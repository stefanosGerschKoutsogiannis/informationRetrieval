2016,Online and Differentially-Private Tensor Decomposition,Tensor decomposition is positioned to be a pervasive tool in the era of big data. In this paper  we resolve many of the key algorithmic questions regarding robustness  memory efficiency  and differential privacy of tensor decomposition. We propose simple variants of the tensor power method which enjoy these strong properties. We propose the first streaming method with a linear memory requirement. Moreover  we present a noise calibrated tensor power method with efficient privacy guarantees. At the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly.,OnlineandDifferentially-PrivateTensorDecompositionYiningWangMachineLearningDepartmentCarnegieMellonUniversityyiningwa@cs.cmu.eduAnimashreeAnandkumarDepartmentofEECSUniversityofCalifornia Irvinea.anandkumar@uci.eduAbstractTensordecompositionisanimportanttoolforbigdataanalysis.Inthispaper weresolvemanyofthekeyalgorithmicquestionsregardingrobustness memoryefﬁciency anddifferentialprivacyoftensordecomposition.Weproposesimplevariantsofthetensorpowermethodwhichenjoythesestrongproperties.Wepresenttheﬁrstguaranteesforonlinetensorpowermethodwhichhasalinearmemoryrequirement.Moreover wepresentanoisecalibratedtensorpowermethodwithefﬁcientprivacyguarantees.Attheheartofalltheseguaranteesliesacarefulperturbationanalysisderivedinthispaperwhichimprovesupontheexistingresultssigniﬁcantly.Keywords:Tensordecomposition tensorpowermethod onlinemethods stream-ing differentialprivacy perturbationanalysis.1IntroductionInrecentyears tensordecompositionhasemergedasapowerfultooltosolvemanychallengingproblemsinunsupervised[1] supervised[18]andreinforcementlearning[4].Tensorsarehigherorderextensionsofmatriceswhichcanrevealfargreaterinformationcomparedtomatrices whileretainingmostoftheefﬁcienciesofmatrixoperations[1].Acentraltaskintensoranalysisistheprocessofdecomposingthetensorintoitsrank-1components whichisusuallyreferredtoasCP(Candecomp/Parafac)decompositionintheliterature.WhiledecompositionofarbitrarytensorsisNP-hard[13] itbecomestractablefortheclassoftensorswithlinearlyindependentcomponents.Throughasimplewhiteningprocedure suchtensorscanbeconvertedtoorthogonallydecomposabletensors.Tensorpowermethodisapopularmethodforcomputingthedecompositionofanorthogonaltensor.Itissimpleandefﬁcienttoimplement andanaturalextensionofthematrixpowermethod.Intheabsenceofnoise thetensorpowermethodcorrectlyrecoversthecomponentsunderarandominitializationfollowedbydeﬂation.Ontheotherhand perturbationanalysisoftensorpowermethodismuchmoredelicatecomparedtothematrixcase.ThisisbecausetheproblemoftensordecompositionisNP-hard andifalargeamountofarbitrarynoiseisaddedtoanorthogonaltensor thedecompositioncanagainbecomeintractable.In[1] guaranteedrecoveryofcomponentswasprovenunderboundednoise andtheboundwasimprovedin[2].Inthispaper wesigniﬁcantlyimproveuponthenoiserequirements i.e.theextentofnoisethatcanbewithstoodbythetensorpowermethod.Inorderfortensormethodstobedeployedinlarge-scalesystems werequirefast parallelizableandscalablealgorithms.Toachievethis weneedtoavoidtheexponentialincreaseincomputationandmemoryrequirementswiththeorderofthetensor;i.e.anaiveimplementationona3rd-orderd-dimensionaltensorwouldrequireO(d3)computationandmemory.Instead weanalyzetheonlinetensorpowermethodthatrequiresonlylinear(ind)memoryanddoesnotformtheentiretensor.Thisisachievedinsettings wherethetensorisanempiricalhigherordermoment computedfromthestreamofdatasamples.Wecanavoidexplicitconstructionofthetensorbyrunningonlinetensor30thConferenceonNeuralInformationProcessingSystems(NIPS2016) Barcelona Spain.powermethoddirectlyoni.i.d.datasamples.Weshowthatthisalgorithmcorrectlyrecoverstensorcomponentsintime1˜O(nk2d)and˜O(dk)memoryforarank-ktensorandnnumberofdatasamples.Additionally weprovideefﬁcientsamplecomplexityanalysis.Asspectralmethodsbecomeincreasinglypopularwithrecommendationsystemandhealthanalyticsapplications[29 17] dataprivacyisparticularlyrelevantinthecontextofpreservingsensitiveprivateinformation.Differentialprivacycouldstillbeusefulevenifdataprivacyisnottheprimeconcern[30].Weproposetheﬁrstdifferentiallyprivatetensordecompositionalgorithmwithbothprivacyandutilityguaranteesvianoisecalibratedpoweriterations.Weshowthatunderthenaturalassumptionoftensorincoherence privacyparametershaveno(polynomial)dependenceontensordimensiond.Ontheotherhand straightforwardinputperturbationtypemethodsleadtofarworseboundsanddonotyieldguaranteedrecoveryforallvaluesofprivacyparameters.1.1RelatedworkOnlinetensorSGDStochasticgradientdescent(SGD)isanintuitiveapproachforonlinetensordecompositionandhasbeensuccessfulinpracticallarge-scaletensordecompositionproblems[16].Despiteitssimplicity theoreticalpropertiesareparticularlyhardtoestablish.[11]consideredavariantoftheSGDobjectiveandproveditscorrectness.However theapproachin[11]onlyworksforeven-ordertensorsanditssamplecomplexitydependencyupontensordimensiondispoor.TensorPCAInthestatisticaltensorPCA[24]modelad×d×dtensorT=v⊗3+EisobservedandonewishestorecovercomponentvunderthepresenceofGaussianrandomnoiseE.[24]showsthatkEkop=O(d−1/2)issufﬁcienttoguaranteeapproximaterecoveryofvand[14]furtherimprovesthenoiseconditiontokEkop=O(d−1/4)viaa4th-ordersum-of-squaresrelaxation.Techniquesinboth[24 14]arerathercomplicatedandcouldbedifﬁculttoadapttomemoryorprivacyconstraints.Furthermore in[24 14]onlyonecomponentisconsidered.Ontheotherhand [25]showsthatkEkop=O(d−1/2)issufﬁcientforrecoveringmultiplecomponentsfromnoisytensors.However [25]assumesexactcomputationofrank-1tensorapproximation whichisNP-hardingeneral.NoisymatrixpowermethodsOurrelaxednoiseconditionanalysisfortensorpowermethodisinspiredbyrecentanalysisofnoisymatrixpowermethods[12 6].Unlikethematrixcase tensordecompositionnolongerrequiresspectralgapamongeigenvaluesandeigenvectorsareusuallyrecoveredoneatatime[1 2].Thisposesnewchallengesandrequiresnon-trivialextensionsofmatrixpowermethodanalysistothetensorcase.1.2NotationandPreliminariesWeuse[n]todenotetheset{1 2 ··· n}.WeuseboldcharactersA T vformatrices tensors vectorsandnormalcharactersλ µforscalars.ApthordertensorTofdimensionsd1 ··· dphasd1×···×dpelements eachindexedbyap-tuple(i1 ··· ip)∈[d1]×···×[dp].AtensorTofdimensionsd×···×dissuper-symmetricorsimplysymmetricifTi1 ··· ip=Tσ(i1) ··· σ(ip)forallpermutationsσ:[p]→[p].ForatensorT∈Rd1×···×dpandmatricesA1∈Rm1×d1 ··· Ap∈Rmp×dp themulti-linearformT(A1 ··· Ap)isam1×···×mptensordeﬁnedas[T(A1 ··· Ap)]i1 ··· ip=Xj1∈[d1]···Xjp∈[dp]Tj1 ··· jp[A1]j1 i1···[Ap]jp ip.Weusekvk2=pPiv2iforvector2-normandkvk∞=maxi|vi|forvectorinﬁnitynorm.WeusekTkoptodenotetheoperatornormorspectralnormofatensorT whichisdeﬁnedaskTkop=supku1k2=···kupk2=1T(u1 ··· up).AneventAissaidtooccurwithoverwhelmingprobabilityifPr[A]≥1−d−10.Welimitourselvestosymmetric3rd-ordertensors(p=3)inthispaper.Theresultscanbedirectlyextendedtoasymmetrictensorssincetheycanﬁrstbesymmetrizedusingsimplematrixoperations(see[1]).Extensiontohigher-ordertensorsisalsostraightforward.Asymmetric3rd-ordertensorTisrank-1ifitcanbewrittenintheformofT=λ·v⊗v⊗v=λv⊗3⇐⇒Ti j ‘=λ·v(i)·v(j)·v(‘) (1)1˜Ohidespoly-logarithmicfactors.2Algorithm1Robusttensorpowermethod[1]1:Input:symmetricd×d×dtensoreT numberofcomponentsk≤d numberofiterationsL R.2:fori=1tokdo3:Initialization:Drawu0uniformlyatrandomfromtheunitsphereinRd.4:Poweriteration:Computeut=eT(I ut−1 ut−1)/keT(I ut−1 ut−1)k2fort=1 ··· R.5:Boosting:RepeatSteps3and4forLtimesandobtainu(1)R ··· u(L)R.Letτ∗=argmaxLτ=1eT(u(τ)R u(τ)R u(τ)R).Setˆvi=u(τ)Randˆλi=eT(u(τ)R u(τ)R u(τ)R).6:Deﬂation:eT←eT−ˆλiˆv⊗3i.7:endfor8:Output:Estimatedeigenvalue/Eigenvectorpairs{ˆλi ˆvi}ki=1.where⊗representstheouterproduct andv∈Rdisaunitvector(i.e. kvk2=1)andλ∈R+.2AtensorT∈Rd×d×dissaidtohaveaCP(Candecomp/Parafac)rankkifitcanbe(minimally)writtenasthesumofkrank-1tensors:T=Xi∈[k]λivi⊗vi⊗vi λi∈R+ vi∈Rd.(2)Atensorissaidtobeorthogonallydecomposableifintheabovedecompositionhvi vji=0fori6=j.Anytensorcanbeconvertedtoanorthogonaltensorthroughaninvertiblewhiteningtransform providedthatv1 v2 ... vkarelinearlyindependent[1].Wethuslimitouranalysistoorthogonaltensorsinthispapersinceitcanbeextendedtothismoregeneralclassinastraightforwardmanner.TensorPowerMethod:Apopularalgorithmforﬁndingthetensordecompositionin(2)isthroughthetensorpowermethod.ThefullalgorithmisgiveninAlgorithm1.Weﬁrstprovideanimprovednoiseanalysisfortherobustpowermethod improvingerrortoleranceboundspreviouslyestablishedin[1].Wenextproposememory-efﬁcientand/ordifferentiallyprivatevariantsoftherobustpowermethodandgiveperformanceguaranteebasedonourimprovednoiseanalysis.2ImprovedNoiseAnalysisforTensorPowerMethodWhenthetensorThasanexactorthogonaldecomposition thepowermethodprovablyrecoversallthecomponentswithrandominitializationanddeﬂation.However theanalysisismoresubtleundernoise.Whilematrixperturbationboundsarewellunderstood itisanopenprobleminthecaseoftensors.ThisisbecausetheproblemoftensordecompositionisNP-hard andbecomestractableonlyunderspecialconditionssuchasorthogonality(andmoregenerallylinearindependence).Ifalargeamountofarbitrarynoiseisadded thedecompositioncanagainbecomeintractable.In[1] guaranteedrecoveryofcomponentswasprovenunderboundednoiseandwerecaptheresultbelow.Theorem2.1([1]Theorem5.1 simpliﬁedversion).SupposeeT=T+∆T whereT=Pki=1λiv⊗3iwithλi>0andorthonormalbasisvectors{v1 ··· vk}⊆Rd d≥k andnoise∆Tsatisﬁesk∆Tkop≤.Letλmax λminbethelargestandsmallestvaluesin{λi}ki=1and{ˆλi ˆvi}ki=1beoutputsofAlgorithm1.ThereexistabsoluteconstantsK0 C1 C2 C3>0suchthatif≤C1·λmin/d R=Ω(logd+loglog(λmax/)) L=Ω(max{K0 k}log(max{K0 k})) (3)thenwithprobabilityatleast0.9 thereexistsapermutationπ:[k]→[k]suchthat|λi−ˆλπ(i)|≤C2 kvi−ˆvπ(i)k2≤C3/λi ∀i=1 ··· k.Theorem2.1istheﬁrstprovablycorrectresultonrobusttensordecompositionundergeneralnoiseconditions.Inparticular thenoiseterm∆Tcanbedeterministicorevenadversarial.However oneimportantdrawbackofTheorem2.1isthatk∆TkopmustbeupperboundedbyO(λmin/d) whichisastrongassumptionformanypracticalapplications[28].Ontheotherhand [2 24]showthatbyusingsmartinitializationstherobusttensorpowermethodiscapableoftoleratingO(λmin/√d)2Onecanalwaysassumewithoutlossofgeneralitythatλ≥0byreplacingvwith−vinstead.3magnitudeofnoise and[25]suggeststhatsuchnoisemagnitudecannotbeimprovedifdeﬂation(i.e. successiverank-oneapproximation)istobeperformed.Inthispaper weshowthattherelaxednoiseboundO(λmin/√d)holdseveniftheinitializationofrobustTPMisassimpleasavectoruniformlysampledfromthed-dimensionalsphere(Algorithm1).Ourclaimisformalizedbelow:Theorem2.2(ImprovednoisetoleranceanalysisforrobustTPM).AssumethesamenotationasinTheorem2.1.Let∈(0 1/2)beanerrortoleranceparameter.ThereexistabsoluteconstantsK0 C0 C1 C2 C3>0suchthatif∆Tsatisﬁesk∆T(I u(τ)t u(τ)t)k2≤ |∆T(vi u(τ)t u(τ)t)|≤min{/√k C0λmin/d}(4)foralli∈[k] t∈[T] τ∈[L]andfurthermore≤C1·λmin/√k R=Ω(log(λmaxd/)) L=Ω(max{K0 k}log(max{K0 k})) (5)thenwithprobabilityatleast0.9 thereexistsapermutationπ:[k]→[k]suchthat|λi−ˆλπ(i)|≤C2 kvi−ˆvπ(i)k2≤C3/λi ∀i=1 ··· k.Duetospaceconstraints proofofTheorem2.2isplacedinAppendixC.Wenextmakeseveralremarksonourresults.Inparticular weconsiderthreescenarioswithincreasingassumptionsimposedonthenoisetensor∆TandcomparethenoiseconditionsinTheorem2.2withexistingresultsonorthogonaltensordecomposition:1.∆Tdoesnothaveanyspecialstructure:inthiscase weonlyhave|∆T(vi ut ut)|≤k∆Tkopandournoiseconditionsreducestotheclassicalone:k∆Tkop=O(λmin/d).2.∆Tis“round”inthesensethat|∆T(vi ut ut)|≤O(1/√d)·k∆T(I ut ut)k2:thisisthetypicalsettingwhenthenoise∆TfollowsGaussianorsub-Gaussiandistributions asweexplaininSec.3and4.Ournoiseconditioninthiscaseisk∆Tkop=O(λmin/√d) strictlyimprovingTheorem2.1onrobusttensorpowermethodwithrandominitializationsandmatchingtheboundformoreadvancedSVDinitializationtechniquesin[2].3.∆Tisweaklycorrelatedwithsignalinthesensethatk∆T(vi I I)k2=O(λmin/d)foralli≤k:inthiscaseournoiseconditionreducestok∆Tkop=O(λmin/√k) strictlyimprovingoverSVDinitialization[2]inthe“undercomplete”regimek=o(d).Notethatthewhiteningtrick[3 1]doesnotattainourbound asweexplaininAppendixB.Finally weremarkthattheloglog(1/)quadraticconvergencerateinEq.(3)isworsenedtolog(1/)linearrateinEq.(5).Wearenotsurewhetherthisisanartifactofouranalysis becausesimilaranalysisforthematrixnoisypowermethod[12]alsorevealsalinearconvergencerate.ImplicationsOurboundsinTheorem2.2resultsinsharperanalysisofbothmemory-efﬁcientanddifferentiallyprivatepowermethodswhichweproposeinSec.3 4.Usingtheoriginalanalysis(Theorem2.1)forthetwoapplications thememory-efﬁcienttensorpowermethodwouldhavesamplecomplexitycubicinthedimensiondandfordifferentiallyprivatetensordecompositiontheprivacylevelεneedstoscaleas˜Ω(√d)asdincreases whichisparticularlybadasthequalityofprivacyprotectioneεdegradesexponentiallywithtensordimensiond.Ontheotherhand ourimprovednoiseconditioninTheorem2.2greatlysharpenstheboundsinbothapplications:formemoryefﬁcientdecomposition wenowrequireonlyquadraticsamplecomplexityandfordifferentiallyprivatedecomposition theprivacylevelεhasnopolynomialdependenceond.Thismakesourresultsfarmorepracticalforhigh-dimensionaltensordecompositionapplications.NumericalveriﬁcationofnoiseconditionsandcomparisonwithwhiteningtechniquesWever-ifyourimprovednoiseconditionsforrobusttensorpowermethodonsimulationtensordata.Inparticular weconsiderthreenoisemodelsanddemonstratevariedasymptoticnoisemagnitudesatwhichtensorpowermethodsucceeds.Thesimulationresultsnicelymatchourtheoreticalﬁndingsandalsosuggest inanempiricalway tightnessofnoiseboundsinTheorem2.2.Duetospaceconstraints simulationresultsareplacedinAppendixA.4Wealsocompareourimprovednoiseboundwiththoseobtainedbywhitening apopulartechniquethatreducestensordecompositiontomatrixdecompositionproblems[1 21 28].WeshowinAppendixBthat withoutsideinformationthestandardanalysisofwhiteningbasedtensordecompositionleadstoworsenoisetoleranceboundsthanwhatweobtainedinTheorem2.2.3Memory-EfﬁcientStreamingTensorDecompositionTensorpowermethodinAlgorithm1requiressigniﬁcantstoragetobedeployed:Ω(d3)memoryisrequiredtostoreadensed×d×dtensor whichisprohibitivelylargeinmanyreal-worldapplicationsastensordimensiondcouldbereallyhigh.Weshowinthissectionhowtocomputetensordecompositioninamemoryefﬁcientmanner withstoragescalinglinearlyind.Inparticular weconsiderthecasewhentensorTtobedecomposedisapopulationmomentEx∼D[x⊗3]withrespecttosomeunknownunderlyingdatadistributionD anddatapointsx1 x2 ···i.i.d.sampledfromDarefedintoatensordecompositionalgorithminastreamingfashion.Oneclassicalexampleistopicmodeling whereeachxirepresentsdocumentsthatcomeinstreamsandconsistentestimationoftopicscanbeachievedbydecomposingvariantsofthepopulationmoment[1 3].Algorithm2displaysmemory-efﬁcienttensordecompositionprocedureonstreamingdatapoints.ThemainideaistoreplacethepoweriterationstepT(I u u)inAlgorithm1witha“dataassociation”stepthatexploitstheempirical-momentstructureofthetensorTtobedecomposedandevaluatesapproximatepoweriterationsfromstochasticdatasamples.Thisprocedureishighlyefﬁcient inthatbothtimeandspacecomplexityscalelinearlywithtensordimensiond:Proposition3.1.Algorithm2runsinO(nkdLR)timeandO(d(k+L))memory withO(nkR)samplecomplexity(numberofdatapointgonethrough).IntheremainderofthissectionweshowAlgorithm2recoverseigenvectorsofthepopulationmomentEx∼D[x⊗3]withhighprobabilityandwederivecorrespondingsamplecomplexitybounds.TofacilitateourtheoreticalanalysisweneedseveralassumptionsonthedatadistributionD.Theﬁrstnaturalassumptionisthelow-ranknessofthepopulationmomentEx∼D[x⊗3]tobedecomposed:Assumption3.1(Low-rankmoment).ThemeantensorT=Ex∼D[x⊗3]admitsalow-rankrepre-sentationT=Pki=1λiv⊗3iforλ1 ··· λk>0andorthonormal{v1 ··· vk}⊆Rd.Wealsoplacerestrictionsonthe“noisemodel” whichimplythatthepopulationmomentEx∼D[x⊗3]canbewellapproximatedbyareasonablenumberofsampleswithhighprobability.Inparticular weconsidersub-GaussiannoiseasformulatedinDeﬁnition3.1andAssumption3.2:Deﬁnition3.1(Multivariatesub-Gaussiandistribution [15]).AD-dimensionalrandomvariablexbelongstothesub-GaussiandistributionfamilySGD(σ)withparameterσ>0ifithaszeromeanandE(cid:2)exp(a>x)(cid:3)≤exp(cid:8)kak22σ2/2(cid:9)foralla∈RD.Assumption3.2(Sub-Gaussiannoise).Thereexistsσ>0suchthatthemean-centeredvectorizedrandomvariablevec(x⊗3−E[x⊗3])belongstoSGd3(σ)asdeﬁnedinDeﬁnition3.1.WeremarkthatAssumption3.2includesawidefamilyofdistributionsthatareofpracticalimportance forexamplenoisethathavecompactsupport.Assumption3.2alsoresembles(B p)-roundnoiseconsideredin[12]thatimposessphericalsymmetryconstraintsontothenoisedistribution.Wearenowreadytopresentthemaintheoremthatboundstherecovery(approximation)errorofeigenvaluesandeigenvectorsofthestreamingrobusttensorpowermethodinAlgorithm2:Theorem3.1(Analysisofstreamingrobusttensorpowermethod).LetAssumptions3.1 3.2holdtrueandsuppose<C1λmin/√kforsomesufﬁcientlysmallabsoluteconstantC1>0.Ifn=eΩ(cid:18)min(cid:26)σ2d2 σ2d2λ2min(cid:27)(cid:19) R=Ω(log(λmaxd/)) L=Ω(klogk) thenwithprobabilityatleast0.9thereexistspermutationπ:[k]→[k]suchthat|λi−ˆλπ(i)|≤C2 kvi−ˆvπ(i)k2≤C3/λi ∀i=1 ··· kforsomeuniversalconstantsC2 C3>0.Corollary3.1isthenanimmediateconsequenceofTheorem3.1 whichsimpliﬁestheboundsandhighlightsasymptoticdependenciesoverimportantmodelparametersd kandσ:5Algorithm2Onlinerobusttensorpowermethod1:Input:datastreamx1 x2 ···∈Rd no.ofcomponentsk parametersL R n.2:fori=1tokdo3:Drawu(1)0 ··· u(L)0i.i.d.uniformlyatrandomfromtheunitsphereSd−1.4:fort=0toR−1do5:Initialization:Setaccumulators˜u(1)t+1 ··· ˜u(L)t+1and˜λ(1) ··· ˜λ(L)to0.6:Dataassociation:Readthenextndatapoints;update˜u(τ)t+1←˜u(τ)t+1+1n(x>‘u(τ)t)2xiand˜λ(τ)←˜λ(τ)+1n(x>‘u(τ)t)3foreach‘∈[n]andτ∈[L].7:Deﬂation:Foreachτ∈[L] update˜u(τ)t+1←˜u(τ)t+1−Pi−1j=1ˆλjξ2j τˆvjand˜λ(τ)←˜λ(τ)−Pi−1j=1ˆλjξ3j τ whereξj τ=ˆv>j˜u(τ)t.8:Normalization:u(τ)t+1=˜u(τ)t+1/k˜u(τ)t+1k2 foreachτ∈[L].9:endfor10:Findτ∗=argmaxτ∈[L]˜λ(τ)andstoreˆλi=˜λ(τ∗) ˆvi=u(τ∗)R.11:endfor12:Output:approximateeigenvalueandeigenvectorpairs{ˆλi ˆvi}ki=1ofˆEx∼D[x⊗3].Corollary3.1.UnderAssumptions3.1 3.2 Algorithm2correctlylearns{λi vi}ki=1uptoO(1/√d)additiveerrorwith˜O(σ2kd2)samplesand˜O(dk)memory.ProofsofTheorem3.1andCorollary3.1arebothdeferredtoAppendixD.ComparedtostreamingnoisymatrixPCAconsideredin[12] theboundisweakerwithanadditional1/kfactorintheterminvolvingand1/dfactorinthetermthatdoesnotinvolve.Weconjecturethistobeafundamentaldifﬁcultyofthetensordecompositionproblem.Ontheotherhand ourboundsresultingfromtheanalysisinSec.2haveaO(1/d)improvementcomparedtoapplyingexistinganalysisin[1]directly.RemarkoncomparisonwithSGD:Ourproposedstreamingtensorpowermethodisnothingbuttheprojectedstochasticgradientdescent(SGD)procedureontheobjectiveofmaximizingthetensornormonthesphere.Theoptimalsolutionofthiscoincideswiththeobjectiveofﬁndingthebestrank-1approximationofthetensor.Here wecanestimateallthecomponentsofthetensorthroughdeﬂation.AnalternativemethodistorunSGDbasedacombinedobjectivefunctiontoobtainallthecomponentsofthetensorsimultaneously asconsideredin[16 11].However theanalysisin[11]onlyworksforeven-ordertensorsandhasworsedependency(atleastd9)ontensordimensiond.4DifferentiallyprivatetensordecompositionTheobjectiveofprivatedataprocessingistoreleasedatasummariessuchthatanyparticularentryoftheoriginaldatacannotbereliablyinferredfromthereleasedresults.Formallyspeaking weadoptthepopular(ε δ)-differentialprivacycriterionproposedin[9]:Deﬁnition4.1((ε δ)-differentialprivacy[9]).LetMdenoteallsymmetricd-dimensionalrealthirdordertensorsandObeanarbitraryoutputset.ArandomizedalgorithmA:M→Ois(ε δ)-differentiallyprivateifforallneighboringtensorsT T0andmeasurablesetO⊆OwehavePr[A(T)∈O]≤eεPr[A(T0)∈O]+δ whereε>0 δ∈[0 1)areprivacyparametersandprobabilitiesaretakenoverrandomnessinA.Sinceourtensordecompositionanalysisconcernssymmetrictensorsprimarily weadopta“symmet-ric”deﬁnitionofneighboringtensorsinDeﬁnition4.1 asshownbelow:Deﬁnition4.2(Neighboringtensors).Twod×d×dsymmetrictensorsT T0areneighboringtensorsifthereexistsi j k∈[d]suchthatT0−T=±symmetrize(ei⊗ej⊗ek)=±(ei⊗ej⊗ek+ei⊗ek⊗ej+···+ek⊗ej⊗ei).Asnotedearlier theabovenotionscanbesimilarlyextendedtoasymmetrictensorsaswellastheguaranteesfortensorpowermethodonasymmetrictensors.Wealsoremarkthatthedifferenceof6Algorithm3Differentiallyprivaterobusttensorpowermethod1:Input:tensorT no.ofcomponentsk numberofiterationsL R privacyparametersε δ.2:Initialization:D=0 ν=6√2ln(1.25/δ0)ε0 δ0=δ2K ε0=ε√K(4+ln(2/δ)) K=kL(R+1).3:fori=1tokdo4:Initialization:Drawu(1)0 ··· u(τ)0uniformlyatrandomfromtheunitsphereinRd.5:fort=0toR−1do6:Poweriteration:compute˜u(τ)t+1=(T−D)(I u(τ)t u(τ)t).7:Noisecalibration:release¯u(τ)t+1=˜u(τ)t+1+νku(τ)tk2∞·z(τ)t wherez(τ)ti.i.d.∼N(0 Id).8:Normalization:u(τ)t+1=¯u(τ)t+1/k¯u(τ)t+1k2.9:endfor10:Compute˜λ(τ)=(T−D)(u(τ)R u(τ)R u(τ)R)+νku(τ)Rk3∞·zτandletτ∗=argmaxτ˜λ(τ).11:Deﬂation:ˆλi=˜λ(τ∗) ˆvi=u(τ∗)R D←D+ˆλiˆv⊗3i.12:endfor13:Output:eigenvalue/eigenvectorpairs{ˆλi ˆvi}ki=1.“neighboringtensors”asdeﬁnedabovehasFrobeniousnormboundedbyO(1).Thisisnecessarybecauseanarbitraryperturbationofatensor evenifrestrictedtoonlyoneentry iscapableofdestroyinganyutilityguaranteepossible.Inanutshell Deﬁnitions4.1 4.2statethatanalgorithmAisdifferentiallyprivateif conditionedonanysetofpossibleoutputsofA onecannotdistinguishwithhighprobabilitybetweentwo“neighboring”tensorsT T0thatdifferonlyinasingleentry(uptosymmetrization) thusprotectingtheprivacyofanyparticularelementintheoriginaltensorT.Hereε δareparameterscontrollingthelevelofprivacy withsmallerε δvaluesimplyingstrongerprivacyguaranteeasPr[A(T)∈O]andPr[A(T0)∈O]areclosertoeachother.Algorithm3describestheprocedureofprivatelyreleasingeigenvectorsofalow-rankinputtensorT.Themainideaforprivacypreservationisthefollowingnoisecalibrationstep¯ut+1=˜ut+1+νkutk2∞·zt whereztisad-dimensionalstandardNormalrandomvariableandνkutk2∞isacarefullydesignednoisemagnitudeinordertoachieveddesiredprivacylevel(ε δ).Onekeyaspectisthatthenoisecalibrationstepoccursateverypoweriteration whichaddstotherobustnessofthealgorithmandachievessharperbounds.Wediscussattheendofthissection.Theorem4.1(Privacyguarantee).Algorithm3satisﬁes(ε δ)-differentialprivacy.Proof.TheonlypoweriterationstepofAlgorithm3canbethoughtofasK=kL(R+1)queriesdirectedtoaprivatedatasanitizerwhichproducesf1(T;u)=T(I u u)orf2(T;u)=T(u u u)eachtime.The‘2-sensitivityofbothqueriescanbeseparatelyboundedas∆2f1=supT0kT(I u u)−T0(I u u)k2≤supi j k2(|uiuj|+|uiuk|+|ujuk|)≤6kuk2∞;∆2f2=supT0(cid:12)(cid:12)T(u u u)−T0(u u u)(cid:12)(cid:12)=supi j k6(cid:12)(cid:12)uiujuk(cid:12)(cid:12)≤6kuk3∞ whereT0=T+symmetrize(ei⊗ej⊗ek)issomeneighboringtensorofT.Thus applyingtheGaussianmechanism[9]wecan(ε δ)-privatelyreleaseoneoutputofeitherf1(u)orf2(u)byf‘(u)+∆2f‘·p2ln(1.25/δ)ε·w where‘=1 2andw∼N(0 I)arei.i.d.standardNormalrandomvariables.Finally applyingadvancedcomposition[9]acrossallK=kL(R+1)privatereleaseswecompletetheproofofthisproposition.NotethatbothnormalizationanddeﬂationstepsdonotaffectthedifferentialprivacyofAlgorithm3duetotheclosenessunderpost-processingpropertyofDP.Therestofthesectionisdevotedtodiscussingthe“utility”ofAlgorithm3;i.e. toshowthatthealgorithmisstillcapableofproducingapproximateeigenvectors despitetheprivacyconstraints.Similarto[12] weadoptthefollowingincoherenceassumptionsontheeigenspaceofT:7Assumption4.1(Incoherentbasis).SupposeV∈Rd×kisthestackedmatrixoforthonormalcomponentvectors{vi}ki=1.Thereexistsconstantµ0>0suchthatdkmax1≤i≤dkV>eik22≤µ0.(6)Notethatbydeﬁnition µ0isalwaysintherangeof[1 d/k].Intuitively Assumption4.1withsmallconstantµ0impliesarelatively“ﬂat”distributionofelementmagnitudesinT.Theincoherencelevelµ0playsanimportantroleintheutilityguaranteeofAlgorithm3 asweshowbelow:Theorem4.2(Guaranteedrecoveryofeigenvectorunderprivacyrequirements).SupposeT=Pki=1λiv⊗3iforλ1>λ2≥λ3≥···≥λk>0withorthonormalv1 ··· vk∈Rd andsupposeAssumption4.1holdswithµ0.Assumeλ1−λ2≥c/√dforsomesufﬁcientlysmalluniversalconstantc>0.IfR=Θ(log(λmaxd)) L=Θ(klogk)andε δsatisfyε=Ω(cid:18)µ0k2log(λmaxd/δ)λmin(cid:19) (7)thenwithprobabilityatleast0.9theﬁrsteigenpair(ˆλ1 ˆv1)returnedbyAlgorithm3satisﬁes(cid:12)(cid:12)λ1−ˆλ1(cid:12)(cid:12)=O(1/√d) kv1−ˆv1k2=O(1/(λ1√d)).Atahighlevel Theorem4.2statesthatwhentheprivacyparameterεisnottoosmall(i.e. privacyrequirementsarenottoostringent) Algorithm3approximatelyrecoversthelargesteigenvalueandeigenvectorwithhighprobability.Furthermore whenµ0isaconstant thelowerboundconditionontheprivacyparameterεdoesnotdependpolynomiallyupontensordimensiond whichisamuchdesiredpropertyforhigh-dimensionaldataanalysis.Ontheotherhand similarresultscannotbeachievedviasimplermethodslikeinputperturbation aswediscussbelow:ComparisonwithinputperturbationInputperturbationisperhapsthesimplestmethodfordif-ferentiallyprivatedataanalysisandhasbeensuccessfulinnumerousscenarios e.g.privatematrixPCA[10].Inourcontext thiswouldentailappendingarandomGaussiantensorEdirectlyontotheinputtensorTbeforetensorpoweriterations.ByGaussianmechanism thestandarddeviationσofeachelementinEscalesasσ=Ω(ε−1plog(1/δ)).Ontheotherhand noiseanalysisfortensordecompositionderivedin[24 2]andinthesubsequentsectionofthispaperrequiresσ=O(1/d)orkEkop=O(1/√d) whichimpliesε=˜Ω(d)(cf.LemmaF.9).Thatis theprivacyparameterεmustscalelinearlywithtensordimensiondtosuccessfullyrecovereventheﬁrstprincipleeigenvector whichrenderstheprivacyguaranteeoftheinputperturbationprocedureuselessforhigh-dimensionaltensors.Thus werequireanon-trivialnewapproachfordifferentiallyprivatetensordecomposition.Finally weremarkthatamoredesiredutilityanalysiswouldboundtheapproximationerrorkvi−ˆvik2foreverycomponentv1 ··· vk andnotjustthetopeigenvector.Unfortunately ourcurrentanalysiscannothandledeﬂationeffectivelyasthedeﬂatedvectorˆvi−vimaynotbeincoherent.Extensiontodeﬂatedtensordecompositionremainsaninterestingopenquestion.5ConclusionWeconsidermemory-efﬁcientanddifferentiallyprivatetensordecompositionproblemsinthispaperandderiveefﬁcientalgorithmsforbothonlineandprivatetensordecompositionbasedonthepopulartensorpowermethodframework.Throughanimprovednoiseconditionanalysisofrobusttensorpowermethod weobtainsharperdimension-dependentsamplecomplexityboundsforonlinetensordecompositionandwiderrangeofprivacyparametersvaluesforprivatetensordecompositionwhilestillretainingutility.Simulationresultsverifythetightnessofournoiseconditionsinprinciple.Oneimportantdirectionoffutureresearchistoextendouronlineand/orprivatetensordecompositionalgorithmsandanalysistopracticalapplicationssuchastopicmodelingandcommunitydetection wheretensordecompositionactsasonecriticalstepfordataanalysis.Anend-to-endanalysisofonline/privatemethodsfortheseapplicationswouldbetheoreticallyinterestingandcouldalsogreatlybeneﬁtpracticalmachinelearningofimportantmodels.AcknowledgementA.AnandkumarissupportedinpartbyMicrosoftFacultyFellowship NSFCareerawardCCF-1254106 ONRAwardN00014-14-1-0665 AROYIPAwardW911NF-13-1-0084andAFOSRYIPFA9550-15-1-0221.8References[1]A.Anandkumar R.Ge D.Hsu S.M.Kakade andM.Telgarsky.Tensordecompositionsforlearninglatentvariablemodels.JournalofMachineLearningResearch 15(1):2773–2832 2014.[2]A.Anandkumar R.Ge andM.Janzamin.Learningovercompletelatentvariablemodelsthroughtensormethods.InProc.ofCOLT 2015.[3]A.Anandkumar Y.-k.Liu D.J.Hsu D.P.Foster andS.M.Kakade.Aspectralalgorithmforlatentdirichletallocation.InNIPS 2012.[4]K.Azizzadenesheli A.Lazaric andA.Anandkumar.ReinforcementlearningofPOMDP’susingspectralmethods.InCOLT 2016.[5]B.W.BaderandT.G.Kolda.Algorithm862:Matlabtensorclassesforfastalgorithmprototyping.ACMTransactionsonMathematicalSoftware 32(4):635–653 2006.[6]M.-F.Balcan S.Du Y.Wang andA.W.Yu.Animprovedgap-dependencyanalysisofthenoisypowermethod.InCOLT 2016.[7]L.Birgé.Analternativepointofviewonlepski’smethod.LectureNotes-MonographSeries pages113–133 2001.[8]B.Cirel’soN I.Ibragimov andV.Sudakov.Normsofgaussiansamplefunctions.LectureNotesinMathematics 550:20–41 1976.[9]C.DworkandA.Roth.Thealgorithmicfoundationsofdifferentialprivacy.FoundationsandTrendsinTheoreticalComputerScience 9(3-4):211–407 2014.[10]C.Dwork K.Talwar A.Thakurta andL.Zhang.Analyzegauss:optimalboundsforprivacy-preservingprincipalcomponentanalysis.InSTOC 2014.[11]R.Ge F.Huang C.Jin andY.Yuan.Escapingfromsaddlepoints—onlinestochasticgradientfortensordecomposition.InCOLT 2015.[12]M.HardtandE.Price.Thenoisypowermethod:Ametaalgorithmwithapplications.InNIPS 2014.[13]C.J.HillarandL.-H.Lim.Mosttensorproblemsarenp-hard.JournaloftheACM(JACM) 60(6):45 2013.[14]S.B.Hopkins J.Shi andD.Steurer.Tensorprincipalcomponentanalysisviasum-of-squaresproofs.InCOLT 2015.[15]D.Hsu S.M.Kakade andT.Zhang.Atailinequalityforquadraticformsofsubgaussianrandomvectors.Electron.Commun.Probab 17(52):1–6 2012.[16]F.Huang U.Niranjan M.U.Hakeem andA.Anandkumar.Onlinetensormethodsforlearninglatentvariablemodels.JournalofMachineLearningResearch 16:2797–2835 2015.[17]F.Huang I.Perros R.Chen J.Sun A.Anandkumar etal.Scalablelatenttreemodelanditsapplicationtohealthanalytics.arXivpreprintarXiv:1406.4566 2014.[18]M.Janzamin H.Sedghi andA.Anandkumar.Beatingtheperilsofnon-convexity:Guaranteedtrainingofneuralnetworksusingtensormethods.arXivpreprintarXiv:1506.08473 2015.[19]G.Kamath.Boundsontheexpectationofthemaximumofsamplesfromagaussian.[Online;accessedApril 2016].[20]T.G.KoldaandJ.R.Mayo.Shiftedpowermethodforcomputingtensoreigenpairs.SIAMJournalonMatrixAnalysisandApplications 32(4):1095–1124 2011.[21]V.Kuleshov A.T.Chaganty andP.Liang.Tensorfactorizationviamatrixfactorization.InAISTATS 2015.[22]B.LaurentandP.Massart.Adaptiveestimationofaquadraticfunctionalbymodelselection.AnnalsofStatistics pages1302–1338 2000.[23]P.Massart.Concentrationinequalitiesandmodelselection volume6.Springer 2007.[24]A.MontanariandE.Richard.AstatisticalmodelfortensorPCA.InNIPS 2014.[25]C.Mu D.Hsu andD.Goldfarb.Successiverank-oneapproximationsfornearlyorthogonallydecompos-ablesymmetrictensors.SIAMJournalonMatrixAnalysisandApplications 36(4):1638–1659 2015.[26]G.W.Stewart J.-g.Sun andH.B.Jovanovich.Matrixperturbationtheory.AcademicpressNewYork 1990.[27]R.TomiokaandT.Suzuki.Spectralnormofrandomtensors.arXiv:1407.1870 2014.[28]Y.Wang H.-Y.Tung A.J.Smola andA.Anandkumar.Fastandguaranteedtensordecompositionviasketching.InNIPS 2015.[29]Y.WangandJ.Zhu.Spectralmethodsforsupervisedtopicmodels.InNIPS 2014.[30]R.Zemel Y.Wu K.Swersky T.Pitassi andC.Dwork.Learningfairrepresentations.InICML 2013.9,Brooks Paige
Frank Wood
Arnaud Doucet
Yee Whye Teh
Yining Wang
Anima Anandkumar