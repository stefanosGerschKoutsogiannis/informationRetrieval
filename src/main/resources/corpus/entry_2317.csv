2019,Global Guarantees for Blind Demodulation with Generative Priors,We study a deep learning inspired formulation for the blind demodulation problem  which is the task of recovering two unknown vectors from their entrywise multiplication. We consider the case where the unknown vectors are in the range of known deep generative models  $\mathcal{G}^{(1)}:\mathbb{R}^n\rightarrow\mathbb{R}^\ell$ and $\mathcal{G}^{(2)}:\mathbb{R}^p\rightarrow\mathbb{R}^\ell$. In the case when the networks corresponding to the generative models are expansive  the weight matrices are random and the dimension of the unknown vectors satisfy $\ell = \Omega(n^2+p^2)$  up to log factors  we show that the empirical risk objective has a favorable landscape for optimization. That is  the objective function has a descent direction at every point outside of a small neighborhood around four hyperbolic curves. We also characterize the local maximizers of the empirical risk objective and  hence  show that there does not exist any other stationary points outside of these neighborhood around four hyperbolic curves and the set of local maximizers. We also implement a gradient descent scheme inspired by the geometry of the landscape of the objective function. In order to converge to a global minimizer  this gradient descent scheme exploits the fact that exactly one of the hyperbolic curve corresponds to the global minimizer  and thus points near this hyperbolic curve have a lower objective value than points close to the other spurious hyperbolic curves. We show that this gradient descent scheme can effectively remove distortions synthetically introduced to the MNIST dataset.,Global Guarantees for Blind Demodulation with

Generative Priors

Dept. of Mathematics and College of Computer Science and Information

Paul Hand

Northeastern University  MA
p.hand@northeastern.edu

Babhru Joshi

Dept. of Mathematics

University of British Columbia  BC

b.joshi@math.ubc.ca

Abstract

We study a deep learning inspired formulation for the blind demodulation prob-
lem  which is the task of recovering two unknown vectors from their entrywise
multiplication. We consider the case where the unknown vectors are in the range
of known deep generative models  G(1) : Rn ! R` and G(2) : Rp ! R`. In the
case when the networks corresponding to the generative models are expansive 
the weight matrices are random and the dimension of the unknown vectors satisfy
` =⌦( n2 + p2)  up to log factors  we show that the empirical risk objective has a
favorable landscape for optimization. That is  the objective function has a descent
direction at every point outside of a small neighborhood around four hyperbolic
curves. We also characterize the local maximizers of the empirical risk objective
and  hence  show that there does not exist any other stationary points outside of
these neighborhood around four hyperbolic curves and the set of local maximizers.
We also implement a gradient descent scheme inspired by the geometry of the
landscape of the objective function. In order to converge to a global minimizer 
this gradient descent scheme exploits the fact that exactly one of the hyperbolic
curve corresponds to the global minimizer  and thus points near this hyperbolic
curve have a lower objective value than points close to the other spurious hyper-
bolic curves. We show that this gradient descent scheme can effectively remove
distortions synthetically introduced to the MNIST dataset.

1 Introduction
We study the problem of recovering two unknown vectors x0 2 R` and w0 2 R` from observations
y0 2 R` of the form
(1)
where  is entrywise multiplication. This bilinear inverse problem (BIP) is known as the blind
demodulation problem. BIPs  in general  have been extensively studied and include problems
such as blind deconvolution/demodulation [Ahmed et al.  2014  Stockham et al.  1975  Kundur and
Hatzinakos  1996  Aghasi et al.  2016  2019]  phase retrieval [Fienup  1982  Candès and Li  2012 
Candès et al.  2013]  dictionary learning [Tosic and Frossard  2011]  matrix factorization [Hoyer 
2004  Lee and Seung  2001]  and self-calibration [Ling and Strohmer  2015]. A signiﬁcant challenge
of BIP is the ambiguity of solutions. These ambiguities are challenging because they cause the set of
solutions to be non-convex.

y0 = w0  x0 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

A common ambiguity  also shared by the BIP in (1)  is the scaling ambiguity. That is any member of
the set {cw0  1
c x0} for c 6= 0 solves (1). In addition to the scaling ambiguity  this BIP is difﬁcult to
solve because the solutions are non-unique  even when excluding the scaling ambiguity. For example 
(w0  x0) and (1  w0  x0) both satisfy (1). This structural ambiguity can be solved by assuming
a prior model of the unknown vectors. In past works relating to blind deconvolution and blind
demodulation [Ahmed et al.  2014  Aghasi et al.  2019]  this structural ambiguity issue was addressed
by assuming a subspace prior  i.e. the unknown signals belong to known subspaces. Additionally  in
many applications  the signals are compressible or sparse with respect to a basis like a wavelet basis
or the Discrete Cosine Transform basis  which can address this structural ambiguity issue.
In contrast to subspace and sparsity priors  we address the structural ambiguity issue by assuming
the signals w0 and x0 belong to the range of known generative models G(1) : Rn ! R` and
G(2) : Rp ! R`  respectively. That is  we assume that w0 = G(1)(h0) for some h0 2 Rn and
x0 = G(2)(m0) for some m0 2 Rp. So  to recover the unknown vectors w0 and x0  we ﬁrst recover
the latent code variables h0 and m0 and then apply G(1) and G(2) on h0 and m0  respectively. Thus 
the blind demodulation problem under generative prior we study is:

ﬁnd h 2 Rn and m 2 Rp  up to the scaling ambiguity  such that y0 = G(1)(h) G (2)(m).

In recent years  advances in generative modeling of images [Karras et al.  2017] has signiﬁcantly
increased the scope of using a generative model as a prior in inverse problems. Generative models
are now used in speech synthesis [van den Oord et al.  2016]  image in-painting [Iizuka et al.  2017] 
image-to-image translation [Zhu et al.  2017]  superresolution [Sønderby et al.  2017]  compressed
sensing [Bora et al.  2017  Lohit et al.  2018]  blind deconvolution [Asim et al.  2018]  blind pty-
chography [Shamshad et al.  2018]  and in many more ﬁelds. Most of these papers empirically show
that using generative model as a prior to solve inverse problems outperform classical methods. For
example  in compressed sensing  optimization over the latent code space to recover images from its
compressive measurements have been empirically shown to succeed with 10x fewer measurements
than classical sparsity based methods [Bora et al.  2017]. Similarly  the authors of Asim et al. [2018]
empirically show that using generative priors in image debluring inverse problem provide a very
effective regularization that produce sharp deblurred images from very blurry images.
In the present paper  we use generative priors to solve the blind demodulation problem (1). The
generative model we consider is the an expansive  fully connected  feed forward neural network with
Rectiﬁed Linear Unit (ReLU) activation functions and no bias terms. Our main contribution is we
show that the empirical risk objective function  for a sufﬁciently expansive random generative model 
has a landscape favorable for gradient based methods to converge to a global minimizer. Our result
implies that if the dimension of the unknown signals satisfy ` =⌦( n2 + p2)  up to log factors  then
the landscape is favorable. In comparison  classical sparsity based methods for similar BIPs like
sparse blind demodulation [Lee et al.  2017] and sparse phase retrieval [Li and Voroninski  2013]
showed that exact recovery of the unknown signals is possible if the number of measurements scale
quadratically  up to a log factor  w.r.t. the sparsity level of the signals. While we show a similar
scaling of the number of measurements w.r.t. the latent code dimension  the latent code dimension can
be smaller than the sparsity level for the same signal  and thus recovering the signal using generative
prior would require less number of measurements.

1.1 Main results
We study the problem of recovering two unknown signals w0 and x0 in R` from observations
y0 = w0  x0  where  denotes entrywise product. We assume  as a prior  that the vectors
w0 and x0 belong to the range of d-layer and s-layer neural networks G(1) : Rn ! R` and
G(2) : Rp ! R`  respectively. The task of recovering w0 and x0 is reduced to ﬁnding the latent
codes h0 2 Rn and m0 2 Rp such that G(1)(h0) = w0 and G(2)(m0) = x0. More precisely  we
consider the generative networks modeled by G(1)(h) = relu(W (1)
1 h)) . . . )
and G(2)(m) = relu(W (2)
1 m)) . . . )  where relu(x) = max(x  0) applies
entrywise  W (1)
i 2
Rpi⇥pi1 for i = 1  . . .   s with p = p0 < p1 < ··· < ps = `. The blind demodulation problem we
consider is:

i 2 Rni⇥ni1 for i = 1  . . .   d with n = n0 < n1 < ··· < nd = `  and W (2)

2 relu(W (2)

d . . . relu(W (1)

2 relu(W (1)

. . . relu(W (2)

s

Let: y0 2 R`  h0 2 Rn  m0 2 Rp such that y0 = G(1)(h0) G (2)(m0) 

2

Given: G(1) G(2) and measurements y0 
Find: h0 and m0  up to the scaling ambiguity.

In order to recover h0 and m0  up to the scaling ambiguity  we consider the following empirical risk
minimization program:

minimize
h2Rn m2Rp

f (h  m) :=

1

2G(1) (h0) G (2) (m0) G (1) (h) G (2) (m)

2

2

.

(2)

1.5

1

0.5

0

-0.5

-1

(a) Landscape of the empirical risk function.

(b) Note the four hyperbolic branches visible.

Figure 1: Plots showing the landscape of the objective function with h0 = 1 and m0 = 1.

-1

-0.5

0

0.5

1

1.5

Figures 1a and 1b show the landscape of the objective function in the case when h0 = m0 = 1 
s = d = 2  the networks are expansive  and the weight matrices W (1)
contain i.i.d.
Gaussian entries. Clearly  the objective function in (2) is non-convex and  as a result  there does not
exist a prior guarantee that gradient based methods will converge to a global minima. Additionally  the
objective function does not contain any regularizer which are generally be used to resolve the scaling

and W (2)

i

i

show that under certain conditions on the networks  the minimizers of (2) are in the neighborhood of
four hyperbolic curves  one of which is the hyperbolic curve containing the global minimizers.
In order to deﬁne these hyperbolic neighborhoods  let

c m0)|c > 0 is a global optima of (2). Nonetheless  we
ambiguity  and thus every point in(ch0  1
A✏ (˜h  ˜m) =⇢(h  m) 2 Rn⇥p9 c > 0 s.t. (h  m) ✓c˜h 
˜m◆2  
where (˜h  ˜m) 2 Rn⇥p is ﬁxed.
c ˜m)|c > 0o. We show that the minimizers of (2) are contained in the four hyperbolic
n(c˜h  1
sets given by A✏ (h0 m0)  A✏ (⇢(1)
s m0). Here  ✏ de-
pends on the expansivity and number of layers in the networks  and both ⇢(1)
and ⇢(2)
are positive
s
d
constants close to 1. We also show that the points in the set {(h  0)|h 2 Rn}[{ (0  m)|m 2 Rp}
are local maximizers. This result holds for networks with the following assumptions:

(3)
is an ✏-neighborhood of the hyperbolic set

˜m◆2  ✏✓c˜h 

d h0 m0)  A✏ (h0 ⇢(2)

s m0)  and A✏ (⇢(1)

d h0 ⇢(2)

This set

1
c

1
c

A1. The weight matrices are random.
A2. The weight matrices of inner layers satisfy ni  cni1 log ni1 for i = 1  . . .   d  1 and
A3. The weight matrices of the last layer for each generator satisfy `  c((nd1 log nd1)2 +

pi  cpi1 log pi1 for i = 1  . . .   s  1.
(ps1 log ps1)2).

In the above assumptions  c is a constant that depends polynomially on the expansivity parameter
of G(1) and G(2) . Figures 1a and 1b show the landscape of the objective function and corroborate
our ﬁndings. In the paper we provide two deterministic conditions that are sufﬁcient to characterize
the landscape of the objective function  and show that Gaussian matrices satisfy these conditions.
In essence  we only require approximate Gaussian matrices. We also note that the state-of-the-art
literature for provable convergence of the training of neural networks for regression and classiﬁcation
admit proofs only in the case that the ﬁnal trained weights are close to their random initialization.

3

Thus  our neural network assumptions are consistent with the best known cases for which networks
can be provably trained.
Theorem 1 (Informal). Let

A = A✏ (h0 m0) [A ✏ (⇢(1)

d h0 m0) [A ✏ (h0 ⇢(2)

s m0) [A ✏ (⇢(1)
d  ⇢ (2)

d h0 ⇢(2)

s m0) 

where ✏> 0 depends on the expansivity of our networks and ⇢(1)
s ! 1 as d  s ! 1  respectively.
Suppose the networks are sufﬁciently expansive such that the number of neurons in the inner layers
and the last layers satisfy assumptions A2 and A3  respectively. Then there exist a descent direction 
given by one of the one-sided partial derivative of the objective function in (2)  for every (h  m) /2
A[{ (h  0)|h 2 Rn}[{ (0  m)|m 2 Rp} with high probability. In addition  elements of the set
{(h  0)|h 2 Rn}[{ (0  m)|m 2 Rp} are local maximizers.
Our main result states that the objective function in (2) does not have any spurious minimizers outside
of the four hyperbolic neighborhoods. Thus  a gradient descent algorithm will converge to a point
inside the four neighborhoods  one of which contains the global minimizers of (2). However  it may
not guarantee convergence to a global minimizer and it may not resolve the inherent scaling ambiguity
present in the problem. So  in order to converge to a global minimizer  we implement a gradient
descent scheme that exploits the landscape of the objective function. That is  we exploit the fact
that points near the hyperbolic curve corresponding to the global minimizer have a lower objective
value than points that are close to the remaining three spurious hyperbolic curves. Second  in order
to resolve the scaling ambiguity  we promote solutions that have equal `2 norm by normalizing
the estimates in each iteration of the gradient descent scheme (See Section 2). In principle  a
convergence result to a global minimizer by gradient descent is possible  and would require showing
a convexity-like property around the hyperbola. We leave this for possible future work.
Theorem 1 also provides a global guarantee of the landscape of the objective function in (2) if the
dimension of the unknown signals scale quadratically w.r.t. to the dimension of the latent codes  i.e.
` =⌦( n2 + p2)  up to log factors. Our result  which we get by enforcing generative priors may enjoy
better sample complexity than classical priors like sparsity because: i) existing recovery guarantee
of unstructured signals require number of measurements that scale quadratically with the sparsity
level  and ii) a signal can have a latent code dimension with respect to a GAN that is smaller than its
sparsity level with respect to a wavelet basis. For example  consider a set of images that correspond to
a single train going down a single track. This set of images form a one dimensional sub-manifold of
the manifold of natural images. If properly parameterized by a generative model  then it would have
a latent dimensionality of approximately 1  whereas the number of wavelet coefﬁcients needed to
describe any of those images is much greater. The work in Bora et al. [2017] shows that compressed
sensing can be done with 5-10x fewer measurements than sparsity models. This provides evidence
for the more economical representation of generative models than of sparsity models. Additionally  it
is more natural to view the natural signal manifold as a low-dimensional manifold  as opposed to
being the combinatorially-many union of low dimensional spaces. Performance gains are provided
by the fact that the natural signal manifold can be directly exploited  whereas the union of subspaces
can only be indirectly exploited via convex relaxations. Thus  our result may be less limiting in terms
of sample complexity.

1.2 Prior work on problems related to blind demodulation
A common approach of solving the BIP in (1) is to assume a subspace or sparsity prior on the
unknown vectors. In these cases the unknown vectors w0 and x0 are assumed to be in the range of
known matrices B 2 R`⇥n and C 2 R`⇥p  respectively. In Ahmed et al. [2014]  the authors assumed
a subspace prior and cast the BIP as a linear low rank matrix recovery problem. They introduced
a semideﬁnite program based on nuclear norm minimization to recover the unknown matrix. For
the case where the rows of B and C are Fourier and Gaussian vectors  respectively  they provide a
recovery guarantee that depend on the number of measurements as ` =⌦( n + p)  up to log factors.
However  because this method operates in the space of matrices  it is computationally prohibitively
expensive. Another limitation of the lifted approach is that recovering a low rank and sparse matrix
efﬁciently from linear observation of the matrix has been challenging. Recently  Lee et al. [2017]
provided a recovery guarantee with near optimal sample complexity for the low rank and sparse
matrix recovery problem using an alternating minimization method for a class of signals that satisfy a
peakiness condition. However  for general signals the same work established a recovery result for the
case where the number of measurements scale quadratically with the sparsity level.

4

In order to address the computational cost of working in the lifted case  a recent theme has been to
introduce convex and non-convex programs that work in the natural parameter space. For example 
in Bahmani and Romberg [2016]  Goldstein and Studer [2016]  the authors introduced PhaseMax 
which is a convex program for phase retrieval that is based on ﬁnding a simple convex relaxation via
the convex hull of the feasibility set. The authors showed that PhaseMax enjoys rigorous recovery
guarantee if a good anchor is available. This formulation was extended to the sparse case in Hand and
Voroninski [2016]  where the authors considered SparsePhaseMax and provided a recovery guarantee
with optimal sample complexity. The idea of formulating a convex program using a simple convex
relaxation via the convex hull of the feasibility set was used in the blind demodulation problem as
well [Aghasi et al.  2019  2018]. In particular  Aghasi et al. [2018] introduced a convex program in
the natural parameter space for the sparse blind demodulation problem in the case where the sign of
the unknown signals are known. Like in Lee et al. [2017]  the authors in Aghasi et al. [2019] provide
a recovery guarantee with optimal sample complexity for a class of signals. However  the result does
not extend to signals with no constraints. Other approaches that operate in the natural parameter space
are methods based on Wirtinger Flow. For example  in Candès et al. [2015]  Wang et al. [2016]  Li
et al. [2016]  the authors use Wirtinger Flow and its variants to solve the phase retrieval and the blind
deconvolution problem. These methods are non-convex and require a good initialization to converge
to a global solution. However  they are simple to solve and enjoys rigorous recovery guarantees.

1.3 Other related work
In this paper  we consider the blind demodulation problem with the unknown signals assumed to be
in the range of known generative models. Our work is motivated by experimental results in deep
compressed sensing and deep blind deconvolution presented in Bora et al. [2017]  Asim et al. [2018]
and theoretical work in deep compressed sensing presented in Hand and Voroninski [2017]. In Bora
et al. [2017]  the authors consider the compressed sensing problem where  instead of a sparsity
prior  a generative prior is considered. They used an empirical risk optimization program over the
latent code space to recover images and empirical showed that their method succeeds with 10x fewer
measurements than previous sparsity based methods. Following the empirical successes of deep
compressed sensing  the authors in Hand and Voroninski [2017] provided a theoretical understanding
for these successes by characterizing the landscape of the empirical risk objective function. In the
random case with the layers of the generative model sufﬁciently expansive  they showed that every
point outside of a small neighborhood around the true solution and a negative multiple of the true
solution has a descent direction with high probability. Another instance where generative model
currently outperforms sparsity based methods is in sparse phase retrieval Hand et al. [2018]. In
sparse phase retrieval  current algorithms that enjoy a provable recovery guarantee of an unknown
n-dimensional k-sparse signal require at least O(k2 log n) measurements; whereas  when assuming
the unknown signal is an output of a known d-layer generator G : Rk ! Rn  the authors in Hand
et al. [2018] showed that  under favorable conditions on the generator and with at least O(kd2 log n)
measurements  the empirical risk objective enjoys a favorable landscape.
Similarly  in Asim et al. [2018]  the authors consider the blind deconvolution problem where a
generative prior over the unknown signal is considered. They empirically showed that using generative
priors in the image deblurring inverse problem provide a very effective regularization that produce
sharp deblurred images from very blurry images. The algorithm used to recovery these deblurred
images is an alternating minimization approach which solves the empirical risk minimization with `2
regularization on the unknown signals. The `2 regularization promotes solution with least `2 norm
and resolves the scaling ambiguity present in the blind deconvolution problem. We consider a related
problem  namely the blind demodulation problem with a generative prior on the unknown signals 
and show that under certain conditions on the generators the empirical risk objective has a favorable
landscape.

1.4 Notations
Vectors and matrices are written with boldface  while scalars and entries of vectors are written in plain
font. We write 1 as the vector of all ones with dimensionality appropriate for the context. Let Sn1
be the unit sphere in Rn. We write I n as the n ⇥ n identity matrix. For x 2 RK and y 2 RN  (x  y)
is the corresponding vector in RK ⇥ RN. Let relu(x) = max(x  0) apply entrywise for x 2 Rn. Let
diag(W x > 0) be the diagonal matrix that is 1 in the (i  i) entry if (W x)i > 0 and 0 otherwise. Let
A  B mean that B  A is a positive semideﬁnite matrix. We will write  = O() to mean that

5

there exists a positive constant C such that   C  where  is understood to be positive. Similarly
we will write c =⌦( ) to mean that there exists a positive constant C such that c  C. When we
say that a constant depends polynomially on ✏1  that means that it is at most C✏k for some positive
C and positive integer k. For notational convenience  we will write a = b + O1(✏) if ka  bk  ✏ 
where the norm is understood to be absolute value for scalars  the `2 norm for vectors  and the spectral
norm for matrices.

2 Algorithm

s

c m0)|c > 0}  {(c⇢(1)

c m0)|c > 0}  where ⇢(1)

d h0  ⇢(2)
c m0) is less than f (ch0  1

In this section  we propose a gradient descent scheme that solves (2). The gradient descent
scheme exploits the global geometry present in the landscape of the objective function in (2)
and avoids regions containing spurious minimizers. The gradient descent scheme is based on
two observations. The ﬁrst observation is that the minimizers of (2) are close to four hyperbolic
c m0)|c > 0}  {(ch0  ⇢(2)
curves given by {(ch0  1
c m0)|c > 0}  and
{(c⇢(1)
are close to 1. The second observation is that
c m0)  and f (ch0  1
c m0) for any c > 0.
f (ch0  1
This is because the curve {(ch0  1
We now introduce some quantities which are useful in stating the gradient descent algorithm. For any
h 2 Rn and W 2 Rl⇥n  deﬁne W + h = diag(W h > 0)W . That is  W + h zeros out the rows of
W that do not have a positive inner product with h and keeps the remaining rows. We will extend
the deﬁnition of W + h to each layer of weights W (1)
i 2 Rn1⇥n
and h 2 Rn  deﬁne W (1)
1 . For each layer i > 1  deﬁne

c m0)|c > 0} corresponds to the global minimizer of (2).

in our neural network. For W (1)
1 h > 0)W (1)

c m0)  f (ch0  1

d h0  1
and ⇢(2)
s

1 + h := (W (1)

d

i

d

1 )+ h = diag(W (1)
i1 + h . . . W (1)

.

i

W (1)

s + mm.

i=d W (1)

2 + hW (1)

1 + hh > 0)W (1)

i + h = diag(W (1)

d + h :=Q1

i W (1)
i + h. Using the above notation  G(1)(h) can be compactly written

d + hh. Similarly  we may write G(2)(m) compactly as ⇤(2)

Lastly  deﬁne ⇤(1)
as ⇤(1)
The gradient descent scheme is an alternating descent direction algorithm. We ﬁrst pick an initial
iterate (h1  m1) such that h1 6= 0 and m1 6= 0. At each iteration i = 1  2  . . .   we ﬁrst compare the
objective value at (h1  m1)  (h1  m1)  (h1 m1)  and (h1 m1) and reset (h1  m1) to be
the point with least objective value. Second we descend along a direction. We compute the descent
direction ˜g1 (h m)  given by the partial derivative of f in (2) w.r.t. h 
h0⌘
s + m0m0⌘ .

and take a step along this direction. Next  we compute the descent direction ˜g2 (h m)  given by the
partial derivative of f in w.r.t. m 

|⇣diag(⇤(2)
|⇣diag(⇤(1)

and again take a step along this direction. Lastly  we normalize the iterate so that at each iteration i
khik2 = kmik2. We repeat this process until convergence. Algorithm 1 outlines this process.
Algorithm 1 Alternating descent algorithm for (2)

s + mm  diag(⇤(1)

d + hh  diag(⇤(2)

s + mm  ⇤(2)

d + hh  ⇤(1)

˜g2 (h m) := ⇤(2)

˜g1 (h m) := ⇤(1)

s + m0m0)⇤(1)

s + mm)2⇤(1)

d + hh)2⇤(2)

h0)⇤(2)

d + h0

d + h

s + m

d + h0

Input: Weight matrices  W (1)
Output: An estimate of a global minimizer of (2)

and W (2)

i

i

  observation y0 and step size ⌘> 0.

1: Choose an arbitrary point (h1  m1) such that h1 6= 0 and m1 6= 0
2: for i = 1  2  . . . do:
3:
4:
5:
6: end for

(hi  mi) arg min(f (hi  mi)  f (hi  mi)  f (hi mi)  f (hi mi))
hi+1 hi  ⌘˜g1 (hi mi)  mi+1 mi  ⌘˜g2 (hi+1 mi)
c pkhi+1k2/kmi+1k2  hi+1 hi+1/c  mi+1 mi+1 · c

6

1

3 Proof Outline

We now present our main results which states that the objective function has a descent direction at
every point outside of four hyperbolic regions. In order to state these directions  we ﬁrst note that the
partial derivatives of f at a differentiable point (h  m) are

rhf (h  m) = ˜g1 (h m) and rmf (h  m) = ˜g2 (h m).

0

0

lim

The function f is not differentiable everywhere because of the behavior of the RELU activation
function in the neural network. However  since G(1) and G(2) are piecewise linear  f is differentiable
at (h  m) + w for all (h  m) and w and sufﬁciently small . The directions we consider are
g1 (h m) 2 Rn+p and g2 (h m) 2 Rn+p  where
!0+ rmf ((h  m) + w)    and
g1 (h m) = lim

   g2 (h m) =

!0+ rhf ((h  m) + w)

(4)
w is ﬁxed. Let Dgf (h  m) be the unnormalized one-sided directional derivative of f (h  m) in the
direction of g: Dgf (h  m) = limt!0+
Theorem 2. Fix ✏> 0 such that K1(d7s2 + d2s7)✏1/4 < 1  d  2  and s  2. Assume the
networks satisfy assumptions A2 and A3. Assume W (1)
I ni1) for i = 1  . . .   d  1
and ith row of W (1)
` I nd1). Sim-
d
ilarly  assume W (2)
satisﬁes
(w(2)
` I ps1). Let K = {(h  0) 2 Rn⇥p |h 2 Rn}[
{(0  m) 2 Rn⇥p |m 2 Rp} and A = AK2d3s3✏
d h0 m0⌘ [
s h0 m0⌘. Then on an event of probability at least
4  ⇣⇢(2)
AK2d3s8✏
i=1 ˜cnieni1Ps
1Pd
i=1 ˜cpiepi1˜ce`/(nd1 log nd1+ps1 log ps1) we have the following:
for (h0  m0) 6= (0  0)  and

d )|
I pi1) for i = 1  . . .   s  1 and ith row of W (2)

satisﬁes (w(1)
i ⇠N (0  1
kwk23pps1/` with w ⇠N (0  1

kwk23pnd1/` with w ⇠N (0  1

s h0 m0⌘ [A K2d8s8✏

4  (h0 m0) [A K2d8s3✏

i ⇠N (0  1

i = w| · 1

f ((h m)+tg)f (h m)

i = w| · 1

4  ⇣⇢(1)

4  ⇣⇢(1)

s )|

1

d ⇢(2)

1

1

t

.

pi

s

ni

(h  m) /2A[K

d

the one-sided directional derivative of f in the direction of g = g1 (h m) or g = g2 (h m)  deﬁned in
(4)  satisfy Dgf (h  m) < 0. Additionally  elements of the set K are local maximizers. Here  ⇢(k)
are positive numbers that converge to 1 as d ! 1  c and 1 are constants that depend polynomially
on ✏1 and ˜c  K1  and K2 are absolute constants.
We prove Theorem 2 by showing that neural networks with random weights satisfy two deterministic
conditions. These conditions are the Weight Distributed Condition (WDC) and the joint Weight
Distributed Condition (joint-WDC). The WDC is a slight generalization of the WDC introduced in
Hand and Voroninski [2017]. We say a matrix W 2 R`⇥n satisﬁes the WDC with constants ✏> 0
and 0 <↵  1 if for all nonzero x  y 2 Rk 
i  ↵Qx y  ✏  with Qx y =
`Xi=1
where wi 2 Rn is the ith row of W ; M ˆx$ˆy 2 Rn⇥n is the matrix such that ˆx ! ˆy  ˆy ! ˆx  and
z ! 0 for all z 2 span({x  y})?; ˆx = x/kxk2 and ˆy = y/kyk2; ✓0 = \(x  y); and 1S is the
indicator function on S. If wi ⇠N (0  1
` I n) for all i  then an elementary calculation shows that
ii = Qx y and if x = y then Qx y is an isometry up to a factor

of 1/2. Also  note that if W satisﬁes WDC with constants ✏ and ↵  then 1p↵ W satisﬁes WDC with
constants ✏/↵ and 1.
We now state the joint Weight Distributed Condition. We say that B 2 R`⇥n and C 2 R`⇥p satisfy
joint-WDC with constants ✏> 0 and 0 <↵  1 if for all nonzero h  x 2 Rn and nonzero m 
y 2 Rp 


EhP`

i=1 1wi·x>01wi·y>0 · wiw|

1wi·x>01wi·y>0 · wiw|

⇡  ✓0
2⇡

M ˆx$ˆy 

sin ✓0

I n +

(5)

2⇡

+ hdiag (C+ mm  C+ yy) B+ x 

B|

m|Qm yy · Qh x 

↵
`

7

✏
`kmk2kyk2  and

(6)

+ mdiag (B+ hh  B+ xx) C+ y 

C|

↵
`

h|Qh xx · Qm y 

✏
`khk2kxk2

(7)

We analyze networks G(1) and G(2) where the weight matrices corresponding to the inner layers
satisfy the WDC with constants ✏> 0 and 1 and for the two matrices corresponding to the outer
layers  we assume that one of them satisﬁes WDC with constants ✏ and 0 <↵ 1  1 and the other
satiﬁes WDC with constants ✏ and 0 <↵ 2  1. We also assume that the two outer layer matrices
satisfy joint-WDC with constants ✏> 0 and ↵ = ↵1 · ↵2. We now state the main deterministic result:
Theorem 3. Fix ✏> 0  0 <↵ 1  1 and 0 <↵ 2  1 such that K1(d7s2 + d2s7)✏1/4/(↵1↵2) < 1 
d  2  and s  2. Let K = {(h  0) 2 Rn⇥p |h 2 Rn}[{ (0  m) 2 Rn⇥p |m 2 Rp}. Suppose that
W (1)
i 2 Rni⇥ni1 for i = 1  . . .   d  1 and W (2)
i 2 Rpi⇥pi1 for i = 1  . . .   s  1 satisfy the WDC
with constant ✏ and 1. Suppose W (1)
d 2 R`⇥nd1 satisfy WDC with constants ✏ and ↵1  and W (2)
s 2
s ⌘ satisfy joint-WDC
R`⇥ps1 satisfy WDC with constants ✏ and ↵2. Also  suppose⇣W (1)
with constants ✏  ↵ = ↵1 · ↵2. Let K = {(h  0) 2 Rn⇥p |h 2 Rn}[{ (0  m) 2 Rn⇥p |m 2 Rp}
and A = AK2d3s3✏
4 ↵1 ⇣⇢(2)
s h0 m0⌘ [
4 ↵1 (h0 m0) [A K2d8s3✏
s h0 m0⌘. Then  for (h0  m0) 6= (0  0)  and
4 ↵1 ⇣⇢(1)
AK2d8s8✏
d ⇢(2)

d   W (2)
d h0 m0⌘ [A K2d3s8✏

4 ↵1 ⇣⇢(1)

1

1

1

1

(h  m) /2A[K

the one-sided directional derivative of f in the direction of g = g1 (h m) or g = g2 (h m) satisfy
Dgf (h  m) < 0. Additionally  elements of the set K are local maximizers. Here  ⇢(k)
d are positive
numbers that converge to 1 as d ! 1  and K1  and K2 are absolute constants.
We prove the theorems by showing that the descent directions g1 (h m) and g2 (h m) concentrate
around its expectation and then characterize the set of points where the corresponding expectations
are simultaneously zero. The outline of the proof is:

(h m) (h0 m0) and t(2)

• The WDC and joint-WDC imply that the one-sided partial directional derivatives of f
concentrate uniformly for all non-zero h  h0 2 Rn and m  m0 2 Rp around continuous
vectors t(1)
(h m) (h0 m0)  respectively  deﬁned in equations (10) and (11)
in the Appendix.
• Direct analysis show that t(1)
mately zero around the four hyperbolic sets A✏ (h0 m0)  A✏ (⇢(1)
and A✏ (⇢(1)
d h0 ⇢(2)
networks  and both ⇢(1)
d
of layers in the two neural networks as well.

(h m) (h0 m0) are simultaneously approxi-
s m0) 
s m0)  where ✏ depends on the expansivity and number of layers in the
are positive constants close to 1 and depends on the number

d h0 m0)  A✏ (h0 ⇢(2)

(h m) (h0 m0) and t(2)

and ⇢(2)
s

• Using sphere covering arguments  Gaussian and truncated Gaussian matrices with appropri-

ate dimensions satisfy the WDC and joint-WDC conditions.

The full proof of Theorem 3 is provided in the Appendix.

4 Numerical Experiment

We now empirically show that Algorithm 1 can remove distortions present in the dataset. We consider
the image recovery task of removing distortions that were synthetically introduced to the MNIST
dataset. The distortion dataset contain 8100 images of size 28⇥28 where the distortions are generated
using a 2D Gaussian function  g(x  y) = e
  where c is the center and  controls its
tail behavior. For each of the 8100 image  we ﬁx c and   which vary uniformly in the intervals [3  3]
and [20  35]  respectively  and x and y are in the interval [5  5]. Prior to training the generators  the
images in the MNIST dataset and the distortion dataset were resized to 64 ⇥ 64 images. We used
DCGAN [Radford et al.  2016] with a learning rate of 0.0002 and latent code dimension of 50 to
train a generator  G(2)  for the distortion images. Similarly  we used the DCGAN with learning rate
of 0.0002 and latent code dimension of 100 to train a generator  G(1)  for the MNIST images. Finally 

(xc)2+(yc)2



8

a distorted image y0 is generated via the pixelwise multiplication of an image w0 from the MNIST
dataset and an image x0 from the distortion dataset  i.e. y0 = w0  x0.

Figure 2: The ﬁgure shows the result removing distortion in an image by solving (2) using Algorithm
1. The top row corresponds to the input distorted image. The second and third row corresponds to the
images recovered using empirical risk minimization.

Figure 2 shows the result of using Algorithm 1 to remove distortion from y0. In the implementation of
Algorithm 1  ˜g1 (hi mi) and ˜g1 (hi mi) corresponds to the partial derivatives of f with the generators
as G(1) and G(2). We used the Stochastic Gradient Descent algorithm with the step size set to 1 and
momentum set to 0.9. For each image in the ﬁrst row of Figure 2  the corresponding images in the
second and third rows are the output of Algorithm 1 after 500 iterations.

References
Ali Ahmed  Benjamin Recht  and Justin Romberg. Blind deconvolution using convex programming.

IEEE Trans. Inform. Theory  60(3):1711–1732  2014.

Thomas G Stockham  Thomas M Cannon  and Robert B Ingebretsen. Blind deconvolution through

digital signal processing. Proceedings of the IEEE  63(4):678–692  1975.

Deepa Kundur and Dimitrios Hatzinakos. Blind image deconvolution. IEEE signal processing

magazine  13(3):43–64  1996.

Alireza Aghasi  Barmak Heshmat  Albert Redo-Sanchez  Justin Romberg  and Ramesh Raskar.
Sweep distortion removal from terahertz images via blind demodulation. Optica  3(7):754–762 
2016.

Alireza Aghasi  Ali Ahmed  and Paul Hand. Branchhull: Convex bilinear inversion from the entrywise
product of signals with known signs. Applied Computational and Harmonic Analysis  2019. doi:
https://doi.org/10.1016/j.acha.2019.03.002.

James R Fienup. Phase retrieval algorithms: a comparison. Applied optics  21(15):2758–2769  1982.
E. Candès and X. Li. Solving quadratic equations via phaselift when there are about as many

equations as unknowns. Found. Comput. Math.  pages 1–10  2012.

E. Candès  T. Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from magnitude

measurements via convex programming. Commun. Pure Appl. Math.  66(8):1241–1274  2013.

Ivana Tosic and Pascal Frossard. Dictionary learning. IEEE Signal Processing Magazine  28(2):

27–38  2011.

Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine

learning research  5(Nov):1457–1469  2004.

Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances

in neural information processing systems  pages 556–562  2001.

Shuyang Ling and Thomas Strohmer. Self-calibration and biconvex compressive sensing. Inverse

Problems  31(11):115002  2015.

Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive growing of gans for
improved quality  stability  and variation. CoRR  abs/1710.10196  2017. URL http://arxiv.
org/abs/1710.10196.

9

Aäron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex Graves 
Nal Kalchbrenner  Andrew W. Senior  and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. CoRR  abs/1609.03499  2016. URL http://arxiv.org/abs/1609.03499.

Satoshi Iizuka  Edgar Simo-Serra  and Hiroshi Ishikawa. Globally and locally consistent image
completion. ACM Trans. Graph.  36(4):107:1–107:14  July 2017. ISSN 0730-0301. doi: 10.1145/
3072959.3073659. URL http://doi.acm.org/10.1145/3072959.3073659.

Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A. Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. CoRR  abs/1703.10593  2017. URL http://arxiv.
org/abs/1703.10593.

Casper Kaae Sønderby  Jose Caballero  Lucas Theis  Wenzhe Shi  and Ferenc Huszár. Amortised MAP
inference for image super-resolution. In 5th International Conference on Learning Representations 
ICLR 2017  Toulon  France  April 24-26  2017  Conference Track Proceedings  2017. URL
https://openreview.net/forum?id=S1RP6GLle.

Ashish Bora  Ajil Jalal  Eric Price  and Alexandros G. Dimakis. Compressed sensing using generative

models. 2017. URL https://arxiv.org/abs/1703.03208.

S. Lohit  K. Kulkarni  R. Kerviche  P. Turaga  and A. Ashok. Convolutional neural networks for
noniterative reconstruction of compressively sensed images. IEEE Transactions on Computational
Imaging  4(3):326–340  Sep. 2018. ISSN 2333-9403. doi: 10.1109/TCI.2018.2846413.

Muhammad Asim  Fahad Shamshad  and Ali Ahmed. Solving bilinear inverse problems using deep
generative priors. CoRR  abs/1802.04073  2018. URL http://arxiv.org/abs/1802.04073.
Fahad Shamshad  Farwa Abbas  and Ali Ahmed. Deep ptych: Subsampled fourier ptychography

using generative priors. CoRR  abs/1812.11065  2018.

Kiryung Lee  Yihing Wu  and Yoram Bresler. Near optimal compressed sensing of a class of sparse

low-rank matrices via sparse power factorization. arXiv preprint arXiv:1702.04342  2017.

Xiaodong Li and Vladislav Voroninski. Sparse signal recovery from quadratic measurements via

convex programming. SIAM Journal on Mathematical Analysis  45(5):3019–3033  2013.

Sohail Bahmani and Justin Romberg. Phase retrieval meets statistical learning theory: A ﬂexible

convex relaxation. arXiv preprint arXiv:1610.04210  2016.

Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. arXiv

preprint arXiv:1610.07531  2016.

Paul Hand and Vladislav Voroninski. Compressed sensing from phaseless gaussian measurements
via linear programming in the natural parameter space. CoRR  abs/1611.05985  2016. URL
http://arxiv.org/abs/1611.05985.

Alireza Aghasi  Ali Ahmed  Paul Hand  and Babhru Joshi. A convex program for bilinear in-
version of sparse vectors.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-
Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems 31 
pages 8548–8558. Curran Associates  Inc.  2018. URL http://papers.nips.cc/paper/
8074-a-convex-program-for-bilinear-inversion-of-sparse-vectors.pdf.

Emmanuel Candès  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow:

Theory and algorithms. IEEE Trans. Inform. Theory  61(4):1985–2007  2015.

Gang Wang  Georgios B Giannakis  and Yonina C Eldar. Solving systems of random quadratic

equations via truncated amplitude ﬂow. arXiv preprint arXiv:1605.08285  2016.

Xiaodong Li  Shuyang Ling  Thomas Strohmer  and Ke Wei. Rapid  robust  and reliable blind

deconvolution via nonconvex optimization. arXiv preprint arXiv:1606.04933  2016.

Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by

empirical risk. CoRR  abs/1705.07576  2017. URL http://arxiv.org/abs/1705.07576.

10

Paul Hand  Oscar Leong  and Vladislav Voroninski. Phase retrieval under a generative prior. CoRR 

abs/1807.04261  2018. URL http://arxiv.org/abs/1807.04261.

Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. In ICLR  2016.

R. Vershynin. Compressed sensing: theory and applications. Cambridge University Press  2012.
Halyun Jeong  Xiaowei Li  Yaniv Plan  and Ozgur Yilmaz. Non-gaussian random matrices on sets:
Optimal tail dependence and applications. In 13th International conference on Sampling Theory
and Applications  2019.

11

,Paul Hand
Babhru Joshi