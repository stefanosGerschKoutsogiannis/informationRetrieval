2018,Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo,A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e.  constant of proportionality). One prevalent example of this is sampling posteriors in parametric 
distributions  such as latent-variable generative models.  However sampling (even very approximately) can be #P-hard.

Classical results (going back to Bakry and Emery) on sampling focus on log-concave distributions  and show a natural Markov chain called Langevin diffusion mix in polynomial time.  However  all log-concave distributions are uni-modal  while in practice it is very common for the distribution of interest to have multiple modes.
In this case  Langevin diffusion suffers from torpid mixing. 

We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for a mixture of (strongly) log-concave distributions of the same shape. In particular  our technique applies to the canonical multi-modal distribution: a mixture of gaussians (of equal variance). Our algorithm efficiently samples from these distributions given only access to the gradient of the log-pdf. To the best of our knowledge  this is the first result that proves fast mixing for multimodal distributions.,Beyond Log-concavity: Provable Guarantees for

Sampling Multi-modal Distributions using Simulated

Tempering Langevin Monte Carlo

Duke University  Computer Science Department

Rong Ge

rongge@cs.duke.edu

Princeton University  Mathematics Department

Holden Lee

holdenl@princeton.edu

Massachusetts Institute of Technology  Applied Mathematics and IDSS

Andrej Risteski

risteski@mit.edu

Abstract

A key task in Bayesian machine learning is sampling from distributions that are
only speciï¬ed up to a partition function (i.e.  constant of proportionality). One
prevalent example of this is sampling posteriors in parametric distributions  such
as latent-variable generative models. However sampling (even very approximately)
can be #P-hard.
Classical results (going back to [BÃ‰85]) on sampling focus on log-concave dis-
tributions  and show a natural Markov chain called Langevin diffusion mixes in
polynomial time. However  all log-concave distributions are uni-modal  while in
practice it is very common for the distribution of interest to have multiple modes.
In this case  Langevin diffusion suffers from torpid mixing.
We address this problem by combining Langevin diffusion with simulated temper-
ing. The result is a Markov chain that mixes more rapidly by transitioning between
different temperatures of the distribution. We analyze this Markov chain for a
mixture of (strongly) log-concave distributions of the same shape. In particular  our
technique applies to the canonical multi-modal distribution: a mixture of gaussians
(of equal variance). Our algorithm efï¬ciently samples from these distributions
given only access to the gradient of the log-pdf. To the best of our knowledge  this
is the ï¬rst result that proves fast mixing for multimodal distributions in this setting.
For the analysis  we introduce novel techniques for proving spectral gaps based on
decomposing the action of the generator of the diffusion. Previous approaches rely
on decomposing the state space as a partition of sets  while our approach can be
thought of as decomposing the stationary measure as a mixture of distributions (a
â€œsoft partitionâ€).
Additional materials for the paper can be found at http://tiny.cc/glr17. Note
that the proof and results have been improved and generalized from the precursor
at http://www.arxiv.org/abs/1710.02736. See Section ?? for a compari-
son.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  MontrÃ©al  Canada.

1

Introduction

ğ‘(ğ‘¥)

to evaluate  the denominator ğ‘(ğ‘¥) =âˆ«ï¸€

Sampling is a fundamental task in Bayesian statistics  and dealing with multimodal distributions is
a core challenge. One common technique to sample from a probability distribution is to deï¬ne a
Markov chain with that distribution as its stationary distribution. This general approach is called
Markov chain Monte Carlo. However  in many practical problems  the Markov chain does not mix
rapidly  and we obtain samples from only one part of the support of the distribution.
Practitioners have dealt with this problem through a variety of heuristics. A popular family of
approaches involve changing the temperature of the distribution. However  there has been little
theoretical analysis of such methods. We give provable guarantees for a temperature-based method
called simulated tempering when it is combined with Langevin diffusion.
More precisely  the setup we consider is sampling from a distribution given up to a constant of
proportionality. This is inspired from sampling a posterior distribution over the latent variables of a
latent-variable Bayesian model with known parameters. In such models  the observable variables
ğ‘¥ follow a distribution ğ‘(ğ‘¥) which has a simple and succinct form given the values of some latent
variables â„  i.e.  the joint ğ‘(â„  ğ‘¥) factorizes as ğ‘(â„)ğ‘(ğ‘¥|â„) where both factors are explicit. Hence 
the posterior distribution ğ‘(â„|ğ‘¥) has the form ğ‘(â„|ğ‘¥) = ğ‘(â„)ğ‘(ğ‘¥|â„)
. Although the numerator is easy
â„ ğ‘(â„)ğ‘(ğ‘¥|â„) can be NP-hard to approximate even for simple
models like topic models [SR11]. Thus the problem is intractable without structural assumptions.
Previous theoretical results on sampling have focused on log-concave distributions  i.e.  distributions
of the form ğ‘(ğ‘¥) âˆ ğ‘’âˆ’ğ‘“ (ğ‘¥) for a convex function ğ‘“ (ğ‘¥). This is analogous to convex optimization
where the objective function ğ‘“ (ğ‘¥) is convex. Recently  there has been renewed interest in analyzing
a popular Markov Chain for sampling from such distributions  when given gradient access to ğ‘“â€”a
natural setup for the posterior sampling task described above. In particular  a Markov chain called
Langevin Monte Carlo (see Section 2.1)  popular with Bayesian practitioners  has been proven to
work  with various rates depending on the precise properties of ğ‘“ [Dal16  DM16  Dal17].
Yet  just as many interesting optimization problems are nonconvex  many interesting sampling prob-
lems are not log-concave. A log-concave distribution is necessarily uni-modal: its density function
has only one local maximum  which is necessarily a global maximum. This fails to capture many
interesting scenarios. Many simple posterior distributions are neither log-concave nor uni-modal  for
instance  the posterior distribution of the means for a mixture of gaussians  given a sample of points
from the mixture of gaussians. In a more practical direction  complicated posterior distributions asso-
ciated with deep generative models [RMW14] and variational auto-encoders [KW13] are believed to
be multimodal as well.
In this work we initiate an exploration of provable methods for sampling â€œbeyond log-concavity â€
in parallel to optimization â€œbeyond convexityâ€. As worst-case results are prohibited by hardness
results  we must make assumptions on the distributions of interest. As a ï¬rst step  we consider a
mixture of strongly log-concave distributions of the same shape. This class of distributions captures
the prototypical multimodal distribution  a mixture of Gaussians with the same covariance matrix.
Our result is also robust in the sense that even if the actual distribution has density that is only close
to a mixture that we can handle  our algorithm can still sample from the distribution in polynomial
time. Note that the requirement that all Gaussians have the same covariance matrix is in some sense
necessary: in Appendix K we show that even if the covariance of two components differ by a constant
factor  no algorithm (with query access to ğ‘“ and âˆ‡ğ‘“) can achieve the same robustness guarantee in
polynomial time.

1.1 Problem statement

We formalize the problem of interest as follows.
Problem 1.1. Let ğ‘“ : Rğ‘‘ â†’ R be a function. Given query access to âˆ‡ğ‘“ (ğ‘¥) and ğ‘“ (ğ‘¥) at any point
ğ‘¥ âˆˆ Rğ‘‘  sample from the probability distribution with density function ğ‘(ğ‘¥) âˆ ğ‘’âˆ’ğ‘“ (ğ‘¥).
In particular  consider the case where ğ‘’âˆ’ğ‘“ (ğ‘¥) is the density function of a mixture of strongly log-
concave distributions that are translates of each other. That is  there is a base function ğ‘“0 : Rğ‘‘ â†’ R 

2

centers ğœ‡1  ğœ‡2  . . .   ğœ‡ğ‘š âˆˆ Rğ‘‘  and weights ğ‘¤1  ğ‘¤2  . . .   ğ‘¤ğ‘š (âˆ‘ï¸€ğ‘š
)ï¸ƒ

(ï¸ƒ ğ‘šâˆ‘ï¸

ğ‘–=1 ğ‘¤ğ‘– = 1) such that

 

ğ‘–=1

ğ‘“ (ğ‘¥) = âˆ’ log

ğ‘¤ğ‘–ğ‘’âˆ’ğ‘“0(ğ‘¥âˆ’ğœ‡ğ‘–)
For notational convenience  we will deï¬ne ğ‘“ğ‘–(ğ‘¥) = ğ‘“0(ğ‘¥ âˆ’ ğœ‡ğ‘–).
The function ğ‘“0 speciï¬es a basic â€œshapeâ€ around the modes  and the means ğœ‡ğ‘– indicate the locations
of the modes.
Without loss of generality we assume the mode of the distribution ğ‘’âˆ’ğ‘“0(ğ‘¥) is at 0 (âˆ‡ğ‘“0(0) =
0). We also assume ğ‘“0 is twice differentiable  and for any ğ‘¥ the Hessian is sandwiched between
ğœ…ğ¼ âª¯ âˆ‡2ğ‘“0(ğ‘¥)) âª¯ ğ¾ğ¼. Such functions are called ğœ…-strongly-convex  ğ¾-smooth functions. The
corresponding distribution ğ‘’âˆ’ğ‘“0(ğ‘¥) are strongly log-concave distributions. 1

(1)

1.2 Our results

ğœ€   1

ğœ…   ğ¾)ï¸€ 

Algorithm 2 with appropriate setting of parameters) with running time poly(ï¸€ğ‘¤min  ğ·  ğ‘‘  1

We show that there is an efï¬cient algorithm that can sample from this distribution given just access to
ğ‘“ (ğ‘¥) and âˆ‡ğ‘“ (ğ‘¥).
Theorem 1.2 (main). Given ğ‘“ (ğ‘¥) as deï¬ned in Equation (1)  where the base function ğ‘“0 satisï¬es
for any ğ‘¥  ğœ…ğ¼ âª¯ âˆ‡2ğ‘“0(ğ‘¥) âª¯ ğ¾ğ¼  and â€–ğœ‡ğ‘–â€– â‰¤ ğ· for all ğ‘– âˆˆ [ğ‘š]  there is an algorithm (given as
which given query access to âˆ‡ğ‘“ and ğ‘“  outputs a sample from a distribution within TV-distance ğœ€ of
ğ‘(ğ‘¥) âˆ ğ‘’âˆ’ğ‘“ (ğ‘¥).
Note that importantly the algorithm does not have direct access to the mixture parameters ğœ‡ğ‘–  ğ‘¤ğ‘–  ğ‘– âˆˆ
[ğ‘›] (otherwise the problem would be trivial). Sampling from this mixture is thus non-trivial: algo-
rithms that are based on making local steps (such as the ball-walk [LS93  Vem05] and Langevin
Monte Carlo) cannot move between different components of the gaussian mixture when the gaus-
sians are well-separated. In the algorithm we use simulated tempering (see Section 2.2)  which is
a technique that adjusts the â€œtemperatureâ€ of the distribution in order to move between different
components.
Of course  requiring the distribution to be exactly a mixture of log-concave distributions is a very
strong assumption. Our results can be generalized to all functions that are â€œcloseâ€ to a mixture of
log-concave distributions.
More precisely  assume the function ğ‘“ satisï¬es the following properties:
âˆƒ Ëœğ‘“ : Rğ‘‘ â†’ R where

â‰¤ ğœ and â€–âˆ‡2 Ëœğ‘“ (ğ‘¥) âˆ’ âˆ‡2ğ‘“ (ğ‘¥)â€–2 â‰¤ ğœ âˆ€ğ‘¥ âˆˆ Rğ‘‘

â‰¤ âˆ†  

âƒ¦âƒ¦âƒ¦ Ëœğ‘“ âˆ’ ğ‘“

âƒ¦âƒ¦âƒ¦âˆ
(ï¸ƒ ğ‘šâˆ‘ï¸

âƒ¦âƒ¦âƒ¦âˆ‡ Ëœğ‘“ âˆ’ âˆ‡ğ‘“
âƒ¦âƒ¦âƒ¦âˆ
)ï¸ƒ

(2)

(3)

(4)

and Ëœğ‘“ (ğ‘¥) = âˆ’ log

ğ‘¤ğ‘–ğ‘’âˆ’ğ‘“0(ğ‘¥âˆ’ğœ‡ğ‘–)

where âˆ‡ğ‘“0(0) = 0  and âˆ€ğ‘¥  ğœ…ğ¼ âª¯ âˆ‡2ğ‘“0(ğ‘¥) âª¯ ğ¾ğ¼.

ğ‘–=1

ğœ€   ğ‘’Î”  ğœ  1

poly(ï¸€ğ‘¤min  ğ·  ğ‘‘  1

ğœ…   ğ¾)ï¸€  which given query access to âˆ‡ğ‘“ and ğ‘“  outputs a sample ğ‘¥ from

That is  ğ‘“ is within a ğ‘’Î” multiplicative factor of an (unknown) mixture of log-concave distributions.
Our theorem can be generalized to this case.
Theorem 1.3 (general case). For function ğ‘“ (ğ‘¥) that satisï¬es Equations (2) (3) and (4)  there is
an algorithm (given as Algorithm 2 with appropriate setting of parameters) that runs in time
a distribution that has TV-distance at most ğœ€ from ğ‘(ğ‘¥) âˆ ğ‘’âˆ’ğ‘“ (ğ‘¥).
Both main theorems may seem simple. In particular  one might conjecture that it is easy to use local
search algorithms to ï¬nd all the modes. However in Section J  we give a few examples to show that
such simple heuristics do not work (e.g. random initialization is not enough to ï¬nd all the modes).
2ğœ2 â€–ğ‘¥â€–2. This corresponds to the case

1On a ï¬rst read  we recommend concentrating on the case ğ‘“0(ğ‘¥) = 1

where all the components are spherical Gaussians with mean ğœ‡ğ‘– and covariance matrix ğœ2ğ¼.

3

The assumption that all the mixture components share the same ğ‘“0 (hence when applied to Gaussians 
all Gaussians have same covariance) is also necessary. In Section K  we give an example where for a
mixture of two gaussians  even if the covariance only differs by a constant factor  any algorithm that
achieves similar gaurantees as Theorem 1.3 must take exponential time.

2 Overview of algorithm

Our algorithm combines Langevin diffusion  a chain for sampling from distributions in the form
ğ‘(ğ‘¥) âˆ ğ‘’âˆ’ğ‘“ (ğ‘¥) given only gradient access to ğ‘“ and simulated tempering  a heuristic used for tackling
multimodality. We brieï¬‚y deï¬ne both of these and recall what is known for both of these techniques.
For technical prerequisites on Markov chains  the reader can refer to Appendix B.
The basic idea to keep in mind is the following: A Markov chain with local moves such as Langevin
diffusion gets stuck in a local mode. Creating a â€œmeta-Markov chainâ€ which changes the temperature
(the simulated tempering chain) can exponentially speed up mixing.

2.1 Langevin dynamics
Langevin Monte Carlo is an algorithm for sampling from ğ‘ âˆ ğ‘’âˆ’ğ‘“ given access to the gradient of the
log-pdf  âˆ‡ğ‘“.
The continuous version  overdamped Langevin diffusion (often simply called Langevin diffusion)  is
a stochastic process described by the stochastic differential equation (henceforth SDE)

ğ‘‘ğ‘‹ğ‘¡ = âˆ’âˆ‡ğ‘“ (ğ‘‹ğ‘¡) ğ‘‘ğ‘¡ +

(5)
where ğ‘Šğ‘¡ is the Wiener process (Brownian motion). For us  the crucial fact is that Langevin dynamics
converges to the stationary distribution given by ğ‘(ğ‘¥) âˆ ğ‘’âˆ’ğ‘“ (ğ‘¥).
Substituting ğ›½ğ‘“ for ğ‘“ in (5) gives the Langevin diffusion process for inverse temperature ğ›½  which
has stationary distribution âˆ ğ‘’âˆ’ğ›½ğ‘“ (ğ‘¥). Equivalently we can consider the temperature as changing the
magnitude of the noise:

2 ğ‘‘ğ‘Šğ‘¡

âˆš

ğ‘‘ğ‘‹ğ‘¡ = âˆ’âˆ‡ğ‘“ (ğ‘‹ğ‘¡)ğ‘‘ğ‘¡ +

âˆšï¸€
ğ‘‹ğ‘¡+1 = ğ‘‹ğ‘¡ âˆ’ ğœ‚âˆ‡ğ‘“ (ğ‘‹ğ‘¡) +âˆšï¸€2ğœ‚ğœ‰ğ‘˜ 

2ğ›½âˆ’1ğ‘‘ğ‘Šğ‘¡.

Of course algorithmically we cannot run a continuous-time process  so we run a discretized version of
the above process: namely  we run a Markov chain where the random variable at time ğ‘¡ is described
as

(6)
ğœ‚ scaling is that running Brownian motion for ğœ‚ of
ğœ‚.) This is analogous to how gradient descent is a discretization of

âˆš

ğœ‰ğ‘˜ âˆ¼ ğ‘ (0  ğ¼)

where ğœ‚ is the step size. (The reason for the
the time scales the variance by
gradient ï¬‚ow.

âˆš

2.1.1 Prior work on Langevin dynamics

For Langevin dynamics  convergence to the stationary distribution is a classic result [Bha78]. Fast
mixing for log-concave distributions is also a classic result: [BÃ‰85  BBCG08] show that log-
concave distributions satisfy a PoincarÃ© and log-Sobolev inequality  which characterize the rate
of convergenceâ€”If ğ‘“ is ğ›¼-strongly convex  then the mixing time is on the order of 1
ğ›¼. Of course 
algorithmically  one can only run a â€œdiscretizedâ€ version of the Langevin dynamics. Analyses of the
discretization are more recent: [Dal16  DM16  Dal17  DK17  DMM18] give running times bounds
for sampling from a log-concave distribution over Rğ‘‘  and [BEL18] give a algorithm to sample
from a log-concave distribution restricted to a convex set by incorporating a projection. We note
these analysis and ours are for the simplest kind of Langevin dynamics  the overdamped case; better
rates are known for underdamped dynamics ([CCBJ17])  if a Metropolis-Hastings rejection step is
used ([DCWY18])  and for Hamiltonian Monte Carlo which takes into account momentum ([MS17]).
[RRT17] consider arbitrary non-log-concave distributions with certain regularity and decay properties 
but the mixing time is exponential in general; furthermore  it has long been known that transition-
ing between different modes can take exponentially long  a phenomenon known as meta-stability
[BEGK02  BEGK04  BGK05]. The Holley-Stroock Theorem (see e.g. [BGL13]) shows that guaran-
tees for mixing extend to distributions ğ‘’âˆ’ğ‘“ (ğ‘¥) where ğ‘“ (ğ‘¥) is a â€œniceâ€ function that is close to a convex

4

function in ğ¿âˆ distance; however  this does not address more global deviations from convexity.
[MV17] consider a more general model with multiplicative noise.

2.2 Simulated tempering

For distributions that are far from being log-concave and have many deep modes  additional techniques
are necessary. One proposed heuristic  out of many  is simulated tempering  which swaps between
Markov chains that are different temperature variants of the original chain. The intuition is that
the Markov chains at higher temperature can move between modes more easily  and hence  the
higher-temperature chain acts as a â€œbridgeâ€ to move between modes.
Indeed  Langevin dynamics corresponding to a higher temperature distributionâ€”with ğ›½ğ‘“ rather
than ğ‘“  where ğ›½ < 1â€”mixes faster. (Here  we use terminology from statistical physics  letting ğœ
denote teh temperature and ğ›½ = 1
ğœ denote the inverse temperature.) A high temperature ï¬‚attens
out the distribution. However  we canâ€™t simply run Langevin at a higher temperature because the
stationary distribution is wrong; the simulated tempering chain combines Markov chains at different
temperatures in a way that preserves the stationary distribution.
We can deï¬ne simulated tempering with respect to any sequence of Markov chains ğ‘€ğ‘– on the same
space â„¦. Think of ğ‘€ğ‘– as the Markov chain corresponding to temperature ğ‘–  with stationary distribution
ğ‘’âˆ’ğ›½ğ‘–ğ‘“ .
Then we deï¬ne the simulated tempering Markov chain as follows.

âˆ™ The state space is â„¦ Ã— [ğ¿]: ğ¿ copies of the state space (in our case Rğ‘‘)  one copy for each
âˆ™ The evolution is deï¬ned as follows.

temperature.

1. If the current point is (ğ‘¥  ğ‘–)  then evolve according to the ğ‘–th chain ğ‘€ğ‘–.
2. Propose swaps with some rate ğœ†. When a swap is proposed  attempt to move to a
neighboring chain  ğ‘–â€² = ğ‘– Â± 1. With probability min{ğ‘ğ‘–â€²(ğ‘¥)/ğ‘ğ‘–(ğ‘¥)  1}  the transition is
successful. Otherwise  stay at the same point. This is a Metropolis-Hastings step; its
purpose is to preserve the stationary distribution.2

The crucial fact to note is that the stationary distribution is a â€œmixtureâ€ of the distributions corre-
sponding to the different temperatures. Namely:
Proposition 2.1. [MP92  Nea96] If the ğ‘€ğ‘˜  1 â‰¤ ğ‘˜ â‰¤ ğ¿ are reversible Markov chains with stationary
distributions ğ‘ğ‘˜  then the simulated tempering chain ğ‘€ is a reversible Markov chain with stationary
distribution

ğ‘(ğ‘¥  ğ‘–) =

ğ‘ğ‘–(ğ‘¥).

1
ğ¿

The typical setting of simulated tempering is as follows. The Markov chains come from a smooth
family of Markov chains with parameter ğ›½ â‰¥ 0  and ğ‘€ğ‘– is the Markov chain with parameter ğ›½ğ‘– 
where 0 â‰¤ ğ›½1 â‰¤ . . . â‰¤ ğ›½ğ¿ = 1. We are interested in sampling from the distribution when ğ›½ is large
(ğœ is small). However  the chain suffers from torpid mixing in this case  because the distribution is
more peaked. The simulated tempering chain uses smaller ğ›½ (larger ğœ) to help with mixing. For us 
the stationary distribution at inverse temperature ğ›½ is âˆ ğ‘’âˆ’ğ›½ğ‘“ (ğ‘¥).

2.2.1 Prior work on simulated tempering

Provable results of this heuristic are few and far between. [WSH09  Zhe03] lower-bound the spectral
gap for generic simulated tempering chains  using a Markov chain decomposition technique due to
[MR02]. However  for the Problem 1.1 that we are interested in  the spectral gap bound in [WSH09]
is exponentially small as a function of the number of modes. Drawing inspiration from [MR02]  we
establish a Markov chain decomposition technique that overcomes this.

2 This can be deï¬ned as either a discrete or continuous Markov chain. For a discrete chain  we propose
a swap with probability ğœ† and follow the current chain with probability 1 âˆ’ ğœ†. For a continuous chain  the
time between swaps is an exponential distribution with decay ğœ† (in other words  the times of the swaps forms a
Poisson process). Note that simulated tempering is traditionally deï¬ned for discrete Markov chains  but we will
use the continuous version. See Deï¬nition C.1 for the formal deï¬nition.

5

One issue that comes up in simulated tempering is estimating the partition functions; various methods
have been proposed for this [PP07  Lia05].

2.3 Main algorithm

Our algorithm is intuitively the following. Take a sequence of inverse temperatures ğ›½ğ‘–  starting at a
small value and increasing geometrically towards 1. Run simulated tempering Langevin on these
temperatures  suitably discretized. Take the samples that are at the ğ¿th temperature.
Note that there is one complication: the standard simulated tempering chain assumes that we can
compute the ratio between temperatures ğ‘ğ‘–â€² (ğ‘¥)
ğ‘ğ‘–(ğ‘¥) . However  we only know the probability density
functions up to a normalizing factor (the partition function). To overcome this  we note that if we use
the ratios ğ‘Ÿğ‘–â€² ğ‘ğ‘–â€² (ğ‘¥)
ğ‘–=1 ğ‘Ÿğ‘– = 1  then the chain converges to the stationary distribution
ğ‘Ÿğ‘–ğ‘ğ‘–(ğ‘¥)
with ğ‘(ğ‘¥  ğ‘–) = ğ‘Ÿğ‘–ğ‘ğ‘–(ğ‘¥). Thus  it sufï¬ces to estimate each partition function up to a constant factor. We
can do this inductively: running the simulated tempering chain on the ï¬rst â„“ levels  we can estimate
the partition function ğ‘â„“+1; then we can run the simulated tempering chain on the ï¬rst â„“ + 1 levels.
This is what Algorithm 2 does when it calls Algorithm 1 as subroutine.
A formal description of the algorithm follows.

instead  forâˆ‘ï¸€ğ¿

Algorithm 1 Simulated tempering Langevin Monte Carlo

INPUT: Temperatures ğ›½1  . . .   ğ›½â„“; partition function estimates Ì‚ï¸€ğ‘1  . . .  Ì‚ï¸€ğ‘â„“; step size ğœ‚  time ğ‘‡   rate

0ğ¼).

ğœ†  variance of initial distribution ğœ0.
OUTPUT: A random sample ğ‘¥ âˆˆ Rğ‘‘ (approximately from the distribution ğ‘â„“(ğ‘¥) âˆ ğ‘’âˆ’ğ›½â„“ğ‘“ (ğ‘¥)).
Let (ğ‘–  ğ‘¥) = (1  ğ‘¥0) where ğ‘¥0 âˆ¼ ğ‘ (0  ğœ2
Let ğ‘› = 0  ğ‘‡0 = 0.
while ğ‘‡ğ‘› < ğ‘‡ do

Determine the next transition time: Draw ğœ‰ğ‘›+1 from the exponential distribution ğ‘(ğ‘¥) = ğœ†ğ‘’âˆ’ğœ†ğ‘¥ 
ğ‘¥ â‰¥ 0.

Let ğœ‰ğ‘›+1 â†(cid:91) min{ğ‘‡ âˆ’ ğ‘‡ğ‘›  ğœ‰ğ‘›+1}  ğ‘‡ğ‘›+1 = ğ‘‡ğ‘› + ğœ‰ğ‘›+1.

âŒˆï¸ ğœ‰ğ‘›+1
âŒˆï¸ ğœ‰ğ‘›+1
âŒ‰ï¸
times: Update ğ‘¥ according to ğ‘¥ â†(cid:91) ğ‘¥ âˆ’ ğœ‚â€²ğ›½ğ‘–âˆ‡ğ‘“ (ğ‘¥) +
{ï¸ ğ‘’
ğ‘–â€² ğ‘“ (ğ‘¥)/Ì‚ï¸€ğ‘ğ‘–â€²
ğ‘’âˆ’ğ›½ğ‘–ğ‘“ (ğ‘¥)/Ì‚ï¸€ğ‘ğ‘–

Let ğœ‚â€² = ğœ‰ğ‘›+1/
Repeat
If ğ‘‡ğ‘›+1 < ğ‘‡ (i.e.  the end time has not been reached)  let ğ‘–â€² = ğ‘– Â± 1 with probability 1
2. If ğ‘–â€² is
out of bounds  do nothing. If ğ‘–â€² is in bounds  make a type 2 transition  where the acceptance
ratio is min

(the largest step size < ğœ‚ that evenly divides into ğœ‰ğ‘›+1).

2ğœ‚â€²ğœ‰  ğœ‰ âˆ¼ ğ‘ (0  ğ¼).

}ï¸

âŒ‰ï¸

âˆš

  1

âˆ’ğ›½

.

ğœ‚

ğœ‚

ğ‘› â†(cid:91) ğ‘› + 1.

end while
If the ï¬nal state is (â„“  ğ‘¥) for some ğ‘¥ âˆˆ Rğ‘‘  return ğ‘¥. Otherwise  re-run the chain.

Algorithm 2 Main algorithm

INPUT: A function ğ‘“ : Rğ‘‘  satisfying assumption (2)  to which we have gradient access.
OUTPUT: A random sample ğ‘¥ âˆˆ Rğ‘‘.
Let 0 â‰¤ ğ›½1 < Â·Â·Â· < ğ›½ğ¿ = 1 be a sequence of inverse temperatures satisfying (117) and (118).
for â„“ = 1 â†’ ğ¿ do

Let Ì‚ï¸€ğ‘1 = 1.
function estimates Ì‚ï¸€ğ‘1  . . .  Ì‚ï¸€ğ‘â„“  step size ğœ‚  time ğ‘‡   and rate ğœ† given by Lemma G.2.
(ï¸ 1
Ì‚ï¸ğ‘â„“

to get ğ‘› = ğ‘‚(ğ¿2 ln(ï¸€ 1

ğ‘—=1 ğ‘’(âˆ’ğ›½â„“+1+ğ›½â„“)ğ‘“ (ğ‘¥ğ‘— ))ï¸
âˆ‘ï¸€ğ‘›

Run the simulated tempering chain in Algorithm 1 with temperatures ğ›½1  . . .   ğ›½â„“  partition

If â„“ = ğ¿  return the sample.
If

and let (cid:91)ğ‘â„“+1 =

â„“ < ğ¿ 

samples 

)ï¸€)

repeat

.

ğ›¿

ğ‘›
end for

6

3 Overview of the proof techniques

We summarize the main ingredients and crucial techniques in the proof  while the full proofs are
included in the appendices.

Step 1: Deï¬ne a continuous version of the simulated tempering Markov chain (Deï¬nition C.1 
Lemma C.2)  where transition times are real numbers determined by an exponential weighting time
distribution.

Step 2: Prove a new decomposition theorem (Theorem D.2) for bounding the spectral gap (or
equivalently  the mixing time) of the simulated tempering chain we deï¬ne. This is the main technical
ingredient  and also a result of independent interest.
While decomposition theorems have appeared in the Markov Chain literature (e.g. [MR02])  typically
one partitions the state space  and bounds the spectral gap using (1) the probability ï¬‚ow of the chain
inside the individual sets  and (2) between different sets.
In our case  we decompose the Markov chain itself; this includes a decomposition of the stationary
distribution into components. (More precisely  we show a decomposition theorem on the generator of
the tempering chain.) We would like to do this because in our setting  the stationary distribution is
exactly a mixture distribution (Problem 1.1).
Our Markov chain decomposition theorem bounds the spectral gap (mixing time) of a simulated
tempering chain in terms of the spectral gap (mixing time) of two chains:

1. â€œcomponentâ€ chains on the mixture components
2. a â€œprojectedâ€ chain whose state space is the set of components  and which captures the
action of the chain between components as well as the ğœ’2-divergence between the mixture
components.

This means that if the Markov chain on the individual components mixes rapidly  and the â€œprojectedâ€
chain mixes rapidly  then the simulated tempering chain mixes rapidly as well.
(Note [MR02 
Theorem 1.2] does partition into mixture components  but they only consider the special case where
they components are laid out in a chain.)
The mixing time of a continuous Markov chain is quantiï¬ed by a PoincarÃ© inequality.
Theorem (Simpliï¬ed version of Theorem D.2). Consider the simulated tempering chain ğ‘€ with
ğ¶   where the Markov chain at the ğ‘–th level (temperature) is ğ‘€ğ‘– = (â„¦  Lğ‘–) with stationary
rate ğœ† = 1
distribution ğ‘ğ‘–  for 1 â‰¤ ğ‘– â‰¤ ğ¿. Suppose we have a decomposition of the Markov chain at each level 
ğ‘—=1 ğ‘¤ğ‘– ğ‘— = 1. If each ğ‘€ğ‘– ğ‘— satisï¬es a PoincarÃ© inequality with
constant ğ¶  and the ğœ’2-projected chain ğ‘€ satisï¬es a PoincarÃ© inequality with constant ğ¶  then ğ‘€
satisï¬es a PoincarÃ© inequality with constant ğ‘‚(ğ¶(1 + ğ¶)).
Here  the projected chain ğ‘€ is the chain on [ğ¿] Ã— [ğ‘š] with probability ï¬‚ow in the same and adjacent
levels given by

ğ‘—=1 ğ‘¤ğ‘– ğ‘—ğ‘ğ‘– ğ‘—ğ‘€ğ‘– ğ‘—  whereâˆ‘ï¸€ğ‘š

ğ‘ğ‘–ğ‘€ğ‘– =âˆ‘ï¸€ğ‘š

ğ‘ƒ ((ğ‘–  ğ‘—)  (ğ‘–  ğ‘—â€²)) =

ğ‘ƒ ((ğ‘–  ğ‘—)  (ğ‘– Â± 1  ğ‘—)) =

where ğœ’2

sym(ğ‘  ğ‘) = max{ğœ’2(ğ‘||ğ‘)  ğœ’2(ğ‘||ğ‘)}.

ğœ’2

sym(ğ‘ğ‘– ğ‘—  ğ‘ğ‘– ğ‘—â€²)

  1

min
sym(ğ‘ğ‘– ğ‘—  ğ‘ğ‘–Â±1 ğ‘—â€²)
ğœ’2

ğ‘¤ğ‘– ğ‘—

ğ‘¤ğ‘– ğ‘—â€²

{ï¸ ğ‘¤ğ‘–Â±1 ğ‘—

}ï¸

(7)

(8)

 

The decomposition theorem is the reason why we use a slightly different simulated tempering chain 
which is allowed to transition at arbitrary times  with some rate ğœ†. Such a chain â€œcomposesâ€ nicely
with the decomposition of the Langevin chain  and allows a better control of the Dirichlet form of the
tempering chain  which governs the mixing time.

Step 3: Finally  we need to apply the decomposition theorem to our setup  namely a distribution
which is a mixture of strongly log-concave distributions. The â€œcomponentsâ€ of the decomposition in

7

our setup are simply the mixture components ğ‘’âˆ’ğ‘“0(ğ‘¥âˆ’ğœ‡ğ‘— ). We rely crucially on the fact that Langevin
diffusion on a mixture distribution decomposes into Langevin diffusion on the individual components.

We actually ï¬rst analyze the hypothetical simulated tempering Langevin chain on Ìƒï¸€ğ‘ğ‘– âˆ
âˆ‘ï¸€ğ‘š
ğ‘—=1 ğ‘¤ğ‘—ğ‘’âˆ’ğ›½ğ‘— ğ‘“0(ğ‘¥âˆ’ğœ‡ğ‘— ) (Theorem E.1)â€”i.e.  where the stationary distribution for each tempera-
we can run  where ğ‘ğ‘– âˆ ğ‘ğ›½. To do this  we use the fact that ğ‘ğ‘– is off fromÌƒï¸€ğ‘ğ‘– by at most
ture is a mixture. Then in Lemma E.5 we compare to the actual simulated tempering Langevin that
. (This is

where the factor of ğ‘¤min comes in.)
To use our Markov chain decomposition theorem  we need to show two things:

1

ğ‘¤min

1. The component chains mix rapidly: this follows from the classic fact that Langevin diffusion

mixes rapidly for log-concave distributions.

2. The projected chain mixes rapidly: The â€œprojectedâ€ chain is deï¬ned as having more prob-
ability ï¬‚ow between mixture components in the same or adjacent temperatures which are
close together in ğœ’2-divergence.
By choosing the temperatures close enough  we can ensure that the corresponding mixture
components in adjacent temperatures are close in ğœ’2-divergence. By choosing the highest
temperature large enough  we can ensure that all the mixture components at the highest
temperature are close in ğœ’2-divergence.
From this it follows that we can easily get from any component to any other (by traveling
up to the highest temperature and then back down). Thus the projected chain mixes rapidly
from the method of canonical paths  Theorem B.4.

Note that the equal variance (for gaussians) or shape (for general log-concave distributions) condition
is necessary here. For gaussians with different variance  the Markov chain can fail to mix between
components at the highest temperature. This is because scaling the temperature changes the variance
of all the components equally  and preserves their ratio (which is not equal to 1).
Step 4: We analyze the error from discretization (Lemma F.1)  and choose parameters so that it is
small. We show that in Algorithm 2 we can inductively estimate the partition functions. When we
have all the estimates  we can run the simulated tempering chain on all the temperatures to get the
desired sample.

4 Conclusion

We initiated a study of sampling â€œbeyond log-convexity." In so doing  we developed a new general
technique to analyze simulated tempering  a classical algorithm used in practice to combat multi-
modality but that has seen little theoretical analysis. The technique is a new decomposition lemma
for Markov chains based on decomposing the Markov chain rather than just the state space. We have
analyzed simulated tempering with Langevin diffusion  but note that it can be applied to any with any
other Markov chain with a notion of temperature.
Our result is the ï¬rst result in its class (sampling multimodal  non-log-concave distributions with
gradient oracle access). Admittedly  distributions encountered in practice are rarely mixtures of
distributions with the same shape. However  we hope that our techniques may be built on to
provide guarantees for more practical probability distributions. An exciting research direction is
to provide (average-case) guarantees for probability distributions encountered in practice  such as
posteriors for clustering  topic models  and Ising models. For example  the posterior distribution for a
mixture of gaussians can have exponentially many terms  but may perhaps be tractable in practice.
Another interesting direction is to study other temperature heuristics used in practice  such as particle
ï¬lters [Sch12  DMHW+12  PJT15  GDM+17]  annealed importance sampling [Nea01]  and parallel
tempering [WSH09].

References

[BBCG08] Dominique Bakry  Franck Barthe  Patrick Cattiaux  and Arnaud Guillin. A simple
proof of the PoincarÃ© inequality for a large class of probability measures including
the log-concave case. Electron. Commun. Probab  13:60â€“66  2008.

8

[BÃ‰85] Dominique Bakry and Michel Ã‰mery. Diffusions hypercontractives. In SÃ©minaire de

ProbabilitÃ©s XIX 1983/84  pages 177â€“206. Springer  1985.

[BEGK02] Anton Bovier  Michael Eckhoff  VÃ©ronique Gayrard  and Markus Klein. Metastability
and low lying spectra in reversible Markov chains. Communications in mathematical
physics  228(2):219â€“255  2002.

[BEGK04] Anton Bovier  Michael Eckhoff  VÃ©ronique Gayrard  and Markus Klein. Metastability
in reversible diffusion processes i: Sharp asymptotics for capacities and exit times.
Journal of the European Mathematical Society  6(4):399â€“424  2004.

[BEL18] SÃ©bastien Bubeck  Ronen Eldan  and Joseph Lehec. Sampling from a log-concave
distribution with projected langevin monte carlo. Discrete & Computational Geometry 
59(4):757â€“783  2018.

[BGK05] Anton Bovier  VÃ©ronique Gayrard  and Markus Klein. Metastability in reversible
diffusion processes ii: Precise asymptotics for small eigenvalues. Journal of the
European Mathematical Society  7(1):69â€“99  2005.

[BGL13] Dominique Bakry  Ivan Gentil  and Michel Ledoux. Analysis and geometry of Markov

diffusion operators  volume 348. Springer Science & Business Media  2013.

[Bha78] RN Bhattacharya. Criteria for recurrence and existence of invariant measures for

multidimensional diffusions. The Annals of Probability  pages 541â€“553  1978.

[CCBJ17] Xiang Cheng  Niladri S Chatterji  Peter L Bartlett  and Michael I Jordan.
arXiv preprint

Underdamped Langevin MCMC: A non-asymptotic analysis.
arXiv:1707.03663  2017.

[Dal16] Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and
log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  2016.

[Dal17] Arnak Dalalyan. Further and stronger analogy between sampling and optimization:
Langevin monte carlo and gradient descent. In Satyen Kale and Ohad Shamir  editors 
Proceedings of the 2017 Conference on Learning Theory  volume 65 of Proceedings
of Machine Learning Research  pages 678â€“689  Amsterdam  Netherlands  07â€“10 Jul
2017. PMLR.

[DCWY18] Raaz Dwivedi  Yuansi Chen  Martin J Wainwright  and Bin Yu. Log-concave sampling:
Metropolis-Hastings algorithms are fast! In Proceedings of the 2018 Conference on
Learning Theory  PMLR 75  2018.

[DK17] Arnak S Dalalyan and Avetik G Karagulyan. User-friendly guarantees for the
Langevin Monte Carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095 
2017.

[DM16] Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the

unadjusted Langevin algorithm. 2016.

[DMHW+12] Pierre Del Moral  Peng Hu  Liming Wu  et al. On the concentration properties
of interacting particle processes. Foundations and Trends Râ—‹ in Machine Learning 
3(3â€“4):225â€“389  2012.

[DMM18] Alain Durmus  Szymon Majewski  and BÅ‚aË™zej Miasojedow. Analysis of Langevin

Monte Carlo via convex optimization. arXiv preprint arXiv:1802.09188  2018.

[GDM+17] FranÃ§ois Giraud  Pierre Del Moral  et al. Nonasymptotic analysis of adaptive and

annealed Feynmanâ€“Kac particle models. Bernoulli  23(1):670â€“709  2017.

[KW13] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv

preprint arXiv:1312.6114  2013.

[Lia05] Faming Liang. Determination of normalizing constants for simulated tempering.

Physica A: Statistical Mechanics and its Applications  356(2-4):468â€“480  2005.

9

[LS93] LÃ¡szlÃ³ LovÃ¡sz and MiklÃ³s Simonovits. Random walks in a convex body and an
improved volume algorithm. Random structures & algorithms  4(4):359â€“412  1993.

[MP92] Enzo Marinari and Giorgio Parisi. Simulated tempering: a new Monte Carlo scheme.

EPL (Europhysics Letters)  19(6):451  1992.

[MR02] Neal Madras and Dana Randall. Markov chain decomposition for convergence rate

analysis. Annals of Applied Probability  pages 581â€“606  2002.

[MS17] Oren Mangoubi and Aaron Smith. Rapid mixing of Hamiltonian Monte Carlo on

strongly log-concave distributions. arXiv preprint arXiv:1708.07114  2017.

[MV17] Oren Mangoubi and Nisheeth K Vishnoi. Convex optimization with nonconvex

oracles. arXiv preprint arXiv:1711.02621  2017.

[Nea96] Radford M Neal. Sampling from multimodal distributions using tempered transitions.

Statistics and computing  6(4):353â€“366  1996.

[Nea01] Radford M Neal. Annealed importance sampling. Statistics and computing  11(2):125â€“

139  2001.

[PJT15] Daniel Paulin  Ajay Jasra  and Alexandre Thiery. Error bounds for sequential Monte
Carlo samplers for multimodal distributions. arXiv preprint arXiv:1509.08775  2015.

[PP07] Sanghyun Park and Vijay S Pande. Choosing weights for simulated tempering.

Physical Review E  76(1):016703  2007.

[RMW14] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic back-
propagation and approximate inference in deep generative models. In International
Conference on Machine Learning  pages 1278â€“1286  2014.

[RRT17] Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via
stochastic gradient langevin dynamics: a nonasymptotic analysis. In Conference on
Learning Theory  pages 1674â€“1703  2017.

[Sch12] Nikolaus Schweizer. Non-asymptotic error bounds for sequential MCMC methods.

2012.

[SR11] David Sontag and Dan Roy. Complexity of inference in latent dirichlet allocation. In

Advances in neural information processing systems  pages 1008â€“1016  2011.

[Vem05] Santosh Vempala. Geometric random walks: a survey. Combinatorial and computa-

tional geometry  52(573-612):2  2005.

[WSH09] Dawn B Woodard  Scott C Schmidler  and Mark Huber. Conditions for rapid mixing
of parallel and simulated tempering on multimodal distributions. The Annals of
Applied Probability  pages 617â€“640  2009.

[Zhe03] Zhongrong Zheng. On swapping and simulated tempering algorithms. Stochastic

Processes and their Applications  104(1):131â€“154  2003.

10

,Holden Lee
Andrej Risteski
Rong Ge