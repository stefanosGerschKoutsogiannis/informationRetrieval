2017,Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction,Adversarial machines  where a learner competes against an adversary  have regained much recent interest in machine learning. They are naturally in the form of saddle-point optimization  often with separable structure but sometimes also with unmanageably large dimension. In this work we show that adversarial prediction under multivariate losses can be solved much faster than they used to be. We first reduce the problem size exponentially by using appropriate sufficient statistics  and then we adapt the new stochastic variance-reduced algorithm of Balamurugan & Bach (2016) to allow any Bregman divergence. We prove that the same linear rate of convergence is retained and we show that for adversarial prediction using KL-divergence we can further achieve a speedup of #example times compared with the Euclidean alternative. We verify the theoretical findings through extensive experiments on two example applications: adversarial prediction and LPboosting.,Bregman Divergence for Stochastic Variance

Reduction: Saddle-Point and Adversarial Prediction

Zhan Shi

Xinhua Zhang

University of Illinois at Chicago
{zshi22 zhangx}@uic.edu

Chicago  Illinois 60661

Yaoliang Yu

University of Waterloo
Waterloo  ON  N2L3G1

yaoliang.yu@uwaterloo.ca

Abstract

Adversarial machines  where a learner competes against an adversary  have re-
gained much recent interest in machine learning. They are naturally in the form of
saddle-point optimization  often with separable structure but sometimes also with
unmanageably large dimension. In this work we show that adversarial prediction
under multivariate losses can be solved much faster than they used to be. We ﬁrst
reduce the problem size exponentially by using appropriate sufﬁcient statistics 
and then we adapt the new stochastic variance-reduced algorithm of Balamurugan
& Bach (2016) to allow any Bregman divergence. We prove that the same linear
rate of convergence is retained and we show that for adversarial prediction using
KL-divergence we can further achieve a speedup of #example times compared
with the Euclidean alternative. We verify the theoretical ﬁndings through extensive
experiments on two example applications: adversarial prediction and LPboosting.

Introduction

1
Many algorithmic advances have been achieved in machine learning by ﬁnely leveraging the separa-
bility in the model. For example  stochastic gradient descent (SGD) algorithms typically exploit the
fact that the objective is an expectation of a random function  with each component corresponding
to a training example. A “dual” approach partitions the problem into blocks of coordinates and
processes them in a stochastic fashion [1]. Recently  by exploiting the ﬁnite-sum structure of the
model  variance-reduction based stochastic methods have surpassed the well-known sublinear lower
bound of SGD. Examples include SVRG [2]  SAGA [3]  SAG [4]  Finito [5]  MISO [6]  and SDCA
[7  8]  just to name a few. Specialized algorithms have also been proposed for accommodating
proximal terms [9]  and for further acceleration through the condition number [10–13].
However  not all empirical risks are separable in its plain form  and in many cases dualization is
necessary for achieving separability. This leads to a composite saddle-point problem with convex-
concave (saddle) functions K and M:

(cid:80)n

(x∗  y∗) = arg minx maxy K(x  y) + M (x  y)  where K(x  y) = 1

n

k=1 ψk(x  y).

(1)

Most commonly used supervised losses for linear models can be written as g(cid:63)(Xw)  where g(cid:63) is
the Fenchel dual of a convex function g  X is the design matrix  and w is the model vector. So
the regularized risk minimization can be naturally written as minw maxα α(cid:48)Xw + Ω(w) − g(α) 
where Ω is a regularizer. This ﬁts into our framework (1) with a bilinear function K and a decoupled
function M. Optimization for this speciﬁc form of saddle-point problems has been extensively
studied. For example  [14] and [15] performed batch updates on w and stochastic updates on α 
while [16] and [17] performed doubly stochastic updates on both w and α  achieving O( 1
 ) and
 ) rates respectively. The latter two also studied the more general form (1). Our interest in this
O(log 1
paper is double stochasticity  aiming to maximally harness the power of separability and stochasticity.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Adversarial machines  where the learner competes against an adversary  have re-gained much recent
interest in machine learning [18–20]. On one hand they ﬁt naturally into the saddle-point optimization
framework (1) but on the other hand they are known to be notoriously challenging to solve. The central
message of this work is that certain adversarial machines can be solved signiﬁcantly faster than they
used to be. Key to our development is a new extension of the stochastic variance-reduced algorithm
in [17] such that it is compatible with any Bregman divergence  hence opening the possibility to
largely reduce the quadratic condition number in [17] by better adapting to the underlying geometry
using non-Euclidean norms and Bregman divergences.
Improving condition numbers by Bregman divergence has long been studied in (stochastic  proximal)
gradient descent [21  22]. The best known algorithm is arguably stochastic mirror descent [23]  which
was extended to saddle-points by [16] and to ADMM by [24]. However  they can only achieve the
sublinear rate O(1/) (for an -accurate solution). On the other hand  many recent stochastic variance-
reduced methods [2–6  9  17] that achieve the much faster linear rate O(log 1/) rely inherently on
the Euclidean structure  and their extension to Bregman divergence  although conceptually clear 
remains challenging in terms of the analysis. For example  the analysis of [17] relied on the resolvent
of monotone operators [25] and is hence restricted to the Euclidean norm. In §2 we extend the notion
of Bregman divergence to saddle functions and we prove a new Pythagorean theorem that may be of
independent interest for analyzing ﬁrst order algorithms. In §4 we introduce a fundamentally different
proof technique (details relegated to Appendix C) that overcomes several challenges arising from a
general Bregman divergence (e.g. asymmetry and unbounded gradient on bounded domain)  and we
recover similar quantitative linear rate of convergence as [17] but with the ﬂexibility of using suitable
Bregman divergences to reduce the condition number.
The new stochastic variance-reduced algorithm Breg-SVRG is then applied to the adversarial pre-
diction framework (with multivariate losses such as F-score) [19  20]. Here we make three novel
contributions: (a) We provide a signiﬁcant reformulation of the adversarial prediction problem that
reduces the dimension of the optimization variable from 2n to n2 (where n is the number of samples) 
hence making it amenable to stochastic variance-reduced optimization (§3). (b) We develop a new
efﬁcient algorithm for computing the proximal update with a separable saddle KL-divergence (§5).
(c) We verify that Breg-SVRG accelerates its Euclidean alternative by a factor of n in both theory
and practice (§6)  hence conﬁrming again the uttermost importance of adapting to the underlying
problem geometry. To our best knowledge  this is the ﬁrst time stochastic variance-reduced methods
have been shown with great promise in optimizing adversarial machines.
Finally  we mention that we expect our algorithm Breg-SVRG to be useful for solving many other
saddle-point problems  and we provide a second example (LPboosting) in experiments (§6).

2 Bregman Divergence and Saddle Functions
In this section we set up some notations  recall some background materials  and extend Bregman
divergences to saddle functions  a key notion in our later analysis.
Bregman divergence. For any convex and differentiable function ψ over some closed convex set
C ⊆ Rd  its induced Bregman divergence is deﬁned as:

∀x ∈ int(C)  x(cid:48) ∈ C  ∆ψ(x(cid:48)  x) := ψ(x(cid:48)) − ψ(x) − (cid:104)∇ψ(x)  x(cid:48) − x(cid:105)  

(2)
where ∇ψ is the gradient and (cid:104)· ·(cid:105) is the standard inner product in Rd. Clearly  ∆ψ(x(cid:48)  x) ≥ 0 since
ψ is convex. We mention two familiar examples of Bregman divergence.
• Squared Euclidean distance: ∆ψ(x(cid:48)  x) = 1
2 (cid:107)x(cid:107)2

• (Unnormalized) KL-divergence: ∆ψ(x(cid:48)  x) =(cid:80)

i + xi  ψ(x) =(cid:80)

2 (cid:107)x(cid:48) − x(cid:107)2
i x(cid:48)

2   ψ(x) = 1
i log x(cid:48)
− x(cid:48)

i
xi

2  where (cid:107) · (cid:107)2 is (cid:96)2 norm.

i xi log xi.

f (x(cid:48)) ≥ f (x) + (cid:104)∂f (x)  x(cid:48) − x(cid:105) + ∆ψ(x(cid:48)  x).

Strong convexity. Following [26] we call a function f ψ-convex if f−ψ is convex  i.e. for all x  x(cid:48)
(3)
Smoothness. A function f is L-smooth wrt a norm (cid:107)·(cid:107) if its gradient ∇f is L-Lipschitz continuous 
i.e.  for all x and x(cid:48)  (cid:107)∇f (x(cid:48)) − ∇f (x)(cid:107)∗ ≤ L(cid:107)x(cid:48) − x(cid:107)   where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107). The
change of a smooth function  in terms of its induced Bregman divergence  can be upper bounded by
the change of its input and lower bounded by the change of its slope  cf. Lemma 2 in Appendix A.

2

Saddle functions. Recall that a function φ(x  y) over Cz = Cx × Cy is called a saddle function if it
is convex in x for any y ∈ Cy  and concave in y for any x ∈ Cx. Given a saddle function φ  we call
(x∗  y∗) its saddle point if

∀x ∈ Cx  ∀y ∈ Cy  φ(x∗  y) ≤ φ(x∗  y∗) ≤ φ(x  y∗) 

(4)
or equivalently (x∗  y∗) ∈ arg minx∈Cx maxy∈Cy φ(x  y). Assuming φ is differentiable  we denote
(5)
Note the negation sign due to the concavity in y. We can quantify the notion of “saddle”: A function
f (x  y) is called φ-saddle iff f − φ is a saddle function  or equivalently  ∆f (z(cid:48)  z) ≥ ∆φ(z(cid:48)  z) (see
below). Note that any saddle function φ is 0-saddle and φ-saddle.
Bregman divergence for saddle functions. We now deﬁne the Bregman divergence induced by a
saddle function φ: for z = (x  y) and z(cid:48) = (x(cid:48)  y(cid:48)) in Cz 

Gφ(x  y) := [∂xφ(x  y);−∂yφ(x  y)].

∆φ(z(cid:48)  z) := ∆φy (x(cid:48)  x) + ∆−φx (y(cid:48)  y) = φ(x(cid:48)  y) − φ(x  y(cid:48)) − (cid:104)Gφ(z)  z(cid:48) − z(cid:105)  

(6)

where φy(x) = φ(x  y) is a convex function of x for any ﬁxed y  and similarly φx(y) = φ(x  y) is a
concave (hence the negation) function of y for any ﬁxed x. The similarity between (6) and the usual
Bregman divergence ∆ψ in (2) is apparent. However  φ is never evaluated at z(cid:48) but z (for G) and the
cross pairs (x(cid:48)  y) and (x  y(cid:48)). Key to our subsequent analysis is the following lemma that extends a
result of [27] to saddle functions (proof in Appendix A).
Lemma 1. Let f and g be φ-saddle and ϕ-saddle respectively  with one of them being dif-
ferentiable. Then  for any z = (x  y) and any saddle point (if exists) z∗ := (x∗  y∗) ∈
arg minx maxy {f (z) + g(z)}   we have f (x  y∗)+g(x  y∗) ≥ f (x∗  y)+g(x∗  y)+∆φ+ϕ(z  z∗).
Geometry of norms. In the sequel  we will design two convex functions ψx(x) and ψy(y) such that
their induced Bregman divergences are “distance enforcing” (a.k.a. 1-strongly convex)  that is  w.r.t.
two norms (cid:107)·(cid:107)x and (cid:107)·(cid:107)y that we also design  the following inequality holds:

∆x(x  x(cid:48)) := ∆ψx(x  x(cid:48)) ≥ 1

2 (cid:107)x − x(cid:48)(cid:107)2

x   ∆y(y  y(cid:48)) := ∆ψy (y  y(cid:48)) ≥ 1

2 (cid:107)y − y(cid:48)(cid:107)2
y .

Further  for z = (x  y)  we deﬁne

∆z(z  z(cid:48)) := ∆ψx−ψy (z  z(cid:48)) ≥ 1

2 (cid:107)z − z(cid:48)(cid:107)2

z   where

(cid:107)z(cid:107)2

z := (cid:107)x(cid:107)2

x + (cid:107)y(cid:107)2

y

When it is clear from the context  we simply omit the subscripts and write ∆  (cid:107)·(cid:107)  and (cid:107)·(cid:107)∗.
3 Adversarial Prediction under Multivariate Loss
A number of saddle-point based machine learning problems have been listed in [17]. Here we
give another example (adversarial prediction under multivariate loss) that is naturally formulated
as a saddle-point problem but also requires a careful adaptation to the underlying geometry—a
challenge that was not addressed in [17] since their algorithm inherently relies on the Euclidean
norm. We remark that adaptation to the underlying geometry has been studied in the (stochastic)
mirror descent framework [23]  with signiﬁcant improvements on condition numbers or gradient
norm bounds. Surprisingly  no analogous efforts have been attempted in the stochastic variance
reduction framework—a gap we intend to ﬁll in this work.
The adversarial prediction framework [19  20  28]  arising naturally as a saddle-point problem  is a
convex alternative to the generative adversarial net [18]. Given a training sample X = [x1  . . .   xn]
and ˜y = [˜y1  . . .   ˜yn] ∈ {0  1}n  adversarial prediction optimizes the following saddle function that
is an expectation of some multivariate loss (cid:96)(y  z) (e.g. F-score) over the labels y  z ∈ {0  1}n of all
data points:

(7)

(8)

(cid:104)

(cid:105)

E

max
q∈∆2n

min
p∈∆2n

(9)
Here the proponent tries to ﬁnd a distribution p(·) over the labeling on the entire training set in
order to minimize the loss (∆2n is the 2n dimensional probability simplex). An opponent in contrast
tries to maximize the expected loss by ﬁnding another distribution q(·)  but his strategy is subject to
the constraint that the feature expectation matches that of the empirical distribution. Introducing a

(cid:96)(y  z)  s.t. E
z∼q

( 1
n Xz) = 1

y∼p z∼q

n X ˜y

3

θ

E

(cid:105)

max

− λ

2 + 1

y∼p z∼q

max
q∈∆2n

n θ(cid:48)Xy

n θ(cid:48)X ˜y + min
p∈∆2n

(cid:104) 2y(cid:48)z
1(cid:48)y+1(cid:48)z − 1

1(cid:48)y+1(cid:48)z and (cid:96)(0  0) := 1  the partial dual problem can be written as

Lagrangian variable θ to remove the feature expectation constraint and specializing the problem to
F-score where (cid:96)(y  z) = 2y(cid:48)z
2 (cid:107)θ(cid:107)2

 
where we use y(cid:48)z to denote the standard inner product and we followed [19] to add an (cid:96)2
2 regularizer
on θ penalizing the dual variables on the constraints over the training data. It appears that solving
(10) can be quite challenging  because the variables p and q in the inner minimax problem have 2n
entries! A constraint sampling algorithm was adopted in [19] to address this challenge  although
no formal guarantee was established. Note that we can maximize the outer unconstrained variable
θ (with dimension the same as the number of features) relatively easily using for instance gradient
ascent  provided that we can solve the inner minimax problem quickly—a signiﬁcant challenge to
which we turn our attention below.
Surprisingly  we show here that the inner minimax problem in (10) can be signiﬁcantly simpliﬁed.
The key observation is that the expectation in the objective depends only on a few sufﬁcient statistics
of p and q. Indeed  by interpreting p and q as probability distributions over {0  1}n we have:

(10)

2y(cid:48)z

1(cid:48)y + 1(cid:48)z

E

= p({0})q({0}) +

= p({0})q({0}) +

n(cid:88)
n(cid:88)

i=1

n(cid:88)
n(cid:88)

j=1

i=1

j=1

(cid:17)

E

(cid:16) 2y(cid:48)z
1(cid:48)y+1(cid:48)z [[1(cid:48)y = i]][[1(cid:48)z = j]]
(cid:124)
(cid:124)

E (y[[1(cid:48)y = i]])

(cid:123)(cid:122)

· 1
i

· 1
j

(cid:125)

(cid:48)

αi

2ij
i + j

E (z[[1(cid:48)z = j]])

 

(cid:123)(cid:122)

βj

(cid:125)

(11)

(12)

(13)

(14)

(cid:88)
n(cid:88)
n(cid:88)

i

1(cid:48)αi =

iαi =

1
i

(cid:88)
(cid:2) 2ijn2
(cid:124)
i+j α(cid:48)

i

where [[·]] = 1 if · is true  and 0 otherwise. Crucially  the variables αi and βj are sufﬁcient for
re-expressing (10)  since

E (1(cid:48)y[[1(cid:48)y = i]]) = E[[1(cid:48)y = i]] = p({1(cid:48)y = i}) 

E (y[[1(cid:48)y = i]]) = Ey 

(cid:3)+Ω(α)−Ω(β)  (15)

and similar equalities also hold for βj. In details  the inner minimax problem of (10) simpliﬁes to:

(cid:123)(cid:122)
iβj +n2α(cid:48)

(cid:125)
i11(cid:48)βj

1
n2

−n1(cid:48)αi−n1(cid:48)βj−θ(cid:48)Xiαi

min
α∈S

max
β∈S

i=1

j=1

fij (αi βj )

where S = {α ≥ 0 : 1(cid:48)α ≤ 1 ∀i  (cid:107)iαi(cid:107)∞ ≤ (cid:107)αi(cid:107)1}  Ω(α) = µ(cid:80)

i j αij log(αij). (16)
Importantly  α = [α1; . . .   αn] (resp. β) has n2 entries  which is signiﬁcantly smaller than the 2n
entries of p (resp. q) in (10). For later purpose we have also incorporated an entropy regularizer for
α and β respectively in (15).
To justify the constraint set S  note from (12) and (13) that for any distribution p of y:

since α ≥ 0 and y ∈ {0  1}n  (cid:107)iαi(cid:107)∞ ≤ E(cid:107)y[[1(cid:48)y = i]](cid:107)∞ ≤ E[[1(cid:48)y = i]] = (cid:107)αi(cid:107)1.

(17)
Conversely  for any α ∈ S  we can construct a distribution p such that iαij = E (yj[[1(cid:48)y = i]]) =
p({1(cid:48)y = i  yj = 1}) in the following algorithmic way: Fix i and for each j deﬁne Yj = {y ∈
{0  1}n : 1(cid:48)y = i  yj = 1}. Let U = {1  . . .   n}. Find an index j in U that minimizes αij and set
p({y}) = iαij/|Yj| for each y ∈ Yj. Perform the following updates:

U ← U \ {j}  ∀k (cid:54)= j  Yk ← Yk \ Yj  αik ← αik − αij|Yk ∩ Yj|/|Yj|

(18)
Continue this procedure until U is empty. Due to the way we choose j  α remains nonnegative and
by construction αij = p({1(cid:48)y = i  yj = 1}) once we remove j from U.
The objective function in (15) ﬁts naturally into the framework of (1)  with Ω(α) − Ω(β) and
constraints corresponding to M  and the rest terms to K. The entropy function Ω is convex wrt the
KL-divergence  which is in turn distance enforcing wrt the (cid:96)1 norm over the probability simplex [23].
In the next section we propose the SVRG algorithm with Bregman divergence (Breg-SVRG) that (a)
provably optimizes strongly convex saddle function with a linear convergence rate  and (b) adapts
to the underlying geometry by choosing an appropriate Bregman divergence. Then  in §5 we apply
Breg-SVRG to (15) and achieve a factor of n speedup over a straightforward instantiation of [17].

4

(cid:46) epoch index

4 Breg-SVRG for Saddle-Point

Algorithm 1: Breg-SVRG for Saddle-Point
1 Initialize z0 randomly. Set ˜z = z0.
2 for s = 1  2  . . . do
˜µ ← ˜µs := ∇K(˜z)  z0 ← zs
3
for t = 1  . . .   m do
4
5
6
7

In this section we propose an efﬁcient algorithm
for solving the general saddle-point problem in (1)
and prove its linear rate of convergence. Our main
assumption is:
Assumption 1. There exist two norms (cid:107)·(cid:107)x and
(cid:107)·(cid:107)y such that each ψk is a saddle function and
L-smooth; M is (ψx − ψy)-saddle; and ψx and ψy
are distance enforcing (cf. (7)).
Note that w.l.o.g. we have scaled the norms so that
the usual strong convexity parameter of M is 1.
Recall we deﬁned (cid:107)z(cid:107)z and ∆z in (8). For saddle-point optimization  it is common to deﬁne a signed
gradient G(z) := [∂xK(z);−∂yK(z)] (since K is concave in y). Recall J = K + M  and (x∗  y∗)
is a saddle-point of J. Using Assumption 1  we measure the gap of an iterate zt = (xt  yt) as follows:

0 := zm
Randomly pick ξ ∈ {1  . . .   n}.
Compute vt using (20).
Update zt using (21).
(1 + η)tzt

(cid:46) m(cid:80)

˜z ← ˜zs :=

(cid:46) iter index

(1 + η)t.

t=1

m(cid:80)

t=1

8

t = (zt) = J(xt  y∗) − J(x∗  yt) ≥ ∆(zt  z∗) ≥ 1

(19)
Inspired by [2  9  17]  we propose in Algorithm 1 a new stochastic variance-reduced algorithm for
solving the saddle-point problem (1) using Bregman divergences. The algorithm proceeds in epochs.
In each epoch  we ﬁrst compute the following stochastic estimate of the signed gradient G(zt) by
drawing a random component from K:

2 (cid:107)zt − z∗(cid:107)2 ≥ 0.

(cid:19)

(cid:18) vx(zt)

−vy(zt)

vt =

where

(cid:26)vx(zt) := ∂xψξ(zt) − ∂xψξ(˜z) + ∂xK(˜z)

vy(zt) := ∂yψξ(zt) − ∂yψξ(˜z) + ∂yK(˜z)

.

(20)

Here ˜z is the pivot chosen after completing the previous epoch. We make two important observations:
(1) By construction the stochastic gradient vt is unbiased: Eξ[vt] = G(zt); (2) The expensive gradient
evaluation ∂K(˜z) need only be computed once in each epoch since ˜z is held unchanged. If ˜z → z∗ 
then the variance of vt would be largely reduced hence faster convergence may be possible.
Next  Algorithm 1 performs the following joint proximal update:

(xt+1  yt+1) = arg min

x

max

y

η (cid:104)vx(zt)  x(cid:105) + η (cid:104)vy(zt)  y(cid:105) + ηM (x  y) + ∆(x  xt) − ∆(y  yt)  (21)

/log(1 + η)

2(cid:107)x − xt(cid:107)2

where we have the ﬂexibility in choosing a suitable Bregman divergence to better adapt to the
underlying geometry. When ∆(x  xt) = 1
2  we recover the special case in [17]. However 
to handle the asymmetry in a general Bregman divergence (which does not appear for the Euclidean
distance)  we have to choose the pivot ˜z in a signiﬁcantly different way than [2  9  17].
We are now ready to present our main convergence guarantee for Breg-SVRG in Algorithm 1.
Theorem 1. Let Assumption 1 hold  and choose a sufﬁciently small η > 0 such that m :=
log

(cid:17)
(cid:108)
(cid:16) 1−ηL
18ηL2 −η−1
E(˜zs) ≤ (1 + η)−ms[∆(z∗  z0) + c(Z + 1)(z0)]  where Z =(cid:80)m−1

(cid:109)≥ 1. Then Breg-SVRG enjoys linear convergence in expectation:
45L2   which leads to c = O(1/L2)  m = Θ(cid:0)L2(cid:1)  (1 + η)m ≥ 64

45 
For example  we may set η = 1
and Z = O(L2). Therefore  between epochs  the gap (˜zs) decays (in expectation) by a factor of 45
64 
and each epoch needs to conduct the proximal update (21) for m = Θ(L2) number of times. (We
remind that w.l.o.g. we have scaled the norms so that the usual strong convexity parameter is 1.) In
total  to reduce the gap below some threshold   Breg-SVRG needs to call the proximal update (21)
O(L2 log 1
Discussions. As mentioned  Algorithm 1 and Theorem 1 extend those in [17] which in turn extend
[2  9] to saddle-point problems. However  [2  9  17] all heavily exploit the Euclidean structure (in
particular symmetry) hence their proofs cannot be applied to an asymmetric Bregman divergence.
Our innovations here include: (a) A new Pythagorean theorem for the newly introduced saddle
Bregman divergence (Lemma 1). (b) A moderate extension of the variance reduction lemma in [9] to
accommodate any norm (Appendix B). (c) A different pivot ˜z is adopted in each epoch to handle

 ) number of times  plus a similar number of component gradient evaluations.

t=0 (1+η)t  c = 18η2L2

1−ηL . (22)

5

asymmetry. (d) A new analysis technique through introducing a crucial auxiliary variable that enables
us to bound the function gap directly. See our proof in Appendix C for more details. Compared with
classical mirror descent algorithms [16  23] that can also solve saddle-point problems with Bregman
divergences  our analysis is fundamentally different and we achieve the signiﬁcantly stronger rate
O(log(1/) than the sublinear O(1/) rate of [16]  at the expense of a squared instead of linear
dependence on L. Similar tradeoff also appeared in [17]. We will return to this issue in Section 5.
Variants and acceleration. Our analysis also supports to use different ξ in vx and vy. The standard
acceleration methods such as universal catalyst [10] and non-uniform sampling can be applied directly
(see Appendix E where L  the largest smoothness constant over all pieces  is replaced by their mean).
5 Application of Breg-SVRG to Adversarial Prediction
The quadratic dependence on L  the smoothness parameter  in Theorem 1 reinforces the need to
choose suitable Bregman divergences. In this section we illustrate how this can be achieved for the
adversarial prediction problem in Section 3. As pointed out in [17]  the factorization of K is important 
and we consider three schemes: (a) ψk = fij; (b) ψk = 1
i=1 fi k.
n
W.l.o.g. let us ﬁx the µ in (16) to 1.
Comparison of smoothness constant. Both α and β are n2-dimensional  and the bilinear function
fij can be written as α(cid:48)Aijβ  where Aij ∈ Rn2×n2 is an n-by-n block matrix  with the (i  j)-th
i+j I + 11(cid:48)) and all other blocks being 0. The linear terms in (15) can be absorbed
block being n2( 2ij
into the regularizer Ω without affecting the smoothness parameter.
For scheme (a)  the smoothness constant L2 under (cid:96)2 norm depends on the spectral norm of Aij:
L2 = maxi j n2(n + 2ij
i+j )) = Θ(n3). In contrast the smoothness constant L1 under (cid:96)1 norm depends
on the absolute value of the entries in Aij: L1 = maxi j n2(1 + 2ij
i+j ) = Θ(n3); no saving is achieved.
j=1 Akjβ. Then L1 = O(n2) while

For scheme (b)  the bilinear function ψk corresponds to 1

j=1 fk j; and (c) ψk = 1

n α(cid:48)(cid:80)n

(cid:80)n

(cid:80)n

n

L2

2 =

1
n2 max

2 ≥ n2 max
(cid:107)v(cid:107)2=1
1 saves a factor of n compared with L2
2.

v:(cid:107)v(cid:107)2=1

max

j=1

k

Therefore  L2
Comparison of smoothness constant for the overall problem. By strong duality  we may push the
maximization over θ to the innermost level of (10)  arriving at an overall problem in α and β only:

(cid:107)11(cid:48)v(cid:107)2 = n5.

(23)

(cid:88)n

(cid:13)(cid:13)Akjv(cid:13)(cid:13)2

(cid:88)n

j=1

(cid:21)

.

n(cid:88)

n(cid:88)

(cid:20)

i=1

j=1

min
{αi}∈S

max
{βj}∈S

1
n2

fij(αi  βj) − i
λn

c(cid:48)Xαi +

α(cid:48)
iX(cid:48)Xαj +

ij
2λ

1

2λn2 (cid:107)c(cid:107)2

2

(24)

2 under (cid:96)2 norm is upper bounded by the sum of
= Ω(n6)  i.e. L2 = Θ(n3). In contrast
1 under (cid:96)1 norm is at most the sum of square of maximum absolute
= Θ(n6) 

where c = X ˜y. The quadratic term w.r.t. α can be written as α(cid:48)Bijα  where Bij ∈ Rn2×n2 is an
2λ X(cid:48)X and all other blocks being 0. And we
n-by-n block matrix  with its (i  j)-th block being ij
assume each (cid:107)xi(cid:107)2 ≤ 1. The smoothness constant can be bounded separately from Aij and Bij; see
(128) in Appendix F.
(cid:0) ij
2λ n(cid:1)2
For scheme (a)  the smoothness constant square L2
2 ≥ maxi j
(cid:16)
spectral norm square of Aij and Bij. So L2
the smoothness constant square L2
1 ≤ maxi j
value of the entries in Aij and Bij. Hence L2
j=1 Akjβ + α(cid:48)(cid:80)n
n (α(cid:48)(cid:80)n
i.e. L1 = Θ(n3). So no saving is achieved here.
(cid:13)(cid:13)(cid:88)n
Akjv(cid:13)(cid:13)2
For scheme (b)  ψk corresponds to 1
(cid:17)2
(cid:16) kj
+
2 ≥ n5 similar to (23). Therefore  L2

1 saves a factor of n
2. Similar results apply to scheme (c) too. We also tried non-uniform sampling  but

and by setting β to 0 in (126)  we get L2
compared with L2

1 ≤ 1
L2
≤ 1

Bkjv(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:88)n

j=1 Bkjα). Then

(cid:17)2(cid:21)

∞ + max

n2 max

n2 max

n2(1+ 2kj

n2(1+ 2ij

max

v:(cid:107)v(cid:107)1=1

(by (128))

(cid:0) ij

v:(cid:107)v(cid:107)1=1

(cid:20)(cid:16)

+ maxi j

(cid:17)2

(cid:1)2

max

j

= n4 

(25)

(26)

j=1

k+j )

(cid:21)

∞

i+j )

(cid:20)

k

k

j=1

2λ

2

6

it does not change the order in n. It can also be shown that if our scheme randomly samples n entries
from {Aij  Bij}  the above L1 and L2 cannot be improved by further engineering the factorization.
Computational complexity. We ﬁnally seek efﬁcient algorithms for the proximal update (21) used
by Breg-SVRG. When M (α  β) = Ω(α) − Ω(β) as in (16)  we can solve α and β separately as:

αik log(αik/bik) − cik 

s.t. 1(cid:48)α ≤ 1  ∀i ∀k  0 ≤ iαik ≤ 1(cid:48)αi.

(27)

(cid:88)

ik

min

α

where bik and cik are constants. In Appendix D we designe an efﬁcient “closed form” algorithm
which ﬁnds an  accurate solution in O(n2 log2 1
 ) time  which is also on par with that for computing
the stochastic gradient in schemes (b) and (c). Although scheme (a) reduces the cost of gradient
computation to O(n)  its corresponding smoothness parameter L2
1 is increased by n2 times  hence
not worthwhile. We did manage to design an ˜O(n) algorithm for the proximal update in scheme (a) 
but empirically the overall convergence is rather slow.
If we use the Euclidean squared distance as the Bregman divergence  then a term (cid:107)α − αt(cid:107)2
2 needs to
be added to the objective (27). No efﬁcient “closed form” solution is available  and so in experiments
we simply absorbed M into K  and then the proximal update becomes the Euclidean projection onto
S  which does admit a competitive O(n2 log2(1/)) time solution.

6 Experimental Results

Our major goal here is to show that empirically Entropy-SVRG (Breg-SVRG with KL divergence) is
signiﬁcantly more efﬁcient than Euclidean-SVRG (Breg-SVRG with squared Euclidean distance) on
some learning problems  especially those with an entropic regularizer and a simplex constraint.

6.1 Entropy regularized LPBoost

We applied Breg-SVRG to an extension of LP Boosting using entropy regularization [29]. In a binary
classiﬁcation setting  the base hypotheses over the training set can be compactly represented as
U = (y1x1  . . .   ynxn)(cid:48). Then the model considers a minimax game between a distribution d ∈ ∆n
over training examples and a distribution w ∈ ∆m over the hypotheses:
d(cid:48)U w + λΩ(d) − γΩ(w).

min

(28)

d∈∆n di≤ν

max
w∈∆m

Here w tries to combine the hypotheses to maximize the edge (prediction conﬁdence) yix(cid:48)
iw  while
the adversary d tries to place more weights (bounded by ν) on “hard” examples to reduce the edge.
Settings. We experimented on the adult dataset from the UCI repository  which we partitioned
into n = 32  561 training examples and 16 281 test examples  with m = 123 features. We set
λ = γ = 0.01 and ν = 0.1 due to its best prediction accuracy. We tried a range of values of the step
size η  and the best we found was 10−3 for Entropy-SVRG and 10−6 for Euclidean-SVRG (larger
step size for Euclidean-SVRG ﬂuctuated even worse). For both methods  m = 32561/50 gave good
results.
The stochastic gradient in d was computed by U:jwj  where U:j is the j-th column and j is randomly
i:. We tried with Uijwj and Uijdi (scheme (a) in §5) 
sampled. The stochastic gradient in w is diU(cid:48)
but they performed worse. We also tried with the universal catalyst in the same form as [17]  which
can be directly extended to Entropy-SVRG. Similarly we used the non-uniform sampling based on
the (cid:96)2 norm of the rows and columns of U. It turned out that the Euclidean-SVRG can beneﬁt slightly
from it  while Entropy-SVRG does not. So we only show the “accelerated” results for the former.
To make the computational cost comparable across machines  we introduced a counter called effective
number of passes: #pass. Assume the proximal operator has been called #po number of times  then
(29)
We also compared with a “convex” approach. Given d  the optimal w in (28) obviously admits a
closed-form solution. General saddle-point problems certainly do not enjoy such a convenience.
However  we hope to take advantage of this opportunity to study the following question: suppose we
solve (28) as a convex optimization in d and the stochastic gradient were computed from the optimal

#pass := number of epochs so far + n+m

nm · #po.

7

(a) Primal gap v.s. #pass

(a) Primal gap v.s. #pass

(b) Primal gap v.s. CPU time

(b) Test accuracy v.s. #pass

(c) Test F-score v.s. #pass

(d) Test F-score v.s. CPU time

Figure 1: Entropy Regularized
LPBoost on adult

Figure 2: Adversarial Prediction on the synthetic dataset.

w  would it be faster than the saddle SVRG? Since solving w requires visiting the entire U  strictly
nm ·#po in the deﬁnition of #pass in (29) should be replaced by #po. However 
speaking the term n+m
we stuck with (29) because our interest is whether a more accurate stochastic gradient in d (based
on the optimal w) can outperform doubly stochastic (saddle) optimization. We emphasize that this
comparison is only for conceptual understanding  because generally optimizing the inner variable
requires costly iterative methods.
Results. Figure 1(a) demonstrated how fast the primal gap (with w optimized out for each d) is
reduced as a function of the number of effective passes. Methods based on entropic prox are clearly
much more efﬁcient than Euclidean prox. This corroborates our theory that for problems like (28) 
Entropy-SVRG is more suitable for the underlying geometry (entropic regularizer with simplex
constraints).
We also observed that using entropic prox  our doubly stochastic method is as efﬁcient as the “convex”
method  meaning that although at each iteration the w in saddle SVRG is not the optimal for the
current d  it still allows the overall algorithm to perform as fast as if it were. This suggests that for
general saddle-point problems where no closed-form inner solution is available  our method will still
be efﬁcient and competitive. Note this “convex” method is similar to the optimizer used by [29].
Finally  we investigated the increase of test accuracy as more passes over the data are performed.
Figure 1(b) shows  once more  that the entropic prox does allow the accuracy to be improved much
faster than Euclidean prox. Again  the convex and saddle methods perform similarly.
As a ﬁnal note  the Euclidean/entropic proximal operator for both d and w can be solved in either
closed form  or by a 1-D line search based on partial Lagrangian. So their computational cost differ
in the same order of magnitude as multiplication v.s. exponentiation  which is much smaller than the
difference of #pass shown in Figure 1.

6.2 Adversarial prediction with F-score
Datasets. Here we considered two datasets. The ﬁrst is a synthetic dataset where the positive
examples are drawn from a 200 dimensional normal distribution with mean 0.1 · 1 and covariance
0.5 · I  and negative examples are drawn from N (−0.1 · 1  0.5 · I). The training set has n = 100
samples  half are positive and half are negative. The test set has 200 samples with the same class
ratio. Notice that n = 100 means we are optimizing over two 100-by-100 matrices constrained to a
challenging set S. So the optimization problem is indeed not trivial.

8

0200400600800Number of effective passes10-510-410-310-210-1100Primal gapEntropy  SaddleEntropy  ConvexEuclidean  SaddleEuclidean  Convex0200400600800Number of effective passes757779818385Test accuracy (%)Entropy  SaddleEntropy  ConvexEuclidean  SaddleEuclidean  Convex0100200300400Number of effective passes10-410-2100Primal gapEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  Catalyst05101520CPU time(mins)10-410-2100Primal gapEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  Catalyst0100200300Number of effective passes0.90.920.940.96Test F-scoreEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  Catalyst051015CPU time(mins)0.90.920.940.96Test F-scoreEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  CatalystThe second dataset  ionosphere 
has 211 training examples (122
pos and 89 neg). 89 examples
were used for testing (52 pos
and 37 neg). Each example has
34 features.
Methods.
To apply saddle
SVRG  we used strong duality
to push the optimization over θ
to the inner-most level of (10) 
and then eliminated θ because
it is a simple quadratic. So
we ended up with the convex-
concave optimization as shown
in (24)  where the K part of (15)
is augmented with a quadratic
term in α. The formulae for
computing the stochastic gra-
dient using scheme (b) are de-
tailed in Appendix G. We ﬁxed
µ = 1  λ = 0.01 for the iono-
sphere dataset  and µ = 1  λ =
0.1 for the synthetic dataset.
We also tried the universal cat-
alyst along with non-uniform
sampling where each i was sam-

(a) Primal gap v.s. #pass

(b) Primal gap v.s. CPU time

(c) Test F-score v.s. #pass

(d) Test F-score v.s. CPU time

Figure 3: Adversarial Prediction on the ionosphere dataset.

pled with a probability proportional to(cid:80)n

k=1 (cid:107)Aik(cid:107)2

F   and similarly for j. Here (cid:107)·(cid:107)F is the Frobe-
nious norm.
Parameter Tuning. Since each entry in the n × n matrix α is relatively small when n is large  we
needed a relatively small step size. When n = 100  we used 10−2 for Entropy-SVRG and 10−6 for
Euclidean-SVRG (a larger step size makes it over-ﬂuctuate). When applying catalyst  the catalyst
regularizor can suppress the noise from larger step size. After a careful trade off between catalyst
regularizor parameter and larger step size  we managed to achieve faster convergence empirically.
Results. The results on the two datasets are shown in Figures 2 and 3 respectively. We truncated
the #pass and CPU time in subplots (c) and (d) because the F-score has stabilized and we would
rather zoom in to see the initial growing phase. In terms of primal gap versus #pass (subplot a)  the
entropy based method is signiﬁcantly more effective than Euclidean methods on both datasets (Figure
2(a) and 3(a)). Even with catalyst  Euclidean-Saddle is still much slower than the entropy based
methods on the synthetic dataset in Figure 2(a). The CPU time comparisons (subplot b) follow the
similar trend  except that the “convex methods” should be ignored because they are introduced only
to compare #pass.
The F-score is noisy because  as is well known  it is not monotonic with the primal gap and glitches
can appear. In subplots 2(d) and 3(d)  the entropy based methods achieve higher F-score signiﬁcantly
faster than the plain Euclidean based methods on both datasets. In terms of passes (subplots 2(c) and
3(c))  Euclidean-Saddle and Entropy-Saddle achieved a similar F-score at ﬁrst because their primal
gaps are comparable at the beginning. After 20 passes  the F-score of Euclidean-Saddle is overtaken
by Entropy-Saddle as the primal gap of Entropy-Saddle become much smaller than Euclidean-Saddle.

7 Conclusions and Future Work
We have proposed Breg-SVRG to solve saddle-point optimization and proved its linear rate of
convergence. Application to adversarial prediction conﬁrmed its effectiveness. For future work  we
are interested in relaxing the (potentially hard) proximal update in (21). We will also derive similar
reformulations for DCG and precision@k  with a quadratic number of variables and with a ﬁnite sum
structure that is again amenable to Breg-SVRG  leading to a similar reduction of the condition number
compared to Euclidean-SVRG. These reformulations  however  come with different constraint sets 
and new proximal algorithms with similar complexity as for the F-score can be developed.

9

0200400600Number of effective passes10-410-2100102Primal gapEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  Catalyst020406080CPU time(mins)10-410-2100102Primal gapEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  Catalyst050100150200Number of effective passes0.750.80.850.9Test F-scoreEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  Catalyst0102030CPU time(mins)0.750.80.850.9Test F-scoreEuclidean  ConvexEuclidean  SaddleEuclidean  Saddle  CatalystEntropy  ConvexEntropy  SaddleEntropy  Saddle  CatalystReferences
[1] S. Lacoste-Julien  M. Jaggi  M. Schmidt  and P. Pletscher. Block-coordinate frank-wolfe

optimization for structural SVMs. In ICML. 2013.

[2] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS. 2013.

[3] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with

support for non-strongly convex composite objectives. In NIPS. 2014.

[4] M. Schmidt  N. L. Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  2016.

[5] A. J. Defazio  T. S. Caetano  and J. Domke. Finito: A faster  permutable incremental gradient

method for big data problems. In ICML. 2014.

[6] J. Mairal. Incremental majorization-minimization optimization with application to large-scale

machine learning. SIAM Journal on Optimization  25(2):829–855  2015.

[7] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. J. Mach. Learn. Res.  14:567–599  2013.

[8] S. Shalev-Shwartz. SDCA without duality  regularization  and individual convexity. In ICML.

2016.

[9] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[10] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In NIPS.

2015.

[11] A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. In NIPS. 2014.
[12] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for

regularized loss minimization. In ICML. 2014.

[13] R. Babanezhad  M. O. Ahmed  A. Virani  M. Schmidt  J. Koneˇcn´y  and S. Sallinen. Stop

wasting my gradients: Practical svrg. In NIPS. 2015.

[14] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk

minimization. In ICML. 2015.

[15] Z. Zhu and A. J. Storkey. Adaptive stochastic primal-dual coordinate descent for separable
saddle point problems. In Machine Learning and Knowledge Discovery in Databases  pp.
645–658. 2015.

[16] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[17] P. Balamurugan and F. Bach. Stochastic variance reduction methods for saddle-point problems.

In NIPS. 2016.

[18] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and

Y. Bengio. Generative adversarial nets. In NIPS. 2014.

[19] H. Wang  W. Xing  K. Asif  and B. D. Ziebart. Adversarial prediction games for multivariate

losses. In NIPS. 2015.

[20] F. Farnia and D. Tse. A minimax approach to supervised learning. In NIPS. 2016.
[21] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming 

103(1):127–152  2005.

[22] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM J. on Opti-

mization  16(1):235–249  2005. ISSN 1052-6234.

[23] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters  31(3):167–175  2003.

[24] H. Wang and A. Banerjee. Bregman alternating direction method of multipliers. In NIPS. 2014.
[25] R. T. Rockafellar. Monotone operators associated with saddle functions and minimax problems.

Nonlinear Functional Analysis  18(part 1):397–407  1970.

10

[26] J. C. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent.

In Proc. Annual Conf. Computational Learning Theory. 2010.

[27] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted

to SIAM Journal on Optimization  2009.

[28] K. Asif  W. Xing  S. Behpour  and B. D. Ziebart. Adversarial cost-sensitive classiﬁcation. In

UAI. 2015.

[29] M. K. Warmuth  K. A. Glocer  and S. V. N. Vishwanathan. Entropy regularized LPBoost. In
Y. Freund  Y. L. Gy¨orﬁ  and G. Tur`an  eds.  Proc. Intl. Conf. Algorithmic Learning Theory 
no. 5254 in Lecture Notes in Artiﬁcial Intelligence  pp. 256 – 271. Springer-Verlag  Budapest 
October 2008.

[30] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. Fixed-Point

Algorithms for Inverse Problems in Science and Engineering  49:185–212  2011.

11

,Zhan Shi
Xinhua Zhang
Yaoliang Yu