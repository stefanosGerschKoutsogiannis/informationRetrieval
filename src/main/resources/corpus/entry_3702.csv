2017,Nearest-Neighbor Sample Compression: Efficiency  Consistency  Infinite Dimensions,We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight  fully empirical generalization bounds  as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension --- the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly  we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting  in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising  since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.,Nearest-Neighbor Sample Compression:
Efﬁciency  Consistency  Inﬁnite Dimensions

Aryeh Kontorovich

Department of Computer Science
Ben-Gurion University of the Negev

karyeh@cs.bgu.ac.il

Sivan Sabato

Department of Computer Science
Ben-Gurion University of the Negev

sabatos@bgu.ac.il

Department of Computer Science and Applied Mathematics

Roi Weiss

Weizmann Institute of Science
roiw@weizmann.ac.il

Abstract

We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based
multiclass learning algorithm. This algorithm is derived from sample compression
bounds and enjoys the statistical advantages of tight  fully empirical generalization
bounds  as well as the algorithmic advantages of a faster runtime and memory
savings. We prove that this algorithm is strongly Bayes-consistent in metric
spaces with ﬁnite doubling dimension — the ﬁrst consistency result for an efﬁcient
nearest-neighbor sample compression scheme. Rather surprisingly  we discover
that this algorithm continues to be Bayes-consistent even in a certain inﬁnite-
dimensional setting  in which the basic measure-theoretic conditions on which
classic consistency proofs hinge are violated. This is all the more surprising  since
it is known that k-NN is not Bayes-consistent in this setting. We pose several
challenging open problems for future research.

1

Introduction

This paper deals with Nearest-Neighbor (NN) learning algorithms in metric spaces. Initiated by
Fix and Hodges in 1951 [16]  this seemingly naive learning paradigm remains competitive against
more sophisticated methods [8  46] and  in its celebrated k-NN version  has been placed on a solid
theoretical foundation [11  44  13  47].
Although the classic 1-NN is well known to be inconsistent in general  in recent years a series of
papers has presented variations on the theme of a regularized 1-NN classiﬁer  as an alternative to the
Bayes-consistent k-NN. Gottlieb et al. [18] showed that approximate nearest neighbor search can
act as a regularizer  actually improving generalization performance rather than just injecting noise.
In a follow-up work  [27] showed that applying Structural Risk Minimization to (essentially) the
margin-regularized data-dependent bound in [18] yields a strongly Bayes-consistent 1-NN classiﬁer.
A further development has seen margin-based regularization analyzed through the lens of sample
compression: a near-optimal nearest neighbor condensing algorithm was presented [20] and later
extended to cover semimetric spaces [21]; an activized version also appeared [25]. As detailed in
[27]  margin-regularized 1-NN methods enjoy a number of statistical and computational advantages
over the traditional k-NN classiﬁer. Salient among these are explicit data-dependent generalization
bounds  and considerable runtime and memory savings. Sample compression affords additional
advantages  in the form of tighter generalization bounds and increased efﬁciency in time and space.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In this work we study the Bayes-consistency of a compression-based 1-NN multiclass learning
algorithm  in both ﬁnite-dimensional and inﬁnite-dimensional metric spaces. The algorithm is
essentially the passive component of the active learner proposed by Kontorovich  Sabato  and Urner
in [25]  and we refer to it in the sequel as KSU; for completeness  we present it here in full (Alg. 1).
We show that in ﬁnite-dimensional metric spaces  KSU is both computationally efﬁcient and Bayes-
consistent. This is the ﬁrst compression-based multiclass 1-NN algorithm proven to possess both of
these properties. We further exhibit a surprising phenomenon in inﬁnite-dimensional spaces  where
we construct a distribution for which KSU is Bayes-consistent while k-NN is not.

Main results. Our main contributions consist of analyzing the performance of KSU in ﬁnite and
inﬁnite dimensional settings  and comparing it to the classical k-NN learner. Our key ﬁndings are
summarized below.

• In Theorem 2  we show that KSU is computationally efﬁcient and strongly Bayes-consistent
in metric spaces with a ﬁnite doubling dimension. This is the ﬁrst (strong or otherwise)
Bayes-consistency result for an efﬁcient sample compression scheme for a multiclass (or
even binary)1 1-NN algorithm. This result should be contrasted with the one in [27]  where
margin-based regularization was employed  but not compression; the proof techniques
from [27] do not carry over to the compression-based scheme. Instead  novel arguments
are required  as we discuss below. The new sample compression technique provides a
Bayes-consistency proof for multiple (even countably many) labels; this is contrasted with
the multiclass 1-NN algorithm in [28]  which is not compression-based  and requires solving
a minimum vertex cover problem  thereby imposing a 2-approximation factor whenever
there are more than two labels.
• In Theorem 4  we make the surprising discovery that KSU continues to be Bayes-consistent
in a certain inﬁnite-dimensional setting  even though this setting violates the basic measure-
theoretic conditions on which classic consistency proofs hinge  including Theorem 2. This
is all the more surprising  since it is known that k-NN is not Bayes-consistent for this
construction [9]. We are currently unaware of any separable2 metric probability space on
which KSU fails to be Bayes-consistent; this is posed as an intriguing open problem.

Our results indicate that in ﬁnite dimensions  an efﬁcient  compression-based  Bayes-consistent
multiclass 1-NN algorithm exists  and hence can be offered as an alternative to k-NN  which is well
known to be Bayes-consistent in ﬁnite dimensions [12  41]. In contrast  in inﬁnite dimensions  our
results show that the condition characterizing the Bayes-consistency of k-NN does not extend to all
NN algorithms. It is an open problem to characterize the necessary and sufﬁcient conditions for the
existence of a Bayes-consistent NN-based algorithm in inﬁnite dimensions.

Related work. Following the pioneering work of [11] on nearest-neighbor classiﬁcation  it was
shown by [13  47  14] that the k-NN classiﬁer is strongly Bayes consistent in Rd. These results
made extensive use of the Euclidean structure of Rd  but in [41] a weak Bayes-consistency result was
shown for metric spaces with a bounded diameter and a bounded doubling dimension  and additional
distributional smoothness assumptions. More recently  some of the classic results on k-NN risk
decay rates were reﬁned by [10] in an analysis that captures the interplay between the metric and the
sampling distribution. The worst-case rates have an exponential dependence on the dimension (i.e. 
the so-called curse of dimensionality)  and Pestov [33  34] examines this phenomenon closely under
various distributional and structural assumptions.
Consistency of NN-type algorithms in more general (and in particular inﬁnite-dimensional) metric
spaces was discussed in [1  5  6  9  30]. In [1  9]  characterizations of Bayes-consistency were
given in terms of Besicovitch-type conditions (see Eq. (3)). In [1]  a generalized “moving window”
classiﬁcation rule is used and additional regularity conditions on the regression function are imposed.
The ﬁltering technique (i.e.  taking the ﬁrst d coordinates in some basis representation) was shown to
be universally consistent in [5]. However  that algorithm suffers from the cost of cross-validating
over both the dimension d and number of neighbors k. Also  the technique is only applicable in

1 An efﬁcient sample compression algorithm was given in [20] for the binary case  but no Bayes-consistency

guarantee is known for it.

2Cérou and Guyader [9] gave a simple example of a nonseparable metric on which all known nearest-neighbor

methods  including k-NN and KSU  obviously fail.

2

Hilbert spaces (as opposed to more general metric spaces) and provides only asymptotic consistency 
without ﬁnite-sample bounds such as those provided by KSU. The insight of [5] is extended to the
more general Banach spaces in [6] under various regularity assumptions.
None of the aforementioned generalization results for NN-based techniques are in the form of
fully empirical  explicitly computable sample-dependent error bounds. Rather  they are stated in
terms of the unknown Bayes-optimal rate  and some involve additional parameters quantifying the
well-behavedness of the unknown distribution (see [27] for a detailed discussion). As such  these
guarantees do not enable a practitioner to compute a numerical generalization error estimate for a
given training sample  much less allow for a data-dependent selection of k  which must be tuned via
cross-validation. The asymptotic expansions in [43  37  23  40] likewise do not provide a computable
ﬁnite-sample bound. The quest for such bounds was a key motivation behind the series of works
[18  28  20]  of which KSU [25] is the latest development.
The work of Devroye et al. [14  Theorem 21.2] has implications for 1-NN classiﬁers in Rd that
are deﬁned based on data-dependent majority-vote partitions of the space. It is shown that under
some conditions  a ﬁxed mapping from each sample size to a data-dependent partition rule induces a
strongly Bayes-consistent algorithm. This result requires the partition rule to have a bounded VC
dimension  and since this rule must be ﬁxed in advance  the algorithm is not fully adaptive. Theorem
19.3 ibid. proves weak consistency for an inefﬁcient compression-based algorithm  which selects
among all the possible compression sets of a certain size  and maintains a certain rate of compression
relative to the sample size. The generalizing power of sample compression was independently
discovered by [31]  and later elaborated upon by [22]. In the context of NN classiﬁcation  [14] lists
various condensing heuristics (which have no known performance guarantees) and leaves open the
algorithmic question of how to minimize the empirical risk over all subsets of a given size.
The ﬁrst compression-based 1-NN algorithm with provable optimality guarantees was given in [20];
it was based on constructing γ-nets in spaces with a ﬁnite doubling dimension. The compression
size of this construction was shown to be nearly unimprovable by an efﬁcient algorithm unless P=NP.
With γ-nets as its algorithmic engine  KSU inherits this near-optimality. The compression-based
1-NN paradigm was later extended to semimetrics in [21]  where it was shown to survive violations
of the triangle inequality  while the hierarchy-based search methods that have become standard for
metric spaces (such as [4  18] and related approaches) all break down.
It was shown in [27] that a margin-regularized 1-NN learner (essentially  the one proposed in [18] 
which  unlike [20]  did not involve sample compression) becomes strongly Bayes-consistent when the
margin is chosen optimally in an explicitly prescribed sample-dependent fashion. The margin-based
technique developed in [18] for the binary case was extended to multiclass in [28]. Since the algorithm
relied on computing a minimum vertex cover  it was not possible to make it both computationally
efﬁcient and Bayes-consistent when the number of lables exceeds two. An additional improvement
over [28] is that the generalization bounds presented there had an explicit (logarithmic) dependence
on the number of labels  while our compression scheme extends seamlessly to countable label spaces.

Paper outline. After ﬁxing the notation and setup in Sec. 2  in Sec. 3 we present KSU  the
compression-based 1-NN algorithm we analyze in this work. Sec. 4 discusses our main contributions
regarding KSU  together with some open problems. High-level proof sketches are given in Sec. 5 for
the ﬁnite-dimensional case  and Sec. 6 for the inﬁnite-dimensional case. Full detailed proofs can be
found in [26].

2 Setting and Notation
Our instance space is the metric space (X   ρ)  where X is the instance domain and ρ is the metric.
(See Appendix A in [26] for relevant background on metric measure spaces.) We consider a countable
label space Y. The unknown sampling distribution is a probability measure ¯µ over X × Y  with
(cid:80)
marginal µ over X . Denote by (X  Y ) ∼ ¯µ a pair drawn according to ¯µ. The generalization error of a
classiﬁer f : X → Y is given by err¯µ(f ) := P¯µ(Y (cid:54)= f (X))  and its empirical error with respect to
(x y)∈S(cid:48) 1[y (cid:54)= f (x)]. The optimal Bayes
¯µ := inf err¯µ(f )  where the inﬁmum is taken over all measurable classiﬁers f : X → Y.
risk of ¯µ is R∗
We say that ¯µ is realizable when R∗
¯µ = 0. We omit the overline in ¯µ in the sequel when there is no
ambiguity.

a labeled set S(cid:48) ⊆ X × Y is given by(cid:99)err(f  S(cid:48)) := 1|S(cid:48)|

3

For a ﬁnite labeled set S ⊆ X × Y and any x ∈ X   let Xnn(x  S) be the nearest neighbor of x with
respect to S and let Ynn(x  S) be the nearest neighbor label of x with respect to S:

(Xnn(x  S)  Ynn(x  S)) := argmin
(x(cid:48) y(cid:48))∈S

ρ(x  x(cid:48)) 

n

n

n

n ⊆ X × Y 
n := Sn  in this work we

n. While the classic 1-NN algorithm sets S(cid:48)

The set of points in S  denoted by X = {X1  . . .   X|S|} ⊆ X  

where ties are broken arbitrarily. The 1-NN classiﬁer induced by S is denoted by hS(x) :=
Ynn(x  S).
induces
:= {V1(X)  . . .   V|S|(X)}  where each Voronoi cell is
a Voronoi partition of X   V(X)
Vi(X) := {x ∈ X : argminj∈{1 ... |S|} ρ(x  Xj) = i}. By deﬁnition  ∀x ∈ Vi(X)  hS(x) = Yi.
A 1-NN algorithm is a mapping from an i.i.d. labeled sample Sn ∼ ¯µn to a labeled set S(cid:48)
yielding the 1-NN classiﬁer hS(cid:48)
study a compression-based algorithm which sets S(cid:48)
) converges to R∗ almost surely 
A 1-NN algorithm is strongly Bayes-consistent on ¯µ if err(hS(cid:48)
) = R∗] = 1. An algorithm is weakly Bayes-consistent on ¯µ if err(hS(cid:48)
that is P[limn→∞ err(hS(cid:48)
)
converges to R∗ in expectation  limn→∞ E[err(hS(cid:48)
)] = R∗. Obviously  the former implies the
latter. We say that an algorithm is Bayes-consistent on a metric space if it is Bayes-consistent on all
distributions in the metric space.
A convenient property that is used when studying the Bayes-consistency of algorithms in metric
spaces is the doubling dimension. Denote the open ball of radius r around x by Br(x) := {x(cid:48) ∈
X : ρ(x  x(cid:48)) < r} and let ¯Br(x) denote the corresponding closed ball. The doubling dimension of a
metric space (X   ρ) is deﬁned as follows. Let n be the smallest number such that every ball in X can
be covered by n balls of half its radius  where all balls are centered at points of X . Formally 
i=1Br/2(xi)}.

n := min{n ∈ N : ∀x ∈ X   r > 0  ∃x1  . . .   xn ∈ X s.t. Br(x) ⊆ ∪n

n adaptively  as discussed further below.

n

Then the doubling dimension of (X   ρ) is deﬁned by ddim(X   ρ) := log2 n.
For an integer n  let [n] := {1  . . .   n}. Denote the set of all index vectors of length d by In d :=
[n]d. Given a labeled set Sn = (Xi  Yi)i∈[n] and any i = {i1  . . .   id} ∈ In d  denote the sub-
sample of Sn indexed by i by Sn(i) := {(Xi1  Yi1)  . . .   (Xid   Yid )}. Similarly  for a vector Y (cid:48) =
{Y (cid:48)
d)}  namely the sub-sample
of Sn as determined by i where the labels are replaced with Y (cid:48). Lastly  for i  j ∈ In d  we denote
Sn(i; j) := {(Xi1  Yj1)  . . .   (Xid   Yjd )}.

d} ∈ Y d  denote by Sn(i  Y (cid:48)) := {(Xi1   Y (cid:48)

1 )  . . .   (Xid   Y (cid:48)

1   . . .   Y (cid:48)

3 1-NN majority-based compression

In this work we consider the 1-NN majority-based compression algorithm proposed in [25]  which
we refer to as KSU. This algorithm is based on constructing γ-nets at different scales; for γ > 0
and A ⊆ X   a set X ⊆ A is said to be a γ-net of A if ∀a ∈ A ∃x ∈ X : ρ(a  x) ≤ γ and for all
x (cid:54)= x(cid:48) ∈ X  ρ(x  x(cid:48)) > γ.3
The algorithm (see Alg. 1) operates as follows. Given an input sample Sn  whose set of points is
denoted Xn = {X1  . . .   Xn}  KSU considers all possible scales γ > 0. For each such scale it
constructs a γ-net of Xn. Denote this γ-net by X(γ) := {Xi1   . . .   Xim}  where m ≡ m(γ) denotes
its size and i ≡ i(γ) := {i1  . . .   im} ∈ In m denotes the indices selected from Sn for this γ-net.
For every such γ-net  the algorithm attaches the labels Y (cid:48) ≡ Y (cid:48)(γ) ∈ Y m  which are the empirical
majority-vote labels in the respective Voronoi cells in the partition V(X(γ)) = {V1  . . .   Vm}.
Formally  for i ∈ [m] 

i ∈ argmax
Y (cid:48)
y∈Y

|{j ∈ [n] | Xj ∈ Vi  Yj = y}| 
(1)
where ties are broken arbitrarily. This procedure creates a labeled set S(cid:48)
n(γ) := Sn(i(γ)  Y (cid:48)(γ)) for
every relevant γ ∈ {ρ(Xi  Xj) | i  j ∈ [n]} \ {0}. The algorithm then selects a single γ  denoted
γ∗ ≡ γ∗
n(γ∗). The scale γ∗ is selected so as to minimize a generalization error
bound  which upper bounds err(S(cid:48)
n(γ)) with high probability. This error bound  denoted Q in the
algorithm  can be derived using a compression-based analysis  as described below.

n  and outputs hS(cid:48)

3 For technical reasons  having to do with the construction in Sec. 6  we depart slightly from the standard
deﬁnition of a γ-net X ⊆ A. The classic deﬁnition requires that (i) ∀a ∈ A ∃x ∈ X : ρ(a  x) < γ and (ii)
∀x (cid:54)= x(cid:48) ∈ X : ρ(x  x(cid:48)) ≥ γ. In our deﬁnition  the relations < and ≥ in (i) and (ii) are replaced by ≤ and >.

4

Algorithm 1 KSU: 1-NN compression-based algorithm
Require: Sample Sn = (Xi  Yi)i∈[n]  conﬁdence δ
Ensure: A 1-NN classiﬁer
1: Let Γ := {ρ(Xi  Xj) | i  j ∈ [n]} \ {0}
2: for γ ∈ Γ do
3:
4:
5:
6:
7: end for
n ∈ argminγ∈Γ Q(n  α(γ)  2m(γ)  δ)  where Q is  e.g.  as in Eq. (2)
9: Find γ∗
10: Set S(cid:48)
n := S(cid:48)
11: return hS(cid:48)

Let X(γ) be a γ-net of {X1  . . .   Xn}
Let m(γ) := |X(γ)|
For each i ∈ [m(γ)]  let Y (cid:48)
Set S(cid:48)

8: Set α(γ) := (cid:99)err(hS(cid:48)

n(γ) := (X(γ)  Y (cid:48)(γ))

n(γ)  Sn)

n(γ∗
n)

n

i be the majority label in Vi(X(γ)) as deﬁned in Eq. (1)

i ∈ In m and(cid:99)err(hS(cid:48)

n is a compression scheme if there is a function C : ∪∞

We say that a mapping Sn (cid:55)→ S(cid:48)
m=0(X ×Y)m →
2X×Y  from sub-samples to subsets of X ×Y  such that for every Sn there exists an m and a sequence
n = C(Sn(i)). Given a compression scheme Sn (cid:55)→ S(cid:48)
i ∈ In m such that S(cid:48)
n and a matching function
n = C(Sn(i)) for some
C  we say that a speciﬁc S(cid:48)
n is an (α  m)-compression of a given Sn if S(cid:48)
  Sn) ≤ α. The generalization power of compression was recognized by [17]
and [22]. Speciﬁcally  it was shown in [21  Theorem 8] that if the mapping Sn (cid:55)→ S(cid:48)
n is a compression
n which is an (α  m)-compression of Sn ∼ ¯µn 
scheme  then with probability at least 1 − δ  for any S(cid:48)
we have (omitting the constants  explicitly provided therein  which do not affect our analysis)

n

err(hS(cid:48)

n

) ≤ n

n − m

α + O(

m log(n) + log(1/δ)

n − m

) + O(

n−m α log(n) + log(1/δ)

n − m

).

(2)

(cid:115) nm

Deﬁning Q(n  α  m  δ) as the RHS of Eq. (2) provides KSU with a compression bound. The following
proposition shows that KSU is a compression scheme  which enables us to use Eq. (2) with the
appropriate substitution.4
Proposition 1. The mapping Sn (cid:55)→ S(cid:48)

n deﬁned by Alg. 1 is a compression scheme whose output S(cid:48)

n

is a ((cid:99)err(hS(cid:48)

n

)  2|S(cid:48)

n|)-compression of Sn.

Proof. Deﬁne the function C by C(( ¯Xi  ¯Yi)i∈[2m]) = ( ¯Xi  ¯Yi+m)i∈[m]  and observe that for all
n = C(Sn(i(γ); j(γ)))  where i(γ) is the γ-net index set as deﬁned above  and
Sn  we have S(cid:48)
i = Yji for every i ∈ [m(γ)].
j(γ) = {j1  . . .   jm(γ)} ∈ In m(γ) is some index vector such that Y (cid:48)
Since Y (cid:48)
n of

i is an empirical majority vote  clearly such a j exists. Under this scheme  the output S(cid:48)

this algorithm is a ((cid:99)err(hS(cid:48)

n

)  2|S(cid:48)

n|)-compression.

KSU is efﬁcient  for any countable Y. Indeed  Alg. 1 has a naive runtime complexity of O(n4)  since
O(n2) values of γ are considered and a γ-net is constructed for each one in time O(n2) (see [20 
Algorithm 1]). Improved runtimes can be obtained  e.g.  using the methods in [29  18]. In this work
we focus on the Bayes-consistency of KSU  rather than optimize its computational complexity. Our
Bayes-consistency results below hold for KSU  whenever the generalization bound Q(n  α  m  δn)
satisﬁes the following properties:
Property 1 For any integer n and δ ∈ (0  1)  with probability 1 − δ over the i.i.d. random sample
n is an (α  m)-compression of Sn  then

Sn ∼ ¯µn  for all α ∈ [0  1] and m ∈ [n]: If S(cid:48)
err(hS(cid:48)

) ≤ Q(n  α  m  δ).

n

Property 2 Q is monotonically increasing in α and in m.
Property 3 There is a sequence {δn}∞
n→∞ sup
lim
α∈[0 1]

n=1  δn ∈ (0  1) such that(cid:80)∞

(Q(n  α  m  δn) − α) = 0.

n=1 δn < ∞ and for all m 

4 In [25] the analysis was based on compression with side information  and does not extend to inﬁnite Y.

5

by Eq. (2) using any convergent series(cid:80)∞

The compression bound in Eq. (2) clearly satisﬁes these properties. Note that Property 3 is satisﬁed
n=1 δn < ∞ such that δn = e−o(n); in particular  the decay

of δn cannot be too rapid.

4 Main results

n

n(γ∗

In this section we describe our main results. The proofs appear in subsequent sections. First  we show
that KSU is Bayes-consistent if the instance space has a ﬁnite doubling dimension. This contrasts
with classical 1-NN  which is only Bayes-consistent if the distribution is realizable.
Theorem 2. Let (X   ρ) be a metric space with a ﬁnite doubling-dimension. Let Q be a generalization
bound that satisﬁes Properties 1-3  and let δn be as stipulated by Property 3 for Q. If the input
n) calculated by KSU is
conﬁdence δ for input size n is set to δn  then the 1-NN classiﬁer hS(cid:48)
strongly Bayes consistent on (X   ρ): P(limn→∞ err(hS(cid:48)
) = R∗) = 1.
The proof  provided in Sec. 5  closely follows the line of reasoning in [27]  where the strong Bayes-
consistency of an adaptive margin-regularized 1-NN algorithm was proved  but with several crucial
differences. In particular  the generalization bounds used by KSU are purely compression-based  as
opposed to the Rademacher-based generalization bounds used in [27]. The former can be much tighter
in practice and guarantee Bayes-consistency of KSU even for countably many labels. This however
requires novel technical arguments  which are discussed in detail in Appendix B.1 in [26]. Moreover 
since the compression-based bounds do not explicitly depend on ddim  they can be used even when
ddim is inﬁnite  as we do in Theorem 4 below. To underscore the subtle nature of Bayes-consistency 
we note that the proof technique given here does not carry to an earlier algorithm  suggested in [20 
Theorem 4]  which also uses γ-nets. It is an open question whether the latter is Bayes-consistent.
Next  we study Bayes-consistency of KSU in inﬁnite dimensions (i.e.  with ddim = ∞) — in partic-
ular  in a setting where k-NN was shown by [9] not to be Bayes-consistent. Indeed  a straightforward
application of [9  Lemma A.1] yields the following result.
Theorem 3 (Cérou and Guyader [9]). There exists an inﬁnite dimensional separable metric space
(X   ρ) and a realizable distribution ¯µ over X × {0  1} such that no kn-NN learner satisfying
kn/n → 0 when n → ∞ is Bayes-consistent under ¯µ. In particular  this holds for any space and
realizable distribution ¯µ that satisfy the following condition: The set C of points labeled 1 by ¯µ
satisﬁes

µ(C) > 0

and

∀x ∈ C 

lim
r→0

µ(C ∩ ¯Br(x))

µ( ¯Br(x))

= 0.

(3)

Since µ(C) > 0  Eq. (3) constitutes a violation of the Besicovitch covering property. In doubling
spaces  the Besicovitch covering theorem precludes such a violation [15]. In contrast  as [35  36]
show  in inﬁnite-dimensional spaces this violation can in fact occur. Moreover  this is not an isolated
pathology  as this property is shared by Gaussian Hilbert spaces [45].
At ﬁrst sight  Eq. (3) might appear to thwart any 1-NN algorithm applied to such a distribution.
However  the following result shows that this is not the case: KSU is Bayes-consistent on a distribution
with this property.
Theorem 4. There is a metric space equipped with a realizable distribution for which KSU is weakly
Bayes-consistent  while any k-NN classiﬁer necessarily is not.

The proof relies on a classic construction of Preiss [35] which satisﬁes Eq. (3). We show that the
structure of the construction  combined with the packing and covering properties of γ-nets  imply that
the majority-vote classiﬁer induced by any γ-net with a sufﬁcienlty small γ approaches the Bayes
error. To contrast with Theorem 4  we next show that on the same construction  not all majority-vote
Voronoi partitions succeed. Indeed  if the packing property of γ-nets is relaxed  partition sequences
obstructing Bayes-consistency exist.
Theorem 5. For the example constructed in Theorem 4  there exists a sequence of Voronoi partitions
with a vanishing diameter such that the induced true majority-vote classiﬁers are not Bayes consistent.

The above result also stands in contrast to [14  Theorem 21.2]  showing that  unlike in ﬁnite dimen-
sions  the partitions’ vanishing diameter is insufﬁcient to establish consistency when ddim = ∞. We
conclude the main results by posing intriguing open problems.

6

Open problem 1. Does there exist a metric probability space on which some k-NN algorithm is
consistent while KSU is not? Does there exist any separable metric space on which KSU fails?

Open problem 2. Cérou and Guyader [9] distill a certain Besicovitch condition which is necessary
and sufﬁcient for k-NN to be Bayes-consistent in a metric space. Our Theorem 4 shows that the
Besicovitch condition is not necessary for KSU to be Bayes-consistent. Is it sufﬁcient? What is a
necessary condition?

5 Bayes-consistency of KSU in ﬁnite dimensions

In this section we give a high-level proof of Theorem 2  showing that KSU is strongly Bayes-
consistent in ﬁnite-dimensional metric spaces. A fully detailed proof is given in Appendix B in
[26].
Recall the optimal empirical error α∗
computed by KSU. As shown in Proposition 1  the sub-sample S(cid:48)
n) is an (α∗
of Sn. Abbreviate the compression-based generalization bound used in KSU by

n) and the optimal compression size m∗
n  2m∗

n ≡ m(γ∗
n) as
n)-compression

n ≡ α(γ∗

n(γ∗

Qn(α  m) := Q(n  α  2m  δn).

n(γ∗

n)) − R∗ =(cid:0)err(hS(cid:48)

To show Bayes-consistency  we start by a standard decomposition of the excess error over the optimal
Bayes into two terms:
err(hS(cid:48)
and show that each term decays to zero with probability one. For the ﬁrst term  Property 1 for Q 
together with the Borel-Cantelli lemma  readily imply lim supn→∞ TI (n) ≤ 0 with probability one.
The main challenge is showing that lim supn→∞ TII (n) ≤ 0 with probability one. We do so in
several stages:

n) − R∗(cid:1) =: TI (n) + TII (n) 

n)(cid:1) +(cid:0)Qn(α∗

n)) − Qn(α∗

n(γ∗

n  m∗

n  m∗

1. Loosely speaking  we ﬁrst show (Lemma 10) that the Bayes error R∗ can be well approxi-
mated using 1-NN classiﬁers deﬁned by the true (as opposed to empirical) majority-vote
labels over ﬁne partitions of X . In particular  this holds for any partition induced by a γ-net
of X with a sufﬁciently small γ > 0. This approximation guarantee relies on the fact that in
ﬁnite-dimensional spaces  the class of continuous functions with compact support is dense
in L1(µ) (Lemma 9).
2. Fix ˜γ > 0 sufﬁciently small such that any true majority-vote classiﬁer induced by a ˜γ-net
has a true error close to R∗  as guaranteed by stage 1. Since for bounded subsets of ﬁnite-
dimensional spaces the size of any γ-net is ﬁnite  the empirical error of any majority-vote
γ-net almost surely converges to its true majority-vote error as the sample size n → ∞. Let
n(˜γ) sufﬁciently large such that Qn(˜γ)(α(˜γ)  m(˜γ)) as computed by KSU for a sample of
size n(˜γ) is a reliable estimate for the true error of hS(cid:48)

n(˜γ)(˜γ).

3. Let ˜γ and n(˜γ) be as in stage 2. Given a sample of size n = n(˜γ)  recall that KSU
selects an optimal γ∗ such that Qn(α(γ)  m(γ)) is minimized over all γ > 0. For margins
γ (cid:28) ˜γ  which are prone to over-ﬁtting  Qn(α(γ)  m(γ)) is not a reliable estimate for
n(γ) since compression may not yet taken place for samples of size n. Nevertheless  these
hS(cid:48)
margins are discarded by KSU due to the penalty term in Q. On the other hand  for γ-nets
with margin γ (cid:29) ˜γ  which are prone to under-ﬁtting  the true error is well estimated by
n) ≈ R∗  implying
Qn(α(γ)  m(γ)). It follows that KSU selects γ∗
lim supn→∞ TII (n) ≤ 0 with probability one.

n ≈ ˜γ and Qn(α∗

n  m∗

As one can see  the assumption that X is ﬁnite-dimensional plays a major role in the proof. A simple
argument shows that the family of continuous functions with compact support is no longer dense
in L1 in inﬁnite-dimensional spaces. In addition  γ-nets of bounded subsets in inﬁnite dimensional
spaces need no longer be ﬁnite.

6 On Bayes-consistency of NN algorithms in inﬁnite dimensions

In this section we study the Bayes-consistency properties of 1-NN algorithms on a classic inﬁnite-
dimensional construction of Preiss [35]  which we describe below in detail. This construction was

7

z1:k−2

γk−1

z1:k−1

γk

γk

γk

γk

z1:k

γk

γk

z

C = Z∞

Figure 1: Preiss’s construction. Encircled is the closed ball ¯Bγk−1(z) for some z ∈ C.

(cid:80)∞

ﬁrst introduced as a concrete example showing that in inﬁnite-dimensional spaces the Besicovich
covering theorem [15] can be strongly violated  as manifested in Eq. (3).
Example 1 (Preiss’s construction). The construction (see Figure 1) deﬁnes an inﬁnite-dimensional
metric space (X   ρ) and a realizable measure ¯µ over X × Y with the binary label set Y = {0  1}.
It relies on two sequences: a sequence of natural numbers {Nk}k∈N and a sequence of positive
numbers {ak}k∈N. The two sequences should satisfy the following:

These properties are satisﬁed  for instance  by setting Nk := k! and ak := 2−k/(cid:81)

(4)
i∈[k] Ni. Let Z0
be the set of all ﬁnite sequences (z1  . . .   zk)k∈N of natural numbers such that zi ≤ Ni  and let Z∞
be the set of all inﬁnite sequences (z1  z2  . . . ) of natural numbers such that zi ≤ Ni.
Deﬁne the example space X := Z0 ∪ Z∞ and denote γk := 2−k  where γ∞ := 0. The metric ρ over
X is deﬁned as follows: for x  y ∈ X   denote by x ∧ y their longest common preﬁx. Then 

k=1 akN1 . . . Nk = 1;

limk→∞ akN1 . . . Nk+1 = ∞;

and

limk→∞ Nk = ∞.

ρ(x  y) = (γ|x∧y| − γ|x|) + (γ|x∧y| − γ|y|).

It can be shown (see [35]) that ρ(x  y) is a metric; in fact  it embeds isometrically into the square
norm metric of a Hilbert space.
To deﬁne µ  the marginal measure over X   let ν∞ be the uniform product distribution measure
over Z∞  that is: for all i ∈ N  each zi in the sequence z = (z1  z2  . . . ) ∈ Z∞ is independently
drawn from a uniform distribution over [Ni]. Let ν0 be an atomic measure on Z0 such that for all
z ∈ Z0  ν0(z) = a|z|. Clearly  the ﬁrst condition in Eq. (4) implies ν0(Z0) = 1. Deﬁne the marginal
probability measure µ over X by

∀A ⊆ Z0 ∪ Z∞  µ(A) := αν∞(A) + (1 − α)ν0(A).

In words  an inﬁnite sequence is drawn with probability α (and all such sequences are equally likely) 
or else a ﬁnite sequence is drawn (and all ﬁnite sequences of the same length are equally likely).
Deﬁne the realizable distribution ¯µ over X × Y by setting the marginal over X to µ  and by setting
the label of z ∈ Z∞ to be 1 with probability 1 and the label of z ∈ Z0 to be 0 with probability 1.
As shown in [35]  this construction satisﬁes Eq. (3) with C = Z∞ and µ(C) = α > 0. It follows
from Theorem 3 that no k-NN algorithm is Bayes-consistent on it. In contrast  the following theorem
shows that KSU is weakly Bayes-consistent on this distribution. Theorem 4 immediately follows
from the this result.
Theorem 6. Assume (X   ρ) Y and ¯µ as in Example 1. KSU is weakly Bayes-consistent on ¯µ.
The proof  provided in Appendix C in [26]  ﬁrst characterizes the Voronoi cells for which the true
majority-vote yields a signiﬁcant error for the cell (Lemma 15). In ﬁnite-dimensional spaces  the total
measure of all such “bad” cells can be made arbitrarily close to zero by taking γ to be sufﬁciently
small  as shown in Lemma 10 of Theorem 2. However  it is not immediately clear whether this can
be achieved for the inﬁnite dimensional construction above.
Indeed  we expect such bad cells  due to the unintuitive property that for any x ∈ C  we have
µ( ¯Bγ(x) ∩ C)/µ( ¯Bγ(x)) → 0 when γ → 0  and yet µ(C) > 0. Thus  if for example a signiﬁcant

8

portion of the set C (whose label is 1) is covered by Voronoi cells of the form V = ¯Bγ(x) with
x ∈ C  then for all sufﬁciently small γ  each one of these cells will have a true majority-vote 0. Thus
a signiﬁcant portion of C would be misclassiﬁed. However  we show that by the structure of the
construction  combined with the packing and covering properties of γ-nets  we have that in any γ-net 
the total measure of all these “bad” cells goes to 0 when γ → 0  thus yielding a consistent classiﬁer.
Lastly  the following theorem shows that on the same construction above  when the Voronoi partitions
are allowed to violate the packing property of γ-nets  Bayes-consistency does not necessarily hold.
Theorem 5 immediately follows from the following result.
Theorem 7. Assume (X   ρ)  Y and ¯µ as in Example 1. There exists a sequence of Voronoi partitions
(Pk)k∈N of X with maxV ∈Pk diam(V ) ≤ γk such that the sequence of true majority-vote classiﬁers
(hPk )k∈N induced by these partitions is not Bayes consistent: lim inf k→∞ err(hPk ) = α > 0.
The proof  provided in Appendix D  constructs a sequence of Voronoi partitions  where each partition
Pk has all of its impure Voronoi cells (those with both 0 and 1 labels) being bad. In this case  C is
incorrectly classiﬁed by hPk  yielding a signiﬁcant error. Thus  in inﬁnite-dimensional metric spaces 
the shape of the Voronoi cells plays a fundamental role in the consistency of the partition.

Acknowledgments. We thank Frédéric Cérou for the numerous fruitful discussions and helpful
feedback on an earlier draft. Aryeh Kontorovich was supported in part by the Israel Science
Foundation (grant No. 755/15)  Paypal and IBM. Sivan Sabato was supported in part by the Israel
Science Foundation (grant No. 555/15).

References
[1] Christophe Abraham  Gérard Biau  and Benoît Cadre. On the kernel rule for function classiﬁca-

tion. Ann. Inst. Statist. Math.  58(3):619–633  2006.

[2] Daniel Berend and Aryeh Kontorovich. The missing mass problem. Statistics & Probability

Letters  82(6):1102–1110  2012.

[3] Daniel Berend and Aryeh Kontorovich. On the concentration of the missing mass. Electronic

Communications in Probability  18(3):1–7  2013.

[4] Alina Beygelzimer  Sham Kakade  and John Langford. Cover trees for nearest neighbor. In
ICML ’06: Proceedings of the 23rd international conference on Machine learning  pages
97–104  New York  NY  USA  2006. ACM.

[5] Gérard Biau  Florentina Bunea  and Marten H. Wegkamp. Functional classiﬁcation in Hilbert

spaces. IEEE Trans. Inform. Theory  51(6):2163–2172  2005.

[6] Gérard Biau  Frédéric Cérou  and Arnaud Guyader. Rates of convergence of the functional

k-nearest neighbor estimate. IEEE Trans. Inform. Theory  56(4):2034–2040  2010.

[7] V. I. Bogachev. Measure theory. Vol. I  II. Springer-Verlag  Berlin  2007.
[8] Oren Boiman  Eli Shechtman  and Michal Irani. In defense of nearest-neighbor based image

classiﬁcation. In CVPR  2008.

[9] Frédéric Cérou and Arnaud Guyader. Nearest neighbor classiﬁcation in inﬁnite dimension.

ESAIM: Probability and Statistics  10:340–355  2006.

[10] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classiﬁ-

cation. In NIPS  2014.

[11] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions

on Information Theory  13:21–27  1967.

[12] Luc Devroye. On the inequality of Cover and Hart in nearest neighbor discrimination. IEEE

Trans. Pattern Anal. Mach. Intell.  3(1):75–78  1981.

[13] Luc Devroye and László Györﬁ. Nonparametric density estimation: the L1 view. Wiley Series
in Probability and Mathematical Statistics: Tracts on Probability and Statistics. John Wiley &
Sons  Inc.  New York  1985.

9

[14] Luc Devroye  László Györﬁ  and Gábor Lugosi. A probabilistic theory of pattern recognition 

volume 31. Springer Science & Business Media  2013.

[15] Herbert Federer. Geometric measure theory. Die Grundlehren der mathematischen Wis-

senschaften  Band 153. Springer-Verlag New York Inc.  New York  1969.

[16] Evelyn Fix and Jr. Hodges  J. L. Discriminatory analysis. nonparametric discrimination:
Consistency properties. International Statistical Review / Revue Internationale de Statistique 
57(3):pp. 238–247  1989.

[17] Sally Floyd and Manfred Warmuth. Sample compression  learnability  and the Vapnik-

Chervonenkis dimension. Machine learning  21(3):269–304  1995.

[18] Lee-Ad Gottlieb  Aryeh Kontorovich  and Robert Krauthgamer. Efﬁcient classiﬁcation for metric
data (extended abstract COLT 2010). IEEE Transactions on Information Theory  60(9):5750–
5759  2014.

[19] Lee-Ad Gottlieb  Aryeh Kontorovich  and Robert Krauthgamer. Adaptive metric dimensionality

reduction. Theoretical Computer Science  620:105–118  2016.

[20] Lee-Ad Gottlieb  Aryeh Kontorovich  and Pinhas Nisnevitch. Near-optimal sample compression

for nearest neighbors. In Neural Information Processing Systems (NIPS)  2014.

[21] Lee-Ad Gottlieb  Aryeh Kontorovich  and Pinhas Nisnevitch. Nearly optimal classiﬁcation for
semimetrics (extended abstract AISTATS 2016). Journal of Machine Learning Research  2017.

[22] Thore Graepel  Ralf Herbrich  and John Shawe-Taylor. PAC-Bayesian compression bounds on
the prediction error of learning algorithms for classiﬁcation. Machine Learning  59(1):55–76 
2005.

[23] Peter Hall and Kee-Hoon Kang. Bandwidth choice for nonparametric classiﬁcation. Ann.

Statist.  33(1):284–306  02 2005.

[24] Olav Kallenberg. Foundations of modern probability. Second edition. Probability and its

Applications. Springer-Verlag  2002.

[25] Aryeh Kontorovich  Sivan Sabato  and Ruth Urner. Active nearest-neighbor learning in metric

spaces. In Advances in Neural Information Processing Systems  pages 856–864  2016.

[26] Aryeh Kontorovich  Sivan Sabato  and Roi Weiss. Nearest-neighbor sample compression:

Efﬁciency  consistency  inﬁnite dimensions. CoRR  abs/1705.08184  2017.

[27] Aryeh Kontorovich and Roi Weiss. A Bayes consistent 1-NN classiﬁer. In Artiﬁcial Intelligence

and Statistics (AISTATS 2015)  2014.

[28] Aryeh Kontorovich and Roi Weiss. Maximum margin multiclass nearest neighbors. In Interna-

tional Conference on Machine Learning (ICML 2014)  2014.

[29] Robert Krauthgamer and James R. Lee. Navigating nets: Simple algorithms for proximity
search. In 15th Annual ACM-SIAM Symposium on Discrete Algorithms  pages 791–801  January
2004.

[30] Sanjeev R. Kulkarni and Steven E. Posner. Rates of convergence of nearest neighbor estimation

under arbitrary sampling. IEEE Trans. Inform. Theory  41(4):1028–1039  1995.

[31] Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. unpub-

lished  1986.

[32] James R. Munkres. Topology: a ﬁrst course. Prentice-Hall  Inc.  Englewood Cliffs  N.J.  1975.

[33] Vladimir Pestov. On the geometry of similarity search: dimensionality curse and concentration

of measure. Inform. Process. Lett.  73(1-2):47–51  2000.

[34] Vladimir Pestov. Is the k-NN classiﬁer in high dimensions affected by the curse of dimensional-

ity? Comput. Math. Appl.  65(10):1427–1437  2013.

10

[35] David Preiss. Invalid Vitali theorems. Abstracta. 7th Winter School on Abstract Analysis  pages

58–60  1979.

[36] David Preiss. Gaussian measures and the density theorem. Comment. Math. Univ. Carolin. 

22(1):181–193  1981.

[37] Demetri Psaltis  Robert R. Snapp  and Santosh S. Venkatesh. On the ﬁnite sample performance
of the nearest neighbor classiﬁer. IEEE Transactions on Information Theory  40(3):820–837 
1994.

[38] Walter Rudin. Principles of mathematical analysis. McGraw-Hill Book Co.  New York  third

edition  1976. International Series in Pure and Applied Mathematics.

[39] Walter Rudin. Real and Complex Analysis. McGraw-Hill  1987.

[40] Richard J. Samworth. Optimal weighted nearest neighbour classiﬁers. Ann. Statist.  40(5):2733–

2763  10 2012.

[41] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  2014.

[42] John Shawe-Taylor  Peter L. Bartlett  Robert C. Williamson  and Martin Anthony. Structural
risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory 
44(5):1926–1940  1998.

[43] Robert R. Snapp and Santosh S. Venkatesh. Asymptotic expansions of the k nearest neighbor

risk. Ann. Statist.  26(3):850–878  1998.

[44] Charles J. Stone. Consistent nonparametric regression. The Annals of Statistics  5(4):595–620 

1977.

[45] Jaroslav Tišer. Vitali covering theorem in Hilbert space. Trans. Amer. Math. Soc.  355(8):3277–

3289  2003.

[46] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest

neighbor classiﬁcation. Journal of Machine Learning Research  10:207–244  2009.

[47] Lin Cheng Zhao. Exponential bounds of mean error for the nearest neighbor estimates of

regression functions. J. Multivariate Anal.  21(1):168–178  1987.

11

,Elad Hazan
Kfir Levy
Shai Shalev-Shwartz
Aryeh Kontorovich
Sivan Sabato
Roi Weiss