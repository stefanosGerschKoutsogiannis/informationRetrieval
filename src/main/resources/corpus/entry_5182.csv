2019,Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks,Spiking neural networks (SNNs) well support spatiotemporal learning and energy-efficient event-driven hardware neuromorphic processors. As an important class of SNNs   recurrent spiking neural networks (RSNNs) possess great computational power. However  the practical application of RSNNs is severely limited by challenges in training.  Biologically-inspired unsupervised learning has limited capability in boosting the performance of RSNNs. On the other hand   existing backpropagation (BP) methods suffer from high complexity of unrolling in time  vanishing and exploding gradients  and approximate differentiation of discontinuous spiking activities when applied to RSNNs.  To enable supervised training of RSNNs under a well-defined loss function   we present a novel Spike-Train level RSNNs Backpropagation (ST-RSBP) algorithm for training deep RSNNs. The proposed ST-RSBP directly computes the gradient of a rated-coded loss function defined at the output layer of the network w.r.t tunable parameters. The scalability of ST-RSBP is achieved by the proposed spike-train level computation during which temporal effects of the SNN is captured in both the forward and backward pass of BP. Our ST-RSBP algorithm can be broadly applied to RSNNs with a single recurrent layer or deep RSNNs with multiple feed-forward and recurrent layers.  Based upon challenging speech and image datasets including TI46  N-TIDIGITS  Fashion-MNIST and MNIST   ST-RSBP is able to train RSNNs with an accuracy surpassing that of the current state-of-art SNN BP algorithms and conventional non-spiking deep learning models.,Spike-Train Level Backpropagation for Training

Deep Recurrent Spiking Neural Networks

University of California  Santa Barbara

University of California  Santa Barbara

Wenrui Zhang

Santa Barbara  CA 93106
wenruizhang@ucsb.edu

Peng Li

Santa Barbara  CA 93106

lip@ucsb.edu

Abstract

Spiking neural networks (SNNs) well support spatio-temporal learning and energy-
efﬁcient event-driven hardware neuromorphic processors. As an important class
of SNNs  recurrent spiking neural networks (RSNNs) possess great computa-
tional power. However  the practical application of RSNNs is severely limited by
challenges in training. Biologically-inspired unsupervised learning has limited
capability in boosting the performance of RSNNs. On the other hand  existing
backpropagation (BP) methods suffer from high complexity of unfolding in time 
vanishing and exploding gradients  and approximate differentiation of discontinu-
ous spiking activities when applied to RSNNs. To enable supervised training of
RSNNs under a well-deﬁned loss function  we present a novel Spike-Train level
RSNNs Backpropagation (ST-RSBP) algorithm for training deep RSNNs. The
proposed ST-RSBP directly computes the gradient of a rate-coded loss function
deﬁned at the output layer of the network w.r.t tunable parameters. The scalability
of ST-RSBP is achieved by the proposed spike-train level computation during
which temporal effects of the SNN is captured in both the forward and backward
pass of BP. Our ST-RSBP algorithm can be broadly applied to RSNNs with a
single recurrent layer or deep RSNNs with multiple feedforward and recurrent
layers. Based upon challenging speech and image datasets including TI46 [25] 
N-TIDIGITS [3]  Fashion-MNIST [40] and MNIST  ST-RSBP is able to train
SNNs with an accuracy surpassing that of the current state-of-the-art SNN BP
algorithms and conventional non-spiking deep learning models.

1

Introduction

In recent years  deep neural networks (DNNs) have demonstrated outstanding performance in natural
language processing  speech recognition  visual object recognition  object detection  and many other
domains [6  14  21  36  13]. On the other hand  it is believed that biological brains operate rather
differently [17]. Neurons in artiﬁcial neural networks (ANNs) are characterized by a single  static  and
continuous-valued activation function. More biologically plausible spiking neural networks (SNNs)
compute based upon discrete spike events and spatio-temporal patterns while enjoying rich coding
mechanisms including rate and temporal codes [11]. There is theoretical evidence supporting that
SNNs possess greater computational power over traditional ANNs [11]. Moreover  the event-driven
nature of SNNs enables ultra-low-power hardware neuromorphic computing devices [7  2  10  28].
Backpropagation (BP) is the workhorse for training deep ANNs [22]. Its success in the ANN world
has made BP a target of intensive research for SNNs. Nevertheless  applying BP to biologically
more plausible SNNs is nontrivial due to the necessity in dealing with complex neural dynamics
and non-differentiability of discrete spike events. It is possible to train an ANN and then convert
it to an SNN [9  10  16]. However  this suffers from conversion approximations and gives up the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

opportunity in exploring SNNs’ temporal learning capability. One of the earliest attempts to bridge
the gap between discontinuity of SNNs and BP is the SpikeProp algorithm [5]. However  SpikeProp
is restricted to single-spike learning and has not yet been successful in solving real-world tasks.
Recently  training SNNs using BP under a ﬁring rate (or activity level) coded loss function has been
shown to deliver competitive performances [23  39  4  33]. Nevertheless  [23] does not consider the
temporal correlations of neural activities and deals with spiking discontinuities by treating them as
noise. [33] gets around the non-differentiability of spike events by approximating the spiking process
via a probability density function of spike state change. [39]  [4]  and [15] capture the temporal effects
by performing backpropagation through time (BPTT) [37]. Among these  [15] adopts a smoothed
spiking threshold and a continuous differentiable synaptic model for gradient computation  which
is not applicable to widely used spiking neuron models such as the leaky integrate-and-ﬁre (LIF)
model. Similar to [23]  [39] and [4] compute the error gradient based on the continuous membrane
waveforms resulted from smoothing out all spikes. In these approaches  computing the error gradient
by smoothing the microscopic membrane waveforms may lose the sight of the all-or-none ﬁring
characteristics of the SNN that deﬁnes the higher-level loss function and lead to inconsistency between
the computed gradient and target loss  potentially degrading training performance [19].
Most existing SNN training algorithms including the aforementioned BP works focus on feedforward
networks. Recurrent spiking neural networks (RSNNs)  which are an important class of SNNs and
are especially competent for processing temporal signals such as time series or speech data [12] 
deserve equal attention. The liquid State Machine (LSM) [27] is a special RSNN which has a single
recurrent reservoir layer followed by one readout layer. To mitigate training challenges  the reservoir
weights are either ﬁxed or trained by unsupervised learning like spike-timing-dependent plasticity
(STDP) [29] with only the readout layer trained by supervision [31  41  18]. The inability in training
the entire network with supervision and its architectural constraints  e.g. only admitting one reservoir
and one readout  limit the performance of LSM. [4] proposes an architecture called long short-term
memory SNNs (LSNNs) and trains it using BPTT with the aforementioned issue on approximate
gradient computation. When dealing with training of general RSNNs  in addition to the difﬁculties
encountered in feedforward SNNs  one has to cope with added challenges incurred by recurrent
connections and potential vanishing/exploding gradients.
This work is motivated by: 1) lack of powerful supervised training of general RSNNs  and 2) an
immediate outcome of 1)  i.e. the existing SNN research has limited scope in exploring sophisticated
learning architectures like deep RSNNs with multiple feedforward and recurrent layers hybridized
together. As a ﬁrst step towards addressing these challenges  we propose a novel biologically non-
plausible Spike-Train level RSNNs Backpropagation (ST-RSBP) algorithm which is applicable to
RSNNs with an arbitrary network topology and achieves the state-of-the-art performances on several
widely used datasets. The proposed ST-RSBP employs spike-train level computation similar to what
is adopted in the recent hybrid macro/micro level BP (HM2-BP) method for feedforward SNNs [19] 
which demonstrates encouraging performances and outperforms BPTT such as the one implemented
in [39].
ST-RSBP is rigorously derived and can handle arbitrary recurrent connections in various RSNNs.
While capturing the temporal behavior of the RSNN at the spike-train level  ST-RSBP directly
computes the gradient of a rate-coded loss function w.r.t tunable parameters without incurring
approximations resulted from altering and smoothing the underlying spiking behaviors. ST-RSBP
is able to train RSNNs without costly unfolding the network through time and performing BP time
point by time point  offering faster training and avoiding vanishing/exploding gradients for general
RSNNs. Moreover  as mentioned in Section 2.2.1 and 2.3 of the Supplementary Materials  since
ST-RSBP more precisely computes error gradients than HM2-BP [19]  it can achieve better results
than HM2-BP even on the feedforward SNNs.
We apply ST-RSBP to train several deep RSNNs with multiple feedforward and recurrent layers
to demonstrate the best performances on several widely adopted datasets. Based upon challenging
speech and image datasets including TI46 [25]  N-TIDIGITS [3] and Fashion-MNIST [40]  ST-RSBP
trains RSNNs with an accuracy noticeably surpassing that of the current state-of-the-art SNN BP
algorithms and conventional non-spiking deep learning models and algorithms. Furthermore  ST-
RSBP is also evaluated on feedforward spiking convolutional neural networks (spiking CNNs) with
the MNIST dataset and achieves 99.62% accuracy  which is the best among all SNN BP rules.

2

2 Background

2.1 SNN Architectures and Training Challenges

Fig. 1A shows two SNN architectures often explored in neuroscience: single layer (top) and liquid
state machine (bottom) networks for which different mechanisms have been adopted for training.
However  typically spike timing dependent plasticity (STDP) [29] and winner-take-all (WTA) [8] are
only for unsupervised training and have limited performance. WTA and other supervised learning
rules [31  41  18] can only be applied to the output layer  obstructing adoption of more sophisticated
deep architectures.

Figure 1: Various SNN networks: (A) one layer SNNs and liquid state machine; (B)
multi-layer feedforward SNNs; (C) deep hybrid feedforward/recurrent SNNs.

While bio-inspired learning mechanisms are yet to demonstrate competitive performance for chal-
lenging real-life tasks  there has been much recent effort aiming at improving SNN performance
with supervised BP. Most existing SNN BP methods are only applicable to multi-layer feedfor-
ward networks as shown in Fig. 1B. Several such methods have demonstrated promising results
[23  39  19  33]. Nevertheless  these methods are not applicable to complex deep RSNNs such as the
hybrid feedforward/recurrent networks shown in Fig. 1C  which are the target of this work. Backprop-
agation through time (BPTT) in principle may be applied to training RSNNs [4]  but bottlenecked
with several challenges in: (1) unfolding the recurrent connections through time  (2) back propagating
errors over both time and space  and (3) back propagating errors over non-differentiable spike events.

Figure 2: Backpropagation in recurrent SNNs: BPTT vs. ST-RSBP.

Fig. 2 compares BPTT and ST-RSBP  where we focus on a recurrent layer since feedforward layer
can be viewed as a simpliﬁed recurrent layer. To apply BPTT  one shall ﬁrst unfold a RSNN in time

3

ReservoirInput LayerOutput LayerSTDP/No TrainingWinner Take All/ Single layer supervised learningInput LayerOutput LayerABInput LayerHidden LayerHidden LayerHidden LayerOutput LayerBackpropagationInput LayerHidden LayerHidden LayerHidden LayerOutput LayerCRecurrentLayerRecurrentLayerHidden LayerInput LayerRecurrentLayeroutput Layerhidden Layerhidden LayerBPTTFiringMembrane potentialBackpropagation (BP) by approximating differentiation of discontinuous spiking activities.jieijwijS-PSPBackpropagation over spike-train}Step1: Unfold the entire recurrent layer through time into a feedforward networkStep2: Backpropagate the errors across the whole unfolded network in time with a suﬃciently small time stept=1t=2t=3inoutST-RSBPinoutin[t1]in[t2]in[t3]out[t2]out[t1]out[t3]to convert it into a larger feedforward network without recurrent connections. The total number of
layers in the feedforward network is increased by a factor equal to the number of times the RSNN
is unfolded  and hence can be very large. Then  this unfolded network is integrated in time with
a sufﬁciently small time step to capture dynamics of the spiking behavior. BP is then performed
spatio-temproally layer-by-layer across the unfolded network based on the same time stepsize used
for integration as shown in Fig. 2. In contrast  the proposed ST-RSBP does not convert the RSNN
into a larger feedforward SNN. The forward pass of BP is based on time-domain integration of the
RSNN of the original size. Following the forward pass  importantly  the backward pass of BP is not
conducted point by point in time  but instead  much more efﬁciently on the spike-train level. We
make use of Spike-train Level Post-synaptic Potentials (S-PSPs) discussed in Section 2.2 to capture
temporal interactions between any pair of pre/post-synaptic neurons. ST-RSBP is more scalable and
has the added beneﬁts of avoiding exploding/vanishing gradients for general RSNNs.

2.2 Spike-train Level Post-synaptic Potential (S-PSP)

S-PSP captures the spike-train level interactions between a pair of pre/post-synaptic neurons. Note that
each neuron ﬁres whenever its post-synaptic potential reaches the ﬁring threshold. The accumulated
contributions of the pre-synaptic neuron j’s spike train to the (normalized) post-synaptic potential of
the neuron i right before all the neuron i’s ﬁring times is deﬁned as the (normalized) S-PSP from the
neuron j to the neuron i as in (6) in the Supplementary Materials. The S-PSP eij characterizes the
aggregated effect of the spike train of the neuron j on the membrane potential of the neuron i and its
ﬁring activities. S-PSPs allow consideration of the temporal dynamics and recurrent connections of
an RSNN across all ﬁring events at the spike-train level without expensive unfolding through time
and backpropagation time point by time point.
The sum of the weighted S-PSPs from all pre-synaptic neurons of the neuron i is deﬁned as the total
post-synaptic potential (T-PSP) ai. ai is the post-synaptic membrane potential accumulated right
before all ﬁring times and relates to the ﬁring count oi via the ﬁring threshold ν [19]:

ai =

wij eij 

oi = g(ai) ≈

ai
ν

.

(1)

(cid:88)

j

ai and oi are analogous to the pre-activation and activation in the traditional ANNs  respectively  and
g(·) can be considered as an activation function converting the T-PSP to the output ﬁring count.
A detailed description of S-PSP and T-PSP can be found in Section 1 in the Supplementary Materials.

3 Proposed Spike-Train level Recurrent SNNs Backpropagation (ST-RSBP)

Nk(cid:88)

Nk+1(cid:88)

We use the generic recurrent spiking neural network with a combination of feedforward and recurrent
layers of Fig. 2 to derive ST-RSBP. For the spike-train level activation of each neuron l in the layer
k + 1  (1) is modiﬁed to include the recurrent connections explicitly if necessary:

ak+1
l =

wk+1

lj

ek+1
lj +

j=1

p=1

wk+1

lp

ek+1
lp

 

ok+1
l = g(ak+1

l

ak+1
l
νk+1 .

) ≈

(2)

lp

are the corresponding S-PSPs  νk+1 is the ﬁring threshold at the layer k + 1  ok+1

Nk+1 and Nk are the number of neurons in the layers k + 1 and k  wk+1
is the feedforward weight
lj
from the neuron j in the layer k to the neuron l in the layer k + 1  wk+1
is the recurrent weight from
the neuron p to the neuron l in the layer k + 1  which is non-existent if the layer k + 1 is feedforward 
and ek+1
and
ek+1
lj
are the ﬁring count and pre-activation (T-PSP) of the neuron l at the layer k + 1  respectively.
ak+1
l
The rate-coded loss is deﬁned at the output layer as:
1
2||o − y||2

(3)
where y  o and a are vectors of the desired output neuron ﬁring counts (labels) and actual ﬁring
counts  and the T-PSPs of the output neurons  respectively. Differentiating (3) with respect to each
trainable weight wk

ij incident upon the layer k leads to:

1
2||

a
ν − y||2
2 

E =

2 =

lp

l

∂E
∂wk
ij

=

∂E
∂ak
i

∂ak
i
∂wk
ij

= δk
i

∂ak
i
∂wk
ij

with δk

i =

∂E
∂ak
i

 

(4)

4

i and ∂ak

where δk
respectively  for the neuron i. ST-RSBP updates wk

are referred to as the back propagated error and differentiation of activation 
  where η is a learning rate.

i
∂wk
ij

ij by ∆wk

ij = η ∂E
∂wk
ij

We outline the key component of derivation of ST-RSBP: the back propagated errors. The full
derivation of ST-RSBP is presented in Section 2 of the Supplementary Materials.

3.1 Outline of the Derivation of Back Propagated Errors

3.1.1 Output Layer

If the layer k is the output  the back propagated error of the neuron i is given by differentiating (3):
(5)

=

∂E
∂ak
i

i − yk
(ok
i )
νk

δk
i =
i the desired ﬁring count (label)  and ak

 

i the T-PSP.

where ok

i is the actual ﬁring count  yk

3.1.2 Hidden Layers

Nk+1(cid:88)

l=1

At each hidden layer k  the chain rule is applied to determine the error δi for the neuron i:

δk
i =

∂E
∂ak
i

=

∂E

∂ak+1

l

l

∂ak+1
∂ak
i

=

δk+1
l

l

∂ak+1
∂ak
i

.

(6)

Nk

1  ···   δk

]  and
]  respectively. Assuming δk+1 is given  which is the case for the output layer  the

Deﬁne two error vectors δk+1 and δk for the layers k + 1 and k : δk+1 = [δk+1
δk = [δk
goal is to back propagate from δk+1 to δk. This entails to compute ∂ak+1
∂ak
i
[Backpropagation from a Hidden Recurrent Layer] Now consider the case that the errors are
back propagated from a recurrent layer k + 1 to its preceding layer k. Note that the S-PSP elj from
any pre-synaptic neuron j to a post-synpatic neuron l is a function of both the rate and temporal
information of the pre/post-synaptic spike trains  which can be made explicitly via some function f:
(7)

 ···   δk+1

elj = f (oj  ol  t(f )

in (6).

  t(f )

Nk+1

) 

1

l

j

l

are the pre/post-synaptic ﬁring counts and ﬁring times  respectively.

Nk+1(cid:88)

l=1

Nk(cid:88)

j

  t(f )

where oj  ol  t(f )
Now based on (2)  ∂ak+1
∂ak
i

j

l

l

is split also into two summations:

l

∂ak+1
∂ak
i

=

wk+1

lj

dek+1
lj
dak
i

+

wk+1

lp

dek+1
lp
dak
i

 

(8)

Nk+1(cid:88)

p

 1

νk

1

where the ﬁrst summation sums over all pre-
synaptic neurons in the previous layer k while
the second sums over the pre-synaptic neurons
in the current recurrent layer as illustrated in
Fig. 3.
On the right side of (8)  dek+1
lj
dak
i

is given by:
∂ek+1
∂ok+1

∂ak+1
∂ak
i

lj

l

l

j = i

dek+1
lj
dak
i

=

∂ek+1
li
∂ok
i
∂ek+1
∂ok+1

lj

+ 1

νk+1
∂ak+1
∂ak
i

l

l

νk+1

j (cid:54)= i.
(9)
νk and νk+1 are the ﬁring threshold voltages for
the layers k and k + 1  respectively  and we have
used that ok
/νk+1
from (1). Importantly  the last term on the right
’s dependency on
side of (9) exists due to ek+1
the post-synaptic ﬁring rate ok+1
per (7) and

i /νk and ok+1

≈ ak+1

i ≈ ak

lj

l

l

l

5

Figure 3: Connections for a recurrent layer
neuron and the dependencies among its S-
PSPs.

kk+1Summation over all neurons in the previous layer kSummation over all connected neurons in the recurrent layerilDependencies of S-PSP:pjek+1lpokiok+1lok+1plek+1ljok+1lokj’s further dependency on the pre-synaptic activation ok

i (hence pre-activation ak

i )  as shown in

ok+1
l
Fig. 3.
On the right side of (8)  dek+1
lp
dak
i
dek+1
lp
dak
i

is due to the recurrent connections within the layer k + 1:

=

1

νk+1

lp

∂ek+1
∂ok+1

l

l

∂ak+1
∂ak
i

+

1

νk+1

lp

∂ek+1
∂ok+1

p

∂ak+1
p
∂ak
i

.

(10)

l

per (7) and ok+1

The ﬁrst term on the right side of (10) is due to ek+1
’s further dependence on the pre-synaptic activation ok
ok+1
l
i ). Per (7)  it is important to note that the second term exists because ek+1
ak
pre-synaptic ﬁring rate ok+1
Fig. 3.
Putting (8)  (9)  and (10) together leads to:

’s dependency on the post-synaptic ﬁring rate
i (hence pre-activation
’s dependency on the
i )  as shown in

i (hence pre-activation ak

  which further depends on ok

lp

lp

p

1 −

1

νk+1

 Nk(cid:88)

j

= wk+1

li

1
νk

∂ek+1
li
∂ok
i

+

Nk+1(cid:88)

wk+1

lj

lj

∂ek+1
∂ok+1

l

+

Nk+1(cid:88)

p

p

1

νk+1

wk+1

lp

wk+1

lp

lp

∂ek+1
∂ok+1

l

lp

∂ek+1
∂ok+1

p

∂ak+1
p
∂ak
i

.

 ∂ak+1

l

∂ak
i

(11)

l

It is evident that all Nk+1× Nk partial derivatives involving the recurrent layer k + 1 and its preceding
layer k  i.e. ∂ak+1
  l = [1  Nk+1]  i = [1  Nk]  form a coupled linear system via (11)  which is written
∂ak
i
in a matrix form as:
(12)
where P k+1 k ∈ RNk+1×Nk contains all the desired partial derivatives  Ωk+1 k ∈ RNk+1×Nk+1 is
diagonal  Θk+1 k ∈ RNk+1×Nk+1  Φk+1 k ∈ RNk+1×Nk  and the detailed deﬁnitions of all these
matrices can be found in Section 2.1 of the Supplementary Materials.
Solving the linear system in (12) gives all ∂ak+1
∂ak
j

Ωk+1 k · P k+1 k = Φk+1 k + Θk+1 k · P k+1 k 

P k+1 k = (Ωk+1 k − Θk+1 k)−1 · Φk+1 k.

(13)
Note that since Ω is a diagonal matrix  the cost in factoring the above linear system can be reduced by
approximating the matrix inversion using a ﬁrst-order Taylor’s expansion without matrix factorization.
Error propagation from the layer k + 1 to layer k of (6) is cast in the matrix form: δk = P T · δk+1.
[Backpropagation from a Hidden Feedforward Layer] The much simpler case of backpropagating
errors from a feedforward layer k + 1 to its preceding layer k is described in Section 2.1 of the
Supplementary Materials.
The complete ST-RSBP algorithm is summarized in Section 2.4 in the Supplementary Materials.

:

i

4 Experiments and Results

4.1 Experimental Settings

All reported experiments below are conducted on an NVIDIA Titan XP GPU. The experimented
SNNs are based on the LIF model and weights are randomly initialized by following the uniform
distribution U [−1  1]. Fixed ﬁring thresholds are used in the range of 5mV to 20mV depending
on the layer. Exponential weight regularization [23]  lateral inhibition in the output layer [23]
and Adam [20] as the optimizer are adopted. The parameters like the desired output ﬁring counts 
thresholds and learning rates are empirically tuned. Table 1 lists the typical constant values adopted
in the proposed ST-RSBP learning rule in our experiments. The simulation step size is set to 1 ms.
The batch size is 1 which means ST-RSBP is applied after each training sample to update the weights.
Using three speech datasets and two image dataset  we compare the proposed ST-RSBP with several
other methods which either have the best previously reported results on the same datasets or represent
the current state-of-the-art performances for training SNNs. Among these  HM2-BP [19] is the
best reported BP algorithm for feedforward SNNs based on LIF neurons. ST-RSBP is evaluated

6

Table 1: Parameters settings

Parameter
Time Constant of Membrane Voltage τm
Time Constant of Synapse τs
Refractory Period
Desired Firing Count for Target Neuron
Desired Firing Count for Non-Target Neuron

Value Parameter
64 ms Threshold ν
8 ms
2 ms
35
5

Synaptic Time Delay
Reset Membrane Voltage Vreset
Learning Rate η
Batch Size

Value
10 mV
1 ms
0 mV
0.001

1

using RSNNs of multiple feedforward and recurrent layers with full connections between adjacent
layers and sparse connections inside the recurrent layers. The network models of all other BP
methods we compare with are fully connected feedforward networks. The liquid state machine (LSM)
networks demonstrated below have sparse input connections  sparse reservoir connections  and a fully
connected readout layer. Since HM2-BP cannot train recurrent networks  we compare ST-RSBP with
HM2-BP using models of a similar number of tunable weights. Moreover  we also demonstrate that
ST-RSBP achieves the best performance among several state-of-the-art SNN BP rules evaluated on
the same or similar spiking CNNs. Each experiment reported below is repeated ﬁve times to obtain
the mean and standard deviation (stddev) of the accuracy.

4.2 TI46-Alpha Speech Dataset

TI46-Alpha is the full alphabets subset of the TI46 Speech corpus [25] and contains spoken English
alphabets from 16 speakers. There are 4 142 and 6 628 spoken English examples in 26 classes for
training and testing  respectively. The continuous temporal speech waveforms are ﬁrst preprocessed
by the Lyon’s ear model [26] and then encoded into 78 spike trains using the BSA algorithm [32].

Table 2: Comparison of different SNN models on TI46-Alpha

Algorithm
HM2-BP [19]
HM2-BP [19]
HM2-BP [19]
Non-spiking BPb [38] LSM: R2000
91.57% 0.20% 91.85%
ST-RSBP (this work)
93.06% 0.21% 93.35%
ST-RSBP (this work)
a We show the number of neurons in each feedforward/recurrent hidden layer. R represent recurrent layer.
b An LSM model. The state vector of the reservoir is used to train the single readout layer using BP.

89.36% 0.30% 89.92%
89.83% 0.71% 90.60%
90.50% 0.45% 90.98%

# Epochs Mean
138
163
174

Stddev Best

78%

Hidden Layersa
800
400-400
800-800

R800
400-R400-400

# Params
83 200
201 600
723 200
52 000
86 280
363 313

75
57

Table 2 compares ST-RSBP with several other algorithms on TI46-Alpha. The result from [38] shows
that only training the single readout layer of a recurrent LSM is inadequate for this challenging
task  demonstrating the necessity of training all layers of a recurrent network using techniques such
as ST-RSBP. ST-RSBP outperforms all other methods. In particular  ST-RSBP is able to train a
three-hidden-layer RSNN with 363 313 weights to increase the accuracy from 90.98% to 93.35%
when compared with the feedforward SNN with 723 200 weights trained by HM2-BP.

4.3 TI46-Digits Speech Datasest

TI46-Digits is the full digits subset of the TI46 Speech corpus [25]. It contains 1 594 training
examples and 2 542 testing examples of 10 utterances for each of digits "0" to "9" spoken by 16
different speakers. The same preprocessing used for TI46-Alpha is adopted. Table 3 shows that
the proposed ST-RSBP delivers a high accuracy of 99.39% while outperforming all other methods
including HM2-BP. On recurrent network training  ST-RSBP produces large improvements over two
other methods. For instance  with 19 057 tunable weights  ST-RSBP delivers an accuracy of 98.77%
while [35] has an accuracy of 86.66% with 32 000 tunable weights.

7

Table 3: Comparison of different SNN models on TI46-Digits

Stddev Best

# Epochs Mean
22
21

Hidden Layers
100-100
200-200
LSM: R500
LSM: R3200

Algorithm
HM2-BP [19]
HM2-BP [19]
Non-spiking BP [38]
SpiLinCa [35]
ST-RSBP (this work) R100-100
ST-RSBP (this work) R200-200
ST-RSBP (this work)
aAn LSM with multiple reservoirs in parallel. Weights between input and reservoirs are trained using STDP.
The excitatory neurons in the reservoir are tagged with the classes for which they spiked at a highest rate during
training and are grouped accordingly. During inference  for a test pattern  the average spike count of every group
of neurons tagged is examined and the tag with the highest average spike count represents the predicted class.

98.42%
98.50%
78%
86.66%
98.77% 0.13% 98.95%
99.16% 0.11% 99.27%
99.25% 0.13% 99.39%

# Params
18 800
57 600
5 000
32 000
19 057
58 230
98 230

200-R200-200

75
28
23

4.4 N-Tidigits Neuromorphic Speech Dataset

The N-Tidigits [3] is the neuromorphic version of the well-known speech dataset Tidigits  and consists
of recorded spike responses of a 64-channel CochleaAMS1b sensor in response to audio waveforms
from the original Tidigits dataset [24]. 2 475 single digit examples are used for training and the same
number of examples are used for testing. There are 55 male and 56 female speakers and each of
them speaks two examples for each of the 11 single digits including “oh ” “zero”  and the digits “1”
to “9”. Table 4 shows that proposed ST-RSBP achieves excellent accuracies up to 93.90%  which
is signiﬁcantly better than that of HM2-BP and the non-spiking GRN and LSTM in [3]. With a
similar/less number of tunable weights  ST-RSBP outperforms all other methods rather signiﬁcantly.

Table 4: Comparison of different models on N-Tidigits

# Epoch Mean

Algorithm
HM2-BP [19]
GRN (NSa) [3]
Phased-LSTM (NS) [3]
ST-RSBP (this work)
ST-RSBP (this work)
aNS represents non-spiking algorithm; bG represents a GRN layer; cL represents an LSTM layer.

Hidden Layers
250-250
2× G200-100b
2× 250Lc
250-R250
400-R400-400

89.69%
90.90%
91.25%
92.94% 0.20% 93.13%
93.63% 0.27% 93.90%

# Params
81 250
109 200
610 500
82 050
351 241

Stddev Best

268
287

Table 5: Comparison of different models on Fashion-MNIST

Stddev Best

# Epochs Mean
15

Hidden Layers
400-400
5× 256
5× 256
3× 512
512-512
400-R400

# Params
Algorithm
477 600
HM2-BP [19]
BP [30]a
465 408
LRA-E [30]b
465 408
DL BP [1]a
662 026
Keras BPc
669706
478 841
ST-RSBP (this work)
a Fully connected ANN trained with the BP algorithm.
b Fully connected ANN with locally deﬁned errors trained using gradient descent. Loss functions are L2 norm
for hidden layers and categorical cross-entropy for the output layer.
c Fully connected ANN trained using the Keras package with RELU activation  categorical cross-entropy loss 
and RMSProp optimizer; a dropout layer applied between each dense layer with rate of 0.2.

88.99%
87.02%
87.69%
89.06%
89.01%
90.00% 0.14% 90.13%

50
36

4.5 Fashion-MNIST Image Dataset

The Fashion-MNIST dataset [40] contains 28x28 grey-scale images of clothing items  meant to serve
as a much more difﬁcult drop-in replacement for the well-known MNIST dataset. It contains 60 000
training examples and 10 000 testing examples with each image falling under one of the 10 classes.
Using Poisson sampling  we encode each 28 × 28 image into a 2D 784 × L binary matrix  where
L = 400 represents the duration of each spike sequence in ms  and a 1 in the matrix represents a

8

spike. The simulation time step is set to be 1ms. No other preprocessing or data augmentation is
applied. Table 5 shows that ST-RSBP outperforms all other SNN and non-spiking BP methods.

4.6 Spiking Convolution Neural Networks for the MNIST

As mentioned in Section 1  ST-RSBP can more precisely compute gradients error than HM2-BP even
for the case of feedforward CNNs. We demonstrate the performance improvement of ST-RSBP over
several other state-of-the-art SNN BP algorithms based on spiking CNNs using the MNIST dataset.
The preprocessing steps are the same as the ones for Fashion-MNIST in Section 4.5. The spiking
CNN trained by ST-RSBP consists of two 5 × 5 convolutional layers with a stride of 1  each followed
by a 2 × 2 pooling layer  one fully connected hidden layer and an output layer for classiﬁcation. In
the pooling layer  each neuron connects to 2 × 2 neurons in the preceding convolutional layer with a
ﬁxed weight of 0.25. In addition  we use elastic distortion [34] for data augmentation which is similar
to [23  39  19]. In Table 6  we compare the results of the proposed ST-RSBP with other BP rules on
similar network settings. It shows that ST-RSBP can achieve an accuracy of 99.62%  surpassing the
best previously reported performance [19] with the same model complexity.

Algorithm
Spiking CNN [23]
STBP [39]
SLAYER [33]
HM2-BP [19]
ST-RSBP (this work)
ST-RSBP (this work)
a 20C5 represents convolution layer with 20 of the 5 × 5 ﬁlters. P2 represents pooling layer with 2 × 2 ﬁlters.

Table 6: Performances of Spiking CNNs on MNIST
Hidden Layers
Stddev Best
20C5-P2-50C5-P2-200a
15C5-P2-40C5-P2-300
12C5-p2-64C5-p2
15C5-P2-40C5-P2-300
12C5-p2-64C5-p2
15C5-P2-40C5-P2-300

Mean

99.31%
99.42%
99.36% 0.05% 99.41%
99.42% 0.11% 99.49%
99.50% 0.03% 99.53%
99.57% 0.04% 99.62%

5 Discussions and Conclusion

In this paper  we present the novel spike-train level backpropagation algorithm ST-RSBP  which can
transparently train all types of SNNs including RSNNs without unfolding in time. The employed S-
PSP model improves the training efﬁciency at the spike-train level and also addresses key challenges
of RSNNs training in handling of temporal effects and gradient computation of loss functions with
inherent discontinuities for accurate gradient computation. The spike-train level processing for
RSNNs is the starting point for ST-RSBP. After that  we have applied the standard BP principle while
dealing with speciﬁc issues of derivative computation at the spike-train level.
More speciﬁcally  in ST-RSBP  the given rate-coded errors can be efﬁciently computed and back-
propagated through layers without costly unfolding the network in time and through expensive time
point by time point computation. Moreover  ST-RSBP handles the discontinuity of spikes during
BP without altering and smoothing the microscopic spiking behaviors. The problem of network
unfolding is dealt with accurate spike-train level BP such that the effect of all spikes are captured and
propagated in an aggregated manner to achieve accurate and fast training. As such  both rate and
temporal information in the SNN are well exploited during the training process.
Using the efﬁcient GPU implementation of ST-RSBP  we demonstrate the best performances for both
feedforward SNNs  RSNNs and spiking CNNs over the speech datasets TI46-Alpha  TI46-Digits 
and N-Tidigits and the image dataset MNIST and Fashion-MNIST  outperforming the current state-
of-the-art SNN training techniques. Moreover  ST-RSBP outperforms conventional deep learning
models like LSTM  GRN  and traditional non-spiking BP on the same datasets. By releasing the GPU
implementation code  we expect this work would advance the research on spiking neural networks
and neuromorphic computing.

9

Acknowledgments

This material is based upon work supported by the National Science Foundation (NSF) under
Grants No.1639995 and No.1948201. This work is also supported by the Semiconductor Research
Corporation (SRC) under Task 2692.001. Any opinions  ﬁndings  conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views of NSF 
SRC  UC Santa Barbara  and their contractors.

References
[1] Abien Fred Agarap. Deep learning using rectiﬁed linear units (relu). arXiv preprint arXiv:1803.08375 

2018.

[2] Filipp Akopyan  Jun Sawada  Andrew Cassidy  Rodrigo Alvarez-Icaza  John Arthur  Paul Merolla  Nabil
Imam  Yutaka Nakamura  Pallab Datta  Gi-Joon Nam  et al. Truenorth: Design and tool ﬂow of a 65 mw
1 million neuron programmable neurosynaptic chip. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems  34(10):1537–1557  2015.

[3] Jithendar Anumula  Daniel Neil  Tobi Delbruck  and Shih-Chii Liu. Feature representations for neuromor-

phic audio spike streams. Frontiers in neuroscience  12:23  2018.

[4] Guillaume Bellec  Darjan Salaj  Anand Subramoney  Robert Legenstein  and Wolfgang Maass. Long short-
term memory and learning-to-learn in networks of spiking neurons. In Advances in Neural Information
Processing Systems  pages 787–797  2018.

[5] Sander M Bohte  Joost N Kok  and Han La Poutre. Error-backpropagation in temporally encoded networks

of spiking neurons. Neurocomputing  48(1-4):17–37  2002.

[6] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine learning 
pages 160–167. ACM  2008.

[7] Mike Davies  Narayan Srinivasa  Tsung-Han Lin  Gautham Chinya  Yongqiang Cao  Sri Harsha Choday 
Georgios Dimou  Prasad Joshi  Nabil Imam  Shweta Jain  et al. Loihi: A neuromorphic manycore processor
with on-chip learning. IEEE Micro  38(1):82–99  2018.

[8] Peter U Diehl and Matthew Cook. Unsupervised learning of digit recognition using spike-timing-dependent

plasticity. Frontiers in computational neuroscience  9:99  2015.

[9] Peter U Diehl  Daniel Neil  Jonathan Binas  Matthew Cook  Shih-Chii Liu  and Michael Pfeiffer. Fast-
classifying  high-accuracy spiking deep networks through weight and threshold balancing. In Neural
Networks (IJCNN)  2015 International Joint Conference on  pages 1–8. IEEE  2015.

[10] Steve K Esser  Rathinakumar Appuswamy  Paul Merolla  John V Arthur  and Dharmendra S Modha.
In Advances in Neural Information

Backpropagation for energy-efﬁcient neuromorphic computing.
Processing Systems  pages 1117–1125  2015.

[11] Wulfram Gerstner and Werner M Kistler. Spiking neuron models: Single neurons  populations  plasticity.

Cambridge university press  2002.

[12] Arfan Ghani  T Martin McGinnity  Liam P Maguire  and Jim Harkin. Neuro-inspired speech recognition
with recurrent spiking neurons. In International Conference on Artiﬁcial Neural Networks  pages 513–522.
Springer  2008.

[13] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep learning. MIT press  2016.

[14] Geoffrey Hinton  Li Deng  Dong Yu  George E Dahl  Abdel-rahman Mohamed  Navdeep Jaitly  Andrew
Senior  Vincent Vanhoucke  Patrick Nguyen  Tara N Sainath  et al. Deep neural networks for acoustic
modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing
Magazine  29(6):82–97  2012.

[15] Dongsung Huh and Terrence J Sejnowski. Gradient descent for spiking neural networks. In Advances in

Neural Information Processing Systems  pages 1433–1443  2018.

[16] Eric Hunsberger and Chris Eliasmith. Spiking deep networks with lif neurons.

arXiv:1510.08829  2015.

arXiv preprint

10

[17] Eugene M Izhikevich and Gerald M Edelman. Large-scale model of mammalian thalamocortical systems.

Proceedings of the national academy of sciences  105(9):3593–3598  2008.

[18] Yingyezhe Jin and Peng Li. Ap-stdp: A novel self-organizing mechanism for efﬁcient reservoir computing.

In 2016 International Joint Conference on Neural Networks (IJCNN)  pages 1158–1165. IEEE  2016.

[19] Yingyezhe Jin  Wenrui Zhang  and Peng Li. Hybrid macro/micro level backpropagation for training deep
spiking neural networks. In Advances in Neural Information Processing Systems  pages 7005–7015  2018.

[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[21] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in neural information processing systems  pages 1097–1105  2012.

[22] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. nature  521(7553):436  2015.

[23] Jun Haeng Lee  Tobi Delbruck  and Michael Pfeiffer. Training deep spiking neural networks using

backpropagation. Frontiers in neuroscience  10:508  2016.

[24] R Gary Leonard and George Doddington. Tidigits speech corpus. Texas Instruments  Inc  1993.

[25] Mark Liberman  Robert Amsler  Ken Church  Ed Fox  Carole Hafner  Judy Klavans  Mitch Marcus  Bob
Mercer  Jan Pedersen  Paul Roossin  Don Walker  Susan Warwick  and Antonio Zampolli. TI 46-word
LDC93S9  1991.

[26] Richard Lyon. A computational model of ﬁltering  detection  and compression in the cochlea. In Acoustics 
Speech  and Signal Processing  IEEE International Conference on ICASSP’82.  volume 7  pages 1282–
1285. IEEE  1982.

[27] Wolfgang Maass  Thomas Natschläger  and Henry Markram. Real-time computing without stable states: A
new framework for neural computation based on perturbations. Neural computation  14(11):2531–2560 
2002.

[28] Paul A Merolla  John V Arthur  Rodrigo Alvarez-Icaza  Andrew S Cassidy  Jun Sawada  Filipp Akopyan 
Bryan L Jackson  Nabil Imam  Chen Guo  Yutaka Nakamura  et al. A million spiking-neuron integrated
circuit with a scalable communication network and interface. Science  345(6197):668–673  2014.

[29] Abigail Morrison  Markus Diesmann  and Wulfram Gerstner. Phenomenological models of synaptic

plasticity based on spike timing. Biological cybernetics  98(6):459–478  2008.

[30] Alexander G Ororbia and Ankur Mali. Biologically motivated algorithms for propagating local target

representations. arXiv preprint arXiv:1805.11703  2018.

[31] Filip Ponulak and Andrzej Kasi´nski. Supervised learning in spiking neural networks with resume: sequence

learning  classiﬁcation  and spike shifting. Neural computation  22(2):467–510  2010.

[32] Benjamin Schrauwen and Jan Van Campenhout. Bsa  a fast and accurate spike train encoding scheme. In
Neural Networks  2003. Proceedings of the International Joint Conference on  volume 4  pages 2825–2830.
IEEE  2003.

[33] Sumit Bam Shrestha and Garrick Orchard. Slayer: Spike layer error reassignment in time. In Advances in

Neural Information Processing Systems  pages 1412–1421  2018.

[34] Patrice Y Simard  David Steinkraus  John C Platt  et al. Best practices for convolutional neural networks

applied to visual document analysis. In ICDAR  volume 3  pages 958–962  2003.

[35] Gopalakrishnan Srinivasan  Priyadarshini Panda  and Kaushik Roy. Spilinc: Spiking liquid-ensemble

computing for unsupervised speech and image recognition. Frontiers in neuroscience  12  2018.

[36] Christian Szegedy  Alexander Toshev  and Dumitru Erhan. Deep neural networks for object detection. In

Advances in neural information processing systems  pages 2553–2561  2013.

[37] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE 

78(10):1550–1560  1990.

[38] Parami Wijesinghe  Gopalakrishnan Srinivasan  Priyadarshini Panda  and Kaushik Roy. Analysis of liquid
ensembles for enhancing the performance and accuracy of liquid state machines. Frontiers in Neuroscience 
13:504  2019.

11

[39] Yujie Wu  Lei Deng  Guoqi Li  Jun Zhu  and Luping Shi. Spatio-temporal backpropagation for training

high-performance spiking neural networks. arXiv preprint arXiv:1706.02609  2017.

[40] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking

machine learning algorithms. arXiv preprint arXiv:1708.07747  2017.

[41] Yong Zhang  Peng Li  Yingyezhe Jin  and Yoonsuck Choe. A digital liquid state machine with biologically
inspired learning and its application to speech recognition. IEEE transactions on neural networks and
learning systems  26(11):2635–2649  2015.

12

,Wenrui Zhang
Peng Li