2019,Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback,In this paper  we study Censored Semi-Bandits  a novel variant of the semi-bandits problem. The learner is assumed to have a fixed amount of resources  which it allocates to the arms at each time step. The loss observed from an arm is random and depends on the amount of resources allocated to it. More specifically  the loss equals zero if the allocation for the arm exceeds a constant (but unknown) threshold that can be dependent on the arm. Our goal is to learn a feasible allocation that minimizes the expected loss. The problem is challenging because the loss distribution and threshold value of each arm are unknown. We study this novel setting by establishing its `equivalence' to Multiple-Play Multi-Armed Bandits (MP-MAB) and Combinatorial Semi-Bandits. Exploiting these equivalences  we derive optimal algorithms for our setting using existing algorithms for MP-MAB and Combinatorial Semi-Bandits. Experiments on synthetically generated data validate performance guarantees of the proposed algorithms.,Censored Semi-Bandits: A Framework for Resource

Allocation with Censored Feedback

Arun Verma

Department of IEOR
IIT Bombay  India

v.arun@iitb.ac.in

Arun Rajkumar
Department of CSE
IIT Madras  India

arunr@cse.iitm.ac.in

Manjesh K. Hanawal
Department of IEOR
IIT Bombay  India

mhanwal@iitb.ac.in

Raman Sankaran

LinkedIn India
Bengaluru  India

rsankara@linkedin.com

Abstract

In this paper  we study Censored Semi-Bandits  a novel variant of the semi-bandits
problem. The learner is assumed to have a ﬁxed amount of resources  which it
allocates to the arms at each time step. The loss observed from an arm is random
and depends on the amount of resources allocated to it. More speciﬁcally  the
loss equals zero if the allocation for the arm exceeds a constant (but unknown)
threshold that can be dependent on the arm. Our goal is to learn a feasible allocation
that minimizes the expected loss. The problem is challenging because the loss
distribution and threshold value of each arm are unknown. We study this novel
setting by establishing its ‘equivalence’ to Multiple-Play Multi-Armed Bandits
(MP-MAB) and Combinatorial Semi-Bandits. Exploiting these equivalences  we
derive optimal algorithms for our setting using the existing algorithms for MP-MAB
and Combinatorial Semi-Bandits. Experiments on synthetically generated data
validate performance guarantees of the proposed algorithms.

1

Introduction

Many real-life sequential resource allocation problems have a censored feedback structure. Consider 
for instance  the problem of optimally allocating patrol ofﬁcers (resources) across various locations
in a city on a daily basis to combat opportunistic crimes. Here  a perpetrator picks a location (e.g.  a
deserted street) and decides to commit a crime (e.g.  mugging) but does not go ahead with it if a patrol
ofﬁcer happens to be around in the vicinity. Though the true potential crime rate depends on the latent
decision of the perpetrator  one observes feedback only when the crime is committed. Thus crimes
that were planned but not committed get censored. This model of censoring is quite general and ﬁnds
applications in several resource allocation problems such as police patrolling (Curtin et al.  2010) 
trafﬁc regulations and enforcement (Adler et al.  2014; Rosenfeld and Kraus  2017)  poaching control
(Nguyen et al.  2016; Gholami et al.  2018)  supplier selection (Abernethy et al.  2016)  advertisement
budget allocation (Lattimore et al.  2014)  among many others.
Existing approaches that deal with censored feedback in resource allocation problems fall into two
broad categories. Curtin et al. (2010); Adler et al. (2014); Rosenfeld and Kraus (2017) learn good
resource allocations from historical data. Nguyen et al. (2016); Gholami et al. (2018); Zhang et al.
(2016); Sinha et al. (2018) pose the problem in a game-theoretic framework (opportunistic security
games) and propose algorithms for optimal resource allocation strategies. While the ﬁrst approach
fails to capture the sequential nature of the problem  the second approach is agnostic to the user

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(perpetrator) behavioral modeling. In this work  we balance these two approaches by proposing a
simple yet novel threshold-based behavioral model  which we term as Censored Semi Bandits (CSB).
The model captures how a user opportunistically reacts to an allocation.
In the ﬁrst variation of our proposed behavioral model  we assume the threshold (user behavioral)
is uniform across arms (locations). We establish that this setup is ‘equivalent’ to Multiple-Play
Multi-Armed Bandits (MP-MAB)  where a ﬁxed number of arms is played in each round. We also
study the more general variation where the threshold is arm dependent. We establish that this setup is
equivalent to Combinatorial Semi-Bandits  where a subset of arms to be played is decided by solving
a combinatorial 0-1 knapsack problem.
Formally  we tackle the sequential nature of the resource allocation problem by establishing its
equivalence to the MP-MAB and Combinatorial Semi-Bandits framework. By exploiting this
equivalence for our proposed threshold-based behavioral model  we develop novel resource allocation
algorithms by adapting existing algorithms and provide optimal regret guarantees for the same.

Related Work: The problem of resource allocation for tackling crimes has received signiﬁcant
interest in recent times. Curtin et al. (2010) employ a static maximum coverage strategy for spatial
police allocation while Nguyen et al. (2016) and Gholami et al. (2018) study game-theoretic and
adversarial perpetrator strategies. We  on the other hand  restrict ourselves to a non-adversarial setting.
(Adler et al.  2014) and Rosenfeld and Kraus (2017) look at trafﬁc police resource deployment and
consider the optimization aspects of the problem using real-time trafﬁc  etc.  which differs from the
main focus of our work. Zhang et al. (2015) investigate dynamic resource allocation in the context
of police patrolling and poaching for opportunistic criminals. Here they attempt to learn a model of
criminals using a dynamic Bayesian network. Our approach proposes simpler and realistic modeling
of perpetrators where the underlying structure can be exploited efﬁciently.
We pose our problem in the exploration-exploitation paradigm  which involves solving the MP-MAB
and combinatorial 0-1 knapsack problem. It is different from the bandits with Knapsacks setting
studied in Badanidiyuru et al. (2018)  where resources get consumed in every round. The work of
Abernethy et al. (2016) and Jain and Jamieson (2018) are similar to us in the sense that they are
also threshold-based settings. However  the thresholding we employ naturally ﬁts our problem and
signiﬁcantly differs from theirs. Speciﬁcally  their thresholding is on a sample generated from an
underlying distribution  whereas we work in a Bernoulli setting where the thresholding is based on the
allocation. Resource allocation with semi-bandits feedback (Lattimore et al.  2014  2015; Dagan and
Crammer  2018) study a related but less general setup where the reward is based only on allocation
and a hidden threshold. Our setting requires an additional unknown parameter for each arm  a ‘mean
loss ’ which also affects the reward.
Allocation problems in the combinatorial setting have been explored in Cesa-Bianchi and Lugosi
(2012); Chen et al. (2013); Rajkumar and Agarwal (2014); Combes et al. (2015); Chen et al. (2016);
Wang and Chen (2018). Even though these are not related to our setting directly  we derive explicit
connections to a sub-problem of our algorithm to the setup of Komiyama et al. (2015) and Wang and
Chen (2018).

2 Problem Setting

We consider a sequential learning problem where K denotes the number of arms (locations)  and
Q denotes the amount of divisible resources. The loss at arm i ∈ [K] where [K] := {1  2  . . .   K} 
is Bernoulli distributed with rate µi ∈ (0  1]. Each arm can be assigned a fraction of resource that
decides the feedback observed and the loss incurred from that arm – if the allocated resource is
smaller than a certain threshold1  the loss incurred is the realization of the arm  and it is observed.
Otherwise  the realization is unobserved  and the incurred loss is zero. Let a := {ai : i ∈ [K]} 
where ai ∈ [0  1] denotes the resource allocated to arm i. For each i ∈ [K]  let θi ∈ (0  1] denote
the threshold associated with arm i and is such that a loss is incurred at arm i only if ai < θi. An
i∈[K] ai ≤ Q and set of all feasible allocations is
denoted as AQ. The goal is to ﬁnd a feasible resource allocation that results in a maximum reduction
in the mean loss incurred.

allocation vector a is said to be feasible if(cid:80)

1One could consider a smooth function instead of a step function  but the analysis is more involved  and our

results need not generalize straightforwardly.

2

In our setup  resources may be allocated to multiple arms. However  loss from each of the allocated
arms may not be observed depending on the amount of resources allocated to them. We thus have a
version of the partial monitoring system (Cesa-Bianchi et al.  2006; Bartók and Szepesvári  2012;
Bartók et al.  2014) with semi-bandit feedback  and we refer to it as censored semi-bandits (CSB). The
vectors θ := {θi}i∈[K] and µ := {µj}i∈[K] are unknown and identify an instance of the CSB problem.
Henceforth we identify a CSB instance as P = (µ  θ  Q) ∈ [0  1]K × (0  1]K × R+ and denote the
collection of CSB instances as PCSB. As the number of arms K = |µ|  K is known (implicitly)
from an instance of CSB. For simplicity of discussion  we assume that µ1 ≤ µ2 ≤ . . . ≤ µK  but
the algorithms are not aware of this ordering. For instance P ∈ PCSB  the optimal allocation can be
computed by the following 0-1 knapsack problem

K(cid:88)

i=1

a(cid:63) ∈ arg min
a∈AQ

µi1{ai<θi}.

(1)

Interaction between the environment and a learner is given in Algorithm 1.

Algorithm 1 CSB Learning Protocol on instance (µ  θ  Q)
For each round t:

E [Xt i] = µi and the sequence (Xt i)t≥1 is i.i.d. for all i ∈ [K].

1. Environment generates a vector Xt = (Xt 1  Xt 2  . . .   Xt K) ∈ {0  1}K  where
2. Learner picks an allocation vector at ∈ AQ.
3. Feedback and Loss: The learner observes a random feedback Yt = {Yt i : i ∈ [K]} 

where Yt i = Xt i1{at i<θi} and incurs loss(cid:80)

i∈[K] Yt i.

The goal of the learner is to ﬁnd a feasible resource allocation strategy at every round such that the
cumulative loss is minimized. Speciﬁcally  we measure the performance of a policy that selects
allocations {at}t≥1 in T steps in terms of expected (pseudo) regret given by

(cid:34) T(cid:88)

K(cid:88)

(cid:35)

K(cid:88)

E[RT ] = E

Xt i1{at i<θi}

− T

t=1

i=1

i=1

µi1{a(cid:63)

i <θi}.

(2)

A good policy should have sub-linear expected regret  i.e.  E [RT ] /T → 0 as T → ∞.

Identical Threshold for All Arms

3
In this section  we focus on the particular case of the CSB problem where θi = θc for all i ∈ [K]. With
abuse of notation  we continue to denote an instance of CSB with the same threshold as (µ  θc  Q) 
where θc ∈ (0  1] is the same threshold.
Deﬁnition 1. For a given loss vector µ and amount of resources Q  we say that thresholds θc and ˆθc
are allocation equivalent if the following holds:

K(cid:88)

i=1

K(cid:88)

i=1

min
a∈AQ

µi1{ai<θc} = min
a∈AQ

µi1{ai<ˆθc}.

Though θc can take any value in the interval (0  1]  an allocation equivalent to it can be conﬁned to a
ﬁnite set. The following lemma shows that the search for an allocation equivalent can be restricted to
(cid:100)K − Q + 1(cid:101) elements.
Lemma 1. For any θc ∈ (0  1] and Q ≥ θc  let M = min{(cid:98)Q/θc(cid:99)   K} and ˆθc = Q/M. Then θc
and ˆθc are allocation equivalent. Further  ˆθc ∈ Θ where Θ = {Q/K  Q/(K − 1) ···   min{1  Q}}.
Let M = min{(cid:98)Q/θc(cid:99)  K}. In the following  when arms are sorted in the increasing order of mean
losses  we refer to the last M arms as the bottom-M arms and the remaining arms as top-(K − M )

3

arms. It is easy to note that an optimal allocation with the same threshold θc is to allocate θc amount
of resource to each of the bottom-M arms and allocate the remaining resources to the other arms.
Lemma 1 shows that range of allocation equivalent ˆθc for any instance (µ  θc  Q) is ﬁnite. Once this
value is found  the problem reduces to identifying the bottom-M arms and assigning resource ˆθc to
each one of them to minimize the mean loss. The latter part is equivalent to solving a Multiple-Play
Multi-Armed Bandits problem  as discussed next.

3.1 Equivalence to Multiple-play Multi-armed Bandits

In stochastic Multiple-Play Multi-Armed Bandits (MP-MAB)  a learner can play a subset of arms in
each round known as superarm (Anantharam et al.  1987). The size of each superarm is ﬁxed (and
known). The mean loss of a superarm is the sum of the means of its constituent arms. In each round 
the learner plays a superarm and observes the loss from each arm played (semi-bandit feedback). The
goal of the learner is to select a superarm that has the smallest mean loss. A policy in MP-MAB
selects a superarm in each round based on the past information. Its performance is measured in terms
of regret deﬁned as the difference between cumulative loss incurred by the policy and that incurred
by playing an optimal superarm in each round. Let (µ  m) ∈ [0  1]K × N+ denote an instance of
MP-MAB where µ denotes the mean loss vector  and m ≤ K denotes the size of each superarm.
Let P c
CSB ⊂ PCSB denote the set of CSB instances with the same threshold for all arms. For any
(µ  θc  Q) ∈ P c
CSB with K arms and known threshold θc  let (µ  m) be an instance of MP-MAB
with K arms and each arm has the same Bernoulli distribution as the corresponding arm in the CSB
instance with m = K − M  where M = min{(cid:98)Q/θc(cid:99)   K} as earlier. Let PMP denote the set of
resulting MP-MAB problems and f : PCSB → PMP denote the above transformation.
Let π be a policy on PMP. π can also be applied on any (µ  θc  Q) ∈ P c
CSB with known θc
in round t  let the information
to decide which set of arms are allocated resource as follows:
(L1  Y1  L2  Y2  . . .   Lt−1  Yt−1) collected on a CSB instance  where Ls is the set of K − M arms
where no resource is applied and Ys is the samples observed from these arms. In round t  this
information is given to π which returns a set Lt with K − M elements. Then all arms other than arms
in Lt are given resource θc. Let this policy on (µ  θc  Q) ∈ P c
CSB be denoted as π(cid:48). In a similar way a
policy β on PCSB can be adapted to yield a policy for PMP as follows: in round t  let the information
(S1  Y1  S2  Y2  . . .   St−1  Yt−1) collected on an MP-MAB instance  where Ss is the superarm played
in round s and Ys is the associated loss observed from each arms in Ss  is given to π which returns a
set St of K − M arms where no resources has to be applied. The superarm corresponding to St is
then played. Let this policy on PMP be denoted as β(cid:48). Note that when θc is known  the mapping is
invertible. The next proposition gives regret equivalence between the MP-MAB problem and CSB
problem with a known same threshold.
Proposition 1. Let P := (µ  θc  Q) ∈ P c
CSB with known θc then the regret of π on P is same as the
regret of π(cid:48) on f (P ). Similarly  let P (cid:48) := (µ  m) ∈ PMP  then the regret of a policy β on P (cid:48) is same
as the regret of β(cid:48) on f−1(P (cid:48)). Thus the set PCSB with a known θc is ’regret equivalent’ to PMP  i.e. 
R(P c
Lower bound: As a consequence of the above equivalence and one-to-one correspondence between
the MP-MAB and CSB with the same known threshold  a lower bound on MP-MAB is also a lower
bound on CSB with the same threshold. Hence the following lower bound given for any strongly
consistent algorithm (Anantharam et al.  1987  Theorem 3.1) is also a lower bound on the CSB
problem with the same threshold:

CSB) = R(PMP).

≥(cid:80)

E[RT ]
log T

lim
T→∞

i∈[K]\[K−M ]

µi − µK−M
d(µK−M   µi)

 

(3)

where d(p  q) is the Kullback-Leibler (KL) divergence between two Bernoulli distributions with
parameter p and q. Also note that we are in loss setting.
The above proposition suggests that any algorithm which works well for the MP-MAB also works
well for the CSB once the threshold is known. Hence one can use algorithms like MP-TS (Komiyama
et al.  2015) and ESCB (Combes et al.  2015) once an allocation equivalent to θc is found. MP-TS
uses Thompson Sampling  whereas ESCB uses UCB (Upper Conﬁdence Bound) and KL-UCB type
indices. One can use any of these algorithms. But we adapt MP-TS to our setting as it gives the better
empirical performance and is shown to achieve optimal regret bound for Bernoulli distributions.

4

3.2 Algorithm: CSB-ST

We develop an algorithm named CSB-ST for solving the Censored Semi Bandits problem with Same
Threshold. It exploits the result in Lemma 1 and equivalence established in Proposition 1 to learn a
good estimate of threshold and minimize the regret using a MP-MAB algorithm. CSB-ST consists of
two phases  namely  threshold estimation and regret minimization.

CSB-ST Algorithm for solving the Censored Semi Bandits problem with Same Threshold

Input: K  Q  δ  
\\ Threshold Estimation Phase \\

1: Initialize C = 0  l = 0  u = |Θ|  i = (cid:100)u/2(cid:101)
2: Set Θ as Lemma 1  Tθs = 0  Wδ = log(log2(|Θ|)/δ)/(max{1 (cid:98)Q(cid:99)} log(1/(1 − )))
3: while i (cid:54)= u do
Set ˆθc = Θ[i]
4:
At ← ﬁrst Q/ˆθc arms. Allocate ˆθc resource to each arm i ∈ At
5:
6:
7:
8:
9: end while

If loss observed for any arm i ∈ At then set l = i  i = l +(cid:6) u−l
If C = Wδ then set u = i  i = u −(cid:4) u−l

(cid:5)   C = 0

(cid:7)   C = 0 else C = C + 1

Tθs = Tθs + 1

2

2

\\ Regret Minimization Phase \\

10: Set M = Q/ˆθc and ∀i ∈ [K] : Si = 1  Fi = 1
11: for t = Tθs + 1  Tθs + 2  . . .   T do
12:
13:
14:
15:
16: end for

∀i ∈ [K] : ˆµi(t) = β(Si  Fi)
Lt ← (K-M) arms with smallest estimates
∀i ∈ Lt : allocate no resource to arm i. ∀j ∈ K \ Lt : allocate ˆθc resources to arm j
∀i ∈ Lt : observe Xt i. Update Si = Si + Xt i and Fi = Fi + 1 − Xt i

Threshold Estimation Phase: This phase ﬁnds a threshold ˆθc that is allocation equivalent to
the underlying threshold θc with high probability by doing a binary search over the set Θ =
{Q/K  Q/(K − 1)  . . .   min{1  Q}}. The elements of Θ are arranged in increasing order and
are candidates for θc. The search starts by taking ˆθc to be the middle element in Θ and allocating
ˆθc resource to the ﬁrst Q/ˆθc arms (denoted as At in Line 5). If a loss is observed at any of these
arms  it implies that ˆθc is an underestimate of θc. All the candidates lower than the current value
of ˆθc in Θ are eliminated  and the search is repeated in the remaining half of the elements again by
starting with the middle element (Line 6). If no loss is observed for consecutive Wδ rounds  then ˆθi is
possibly an overestimate. Accordingly  all the candidates larger than the current value of ˆθc in Θ are
eliminated  and the search is repeated starting with the middle element in the remaining half (Line 7).
The variable C keeps track of the number of the consecutive rounds for which no loss is observed. It
changes to 0 either after observing a loss or if no loss is observed for consecutive Wδ rounds.
Note that if ˆθc is an underestimate and no loss is observed for consecutive Wδ rounds  then ˆθc will be
reduced  which leads to a wrong estimate of ˆθc. To avoid this  we set the value of Wδ such that the
probability of happening of such an event is upper bounded by δ. The next lemma gives a bound on
the number of rounds needed to ﬁnd allocation equivalent for threshold θc with high probability.
Lemma 2. Let (µ  θc  Q) be an CSB instance such that µ1 ≥  > 0. Then with probability at
least 1 − δ  the number of rounds needed by the threshold estimation phase of CSB-ST to ﬁnd the
allocation equivalent for threshold θc is bounded as

Tθs ≤

log(log2(|Θ|)/δ)

max{1 (cid:98)Q(cid:99)} log (1/(1 − ))

log2(|Θ|).

Once ˆθc is known  µ needs to be estimated. The resources can be allocated such that no losses are
observed for maximum M arms. As our goal is to minimize the mean loss  we have to select M arms
with highest mean loss and then allocate ˆθc to each of them. It is equivalent to ﬁnd K − M arms with

5

the least mean loss then allocate no resources to these arms and observe their losses. These losses are
then used for updating the empirical estimate of the mean loss of arms.
Regret Minimization Phase: The regret minimization phase of CSB-ST adapts Multiple-Play
Thompson Sampling (MP-TS) (Komiyama et al.  2015) for our setting. It works as follows: initially
we set the prior distribution of each arm as the Beta distribution β(1  1)  which is same as Uniform
distribution on [0  1]. Si represents the number of rounds when loss is observed whereas Fi represents
the number of round when loss is not observed. Let Si(t) and Fi(t) denote the values of Si and Fi in
the beginning of round t. In round t  a sample ˆµi is independently drawn from β(Si(t)  Fi(t)) for
each arm i ∈ [K]. ˆµi values are ranked by their increasing values. The ﬁrst K − M arm are assigned
no resources (denoted as set Lt in Line 13) while each of the remaining M arms are allocated ˆθc
resources. The loss Xt i is observed for each arm i ∈ Lt and then success and failure counts are
updated by setting Si = Si + Xt i and Fi = Fi + 1 − Xt i.
3.2.1 Regret Upper Bound

For instance (µ  θ  Q) and any feasible allocation a ∈ AQ  we deﬁne ∇a =(cid:80)K
i <θi}(cid:1) and ∇m = maxa∈AQ ∇a. We are now ready to state the regret bounds.

1{a(cid:63)
Theorem 1. Let µ1 ≥  > 0  Wδ = log(log2(|Θ|)/δ)/max{1 (cid:98)Q(cid:99)} log(1/(1 − ))  µK−M <
µK−M +1  and T > Wδ log2(|Θ|). Set δ = T −(log T )−α in CSB-ST such that α > 0. Then the regret
of CSB-ST is upper bounded as

(cid:0)1{ai<θi} −

i=1 µi

(cid:16)

(log T )2/3(cid:17)

+

(cid:88)

E [RT ] ≤ Wδ log2 (|Θ|)∇m + O

(µi − µK−M ) log T

.

i∈[K]\[K−M ]

d(µK−M   µi)

The ﬁrst term in the regret bound of Theorem 1 corresponds to the length of the threshold estimation
phase  and the remaining parts correspond to the expected regret in the regret minimization phase.
Note that the assumption µ1 ≥  is only required to guarantee that the threshold estimation
phase terminates in ﬁnite number of rounds. This assumption is not needed to get the bound
on expected regret in the regret minimization phase. The assumption µK−M < µK−M +1 ensures
that Kullback-Leibler divergence in the bound is well deﬁned. This assumption is also equivalent to
assuming that the set of top-M arms is unique.
Corollary 1. The regret of CSB-ST is asymptotically optimal.

Note that Wδ = O(cid:0)(log T )1−α(cid:1) for any α > 0 and δ = T −(log T )−α in CSB-ST. Now the proof of

Corollary 1 follows by comparing the above bound with the lower bound given in Eq. (3).

4 Different Thresholds

In this section  we consider a more difﬁcult problem where the threshold may not be the same for all
arms. Let KP (µ  θ  Q) denote a 0-1 knapsack problem with capacity Q and K items where item i
has weight θi and value µi. Our next result gives the optimal allocation for an instance in PCSB.
Proposition 2. Let P = (µ  θ  Q) ∈ PCSB. Then the optimal allocation for P is a solution of
KP (µ  θ  Q) problem.

The proof of the above proposition and computational issues of the 0-1 knapsack with fractional
values are given in the supplementary. We next discuss the condition when two threshold vectors are
allocation equivalent. Extending the deﬁnition of allocation equivalence to threshold vectors  we say
that two vectors θ and ˆθ are allocation equivalent if minimum mean loss in instances (µ  θ  Q) and
(µ  ˆθ  Q) are the same for any loss vector µ and resource Q. This equivalence allows us to look for
estimated thresholds within some tolerance. We need the following notations to formalize this notion.
For an instance P := (µ  θ  Q)  recall that a(cid:63) = (a(cid:63)
K) denotes the optimal allocation. Let
θi  where r is the residual resources after the optimal allocation. Deﬁne γ := r/K.
Any problem instance with γ = 0 becomes a ‘hopeless’ problem instance as the only vector that is
allocation equivalent to θ is θ itself  which demands θi values to be estimated with full precision to
achieve optimal allocation. However  if γ > 0  an optimal allocation can still be found with small
errors in the estimates of θi as shown next.

r = Q−(cid:80)

1  . . .   a(cid:63)

i ≥θi

i:a(cid:63)

6

Lemma 3. Let γ > 0 and ∀i ∈ [K] : ˆθi ∈ [θi  θi + γ]. Then for any µ ∈ [0  1]K and Q  the θ and ˆθ
are allocation equivalent.

The proof follows by an application of Theorem 3.2 in Hiﬁ and Mhalla (2013) which gives conditions
for two weight vectors θ1 and θ2 to have the same solution in KP (µ  θ1  Q) and KP (µ  θ2  Q) for
any µ and Q. Once we accurately estimate the threshold θ so that the estimate ˆθ is an allocation
equivalent of θ  the problem is equivalent to solving the KP (µ  ˆθ  Q). The latter part is equivalent
to solving a Combinatorial Semi-Bandits as we establish next. Combinatorial Semi-Bandits are the
generalization of MP-MAB  where the size of the superarms need not be the same in each round.
Proposition 3. The CSB problem with known threshold vector θ is regret equivalent to a
Combinatorial Semi-Bandits where Oracle uses KP (µ  θ  Q) to identify the optimal subset of
arms.

4.1 Algorithm: CSB-DT

We develop an algorithm named CSB-DT for solving the Censored Semi Bandits problem with
Different Threshold. It exploits the result of Lemma 3 and equivalence established in Proposition 3 to
learn a good estimate of the threshold for each arm and minimizes the regret using an algorithm from
Combinatorial Semi-Bandits. CSB-DT also consists of two phases: threshold estimation and regret
minimization.

CSB-DT Algorithm for solving the Censored Semi Bandits problem with Different Threshold

Input: K  Q  δ    γ
\\ Threshold Estimation Phase \\

K−(cid:98)Q(cid:99) resources

for i = 1  . . .   K do

else

end if

end if

if loss observe for arm i with θg i = 0 and θl i < ˆθi then

Set θl i = ˆθi  ˆθi = (θu i + θl i)/2  Ci = 0. If available allocate resource ˆθi

If allocated resources is ˆθi then reset Ci = Ci + 1
if Ci = Wδ and θg i = 0 then

Set θu i = ˆθi  ˆθi = (θu i + θl i)/2  Ci = 0. If available allocate resource ˆθi
If θu i − θl i ≤ γ then set θg i = 1 and ˆθi = θu i

1: Initialize ∀i ∈ [K] : θl i = 0  θu i = 1  θg i = 0  Ci = 0.
2: Set Tθd = 0  Wδ = log(K log2((cid:100)1 + 1/γ(cid:101))/δ)/log(1/(1 − ))
3: ∀i ∈ [(cid:98)Q(cid:99)] : allocate ˆθi = 0.5 resource. ∀j ∈ [(cid:98)Q(cid:99) + 1  K] : allocate ˆθj = Q−(cid:98)Q(cid:99)/2
4: while θg i = 0 for any i ∈ [K] do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: end while
21: ∀i ∈ [K] : Si = 1  Fi = 1
22: for t = Tθd + 1  Tθd + 2  . . .   T do
∀i ∈ [K] : ˆµi(t) ← Beta(Si  Fi)
23:
24:
∀i ∈ Lt : allocate no resource to arm i. ∀j ∈ K \ Lt : allocate ˆθj resources to arm j
25:
∀i ∈ Lt : observe Xt i. Update Si = Si + Xt i and Fi = Fi + 1 − Xt i
26:
27: end for

Lt ← Oracle(cid:0)KP ( ˆµ(t)  ˆθ  Q)(cid:1)

end for
while free resources are available do

end while
Tθd = Tθd + 1

\\ Regret Minimization Phase \\

Allocate ˆθi resources to a new randomly chosen arm i from the arms having θg i = 1

Threshold Estimation Phase: This phase ﬁnds a threshold that is allocation equivalent of θ with
high probability. This is achieved by ensuring that ˆθi ∈ [θi  θi + γ] for all i (Lemma 3). For each arm

7

i ∈ [K] a binary search is performed over the interval [0  1] by maintaining variables ˆθi  θl i  θu i  θg i 
and Ci where ˆθi is the current estimate of θi; θl i and θu i denote the lower and upper bound of
the binary search region for arm i; and θg i indicates whether current estimate lies in the interval
[θi  θi + γ]. In each round  the threshold estimate of arms are ﬁrst updated sequentially and then
tested on their respective arms. Ci keeps count of consecutive rounds without no loss for ˆθi. It
changes to 0 either after observing a loss or if no loss is observed for consecutive Wδ rounds.
The threshold estimation phase starts with allocating 0.5 resource for ﬁrst (cid:98)Q(cid:99) arms and (Q −
(cid:98)Q(cid:99) /2)/(K − (cid:98)Q(cid:99)) for the remaining arms (Line 3). In each round  allocated resource are applied
on each arm and based on the observations  their estimates and the allocated resource are updated
sequentially starting from 1 to K as follows. If a loss is observed for arm i having bad threshold
estimate (θg i = 0) and θl i < ˆθi  then it implies that ˆθi is an underestimate of θi and the following
actions are performed – 1) lower end of search region is increased to ˆθi  i.e.  θl i = ˆθi; 2) its estimate
ˆθi is set to (θu i + θl i)/2; 3) if available allocate ˆθi resource to arm i; and 4) set Ci = 0 (Line 7).
If no loss is observed after allocating ˆθi resources for Wδ successive rounds for arm i with bad
threshold estimate  then it implies that ˆθi is overestimated and following actions are performed –
1) the upper end of the search region is changed to ˆθi  i.e  θu i = ˆθi; 2) its estimate ˆθi is set to
(θu i + θl i)/2; and 3) if available allocate ˆθi resource to arm i (Line 11). Further  whether goodness
of ˆθi holds  i.e.  ˆθi ∈ [θi  θi + γ] is checked by condition θu i − θl i ≤ γ. If the condition holds 
the threshold estimation of arm is within desired accuracy and this is indicated by setting θg i to 1
and ˆθi = θu i (Line 12). Any unassigned resources are given to randomly chosen arms having good
threshold estimates (all arms with θg i = 1) where each arm i gets only ˆθi resources (Line 17).
The value of Wδ in CSB-DT is set such that the probability of estimated threshold does not lie in
[θi  θi + γ] for all arms is upper bounded by δ. Following lemma gives the bounds on the number of
rounds needed to ﬁnd the allocation equivalent for threshold vector θ with high probability.
Lemma 4. Let (µ  θ  Q) be an instance of CSB such that γ > 0 and µ1 ≥  > 0. Then with
probability at least 1 − δ  the number of rounds needed by threshold estimation phase of CSB-DT to
ﬁnd the allocation equivalent for threshold vector θ is bounded as

Tθd ≤ K log(K log2((cid:100)1 + 1/γ(cid:101))/δ)
max{1 (cid:98)Q(cid:99)} log(1/(1 − ))

log2((cid:100)1 + 1/γ(cid:101)).

Regret Minimization Phase: For this phase  we could use an algorithm that works well for the
Combinatorial Semi-Bandits  like SDCB (Chen et al.  2016) and CTS (Wang and Chen  2018). CTS
uses Thompson Sampling  whereas SDCB uses the UCB type index. We adapt the CTS to our
setting due to better empirical performance. This phase is similar to the regret minimization phase of
CSB-ST except that superarm to play is selected by Oracle that uses KL( ˆµ(t)  ˆθ  Q) to identify the
arms where the learner has to allocate no resources.

4.1.1 Regret Upper Bound
Let ∇a and ∇m be deﬁned as in Section 3.2.1. Let γ > 0  Sa = {i : ai < θi} for any feasible
allocation a and k(cid:63) = |Sa(cid:63)|. We redeﬁne Wδ = log(K log2((cid:100)1 + 1/γ(cid:101))/δ)/ log(1/(1 − )).
Theorem 2. Let (µ  θ  Q) ∈ PCSB such that γ > 0  µ1 ≥   and T > Wδlog2((cid:100)1 + 1/γ(cid:101)). Set δ =
T −(log T )−α in CSB-DT such that α > 0. Then the expected regret of CSB-DT is upper bounded as

(cid:18) KWδlog2 ((cid:100)1 + 1/γ(cid:101))

(cid:19)

max{1 (cid:98)Q(cid:99)}

∇m +

E [RT ] ≤

(cid:32)

K(K − Q)2

η2

+

8α1
η2

(cid:88)
(cid:18) 4

i∈[K]

η2 + 1

max
Sa:i∈Sa

(cid:19)k(cid:63)

8|Sa| log T

∇a − 2(k(cid:63)2 + 2)η

(cid:33)

log

k(cid:63)
η2 + 3K

∇m 

 +

for any η such that ∀a ∈ AQ ∇a > 2(k(cid:63)2 + 2)η and a problem independent constant α1.
The ﬁrst term of expected regret is due to the threshold estimation phase. Threshold estimation takes
Tθd rounds to complete  and ∇m is the maximum regret that can be incurred in any round. Then the

8

maximum regret due to threshold estimation is bounded by Tθd∇m. The remaining terms correspond
to the regret due to the regret minimization phase. Further  the expected regret of CSB-DT can be
shown be E [RT ] ≤ O(K log T /∇min)  where ∇min is the minimum gap between the mean loss of
optimal allocation and any non-optimal allocation.

5 Experiments

We ran computer simulations to evaluate the empirical performance of proposed algorithms. Our
simulations involve two synthetically generated instances. In Instance 1  the threshold is the same for
all arm  whereas in Instance 2  it varies across arms. The details of the instances are as follows:
Instance 1 (Identical Threshold): It has K = 20  Q = 7  θc = 0.7  δ = 0.1   = 0.1 and
T = 10000. The loss of arm i is Bernoulli distribution with parameter 0.25 + (i − 1)/50.
Instance 2 (Different Thresholds): It has K = 5  Q = 2  δ = 0.1   = 0.1  γ = 10−3 and
T = 5000. The mean loss vector is µ = [0.9  0.89  0.87  0.58  0.3] and corresponding threshold
vector is θ = [0.7  0.7  0.7  0.6  0.35]. The loss of arm i is Bernoulli distributed with parameter µi.
For Instance 1  we only varied the number of resource Q and observed the regret of CSB-ST as given
in Fig. 1. We observe that when resources are small  the learner can allocate resources to a few arms
but observes loss from more arms. On the other hand  when resources are more  the learner allocates
resources to more arms and observes loss from fewer arms. Thus as resources increase  we move
from semi-bandit feedback to bandit feedback and hence regret increases with increase in the amount
of resources. Next  we only varied θc in Instance 1  and the regret of CSB-ST is shown in Fig. 2.
Similar trends are observed as the decrease in threshold leads to an increase in the number of arms
that can be allocated resources and vice-versa. Therefore the amount of feedback decreases as the
threshold decreases and leads to more regret. We repeated the experiment 100 times and plotted the
regret with a 95% conﬁdence interval (the vertical line on each curve shows the conﬁdence interval).
The empirical results validate sub-linear bounds for our algorithms.

Figure 2: Varying value of same threshold.

Figure 1: Varying amount of resources.
Figure 3: UCB and TS based Algorithms.
We also compare the performance of CSB-DT against the CSB-DT-UCB algorithm  which uses the
UCB type index as used in the SDCB algorithm (Chen et al.  2016) on Instance 2. As shown in
Fig. 3  as expected  Thompson Sampling (TS) based CSB-DT outperforms its UCB based counterpart
CSB-DT-UCB. The pseudo-code of CSB-DT-UCB is given in the supplementary material.

6 Conclusion and Future Extensions

In this work  we proposed a novel framework for resource allocation problems using a variant of
semi-bandits named censored semi-bandits. In our setup  loss observed from an arm depends on the
amount of resource allocated  and hence  the loss can be censored. We consider a threshold-based
model where loss from an arm is observed when allocated resource is below a threshold. The goal is
to assign a given resource to arms such that total expected loss is minimized. We considered two
variants of the problem  depending on whether or not the thresholds are the same across the arms. For
the variant where thresholds are the same across the arms  we established that it is equivalent to the
Multiple-Play Multi-Armed Bandit problem. For the second variant where threshold can depend on
the arm  we established that it is equivalent to a more general Combinatorial Semi-Bandit problem.
Exploiting these equivalences  we developed algorithms that enjoy optimal performance guarantees.
We decoupled the problem of threshold and mean loss estimation. It would be interesting to explore
if this can be done jointly  leading to better performance guarantees. Another new extension of work
is to relax the assumptions that mean losses are strictly positive  and time horizon T is known.

9

Acknowledgments

Arun Verma would like to thank travel support from Google and NeurIPS. Manjesh K. Hanawal
would like to thank the support from INSPIRE faculty fellowships from DST  Government of India 
SEED grant (16IRCCSG010) from IIT Bombay  and Early Career Research (ECR) Award from
SERB. Initial discussions of this work were done when Raman Sankaran was at Conduent Labs India.

References
Kevin M Curtin  Karen Hayslett-McCall  and Fang Qiu. Determining optimal police patrol areas
with maximal covering and backup covering location models. Networks and Spatial Economics 
10(1):125–145  2010.

Nicole Adler  Alfred Shalom Hakkert 

Jonathan Kornbluth  Tal Raviv  and Mali Sher.
Location-allocation models for trafﬁc police patrol vehicles on an interurban network. Annals of
Operations Research  221(1):9–31  2014.

Ariel Rosenfeld and Sarit Kraus. When security games hit trafﬁc: Optimal trafﬁc enforcement under

one sided uncertainty. In IJCAI  pages 3814–3822  2017.

Thanh H Nguyen  Arunesh Sinha  Shahrzad Gholami  Andrew Plumptre  Lucas Joppa  Milind
Tambe  Margaret Driciru  Fred Wanyama  Aggrey Rwetsiba  Rob Critchlow  and Colin M. Beale.
In Proceedings of the
Capture: A new predictive anti-poaching tool for wildlife protection.
2016 International Conference on Autonomous Agents & Multiagent Systems  pages 767–775.
International Foundation for Autonomous Agents and Multiagent Systems  2016.

Shahrzad Gholami  Sara Mc Carthy  Bistra Dilkina  Andrew Plumptre  Milind Tambe  Margaret
Driciru  Fred Wanyama  Aggrey Rwetsiba  Mustapha Nsubaga  Joshua Mabonga  Tom Okello  and
Eric Enyel. Adversary models account for imperfect crime data: Forecasting and planning against
real-world poachers. In Proceedings of the 17th International Conference on Autonomous Agents
and MultiAgent Systems  pages 823–831. International Foundation for Autonomous Agents and
Multiagent Systems  2018.

Jacob D Abernethy  Kareem Amin  and Ruihao Zhu. Threshold bandits  with and without censored

feedback. In Advances In Neural Information Processing Systems  pages 4889–4897  2016.

Tor Lattimore  Koby Crammer  and Csaba Szepesvári. Optimal resource allocation with semi-bandit
feedback. In Proceedings of the Thirtieth Conference on Uncertainty in Artiﬁcial Intelligence 
pages 477–486. AUAI Press  2014.

Chao Zhang  Victor Bucarey  Ayan Mukhopadhyay  Arunesh Sinha  Yundi Qian  Yevgeniy
Vorobeychik  and Milind Tambe. Using abstractions to solve opportunistic crime security games at
scale. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent
Systems  pages 196–204  2016.

Arunesh Sinha  Fei Fang  Bo An  Christopher Kiekintveld  and Milind Tambe. Stackelberg security

games: Looking beyond a decade of success. In IJCAI  pages 5494–5501  2018.

Chao Zhang  Arunesh Sinha  and Milind Tambe. Keeping pace with criminals: Designing patrol
allocation against adaptive opportunistic criminals. In Proceedings of the 2015 international
conference on Autonomous agents and multiagent systems  pages 1351–1359  2015.

Ashwinkumar Badanidiyuru  Robert Kleinberg  and Aleksandrs Slivkins. Bandits with knapsacks.

Journal of the ACM (JACM)  65(3):13  2018.

Lalit Jain and Kevin Jamieson. Firing bandits: Optimizing crowdfunding. In International Conference

on Machine Learning  pages 2211–2219  2018.

Tor Lattimore  Koby Crammer  and Csaba Szepesvári. Linear multi-resource allocation with
semi-bandit feedback. In Advances in Neural Information Processing Systems  pages 964–972 
2015.

10

Yuval Dagan and Koby Crammer. A better resource allocation algorithm with semi-bandit feedback.

In Algorithmic Learning Theory  pages 268–320  2018.

Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences  78(5):1404–1422  2012.

Wei Chen  Yajun Wang  and Yang Yuan. Combinatorial multi-armed bandit: General framework and

applications. In International Conference on Machine Learning  pages 151–159  2013.

Arun Rajkumar and Shivani Agarwal. Online decision-making in general combinatorial spaces. In

Advances in Neural Information Processing Systems  pages 3482–3490  2014.

Richard Combes  Mohammad Sadegh Talebi Mazraeh Shahi  Alexandre Proutiere  and Marc Lelarge.
Combinatorial bandits revisited. In Advances in Neural Information Processing Systems  pages
2116–2124  2015.

Wei Chen  Wei Hu  Fu Li  Jian Li  Yu Liu  and Pinyan Lu. Combinatorial multi-armed bandit
with general reward functions. In Advances in Neural Information Processing Systems  pages
1659–1667  2016.

Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In International

Conference on Machine Learning  pages 5101–5109  2018.

Junpei Komiyama  Junya Honda  and Hiroshi Nakagawa. Optimal Regret Analysis of Thompson
In International

Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays.
Conference on Machine Learning  pages 1152–1161  2015.

Nicolo Cesa-Bianchi  Gábor Lugosi  and Gilles Stoltz. Regret minimization under partial monitoring.

Mathematics of Operations Research  31(3):562–580  2006.

Gábor Bartók and Csaba Szepesvári. Partial monitoring with side information. In International

Conference on Algorithmic Learning Theory  pages 305–319. Springer  2012.

Gábor Bartók  Dean P Foster  Dávid Pál  Alexander Rakhlin  and Csaba Szepesvári. Partial
monitoring—classiﬁcation  regret bounds  and algorithms. Mathematics of Operations Research 
39(4):967–997  2014.

Venkatachalam Anantharam  Pravin Varaiya  and Jean Walrand. Asymptotically Efﬁcient Allocation
IEEE

Rules for the Multiarmed Bandit Problem with Multiple Plays-Part I: I.I.D. Rewards.
Transactions on Automatic Control  32(11):968–976  1987.

Mhand Hiﬁ and Hedi Mhalla. Sensitivity analysis to perturbations of the weight of a subset of items:

The knapsack case study. Discrete Optimization  10(4):320–330  2013.

11

,Arun Verma
Manjesh Hanawal
Arun Rajkumar
Raman Sankaran