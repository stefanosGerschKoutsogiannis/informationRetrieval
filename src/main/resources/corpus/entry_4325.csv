2019,Polynomial Cost of Adaptation for X-Armed Bandits,In the context of stochastic continuum-armed bandits  we present an algorithm that adapts to the unknown smoothness of the objective function. We exhibit and compute a polynomial cost of adaptation to the Hölder regularity for regret minimization. To do this  we first reconsider the recent lower bound of Locatelli and Carpentier  2018  and define and characterize admissible rate functions. Our new algorithm matches any of these minimal rate functions. We provide a finite-time analysis and a thorough discussion about asymptotic optimality.,Polynomial Cost of Adaptation for X -Armed Bandits

Hédi Hadiji

Laboratoire de Mathématiques d’Orsay

Université Paris-Sud  Orsay  France
hedi.hadiji@math.u-psud.fr

Abstract

In the context of stochastic continuum-armed bandits  we present an algorithm
that adapts to the unknown smoothness of the objective function. We exhibit
and compute a polynomial cost of adaptation to the Hölder regularity for regret
minimization. To do this  we ﬁrst reconsider the recent lower bound of Locatelli
and Carpentier [21]  and deﬁne and characterize admissible rate functions. Our new
algorithm matches any of these minimal rate functions. We provide a ﬁnite-time
analysis and a thorough discussion about asymptotic optimality.

1 Introduction

Multi-armed bandits are a well-known sequential learning problem. When the number of available
decisions is large  some assumptions on the environment have to be made. In a vast line of work
(see the literature discussion in Section 1.1)  these assumptions show up as nonparametric regularity
conditions on the mean-payoff function. If this function is Hölder continuous with constant L and
exponent ↵  and if the values of L and ↵ are given to the player  then natural strategies can ensure
that the regret is upper bounded by

L1/(2↵+1)T (↵+1)/(2↵+1) .

(1)
Of course  assuming that the player knows ↵ and L is often not realistic. Thus the need for
adaptive methods  that are agnostic with respect to the true regularity of the mean-payoff function.
Unfortunately  Locatelli and Carpentier [21] recently showed that full adaptation is impossible  and
that no algorithm can enjoy the same minimax guarantees as when the regularity is given to the player.
We persevere and address the question:

What can the player achieve when the true regularity is completely unknown?

A polynomial cost of adaptation In statistics  minimax adaptation for nonparametric function
estimation is a deep and active research domain. In many contexts  sharp adaptation is possible;
often  an additional logarithmic factor in the error has to be paid when the regularity is unknown:
this is known as the cost of adaptation. See e.g.  Lepskii [20]  Birgé and Massart [4]  Massart
[22] for adaptive methods  and Cai [9] for a detailed survey of the topic. Under some more exotic
assumptions —see e.g.  Example 3 of Cai and Low [10] — adapting is signiﬁcantly harder: there
may be a polynomial cost of adaptation.
In this paper  we show that in the sequential setting of multi-armed bandits  the necessary exploration
forces a similar phenomenon  and we exhibit this polynomial cost of adaptation. To do so  we revisit
the lower bounds of Locatelli and Carpentier [21]  and design a new algorithm that matches these
lower bounds.
As a representative example of our results  our algorithm can achieve  without the knowledge of ↵
and L  an unimprovable (up to logarithmic factors) regret bound of order

L1/(1+↵)T (↵+2)/(2↵+2) .

(2)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Related work
Continuum-armed bandits Continuum-armed bandits  with nonparametric regularity assumptions 
were introduced by Agrawal [1]. Kleinberg [16] established the minimax rates in the Hölder setting
and introduced the CAB1 algorithm. Auer et al. [3] studied the problem with additional regularity
assumptions under which the minimax rates are improved. Via different roads  Bubeck et al. [7] and
Kleinberg et al. [17] explored further generalizations of these types of regularity  namely the zooming
dimension and the near-optimality dimension. Bull [8] exhibited an algorithm that essentially adapts
to some cases when the near-optimality dimension is zero.
In all these articles  the mean-payoff function needs to satisfy simultaneously two sets of regularity
conditions. The ﬁrst type is a usual Hölder condition  which ensures that the function does not vary
too much around (one of) its maxima. The second type is a “margin condition” that lower bounds the
number of very suboptimal arms; in the literature these are deﬁned in many technically different ways.
Adapting to the margin conditions is often possible when the Hölder regularity is known. However 
all these algorithms require some prior knowledge about the Hölder regularity.
In this paper  we focus on the problem of adapting to Hölder regularity. Accordingly  we call adaptive
the algorithms that assume no knowledge of the Hölder exponent nor of the Lipschitz constant.

Adaptation for cumulative regret Bubeck et al. [5] introduced the problem of adaptation  and
adapted to the Lipschitz constant under extra requirements. An important step was made in Locatelli
and Carpentier [21]  where it is shown that adaptation at the classical minimax rates is impossible. In
the same paper  the authors exhibited some conditions under which full adaptation is achievable  e.g. 
with knowledge of the value of the maximum  or when the near-optimality dimension is zero.

Other settings For simple regret  the objections against adaptation do not hold  as the objective does
not penalize exploration. Adaptation up to polylog factors is done with various (meta-)algorithms.
Locatelli and Carpentier [21] sketch out an aggregation approach inspired by Lepski’s method  while
Valko et al. [24]  Grill et al. [14]  Shang et al. [23] describe cross-validation methods thanks to which
they adapt to the near-optimality dimension with unknown smoothness. As it turns out  this last
approach yields clean results with our smoothness assumptions; we write the details in Appendix E.
Recently  Krishnamurthy et al. [18] studied continuum-armed contextual bandits and use a sophisti-
cated aggregation scheme to derive an algorithm that adapts to the Lipschitz constant when L > 1.

1.2 Contributions and outline

In this paper  we fully compute the cost of adaptation for bandits with Hölder regularity. In Section 2
we discuss the adaptive (and nonadaptive) lower bounds. We take an asymptotic stance in order
to precisely deﬁne the objective of adaptation. Doing so  we uncover a family of noncomparable
lower bounds for adaptive algorithms (Theorem 1)  and deﬁne the corresponding notion of optimality:
admissibility.
Section 3 contains our main contribution: an admissible adaptive algorithm. We ﬁrst recall the CAB1
algorithm  which is nonadaptive minimax  and use it as a building block for our new algorithm
(Subsection 3.1). This algorithm works in a regime-based fashion. Between successive regimes of
doubling lengths  we reset the algorithm and use a new discretization with fewer arms. In order to
carry information between the different stages  we use CAB1 in a clever way: besides partitioning
the arm space  we add summaries of previous regimes by allowing the algorithm to play according to
the empirical distributions of past plays. This is formally described in Subsection 3.2.
A salient difference with all previous approaches is that we zoom out by using fewer and fewer arms.
To our knowledge  this is unique  as all other algorithms for bandits zoom in in a way that crucially
depends on the regularity parameters. Another important feature of our analysis is that we adapt both
to the Hölder exponent ↵ and to the Lipschitz constant L. On a technical level  this is thanks to the
fact that we do not explicitly choose a grid of regularity parameters  which means that we implicitly
handle all values (L  ↵) simultaneously.
We ﬁrst give a regret bound in the known horizon case (Subsection 3.2)  then we provide an anytime
version and we show that they match the lower bounds of adaptation (Subsection 3.3). Finally
Section 4 provides the proof of our main regret bound.

2

2 Setup  preliminary discussion

2.1 Notation and known results
Let us reintroduce brieﬂy the standard bandit terminology. We consider the arm space X = [0  1].
The environment sets a reward function f : X! [0  1]. At each time step t  the player chooses an arm
Xt 2X   and the environment then displays a reward Yt such that E[Yt | Xt] = f (Xt)  independently
from the past. We assume that the variables Yt  f (Xt) are (1/4)-subgaussian conditionnally on Xt;
this is satisﬁed if the payoffs are bounded in [0  1] by Hoeffding’s lemma.
The objective of the player is to ﬁnd a strategy that minimizes her expected cumulative (pseudo-)regret.
If M (f ) denotes the maximum value of f  the regret at time T is deﬁned as

RT = T M (f )  E" TXt=1

Yt# = T M (f )  E" TXt=1

f (Xt)# .

(3)

In this paper  we assume that the function f satisﬁes a Hölder assumption around one of its maxima:
Deﬁnition 1. For ↵> 0 and L > 0  we denote by H(L  ↵) the set of functions that satisfy

9 x? 2 [0  1] s.t. f (x?) = M (f ) and 8 x 2 [0  1]

|f (x?)  f (x)| 6 L|x?  x|↵ .

(4)

We are interested in minimax rates of regret when the mean-payoff function f belongs to these
Hölder-type classes  i.e.  the quantity inf

sup

RT .

algorithms

f2H(L ↵)

MOSS Throughout this paper  we exploit discretization arguments and use a minimax optimal
algorithm for ﬁnite-armed bandits: MOSS  from Audibert and Bubeck [2]. When run for T rounds
on a K-armed bandit problem with (1/4)-subgaussian noise  and when T > K  its regret is upper-
bounded by 18pKT (the improved constant is from Garivier et al. [12]).
Non-adaptive minimax rates When the regularity is given to the player  for any ↵  L and T :

0.001 L1/(2↵+1)T (↵+1)/(2↵+1) 6 inf

algorithms

RT 6 28 L1/(2↵+1)T (↵+1)/(2↵+1) .

(5)

This is well-known since Kleinberg [16]. For completeness  we recall how to derive the upper bound
in Section 3.1  and the lower bound in Section 2.2.

sup

f2H(L ↵)

2.2 Lower bounds: adaptation at usual rates is not possible
Locatelli and Carpentier [21] prove a version of the following theorem; see our reshufﬂed and slightly
improved proof in Appendix F.
Theorem (Variation on Th.3 from [21]). Let B > 0 be a positive number. Let ↵   > 0 and L  ` > 0
be regularity parameters that satify ↵ 6  and L > `.
Assume moreover that 23 12↵B1 6 L 6 `1+↵ T ↵/2 2(1+↵)(82). If an algorithm is such that
supf2H(` ) RT 6 B   then the regret of this algorithm is lower bounded on H(L  ↵):

RT > 210 T L1/(↵+1)B↵/(↵+1) .

(6)

Remark (Bibliographical note). Locatelli and Carpentier [21] consider a more general setting
where additional margin conditions are exploited. In our setting  we slightly improve their result by
dealing with the dependence on the Lipschitz constant  and by removing a requirement on B.
In a different context  Krishnamurthy et al. [18] show a variation of this bound where the Lipschitz
constant is considered  but only in the case where ↵ =  = 1  for ` = 1 and L > 1.
As explained in Locatelli and Carpentier [21] this forbids adaptation at the usual minimax rates over
two regularity classes; we recall how in the paragraph that follows Theorem 1. However this is not
the end of the story  as one naturally wonders what is the best the player can do.
To further investigate this question  we discuss it asymptotically by considering the rates at which the
minimax regret goes to inﬁnity  therefore focusing on the dependence on T . Our main results are
completely nonasymptotic  yet we feel the asymptotic analysis of optimality is clearer.

sup

f2H(L ↵)

3

Deﬁnition 2. Let ✓ : [0  1] ! [0  1] denote a nonincreasing function. We say an algorithm achieves
adaptive rates ✓ if

8 "> 0   8 ↵  L > 0  

lim sup
T!1

supf2H(L ↵) RT

T ✓(↵)+"

< +1 .

We include the " in the deﬁnition in order to neglect the potential logarithmic factors.
As rate functions are not always comparable for pointwise order  the good notion of optimality is the
standard statistical notion of admissibility (akin to “Pareto optimality” for game-theorists).
Deﬁnition 3. A rate function is said to be admissible if it is achieved by some algorithm  and if no
other algorithm achieves stricly smaller rates for pointwise order. An algorithm is admissible if it
achieves an admissible rate function.
We recall that a function ✓0 is stricly smaller than ✓ for pointwise order if ✓0(↵) 6 ✓(↵) for all ↵ and
✓0(↵0) <✓ (↵0) for at least one value of ↵0.
It turns out we can fully characterize the admissible rate functions by inspecting the lower bounds (6).
Theorem 1. The admissible rate functions are exactly the family

✓m : ↵ 7! max✓m   1  m

↵

↵ + 1◆   m 2 [1/2  1] .

(7)

This theorem contains two assertions. The lower bound side states that no smaller rate function may
be achieved by any algorithm. This side is derived from an asymptotic rewording of lower bound (6) 
see Proposition 1 stated below (proofs are in Appendix A). The second statement is that the ✓m’s are
indeed achieved by an algorithm  which is the subject of Section 3.2.
Figure 1 illustrates how these admissible rates compare to each other  and to the usual minimax rates.

Figure 1: The lower bounds on adaptive rates: plots of the admissible rate functions ↵ 7! ✓m(↵). If an
algorithm has regret of order OT ✓(↵)  then ✓ is everywhere above one of these curves.

In particular  we see that reaching the nonadaptive minimax rates for multiple values of ↵ is impossible.
Moreover  at m = ( + 1)/(2 + 1)  we have ✓m() = ( + 1)/(2 + 1)  which is the usual minimax
rate (1) when  is known. This yields an alternative parameterization of the family ✓m: one may
choose to parameterize the functions either by their value at inﬁnity m 2 [1/2  1]  or by the only
point  2 [0  +1] at which they coincide with the usual minimax rates function (1).
Proposition 1. Assume an algorithm achieves adaptive rates ✓. Then ✓ satisﬁes the functional
inequation

8 > 0  

8 ↵ 6  

✓ (↵) > 1  ✓()

4

↵

↵ + 1

.

(8)

2.3 Yet can we adapt in some way?
We have described in (7) the minimal rate functions that are compatible with the lower bounds of
adaptation: no algorithm can enjoy uniformly better rates. Of course  at this point  the next natural
question is whether any of these adaptive rate functions may indeed be reached by an algorithm.
All previous algorithms for continuum-armed bandits require the regularity as an input in some way
(see the literature discussion in Section 1.1). Such algorithms are ﬂawed: if the true regularity is
underestimated then we only recover the guarantees that correspond to the smaller regularity  which
is often far worse than the lower bounds of Theorem 1. More dramatically  if the true regularity is
overestimated  then  a priori  no guarantees hold at all.
We prove that all these rate functions may be achieved by a new algorithm. More precisely  if the
player wishes to reach one of the lower bounds ✓m  she may select a value of the input accordingly
and match the chosen ✓m. This is our main contribution and is described in the next section.

3 An admissible adaptive algorithm and its analysis

We discuss in Section 3.1 how the well-known CAB1 algorithm can be generalized for our purpose.
In Section 3.2 we describe our algorithm and the main upper bound on its regret. Section 3.3 is
devoted to the anytime version of the algorithm and to a discussion on optimality.

3.1 An abstract version of CAB1 as a building block towards adaptation
We describe a generalization of the CAB1 algorithm from Kleinberg [16]  where we include arbitrary
measures in the discretization. Although this extension is straightforward  we detail it as we will use
this algorithm repeatedly further in this paper. In the original CAB1  the space of arms is discretized
into a partition of K subsets  and an algorithm for ﬁnite-armed bandits plays on the K midpoints of
the sets. Auer et al. [3] replace the midpoints by a random point uniformly chosen in the subset.
We introduce a generic version of this algorithm we call CAB1.1. We consider K arbitrary probability
distributions over X   which we denote by (⇡i)16i6K. Denote also by ⇡(f ) the expectation of f (X)
when X ⇠ ⇡. At each time step  the decision maker chooses one distribution  ⇡It  and plays an arm
picked according to that distribution. By the tower rule  she receives a reward such that

E[Yt | It] = E[f (Xt) | It] = ⇡It(f ) .

RT = TM (f )  max

i=1 ... K

As the player uses a ﬁnite-arm algorithm A to select It  the regret she suffers can be decomposed as
the sum of two terms (denoting by ˜RT the expected regret of the ﬁnite-armed algorithm):
(9)

⇡i(f ) + ˜RT⇡i(f )16i6K;A .

This identity is central to the construction of our algorithm. Using terminology from Auer et al. [3] 
the ﬁrst term measures an approximation error of the maximum of f  and the other the actual cost of
learning in the approximate problem. Parameters are chosen to balance these two sources of error.
Algorithm 1 CAB1.1 (Continuum-Armed Bandit  adapted from Kleinberg [16])
1: Input: T the time horizon  K probability measures over X denoted by ⇡1  . . .  ⇡ K  discrete
2: for t = 1  2  . . .   T do
3:
4:
5:
6: end for

Deﬁne It the arm in {1  . . .   K} recommended by A
Play Xt 2X drawn according to ⇡It  and receive Yt such that E[Yt|Xt] = f (Xt)
Give Yt as input to A corresponding to It

K-armed bandit algorithm A

The canonical example is that for which the space of arms is cut into a partition. Denote by Disc(K)
the family of the uniform measures over the intervals [(i  1)/K  i/K] for 1 6 i 6 K. We state this
results (and prove it in Appendix A.1) to recall the non-adaptive minimax bound (1).
Proposition 2. Let ↵> 0 and L > 1/pT be regularity parameters  and deﬁne the number of discrete
arms K? = min⌃L2/(2↵+1)T 1/(2↵+1)⌥   T. Algorithm CAB1.1 run with the uniform discretization
Disc(K?) and A =MOSS enjoys the bound

RT 6 28 L1/(2↵+1) T (↵+1)/(2↵+1) .

sup

f2H(L ↵)

5

3.2 Memorize past plays  Discretize the arm space  and Zoom Out: the MeDZO algorithm

To achieve adaptation  we combine two tricks: going from ﬁne to coarser discretizations while
keeping a summary of past plays in memory.
Our algorithm works in successive regimes. At each time regime i  we reset the algorithm and start
over a new regime of length double the previous one (Ti = 2p+i)  and with fewer discrete arms
(Ki = 2p+2i). While doing this  we keep in memory the previous plays: in addition to the uniform

distributions over the subsets of partitions  we include the empirical measuresb⌫j of the actions played
in the past regimes  for j < i.
Algorithm 2 MeDZO (Memorize  Discretize  Zoom Out)
1: Input: parameter B  time horizon T
2: Set: p = dlog2 Be  Ki = 2p+2i and Ti = 2p+i
3: for i = 1  . . .   p do
4:

For Ti rounds  run algorithm CAB1.1 with the uniform discretization in Ki pieces and the

5:
6: end for

empirical measures of the previous playsb⌫j for j < i; use MOSS as the discrete algorithm.a
Set:b⌫i the empirical measure of the plays during regime i.
aNob⌫ is used for i = 0

Appendix C provides a ﬁgure illustrating the behavior of the algorithm.
Our construction is based on the following remark. Consider the approximation error suffered during
regime i. Denoting the by ⇧i the set of measures given to the player during regime i  that is  the
uniform measures over the regular Ki-partition and the empirical measures of arms played during the
regimes j < i  the approximation error is bounded as follows:

Ti✓M (f )  Ehmax

⇡2⇧i

⇡(f )i◆6 TiM (f )E[b⌫j(f )] =

Ti

Tj Xt2Regime jM (f )E[f (Xt)]

(10)

and this bound is proportional to the regret suffered during regime j. This means that even though we
zoom out by using fewer arms  we can make sure that the average approximation error in regime i is
less than the regret previously suffered. Moreover  the ﬁrst discretizations are ﬁne enough to ensure a
small regret in the ﬁrst regimes  thanks to the Hölder property. This argument is formalized in the
proof (Lemma 1)  and shows that MeDZO maintains a balance between approximation and cost of
learning that yields optimal regret.
A surprising fact here is that we go from ﬁner to coarser discretizations during the different phases.
Thus  paradoxically  the algorithm zooms out as time passes. Note also that although this regime-
based approach is reminiscent of the doubling trick  there is an essential difference in that information
is carried between the regimes via the distribution of the previous plays.
We ﬁrst state our central result  a generic bound that holds for any input parameter B. We discuss the
optimality of these adaptive bounds in the next subsection.
Theorem 2. Algorithm 2 run with the knowledge of T and input B > pT enjoys the following
guarantee: for all ↵> 0 and L > 0 

sup

f2H(L ↵)

RT 6 412 (log2 B)3/2 maxB  T L1/(↵+1)B↵/(↵+1) .

(11)

We provide some illustrative numerical experiments in Appendix D  comparing the results of MeDZO
with other non-adaptive algorithms.

3.3 Discussion: anytime version and admissibility

Anytime version via the doubling trick The dependence of Algorithm 2 on the parameter B
makes it horizon-dependent. We use the doubling trick to build an anytime version of the algorithm.
At each new doubling-trick regime  we input a value of B that depends on the length of the k-th
regime. If it is of length T (k)  one typically thinks of Bk = (T (k))m for some exponent m. In that
case  we get the following bound —see the proof and description of the algorithm in Appendix B.

6

Corollary 1 (Doubling trick version). Choose m 2 [1/2  1]. The doubling-trick version of MeDZO 
run with m as sole input (and without the knowledge of T) ensures that for all regularity parameters
↵> 0 and L > 0 and for T > 1
RT 6 4000(log2 T m)3/2 maxT m  T L1/(↵+1)(T m)↵/(↵+1) = O(log T )3/2 T ✓m(↵).

f2H(L ↵)

sup

Admissibility of Algorithm 2 The next result is a direct consequence of Corollary 1. This echoes
the discussion following Theorem 1  and shows that for any input parameter m  the anytime version
of MeDZO cannot be improved uniformly for all ↵.
Corollary 2. For any m 2 [1/2  1]  the doubling trick version of MeDZO (see App. B) with input m
achieves rate function ✓m  and is therefore admissible.
3.4 About the remaining parameter: the B = pT case
Tuning the value of B amounts to selecting one of the minimal curves in Figure 1. Therefore this
parameter is a feature of the algorithm  as it allows the player to choose between the possible optimal
behaviors. The tuning of this parameter is an unavoidable choice for the player to make.
The next example illustrates well the performance of MeDZO  as it is easily comparable to the usual
minimax bounds. Looking at Figure 1  this choice corresponds to m = 1/2  i.e.  the only choice of
parameter that reaches the usual minimax rates as ↵ ! 1. In other words  if the players wishes to
ensure that her regret on very regular functions is of order pT   then she has to pay a polynomial cost
of adaptation for not knowing ↵ and that price is exactly the ratio between (1) and (2).
Corollary 3. Set a horizon T and run Algorithm 2 with B = pT . Then for ↵> 0 and L > 1/pT  
(12)

RT 6 146 (log2 T )3/2 L1/(↵+1) T (↵+2)/(2↵+2) .

sup

f2H(L ↵)

We bound this quantity thanks to the decomposition (9)  by ﬁrst conditioning on the past up to time
Ti1. Since there are Ki + i discrete actions  the regret bound on MOSS ensures that

This is straightforward from Theorem 2  since the inequality B = pT 6 T L1/(↵+1)pT ↵/(↵+1)
holds whenever L > 1/pT . An anytime version of this result can be obtained from Corollary 1.
4 Proof of Theorem 2
Full proof of Theorem 2. Let Ft = (I1  X1  Y1  . . .   It  Xt  Yt) be the -algebra corresponding to
the information available at the end of round t. Deﬁne also the transition times Ti =Pi
j=1 Tj with
the convention T0 = 0. Let us ﬁrst verify that T is smaller than the total length of the regimes. By
deﬁnition of p  we have B 6 2p < 2B. Thus Tp = 2p+1(2p  1) > 2B(B  1) > B2 > T   and the
algorithm is indeed well-deﬁned up to time T .
t=Ti1+1 E⇥f (Xt)⇤ .
Consider the regret suffered during the i-th regime RTi1 Ti := Ti M (f )PTi
E24
j 2 Disc(Ki)}[b⌫`(f ) | ` = 0  . . .   i 1 . Notice that this bound
(b⌫j)j<i are ﬁxed at time Ti1 + 1 (i.e.  they are FTi1-measurable). Integrating once more  we obtain
Bounding the cost of learning. By deﬁnition of Ki and Ti  we have KiTi = 22p+2 6 16B2 .
Therefore  since p and Ki are integers greater than 1  using a + b  1 6 ab for positive integers 

TiXt=Ti1+1M (f )  f (Xt) FTi135 6 TiM (f )  M ?

i ] + 18p(Ki + i)Ti .
RTi1 Ti 6 TiM (f )  E[M ?
p(Ki + i)Ti 6p(Ki + p  1)Ti 6ppKiTi 6 4ppB .

j (f ) | ⇡(i)
i is a random variable  as the algorithm is completely reset  and the measures

i + 18p(Ki + i)Ti

i = max{⇡(i)
where M ?
holds even though M ?

(13)

(14)

(15)

7

Bounding the approximation error. The key ingredient for this part is the following fact  that
synthetizes the beneﬁts of our construction as hinted in (10) and the surrounding discussion.
Lemma 1. The total approximation error of MeDZO in regime i is controlled by the Hölder bound
on the grid of mesh size 1/Ki  and by the regret suffered during the previous regimes 

TiM (f )  E[M ?

i ] 6 Ti min L

Tj ◆!
j<i ✓ RTj1 Tj

  min

1
K↵
i

(16)

Proof. This derives easily from the construction of the algorithm  i.e.  from the deﬁnition of M ?
i .
Considering an interval in the regular Ki-partition that contains a maximum of f  by the Hölder
property  M (f )  M ?

i . For the second minimum  as described in Eq. (10)  for j < i 

i 6 L/K↵

M (f )  M ?

i 6 M (f ) b⌫j(f ) =

1

Tj

TjXt=Tj1+1M (f )  f (Xt) .

Taking an expectation  RTj1 Tj appears  and we conclude by taking the minimum over j.
Remember that since KiTi = 22p+2  we have L Ti/K↵
. Therefore  the ﬁrst
bound on the approximation error in (16) increases with i  as Ki decreases with i. Denote by i0 the
last time regime i for which

i = L22p+2/K1+↵

i

L

Ti0
K↵
i0

6 B .

(17)

If this is never satisﬁed  i.e.  not even for i = 1  then L2p+1/2↵(p+1) > B which yields  using
B 6 2p 6 2B  that 4LB > 2↵+1B↵B and then L > B↵/2. In that case  L1/(↵+1)B↵/(↵+1) > 1
and the total regret bound (11) is true as it is weaker than the trivial bound RT 6 T .
Hence we may assume that i0 > 1 is well deﬁned. By comparing i to i0  we now show the inequality

B +

i0Xi=1

pXi=i0+1

2(1 + 72pp)Ti L1/(↵+1)B↵/(↵+1) .

TiM (f )  E[M ?
i ] 6

pXi=1
For all i 6 i0 the approximation error is smaller than the ﬁrst argument of the minimum in (16)  and
i ] 6 B . In particular  this together with
this term is smaller than B. Therefore TiM (f )  E[M ?
(14) and (15) implies that the total regret suffered during regime i0 is RTi01 Ti0 6 (1 + 72pp)B.
For the later time regimes i0 < i 6 p  we use the fact that preceding empirical measures were kept as
discrete actions  and in particular the one of the i0-th regime: (16) instantiated with j = i0 yields

(18)

TiM (f )  E[M ?

i ] 6 Ti

RTi01 Ti0

Ti0

61 + 72ppTi

B

Ti0

.

(19)

Solving equations LTi0/K↵
(details are in Appendix A.4). Therefore for i0 < i 6 p  using (19) 

i0 ⇡ B ⇡ 4pTi0Ki0  we get B/Ti0 6 2 L1/(↵+1)B↵/(↵+1)  

and we obtain (18) by summing over i.

TiM (f )  E[M ?

i ] 6 2(1 + 72pp) Ti L1/(↵+1)B↵/(↵+1)  

Conclusion We conclude with some crude boundings. First  as i0 6 p and the sum of the Ti’s is
smaller than T   the total approximation error is less than pB+2(1+72pp)T L1/(↵+1)B↵/(↵+1). Let
us include the cost of learning  which is smaller than 72pppB and conclude  using a + b 6 max(a  b)

RT 6 2(1 + 72pp)T L1/(↵+1)B↵/(↵+1) + pB + 72p3/2B
= 2(1 + 72pp)T L1/(↵+1)B↵/(↵+1) + p(1 + 72pp)B

6⇣2(1 + 72pp) + p1 + 72pp⌘ maxB  T L1/(↵+1)B↵/(↵+1)

from which the desired bound follows  using 1 6 p  and p 6 2 log2 B and 4(1 + 72p2) 6 412.

(20)

8

5 Further considerations
Local regularity assumption Theorem 2 holds under a relaxed smoothness assumption  namely
that the function satisﬁes the Hölder condition only in a small cell containing the maximum. By
looking carefully at the proof  we observe that the condition is only required up to the i0-th epoch
(deﬁned in (17))  at which the size of the cells in the discretization is of order 1/Ki0 ⇡ (LB)1/(1+↵).
Therefore we only need condition (4) to be satisﬁed for points x in an interval of size (LB)1/(1+↵)
around the maximum.

Higher dimension Our results can be generalized to functions [0  1]d ! [0  1] that are k·k1-Hölder.
For MeDZO to be well-deﬁned  take Ki = 2d(p+2i) and Ti = 2d(p+i)  with p ⇡ (log B)/d. The
bounds are similar to their one-dimensional counterparts  up to replacing ↵ by ↵/d in the exponents 
but the constants are deteriorated by a factor that is exponential in d. The bound in Theorem 2
changes to maxB  Ld/(↵+d)T B↵/(↵+d)).

Acknowledgements

We would like to thank Gilles Stoltz and Pascal Massart for their valuable comments and suggestions.

References
[1] Rajeev Agrawal. The Continuum-Armed Bandit Problem. SIAM Journal on Control and Optimization 
33(6):1926–1951  1995. ISSN 0363-0129  1095-7138. doi: 10.1137/S0363012992237273. URL http:
//epubs.siam.org/doi/10.1137/S0363012992237273.

[2] Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic bandits. In

COLT  pages 217–226.

[3] Peter Auer  Ronald Ortner  and Csaba Szepesvári. Improved rates for the stochastic continuum-armed
bandit problem. In International Conference on Computational Learning Theory  pages 454–468. Springer.

[4] Lucien Birgé and Pascal Massart. Estimation of Integral Functionals of a Density. The Annals of Statistics 

23(1):11–29  1995. URL https://doi.org/10.1214/aos/1176324452.

[5] Sébastien Bubeck  Gilles Stoltz  and Jia Yuan Yu. Lipschitz bandits without the Lipschitz constant. In

International Conference on Algorithmic Learning Theory  pages 144–158. Springer.

[6] Sébastien Bubeck  Rémi Munos  and Gilles Stoltz. Pure exploration in ﬁnitely-armed and continuous-
armed bandits. Theoretical Computer Science  412(19):1832–1852  2011. ISSN 03043975. doi: 10.1016/j.
tcs.2010.12.059. URL https://linkinghub.elsevier.com/retrieve/pii/S030439751000767X.

[7] Sébastien Bubeck  Rémi Munos  Gilles Stoltz  and Csaba Szepesvári. X-armed bandits. Journal of Machine

Learning Research  12:1655–1695  2011.

[8] Adam D. Bull. Adaptive-treed bandits. Bernoulli  21(4):2289–2307  2015. doi: 10.3150/14-BEJ644. URL

https://doi.org/10.3150/14-BEJ644.

[9] T. Tony Cai. Minimax and Adaptive Inference in Nonparametric Function Estimation. Statistical Science 

27(1):31–50  2012. doi: 10.1214/11-STS355. URL https://doi.org/10.1214/11-STS355.

[10] T. Tony Cai and Mark G. Low. On adaptive estimation of linear functionals. 33(5):2311–2343. doi:

10.1214/009053605000000633. URL https://doi.org/10.1214/009053605000000633.

[11] Pierre-Arnaud Coquelin and Rémi Munos. Bandit algorithms for tree search. In Uncertainty in Artiﬁcial

Intelligence  2007.

[12] Aurélien Garivier  Hédi Hadiji  Pierre Menard  and Gilles Stoltz. KL-UCB-switch: Optimal regret
bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints. URL
http://arxiv.org/abs/1805.05071.

[13] Aurélien Garivier  Pierre Ménard  and Gilles Stoltz. Explore First  Exploit Next: The True Shape of Regret
in Bandit Problems. Mathematics of Operations Research  2018. ISSN 0364-765X. doi: 10.1287/moor.
2017.0928. URL https://pubsonline.informs.org/doi/10.1287/moor.2017.0928.

9

[14] Jean-Bastien Grill  Michal Valko  and Rémi Munos. Black-box optimization of noisy functions with

unknown smoothness. In Advances in Neural Information Processing Systems  pages 667–675.

[15] Olav Kallenberg. Foundations of modern probability. Springer Science & Business Media  2006.

[16] Robert Kleinberg. Nearly Tight Bounds for the Continuum-armed Bandit Problem. In Proceedings of the

17th International Conference on Neural Information Processing Systems  pages 697–704. MIT Press.

[17] Robert Kleinberg  Aleksandrs Slivkins  and Eli Upfal. Bandits and Experts in Metric Spaces. abs/1312.1277.

URL http://arxiv.org/abs/1312.1277.

[18] Akshay Krishnamurthy  John Langford  Aleksandrs Slivkins  and Chicheng Zhang. Contextual Bandits with
Continuous Actions: Smoothing  Zooming  and Adapting. URL http://arxiv.org/abs/1902.01520.
[19] Tor Lattimore and Csaba Szepesvári. Bandit Algorithms  volume Bubeck  Sébastien and Munos  Rémi and

Stoltz  Gilles and Szepesvári  Csaba. Cambridge University Press (preprint)  2019.

[20] O. Lepskii. On a Problem of Adaptive Estimation in Gaussian White Noise. Theory of Probability
ISSN 0040-585X. doi: 10.1137/1135065. URL https:

& Its Applications  35(3):454–466  1991.
//epubs.siam.org/doi/10.1137/1135065.

[21] Andrea Locatelli and Alexandra Carpentier. Adaptivity to Smoothness in X-armed bandits. In Conference

on Learning Theory  pages 1463–1492.

[22] Pascal Massart. Concentration Inequalities and Model Selection. Ecole d’Eté de Probabilités de Saint-Flour
XXXIII - 2003. Springer Berlin Heidelberg. ISBN 978-3-540-48503-2. URL https://doi.org/10.
1007/978-3-540-48503-2.

[23] Xuedong Shang  Emilie Kaufmann  and Michal Valko. General parallel optimization without a metric. In

Algorithmic Learning Theory  volume 98. URL https://hal.inria.fr/hal-02047225.

[24] Michal Valko  Alexandra Carpentier  and Rémi Munos. Stochastic Simultaneous Optimistic Optimization.

In International Conference on Machine Learning.

10

,Hedi Hadiji