2009,Neurometric function analysis of population codes,The relative merits of different population coding schemes have mostly been analyzed in the framework of stimulus reconstruction using Fisher Information. Here  we consider the case of stimulus discrimination in a two alternative forced choice paradigm and compute neurometric functions in terms of the minimal discrimination error and the Jensen-Shannon information to study neural population codes. We first explore the relationship between minimum discrimination error  Jensen-Shannon Information and Fisher Information and show that the discrimination framework is more informative about the coding accuracy than Fisher Information as it defines an error for any pair of possible stimuli. In particular  it includes Fisher Information as a special case. Second  we use the framework to study population codes of angular variables. Specifically  we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows. That is  for long time window we use the common Gaussian noise approximation. To address the case of short time windows we analyze the Ising model with identical noise correlation structure. In this way  we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding.,Neurometric function analysis of population codes

Philipp Berens  Sebastian Gerwinn  Alexander S. Ecker and Matthias Bethge

Max Planck Institute for Biological Cybernetics

Center for Integrative Neuroscience  University of T¨ubingen

Computational Vision and Neuroscience Group
Spemannstrasse 41  72076  T¨ubingen  Germany

first.last@tuebingen.mpg.de

Abstract

The relative merits of different population coding schemes have mostly been ana-
lyzed in the framework of stimulus reconstruction using Fisher Information. Here 
we consider the case of stimulus discrimination in a two alternative forced choice
paradigm and compute neurometric functions in terms of the minimal discrimina-
tion error and the Jensen-Shannon information to study neural population codes.
We ﬁrst explore the relationship between minimum discrimination error  Jensen-
Shannon Information and Fisher Information and show that the discrimination
framework is more informative about the coding accuracy than Fisher Informa-
tion as it deﬁnes an error for any pair of possible stimuli. In particular  it includes
Fisher Information as a special case. Second  we use the framework to study pop-
ulation codes of angular variables. Speciﬁcally  we assess the impact of different
noise correlations structures on coding accuracy in long versus short decoding
time windows. That is  for long time window we use the common Gaussian noise
approximation. To address the case of short time windows we analyze the Ising
model with identical noise correlation structure. In this way  we provide a new
rigorous framework for assessing the functional consequences of noise correla-
tion structures for the representational accuracy of neural population codes that is
in particular applicable to short-time population coding.

1 Introduction

The relative merits of different population coding schemes have mostly been studied (e.g. [1  12] 
for a review see [2]) in the framework of stimulus reconstruction (ﬁgure 1a)  where the performance
of a code is judged on the basis of the mean squared error E[(θ − ˆθ)2]. That is  if a stimulus θ is
encoded by a population of N neurons with tuning curves fi  we ask how well  on average  can an
estimator reconstruct the true value of the presented stimulus based on the neural responses r  which
were generated by the density p(r|θ). The average reconstruction error can be written as

Eθ r[(θ − ˆθ(r))2] = Eθ[Varˆθ|θ] + Eθ[b2
θ].

Here Varˆθ|θ = Er[(θ − ˆθ(r))2|θ] denotes the error variance and bθ = Er[ˆθ(r)|θ] − θ the bias of the
estimator ˆθ. For the sake of analytical tractability  most studies have employed Fisher Information
(FI) (e.g. [1  12])

(cid:29)

(cid:12)(cid:12)(cid:12)(cid:12) θ

(cid:28)

Jθ =

− ∂2
∂θ2 log p(r|θ)

to bound the conditional error variance Varˆθ|θ of an unbiased estimator from below according to the
Cramer-Rao bound:

Varˆθ|θ ≥ 1
Jθ

.

1

Figure 1: Illustration of the two frameworks for studying population codes. a. In stimulus reconstruction  an
estimator tries to reconstruct the orientation of a stimulus based on a noisy neural response. The quality of a
code is based on the average error of this estimator. b. In stimulus discrimination  an ideal observer needs to
choose one of two possible stimuli based on a noisy neural response (2AFC task). c. A neurometric function
shows the error E as a function of ∆θ  the difference between a reference direction θ1 and a second direction
θ2. This framework is often used in psychophysical studies.

For the comparison of different coding schemes  it is important that an estimator exists which can
actually attain this lower bound. For short time windows and certain types of tuning functions  this
may not always be the case [4]. In particular  it is unclear how different population coding schemes
affect the ﬁdelity with which a population of binary neurons can encode a stimulus variable.

1.1 A new approach for the analysis of population coding

Here we view the population coding problem from a different perspective: We consider the case of
stimulus discrimination in a two alternative forced choice paradigm (2AFC  ﬁgure 1b) with equally
probable stimuli and compute two natural measures of coding accuracy: (1) the minimal discrimina-
tion error E(θ1  θ2) of an ideal observer classifying a stimulus s based on the response distribution as
either being θ1 or θ2 and (2) the Jensen-Shannon information IJS between the response distributions
p(r|θ1) and p(r|θ2). The minimal discrimination error is achieved by the Bayes optimal classiﬁer
ˆθ = argmaxs p(s|r) where s ∈ {θ1  θ2} and the prior distribution p(s) = 1

2. It is given by

(cid:90)

=

1
2

E(θ1  θ2) =

(cid:90)

min (p(s = θ1  r)  p(s = θ2  r)) dr
min (p(r|θ1)  p(r|θ2)) dr

(1)

and the Jensen-Shannon Information [13] is deﬁned as

1
2 DKL [p(r|θ1)(cid:107)p(r)] +

1
2 DKL [p(r|θ2)(cid:107)p(r)]  

IJS(θ1  θ2) =
s∈θ1 θ2 p(s)p(r|s) = 1

where p(r) = (cid:80)
(cid:82) q1(x) log q1(x)

(2)
2(p(r|θ1) + p(r|θ2)) is the arithmetic average between
the two densities  which in our case is the same as the marginal distribution. DKL[q1(cid:107)q2] =
q2(x)dx is the Kullback-Leibler divergence. IJS is an interesting measure of coding
accuracy since it directly measures the mutual information between the neural responses and the
‘class label’  i.e. the stimulus identity. By observing a population response pattern r  the uncertainty
(in terms of entropy) about the stimulus is reduced by
p(r|s) log

MI(r  s) =(cid:88)

dr = IJS 

(cid:80)
p(r|s)
s p(r|s)p(s)

(cid:90)

p(s)

s

with prior distribution as above. In the following  we will restrict our analysis to the special case
of shift-invariant population codes for angular variables and compute neurometric functions E(∆θ)
and IJS(∆θ) (ﬁgure 1c) by setting θ1 = θ and θ2 = θ + ∆θ. In the limit of large populations  the
dependence of these curves on θ can be ignored.

2

aStimulusNeural ResponseStimulus reconstructionbStimulus discriminationcErrorθθ12Figure 2: a. Illustration of equations 5: The entropy H[E] (black) intersects 1 − IJS (grey) at E∗ (dashed).
Because of Fano’s inequality  E > E∗. b. Functional form of the bounds in equations 4 and 5 (black). Our lower
bound is tighter than the lower bound proposed in [13] (grey). c. Illustration of the connections between the
proposed measures of coding accuracy. Minimal discrimination error E(∆θ) (red) is shown as a neurometric
curve as a function of ∆θ and is bounded in terms of the Jensen-Shannon information IJS(∆θ) via equations
4 and 5 (black). Fisher Information links to E via equation 3 and the bounds imposed by IJS (grey). This
approximation is only valid for small ∆θ. The computations have been caried out for a population of N = 50
neurons  with average correlations ¯ρ = .15 and correlation structure as in ﬁgure 3e.

1.2 Computing E and IJS
While the integrals in equation (1) and (2) often cannot be solved  they are relatively easy to evaluate
numerically using Monte-Carlo techniques [10]. For the minimal discrimination error  we use

(cid:90)
M(cid:88)

i=1

E(∆θ) =

1
2
≈ 1
2

min (p(r|θ)  p(r|θ + ∆θ)) dr

(cid:16)

min

p(r(i)|θ)  p(r(i)|θ + ∆θ)

/p(r(i)) 

(cid:17)

where r(i)
2 (p(r|θ) + p(r|θ + ∆θ)). To approximate IJS  we evaluate each DKL term separately as
1

is one of M samples 

(cid:90)

drawn from the mixture distribution p(r) =
p(r|θ) log p(r|θ)
M(cid:88)
p(r)

log p(r(i)|θ) − log p(r(i))

dr

DKL [p(r|θ)(cid:107)p(r)] =

≈ 1
M

i=1

from p(r(i)|θ).

where we draw samples r(i)
We use an analogous expression for
DKL [p(r|θ + ∆θ)(cid:107)p(r)] and plug these estimates into equation 2. This scheme provides consis-
tent estimates of the desired quantities. For all simulations below we used M = 105 samples.

2 Links between the proposed measures
In this section  we link the Fisher Information Jθ of a population code p(r|θ) to the minimum dis-
crimination error E(∆θ) and the Jensen-Shannon Information IJS(∆θ) in the 2AFC paradigm. First 
we link Fisher Information to Jensen-Shannon information IJS. Second  we bound the minimum dis-
crimination error in terms of the Jensen-Shannon information.

2.1 From Fisher Information to Jensen-Shannon Information
In order to obtain a relationship between IJS and Fisher Information  we use an expression already
derived in [7]  where p(r|θ + ∆θ) is expanded up to second order in ∆θ  which yields:

IJS(∆θ) ≈ 1
8

(∆θ)2Jθ.

3

(3)

00.20.40.60.81H[E] (bits)00.20.40.60.810246810  MDEUpper/Lower BoundUpper/LowerBound (FI)00.10.30.5Error00.10.30.5Error00.20.40.60.81abc∆ θ (deg)JS InformationErrorUpper/Lower Bound (equ. 4  5)Lin’s lower boundFigure 3: Illustration of the model. Tuning functions: a. Cosine-type tuning functions with rates between 5
and 50 Hz. b. Box-like tuning function with matched minimal and maximal ﬁring rates. Cosine tuning function
resembles the orientation tuning functions of many cortical neurons. They are characterized by approximately
constant Fisher Information independent of the stimulus orientation. Box-like tuning functions  in contrast 
have non-constant Fisher Information due to their steep non-linearity. They have been shown to exhibit superior
performance over cosine-like tuning functions with respect to the mean squared error [4]. Correlation matrices:
c. stimulus-independent  no limited range (SI  α = ∞)   d. stimulus-independent  limited range (SI  α = 2) 
e. stimulus-dependent  no limited range (SD  α = ∞)  f. stimulus-dependent  limited range (SD  α = 2)

Therefore  Fisher Information provides a good approximation of the Jensen-Shannon Information
for sufﬁciently small ∆θ.

2.2 From Jensen-Shannon Information to Minimal Discrimination Error
The minimal discrimination error E(∆θ) of an ideal observer is bounded from above and below in
terms of IJS(∆θ). An upper bound derived by [13] is given by

E(∆θ) ≤ 1
2

− 1
2

IJS(∆θ).

(4)

Next  we derive a new lower bound on E  which is tighter than a bound derived by Lin [13]. To this
end  we observe that from Fano’s inequality [8] it follows that

H [E] ≥ H[s|r] − E log(|s| − 1)
H[s|r]
H[s] − MI[r  s]
1 − IJS(∆θ) 

=
=
=

(5)

where H[E] is the entropy of a Bernoulli distribution with p = E. The equality from ﬁrst to second
line follows as the number of stimuli or classes |s| = 2. Since the entropy is monotonic in E on the
interval [0  0.5]  we have the lower bound E ≥ E∗  where E∗ is chosen such that equality holds. For
an illustration  see ﬁgure 2a. The shape of both bounds  as well as Lin’s lower bound  are illustrated
in ﬁgure 2b.
In ﬁgure 2c we show the minimal discrimination error for a population code (red) together with the
upper and lower bound (black) obtained by inserting IJS(∆θ) into equations 4 and 5. Both bounds
follow nicely the neurometric function E(∆θ). For comparison  we also show the upper and lower
bound obtained by plugging Fisher Information into equation 3 and computing the bounds 4 and 5
based on this approximation of IJS(∆θ) (grey). Clearly  the approximation is valid for small ∆θ and
becomes successively worse for large ones.

4

01020304050Response (Hz)dacef05010015001020304050Stimulus orientation (deg)Response (Hz)bFigure 4: Comparison of box-like (red) vs. cosine (black) tuning functions in short-term population codes of a.
N = 10 b. N = 50 c. N = 250 independent neurons. Although box-like tuning functions are much broader
than cosine tuning functions  Ebox lies usually below Ecos. For the cosine case  FI (dashed  approximation as in
ﬁgure 2c and Ed(cid:48) (grey) provide accurate accounts of coding accuracy. In contrast  FI grossly overestimates the
discrimination error for box-like tuning functions in small and medium sized populations. In this case  Ed(cid:48) is
only a good approximation of E in the range where ∆θ is small (dark red). Beyond this point  it underestimates
E (a b). For N = 250  bounds are not shown for clarity but they capture the true beaviour of E better than in
ﬁgure 4a and b.

2.3 Previous work

Only a small number of studies on neural population coding have used other measures than Fisher
Information [18  3  6  4]. Two approaches are most closely related to ours: Snippe and Koenderink
[18] and Averbeck and Lee [3] used a measure analogous to the sensitivity index d(cid:48)

(d(cid:48))2 = ∆µΣ−1∆µ
∆µ := f(θ + ∆θ) − f(θ)

(6)

as a measure of coding accuracy. While Snippe and Koenderink have considered only the limit
∆θ → 0  Averbeck and Lee evaluated equation 6 for ﬁnite ∆θ using Σ = 1
2(Σθ + Σθ+∆θ) and
converted d(cid:48) to a discrimination error Ed(cid:48) = 1 − erf(d(cid:48)/2). This approximation is exact only if the
class conditional distribution p(r|θ) is Gaussian with ﬁxed covariance Σθ = Σ for all ∆θ. In that
particular case  the entire neurometric function is fully determined by the Fisher Information [9]:

d(cid:48) = (∆θ)(cid:112)

Jθ = (∆θ) Jmean

Jmean is the linear part of the Fisher Information (cf. equation 7). In the general case  it is not obvious
what aspects of the quality of a population code are captured by the above measure. Therefore  both
Fisher Information and the class-conditional second-order approximation used by Averbeck and
Lee have shortcomings: The latter does not account for information originating from changes in
the covariance matrix as is quantiﬁed by Jcov (cf. equation 7). Fisher Information  on the other
hand  can be quite uninformative about the coding accuracy of the population  especially when the
tuning functions are highly nonlinear (see ﬁgure 3) or noise is large  as in these cases it is not certain
whether the Cramer-Rao bound can actually be attained [4]. The examples studied in the next
section demonstrate how these shortcomings can be overcome using the minimal discrimination
error (equation 1).

3 Results

After describing the population model used in this study  we will illustrate in a simple example  how
our proposed framework is more informative than previous approaches. Second  we will investigate
how different noise correlations structures impact population coding on different timescales.

3.1 The population model

In this section  we describe in detail the population model used in the remainder of the study. To
facilitate comparability  we closely follow the model used in a recent study by Josic et al. [12]

5

abc02040608000.20.40.6∆ θ (deg)Error02040608000.20.40.6∆ θ (deg)Error02040608000.20.40.6∆ θ (deg)ErrorCosineCosine FI boundBoxBox FI boundBox d’Cosine d’where applicable. We consider a population of N neurons tuned to orientation  where the ﬁring rate
of neuron i follows an average tuning proﬁle fi(θ) with (a) a cosine-like shape

fi(θ) = λ1 + λ2ak(θ − φi)
with k = 1 in section 3.2 and k = 6 in section 3.3 and a(φ) = 1

(cid:16)|cos(θ − φi)| 1

fi(θ) =

2(1 + cos(φ)) or (b) a box-like shape

j · sgn cos(θ − φi) + 1

+ λ1.

(cid:17) · λ2

2

Here  φi is the preferred orientation of neuron i and we use j = 12. We consider two scenarios:

1. Long-term coding: r(θ) ∼ N (f(θ)  Σ(θ))  where the trial-to-trial ﬂuctuations are assumed

to be normally distributed with mean f(θ) and covariance matrix Σ(θ).

2. Short-term coding: r(θ) ∼ I (f(θ)  Σ(θ))  where ri ∈ {0  1} and I(µ  Σ) is the maximum
entropy distribution consistent with the constraints provided by µ and Σ  the Ising model
[16]. That is  for short-term population coding  we assume the population acitivity to be
binary with each neuron either emitting one spike or none. The parameters of the Ising
model were computed using gradient descent on the log likelihood.

δijvi(θ) + (1 − δij)ρij(θ)(cid:112)vi(θ)vj(θ)  where vi(θ) is the variance of cell i and ρij(θ) the cor-

[12]  we model the stimulus-dependent covariance matrix as Σij(θ) =

Following Josic et al.

relation coefﬁcient. For long-term coding  we set vi(θ) = fi(θ) and for short-term coding  we
set vi(θ) = fi(θ)(1 − fi(θ)). We allow for both stimulus and spatial inﬂuences on ρ by set-
ting ρij(θ) = σij(θ)c(φi − φj)  where φi is the preferred orientation of neuron i. The func-
tion s models the inﬂuence of the stimulus  while the function c models the spatial component
of the correlation structure. We use σij(θ) = σi(θ)σj(θ)  where σi(θ) = κ1 + κ2a2(θ). We set
c(φi − φj) = C exp (−|φi − φj|/α)  where α controls the length of the spatial decay. To obtain a
desired mean level of correlation ¯ρ  we use the method described in [12].

3.2 Minimum discrimination error is more informative than Fisher Information

As has been pointed out in [4]  the shape of unimodal tuning functions can strongly inﬂuence the
coding accuracy of population codes of angular variables. In particular  box-like tuning functions
can be superior to cosine tuning functions. However  numerical evaluation of the minimum mean
squared error for angular variables is much more difﬁcult than the evaluation of the minimal dis-
crimination error proposed here  and the above claim has only been veriﬁed up to N = 20 neurons.
Here we compute the full neurometric functions for N = 10  50  250 binary neurons (ﬁgure 4). In
this way  we show that the advantage of box-like tuning functions also holds for large numbers of
neurons (compare red and black curves in ﬁgure 4 a-c). In addition  we note that Fisher Information
does not provide an accurate account of the performance of box-like tuning functions: it fails as soon
as the nonlinearity in the tuning functions becomes effective and overestimates the true minimal
discrimination error E. Similarly  the approximate neurometric functions Ed(cid:48)(∆θ) obtained from
equation 6 do not capture the shape of neurometric functions E(∆θ) but underestimate the minimal
discrimination error. In contrast  the deviation between both curves stays rather small for cosine
tuning functions.

3.3 Stimulus-dependent correlations have opposite effects for long- and short-term

population coding

The shape of the noise covariance matrix Σθ can strongly inﬂuence the coding ﬁdelity of a neural
population. In order to evaluate these effects it is important to take differences in the noise covariance
for different stimuli into account. In this section  we will use our new framework to study different
noise correlation structures for short- and long-term population coding.
Previous studies so far have investigated the effect of noise correlations in the long-term case: Most
studies assumed p(r|θ) to follow a multivariate Gaussian distribution  so that ﬁring rates r|θ ∼
N (f(θ)  Σ(θ)) (for detailed description of the population model see section 3.1). In this case  the

6

Figure 5: Neurometric functions E(∆θ) (a-c) and IJS(∆θ) (d-f) for four different noise correlation structures.
a. and d. Large population (N = 100) and long-term coding. b. and e. Medium sized population (N = 15)
and long-term coding. The inset is a magniﬁcation for clarity. c. and f. Medium sized population (N = 15)
and short-term coding. The impact of stimulus-dependent noise correlations in the absence of limited range
correlations changes from b/e to c/f (red line). While they are beneﬁcial in long-term coding  they are beneﬁcial
in short-term coding only for close angles. The exact point of this transition is not the same for E and IJS  since
they are only related via the bounds described in section 2.2. Note that the scale of the x-axis varies.

FI of the population takes a particularly simple form. It can be decomposed into:

Jmean = f(cid:48)(cid:62)Σ−1f(cid:48)

 

Jθ = Jmean + Jcov
1
2

Jcov =

Tr[Σ(cid:48)Σ−1Σ(cid:48)Σ−1] 

(7)

where we omit the dependence on θ for clarity and f(cid:48)
  Σ(cid:48) are the derivatives of f and Σ with respect
to θ. Jmean  Jcov are the Fisher information  when either only the mean or only the covariance are
assumed to depend on θ. For this case  various studies have investigated noise structures where
correlations were either uniform across the population (ﬁgure 3c) or their magnitude decayed with
difference in preferred orientations (ﬁgure 3d)  ‘limited range structure’ or ‘spatial decay’  see e.g.
[1]). Only recently have stimulus-dependent correlations been analyzed in terms of Fisher informa-
tion [12]. Josic et al. ﬁnd that in the absence of limited range correlations  stimulus-dependent noise
correlations (ﬁgure 3e) are beneﬁcial for a population code  while in their presence (ﬁgure 3f)  they
are detrimental.
We ﬁrst compute the neurometric functions E(∆θ) and IJS(∆θ) for a population of 100 neurons
in the case of long-term coding with a Gaussian noise model for the four possible noise correlation
structures (ﬁgure 5a). We corroborate the results of Josic et al. in that we ﬁnd that the lowest E or the
highest IJS is achieved for a population with stimulus-dependent noise correlations and no limited
range structure  while a population with stimulus-dependent noise correlations in the presence of
spatial decay performs worst. Spatially uniform correlations (ﬁgure 3c) provide almost as good a
code as the best coding scheme.

7

05010000.20.40.60.81∆ θ (deg)0102000.20.40.60.81∆ θ (deg)051000.20.40.60.81∆ θ (deg)Information (bits)05010000.10.20.30.40.5 0102000.10.20.30.40.5051000.10.20.30.40.5 Error  SI α=infSI α=1SD α=infSD α=1badefc 789100.050.10.15 Next  we directly compare long- and short-term population coding in a population of 15 neurons1.
For short-term coding  we assume that the population activity is of binary nature  i.e. each neuron
spikes at most once. Again  we compute neurometric functions E(∆θ) and IJS(∆θ) for all four
possible correlation structures. The results for long-term coding do not differ between large and
small populations (ﬁgure 5b)  although relative differences between different coding schemes are
less prominent. In contrast  we ﬁnd that the beneﬁcial impact of stimulus-dependent correlations in
the absence of limited range structure reverses in short-term codes for large ∆θ (ﬁgure 5c).

4 Discussion

In this paper  we introduce the computation of neurometric functions as a new framework for study-
ing the representational accuracy of neural population codes. Importantly  it allows for a rigorous
treatment of nonlinear population codes (e.g. box-like tuning functions) and noise correlations for
non-Gaussian noise models. This is particularly important for binary population codes on timescales
where neurons ﬁre at most one spike. Such codes are of special interest since psychophysical ex-
periments have demonstrated that efﬁcient computations can be performed in cortex on short time
scales [19]. Previous studies have mostly focussed on long-term population codes  since in this case
it is possible to study many question analytically using Fisher Information. Although the structure
of neural population acitivity on short timescales has recently attracted much interest [16  17  15] 
population codes for binary population activity and  in particular  the impact of different noise corre-
lation structures on such codes are not well understood. In contrast to previous work [14]  neuromet-
ric function analysis allows for a comprehensive treatment of both short- and long-term population
codes in a single framework. In section 3.3  we have started to study population codes on short
timescales and found important differences in the effect of noise correlations between short- and
long-term population codes. In the future  we will extend these results to much larger populations
adapting new techniques for approximate ﬁtting of Ising models [15].
The example discussed in section 3.2 demonstrates that neurometric functions can provide addi-
tional information compared to Fisher Information: While Fisher Information is a single number for
each potential population code  neurometric functions in terms of E or IJS assess the coding quality
for each pair of stimuli. This also enables us to detect effects like the dependence of the relative
performance of different population codes on ∆θ as shown in ﬁgure 5 c and f. We can furthermore
easily extend the framework to take unequal prior probabilities into account. In equations 1 and 2
2. Both E and IJS  however  are also
we have assumed equal prior probabilities p(θ1) = p(θ2) = 1
well deﬁned if this is not the case.
The framework of stimulus discrimination in a 2AFC task has long been used in psychophysical and
neurophysiological studies for measuring the accuracy of orientation coding in the visual system
(e.g. [5  21]). It is therefore appealing to use the same framework also in theoretical investigations
on neural population coding since this facilitates the comparison with experimental data. Further-
more  it allows studying population codes for categorial variables since  in contrast to Fisher Infor-
mation  it does not require the variable of interest to be continuous. This is of advantage  as many
neurophysiological studies investigate the encoding of categories  such as objects [11] or numbers
[20].

Acknowledgments

We thank A. Tolias and J. Cotton for discussions. This work has been supported by the Bernstein
award to MB (BMBF; FKZ: 01GQ0601) and a scholarship of the German National academic foun-
dation to PB.

1We are limited in the number of neurons as ﬁtting the required Ising model is computationally very expen-

sive. For the present purpose  we chose N = 15  which is sufﬁcient to demonstrate our point.

8

References
[1] L. F. Abbott and Peter Dayan. The effect of correlated variability on the accuracy of a popula-

tion code. Neural Comp.  11(1):91–101  1999.

[2] B. B. Averbeck  P. E. Latham  and A. Pouget. Neural correlations  population coding and

computation. Nat Rev Neurosci  7(5):358–366  2006.

[3] B. B. Averbeck and D. Lee. Effects of noise correlations on information encoding and decod-

ing. J Neurophysiol  95(6):3633–3644  2006.

[4] M. Bethge  D. Rotermund  and K. Pawelzik. Optimal Short-Term population coding: When

ﬁsher information fails. Neural Comp.  14(10):2317–2351  2002.

[5] A. Bradley  B. C. Skottun  I. Ohzawa  G. Sclar  and R. D. Freeman. Visual orientation and
spatial frequency discrimination: a comparison of single neurons and behavior. J Neurophysiol 
57(3):755–772  1987.

[6] N. Brunel and J. P. Nadal. Mutual information  ﬁsher information  and population coding.

Neural Computation  10(7):1731–1757  1998.

[7] M. Casas  P. W. Lamberti  A. Plastino  and A. R. Plastino. Jensen-Shannon divergence  ﬁsher

information  and wootters’ hypothesis. Arxiv preprint quant-ph/0407147  2004.

[8] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience  2006.
[9] P. Dayan and L. F. Abbott. Theoretical neuroscience: Computational and mathematical mod-

eling of neural systems. MIT Press  2001.

[10] J.R. Hershey and P.A. Olsen. Approximating the kullback leibler divergence between gaus-
sian mixture models. In Acoustics  Speech and Signal Processing  2007. ICASSP 2007. IEEE
International Conference on  volume 4  pages IV–317–IV–320  2007.

[11] C. P. Hung  G. Kreiman  T. Poggio  and J. J. DiCarlo. Fast readout of object identity from

macaque inferior temporal cortex. Science  310(5749):863–866  2005.

[12] K. Josic  E. Shea-Brown  B. Doiron  and J. de la Rocha. Stimulus-dependent correlations and

population codes. Neural Computation  21(10):2774–2804  2009.

[13] J. Lin. Divergence measures based on the shannon entropy. Information Theory  IEEE Trans-

actions on  37(1):145–151  1991.

[14] S. Panzeri  A. Treves  S. Schultz  and E. T. Rolls. On decoding the responses of a population

of neurons from short time windows. Neural Computation  11(7):1553–1577  1999.

[15] Y. Roudi  J. Tyrcha  and J. Hertz. The ising model for neural data: Model quality and ap-
proximate methods for extracting functional connectivity. Phys. Rev. E  79:051915  February
2009.

[16] E. Schneidman  M. J. Berry  R. Segev  and W. Bialek. Weak pairwise correlations imply
strongly correlated network states in a neural population. Nature  440(7087):1007–1012  2006.
[17] J. Shlens  G. D. Field  J. L. Gauthier  M. Greschner  A. Sher  A. M. Litke  and E. J.
Chichilnisky. The structure of Large-Scale synchronized ﬁring in primate retina. Journal
of Neuroscience  29(15):5022  2009.

[18] H. Snippe and J. Koenderink.

Information in channel-coded systems: correlated receivers.

Biological Cybernetics  67(2):183–190  June 1992.

[19] S. Thorpe  D. Fize  and C. Marlot. Speed of processing in the human visual system. Nature 

381(6582):520–522  1996.

[20] O. Tudusciuc and A. Nieder. Neuronal population coding of continuous and discrete quantity
in the primate posterior parietal cortex. Proceedings of the National Academy of Sciences of
the United States of America  104(36):14513–8  2007.

[21] P. Vazquez  M. Cano  and C. Acuna. Discrimination of line orientation in humans and mon-

keys. J Neurophysiol  83(5):2639–2648  2000.

9

,Simon Du
Yining Wang
Xiyu Zhai
Sivaraman Balakrishnan
Russ Salakhutdinov
Aarti Singh