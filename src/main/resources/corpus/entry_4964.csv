2019,Semi-flat minima and saddle points by embedding neural networks to overparameterization,We theoretically study the landscape of the training error for neural networks in overparameterized cases.  We consider three basic methods for embedding a network into a wider one with more hidden units  and discuss whether a minimum point of the narrower network gives a minimum or saddle point of the wider one. Our results show that the networks with smooth and ReLU activation have different partially flat landscapes around the embedded point.  We also relate these results to a difference of their generalization abilities in overparameterized realization.,Semi-ﬂat minima and saddle points by embedding

neural networks to overparameterization

Kenji Fukumizu† ‡

Shoichiro Yamaguchi‡

Yoh-ichi Mototake†

Mirai Tanaka†

†The Institute of Statistical Mathematics

Tachikawa  Tokyo 190-8562  Japan

{fukumizu  mototake  mirai}@ism.ac.jp

‡Preferred Networks  Inc.

Chiyoda-ku  Tokyo 100-0004  Japan

guguchi@preferred.jp

Abstract

We theoretically study the landscape of the training error for neural networks
in overparameterized cases. We consider three basic methods for embedding a
network into a wider one with more hidden units  and discuss whether a minimum
point of the narrower network gives a minimum or saddle point of the wider one.
Our results show that the networks with smooth and ReLU activation have different
partially ﬂat landscapes around the embedded point. We also relate these results to
a difference of their generalization abilities in overparameterized realization.

1

Introduction

Deep neural networks (DNNs) have been applied to many problems with remarkable successes. On
the theoretical understanding of DNNs  however  many problems are still unsolved. Among others 
local minima are important issues on learning of DNNs; existence of many local minima is naturally
expected by its strong nonlinearity  while people also observe that  with a large network and the
stochastic gradient descent  training of DNNs may avoid this issue [8  9]. For a better understanding
of learning  it is essential to clarify the landscape of the training error.
This paper focuses on the error landscape in overparameterized situations  where the number of units
is surplus to realize a function. This naturally occurs when a large network architecture is employed 
and has been recently discussed in connection to optimization and generalization of neural networks
([14  2  1] to list a few). To formulate overparameterization rigorously  this paper introduces three
basic methods  unit replication  inactive units  and inactive propagation  for embedding a network to
a network of more units in some layer. We investigate especially the landscape of the training error
around the embedded point  when we embed a minimizer of the error for a smaller model.
A relevant topic to this paper is ﬂat minima [6  7]  which attract much attention in literature. Such
ﬂatness of minima is often observed empirically  and is connected to generalization performance [3  8].
There are also some works on how to deﬁne ﬂatness appropriately and its relations to generalization
[15  17]. Different from these works  this paper shows some embeddings cause semi-ﬂat minima 
at which a lower dimensional afﬁne subset in the parameter space gives a constant value of error
(see Sec. A). We will also discuss difference between smooth activation and Rectiﬁed Linear Unit
(ReLU); at a semi-ﬂat minimum obtained by embedding a network of zero training error  the ReLU
networks have more ﬂat directions. Using PAC-Bayes arguments [11]  we relate this to the difference
of generalization bounds between ReLU and smooth networks in overparameterized situations.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

This paper extends [4]  in which the three embedding methods are discussed and some conditions on
minimum points are shown. However  the paper is limited to three-layer networks of smooth activation
with one-dimensional output  and addition of one hidden unit is discussed. The current paper covers
a much more general class of networks including ReLU activation and arbitrary number of layers 
and discusses the difference based on the activation functions as well as a link to generalization.
The main contributions of this paper are summarized as follows.
• Three methods of embedding are introduced for the general J-layer networks as basic construction
• For smooth activation  the unit replication method embeds a minimum to a saddle point under
• It is shown theoretically that  for ReLU activation  a minimum is always embedded as a minimum
by the method of inactive units. The surplus parameters correspond to a ﬂat subset of the training
error (Theorem 9). The unit replication gives only saddles under mild conditions (Theorem 10).
• When a network attains zero training error  the embedding by inactive units gives semi-ﬂat minima
in both activation models. The ReLU networks give ﬂatter minima in the overparameterized
realization  which suggests better generalization through the PAC-Bayes bounds (Sec. 5.2).

of overparameterized realization of a function (Sec. 2).

some assumptions (Theorem 5).

All the proofs of the technical results are given in Supplements.

2 Neural network and its embedding to a wider model

i = ϕ(zq−1; wq

We discuss J layer  fully connected neural networks that have an activation function ϕ(z; w)  where
z is the input to a unit and w is a parameter vector. The output of the i-th unit U q
in the q-th
layer is recursively deﬁned by zq
i and the
(q − 1)-th layer. The activation function ϕ(z; w) is any nonlinear function  which often takes the
wgtz − wbias) with w = (wwgt  wbias); typical examples are the sigmoidal function
form ϕ(wT
wgtz − wbias  0}. This paper
ϕ(z; w) = tanh(wT
assumes that there is w(0) such that ϕ(x; w(0)) = 0 for any x. Focusing the q-th layer  with size of
the other layers ﬁxed  the set of networks having H units in the q-th layer is denoted by NH. With a
parameter θ(H) = (W0  w1  . . .   wH   v1  . . .   vH   V0)  the function f (H)

wgtz − wbias) and ReLU ϕ(z; w) = max{wT

i is the weight between U q

θ(H) of NH is deﬁned by

i )  where wq

i

θ(H) (x) := f (H)(x; θ(H)) = ψ(cid:0)(cid:80)H

f (H)

(cid:1) 

j=1vjϕ(x; wj  W0); V0

(1)
where ϕ(x; wj  W0) is the output of U q
i with a summarized parameter W0 in the previous layers  and
ψ(zq+1; V0) is all the parts after zq+1 with parameter V0. Note that vj is a connection weight from
the unit U q
j to the units in the (q + 1)-th layer (we omit the bias term for simplicity). The number of
units in the (q − 1)-th and (q + 1)-th layers are denoted by D and M  respectively.
Embedding of a network refers to a map associating a narrower network in NH0 (H0 < H) with a
network of a speciﬁc parameter in a wider model NH to realize the same function  keeping other
layers unchanged. For clarity  we use (ζi  ui) instead of (vj  wj) for the parameter θ(H0) of NH0;
(2)

θ(H0)(x) := f (H0)(x; θ(H0)) = ψ(cid:0)(cid:80)H0

i=1ζiϕ(x; ui  W0); V0

(cid:1).

f (H0)

We consider minima and stationary points of the empirical risk (or training error)

(3)
where (cid:96)(y  f ) is a loss function to measure the discrepancy between a teacher y and network output
f  and (x1  y1)  . . .   (xn  yn) are given training data. Typical examples of (cid:96)(y  f ) include the square
error (cid:107)y − f(cid:107)2/2 and logistic loss −y log f − (1 − y) log(1 − f ) for y ∈ {0  1} and f ∈ (0  1). In
the sequel  we assume the second order differentiability of (cid:96)(y  f ) with respect to f for each y.

ν=1(cid:96)(yν  f (H)(xν; θ(H))) 

LH (θ(H)) :=(cid:80)n

2.1 Three embedding methods of a network

θ(H0 ) into NH
To fomulate overparameterization  we introduce three basic methods for embedding f (H0)
so that it realizes exactly the same function as f (H0)
θ(H0). See Table 1 and Figure 1 for the deﬁnitions.
(I) Unit replication: We ﬁx a unit  say the H0-th unit U q
  in NH0  and replicate it. Simply  θ(H)
has H − H0 + 1 copies of uH0  and divides the weight ζH0 by vH0  . . .   vH  keeping the other

H0

2

Figure 1: Embedding of a narrower network to a wider one.

Unit replication Πrepl(θ(H0))
wi = ui (1 ≤ i ≤ H0 − 1)
vi = ζi (1 ≤ i ≤ H0 − 1)
wH0 = ··· = wH = uH0
vH0 + ··· + vH = ζH0

Inactive units Πiu(θ(H0))
wi = ui (1 ≤ i ≤ H0)
vi = ζi (1 ≤ i ≤ H0)
wH0+1 = ··· = wH = w(o) wH0+1  . . .   wH: arbitrary
vH0+1  . . .   vH: arbitrary

Inactive propagation Πip(θ(H0))
wi = ui (1 ≤ i ≤ H0)
vi = ζi (1 ≤ i ≤ H0)
vH0+1 = ··· = vH = 0

Table 1: Three methods of embedding

parts unchanged. A choice of ui (1 ≤ i ≤ H0) to replicate is arbitrary  and a different choice
deﬁnes a different network. We use uH0 for simplicity. The parameters vH0  . . .   vH consist of an
(H − H0) × M dimensional afﬁne subspace  denoted by Πrepl(θ(H0))  in the parameters for NH.
(II) Inactive units: This embedding uses the special weight w(0) to make the surplus units inactive.
The set of parameters is denoted by Πiu(θ(H0))  which is of (H − H0) × M dimension.
(III) Inactive propagation: This embedding cuts off the weights to the (q + 1)-th layer for the
surplus part. The weights wj of the surplus units are arbitrary. The set of parameters is denoted by
Πip(θ(H0))  which is of (H − H0) × D dimension.
All the above embeddings give the same function as the narrower network.
Proposition 1. For any θ(H) ∈ Πrepl(θ(H0)) ∪ Πiu(θ(H0)) ∪ Πip(θ(H0))  we have f (H)
θ(H) = f (H0)
θ(H0).
It is important to note that a network is not uniquely embedded in a wider model  in contrast to ﬁxed
bases models such as the polynomial model. This unidentiﬁability has been clariﬁed for three-layer
networks [10  16]; in fact  for three layer networks of tanh activation  [16] shows that the three
methods essentially cover all possible embedding. For three-layer networks of 1-dimensional output
and smooth activation  [4] shows that this unidentiﬁable embedding causes minima or saddle points.
The current paper extends this result to general networks with ReLU as well as smooth activation.

3 Embedding of smooth networks

This section assumes the second order differentiability of ϕ(x; w) on w. The case of ReLU will be
discussed in Sec. 4. Let θ(H0)
∂θ(H0 ) = 0. We are interested
in whether the embedding in Sec. 2 also gives a stationary point of LH. More importantly  we wish
to know if a minimum of LH0 is embedded to a minimum of LH. A network can be embedded by
any combination of the three methods  but we consider their effects separately for simplicity. The
deﬁnition of minimum  saddle point  and related notions are given by Sec. A.

be a stationary point of LH0  i.e.  ∂LH0 (θ(H0)

∗

∗

)

3.1 Stationary properties of embedding

To discuss the stationarity for the case (I) unit replication  we need to restrict Πrepl(θ(H0)) to a subset.
For θ(H0)  deﬁne θ(H)

for every λ = (λH0  . . .   λH ) ∈ RH−H0+1 with(cid:80)H

λj = 1 by

j=H0

λ
wi = ui 
wH0 = ··· = wH = uH0  

vi = ζi

(1 ≤ i ≤ H0 − 1) 

λ ∈ Πrepl(θ(H0)) so that f (H)(x; θ(H)

(4)
vj = λjζH0
Obviously  θ(H)
λ ) = f (H0)(x; θ(H0)). The next theorem tells
that a stationary point of NH0 is embedded to an (H − H0)-dimensional stationary subset of NH.
(cid:80)H
Theorem 2. Let θ(H0)
be a stationary point of LH0. Then  for any λ = (λH0   . . .   λH ) with

deﬁned by Eq. (4) is a stationary point of LH.

λj = 1  the point θ(H)

∗

λ

j=H0

(H0 ≤ j ≤ H).

3

(cid:3037)((cid:3042))(cid:3037)(cid:2868)Embedding(cid:3037)(cid:3037)(cid:3009)(cid:3116)(cid:3009)(cid:3116)(cid:3037)(cid:3009)(cid:3116)(cid:3037)(I) Unit replication(II) Inactive units(III) Inactive propagationThe basic idea for the proof is to separate the subset of parameters (vH0   wH0  . . .   vH   wH ) into
a copy of (ζH0  uH0) and the remaining ones  the latter of which do not contribute to change the
function f (H)
It is easy to see that the inactive units or propagations does not generally embed a stationary point to
a stationary one (see also Theorems 2 and 4 in [4]). The details will be given in Sec. C.

λ . We will see this reparameterization in Sec. 3.2 in detail.

θ(H) at θ(H)

3.2 Embedding of a minimum point in the case of smooth networks

λ

∗

of a mininum point θ(H0)

NH : f (H)(x; θ(H)) =(cid:80)H

We next consider the embedding θ(H)
of LH0. In the sequel  for readability 
we discuss three-layer models (J = 3) and linear output units. Note however that  for general J  the
derivatives and Hessian of LH for the other parameters are exactly the same as those of LH0 for the
corresponding parameters. We omit the full description here. The two models are simply given by
i=1ζiϕ(x; ui).
(5)
To simplify the Hessian for unit replication  we introduce a new parameterization of NH. Let
λ ∈ RH−H0+1 be ﬁxed such that λH0 +··· + λH = 1 and λj (cid:54)= 0. For such λ  take an (H − H0)×
(H − H0 + 1) matrix A = (αcj) (H0 + 1 ≤ c ≤ H  H0 ≤ j ≤ H) that satisﬁes the two conditions:

and NH0 : f (H0)(x; θ(H0)) =(cid:80)H0

j=1vjϕ(x; wj)

(cid:1) is invertible  where 1d = (1  . . .   1)T ∈ Rd 

A

j=H0

αcjλj = 0 for any H0 + 1 ≤ c ≤ H.
To ﬁnd such A  take A = (aH0+1  . . .   aH )T so that aT
for some scalars sc  taking the inner product with λ causes a contradiction.
Given such λ and A = (αcj)  deﬁne a bijective linear transform from (vH0  . . .   vH ; wH0  . . .   wH )
to (a  ξH0+1  . . .   ξH ; b  ηH0+1  . . .   ηH ) by

c λ = 0. Then  if(cid:80)H

c=H0+1 scac = 1H−H0+1

(A1) (cid:0) 1T
(A2) (cid:80)H

H−H0+1

c=H0+1αcjηc

(6)
The parameter b serves as the direction that makes all the hidden units behave equally  and (ηj)
deﬁne the remaining H − 1 directions that differentiate them. The parameter b thus essentially plays
the role of uH0 for NH0. Also  a works as ζH0 when all wj are equal. The next lemma conﬁrms this
role of (a  b) and shows that the directions ηc and ξc do not change the function f (H) at θ(H0)
Lemma 3. Let θ(H0) be any parameter of NH0  and θ(H)

be its embedding deﬁned by Eq. (4). Then 

c=H0+1λjαcjξc

λ

.

λ

(H0 ≤ j ≤ H).

and vj = λja +(cid:80)H

wj = b +(cid:80)H

(cid:12)(cid:12)(cid:12)θ(H)=θ(H)
(cid:12)(cid:12)(cid:12)θ(H)=θ(H)

λ

λ

∂f (H)(x;θ(H))

∂b

∂f (H)(x;θ(H))

∂a

= ∂f (H0)(x;θ(H0))

∂uH0

= ∂f (H0)(x;θ(H0))

∂ζH0

 

 

∂f (H)(x;θ(H))

∂ηc

∂f (H)(x;θ(H))

∂ξc

(cid:12)(cid:12)(cid:12)θ(H)=θ(H)
(cid:12)(cid:12)(cid:12)θ(H)=θ(H)

λ

λ

= 0 

= 0.

(7)

From Lemma 3  the Hessian takes a simple form:
is a stationary point of NH0 and θ(H)
Lemma 4. Let λ and A be as above. Suppose θ(H0)
is its embedding deﬁned by Eq. (4). Then  the Hessian matrix of LH with respect to ω =
(a  b  ξH0+1  . . .   ξH   ηH0+1  . . .   ηH ) at θ(H) = θ(H)

is given by

∗

λ

λ



∂2LH (θ(H)
λ )

∂ω∂ω

=

a

b

ξc

a

b

∗

∂2LH0 (θ(H0)
∂ζH0 ∂ζH0
∂2LH0 (θ(H0)
∂uH0 ∂ζH0

∗

)

)

∗

∂2LH0 (θ(H0)
∂ζH0 ∂uH0
∂2LH0 (θ(H0)
∂uH0 ∂uH0

∗

)

)

.

ξd

ηd

O O

O O
˜F
˜G

(8)

is given by (cid:0)AΛAT(cid:1) ⊗ G with Λ = Diag(λH0  . . .   λH ) and G :=

O
O
)cd  which is a symmetric matrix of (H − H0) ×

O
˜F T

O
O

ηc

The lower-right block ˜G := ( ∂2LH (θ(H)
λ )
∂ηc∂ηd
D dimension 

(cid:80)n
H0)× M dimension  is given by(cid:0)AΛAT(cid:1)⊗ F with F :=(cid:80)n

ζH0∗ ∂2ϕ(xν ;uH0∗)
∂uH0 ∂uH0

∂(cid:96)(yν  f (H0 )(xν ;θ(H0)

ν=1

∂z

))

∗

; and ˜F := ( ∂2LH (θ(H)
λ )
∂ξc∂ηd

)cd  which is of size (H −
∂ϕ(xν ;uH0∗)
.

))

∗

∂z

∂uH0

∂(cid:96)(yν  f (H0)(xν ;θ(H0)

ν=1

4

Lemma 4 shows that  with the reparametrization  the Hessian at the embedded stationary point θ(H)
contains the Hessian of LH0 with a  b  and that the cross blocks between (a  b) and (ξc  ηd) are zero.
Note that the ξ-ξ block is zero  which is important when we prove Theorem 5.
Theorem 5. Consider a three layer network given by Eq. (5). Suppose that the the output dimension
M is greater than 1 and θ(H0)
is a minimum of LH0. Let the matrices G  F and the parameter θ(H)
be used in the same meaning as in Lemma 4 (unit replication). Then  if either of the conditions
(i) G is positive or negative deﬁnite  and F (cid:54)= O 
(ii) G has positive and negative eigenvalues 

∗

λ

λ

holds  then for any λ with(cid:80)H

is a saddle point of LH.

λj = 1 and λj (cid:54)= 0  θ(H)

λ

j=H0

Theorem 5 is easily proved from Lemma 4. From the form of the lower-right four blocks of Eq. (8)  it
has positive and negative eigenvalues if ˜G is positive (or negative) deﬁnite and ˜F (cid:54)= O. See Sec. D.3
in Supplements for a complete proof. The assumption M ≥ 2 is necessary for the condition (i) to
happen. In fact  [4] discussed the case of M = 1  in which F = O is derived. The paper also gave a
sufﬁcient condition that the embedded point θ(H)
is a local minimum when G is positive (or negative)
deﬁnite. See Sec. E for more details on the special case of M = 1.
Suppose that θ(H0)
can never be a saddle point but a global
minimum. Therefore  the situation (ii) can never happen. In that case  if G is invertible  it must be
positive deﬁnite and F = O. We will discuss this case further in Sec. 5.1.

attains zero training error. Then  θ(H)

∗

λ

λ

4 Semi-ﬂat minima by embedding of ReLU networks

This section discusses networks with ReLU. Its special shape causes different results. Let φ(t) be
the ReLU function: φ(t) = max{t  0}  which is used very often in DNNs to prevent vanishing
wgtx − wbias. It
gradients [12  5]. The activation is given by ϕ(x; w) = φ(wT ˜x) with wT ˜x := wT
is important to note that the ReLU function satisﬁes positive homogeneity; i.e.  φ(αt) = αφ(t) for
any α ≥ 0. This causes special properties on ϕ  that is  (a) ϕ(x; rw) = rϕ(x; w) for any r ≥ 0  (b)

∂ϕ(x;w)

∂w

= ∂ϕ(x;w)

∂w

if r > 0  wT ˜x (cid:54)= 0  and (c) ∂2ϕ(x;w)

∂w∂w = 0 if wT ˜x (cid:54)= 0.

(cid:12)(cid:12)(cid:12)w=rw∗

(cid:12)(cid:12)(cid:12)w=w∗

From the positive homogeneity  effective parameterization needs some normalization of vj or wj.
However  this paper uses the redundant parameterization. In our theoretical arguments  no problem is
caused by the redundancy  while it gives additional ﬂat directions in the parameter space.

4.1 Embeddings of ReLU networks

Reﬂecting the above special properties  we introduce modiﬁed versions for embeddings of θ(H0)
(I)R Unit replication: Fix U q

.
  and take γ = (γH0  . . .   γH ) ∈ RH−H0+1 and β = (βH0   . . .   βH )

such that βj > 0 (H0 ≤ ∀j ≤ H) and(cid:80)H

H0

∗

j=H0
vi = ζi

γjβj = 1. Deﬁne θ(H)
(1 ≤ i ≤ H0 − 1) 

γ β by

vj = γjζH0

(H0 ≤ j ≤ H).

wi = ui 
wj = βjuH0  

(9)

(II)R Inactive units: Deﬁne a parameter ˆθ(H) by

(1 ≤ i ≤ H0) 

vj : arbitrary

(H0 + 1 ≤ j ≤ H)

wi = ui 
wj such that wT

vi = ζi

(∀ν  H0 + 1 ≤ j ≤ H).

j ˜xν < 0

(10)
Note that the deﬁnition (II)R is different from the smooth activation case. The last condition is easily
satisﬁed if wbias is large. Note also that ϕ(xν; wj) = 0 for each ν  but ϕ(x; wj) (cid:54)≡ 0 in general.
Since a small change of wj (H0 + 1 ≤ j ≤ H) does not alter ϕ(xν; wj) = 0  the function LH is
constant locally on vj and wj (H0 + 1 ≤ j ≤ H) at ˆθ(H). This is clear difference from the smooth
case  where changing wj from w(0) may cause a different function.
(III)R Inactive propagation: The inactive propagation is exactly the same as the smooth activation
case. The embedded point is denoted by ˜θ(H).
The following proposition is obvious from the deﬁnitions.

5

Proposition 6. For the unit replication and inactive propagation  we have f (H)
θ(H)
γ β

= f (H)

˜θ(H) = f (H0)
θ(H0)
∗

.

We see that there are some other ﬂat directions in addition to the general cases. In the embedding by
j ˜xν ≤ 0 is maintained  LH has the same value. Assume (cid:107)xν(cid:107) ≤ 1
inactive units  if the condition wT
without loss of generality  and ﬁx K > 1 as a constant. Deﬁne ˆwj wgt = 0 and ˆwj bias = 2K for
H0 + 1 ≤ j ≤ H. From wT
j ˜xν ≤ (cid:107)wj wgt(cid:107) − wj bias ≤ 0 for wj ∈ BK := {wj | (cid:107)wj wgt(cid:107) ≤
K and K ≤ wj bias ≤ 3K} and any vj (H0 + 1 ≤ j ≤ H)  we have the following result  showing
that an (H − H0) × (M + D) dimensional afﬁne subset at ˆθ(H) gives the same value at xν.
Proposition 7. Assume (cid:107)xν(cid:107) ≤ 1 (∀ν). If (vi  wi) = (ζi∗  ui∗) (1 ≤ i ≤ H0) and (vj  wj) ∈
RM × BK (H0 + 1 ≤ j ≤ H)  we have for any ν = 1  . . .   n

f (H)(xν; θ(H)) = f (H0)(xν; θ(H0)

∗

).

ﬂat directions. To see this  for a ﬁxed (γ  β) with(cid:80)
αcjγjβj = 0 (∀c) and(cid:0) 1T
matrix such that(cid:80)H

Next  for the unit replication of ReLU networks  the piecewise linearity of ReLU causes additional
j γjβj = 1  we introduce a parametriza-
tion in a similar manner to the smooth case. Let A = (αcj) be an (H − H0) × (H − H0 + 1)

(cid:1) is invertible. Fix such A and deﬁne

H−H0+1

j=H0+1 does not alter the value LH (θ(H)) = LH0(θ(H0)

(a  ξH0+1  . . .   ξH ; b  ηH0+1  . . .   ηH ) by Eq. (6). The next proposition shows that a small change of
δ (θ(H)) denote the intersection
(ηj)H
of the ball of radius δ > 0 at θ(H) and the afﬁne subspace spanned by ηH0+1  . . .   ηH at θ(H).
Proposition 8. Let {xν}n
and θ(H)

be any parameter of the ReLU network NH0 
∗
H0∗xν (cid:54)= 0 for all ν. Then  there is δ > 0 such that

γ β be deﬁned by Eq. (9). Assume that uT

ν=1 be any data set  θ(H0)

). Let Bη

∗

j=H0

A

f (H)(xν; θ(H)) = f (H0)(xν; θ(H0)

∗

)

(∀θ(H) ∈ Bη

δ (θ(H)

γ β )  ∀ν = 1  . . .   n).

See Sec. F.1 for the proof. The situation uT

H0∗xν (cid:54)= 0 may easily occur in practice (Fig. 2(a)).

4.2 Embedding a local minimum of ReLU networks

∗

We ﬁrst consider the embedding of a minimum by inactive units. Let ˆθ(H) be an embedding of θ(H0)
j=H0+1 around ˆθ(H) but
by Eq. (10). From Proposition 7  LH (θ(H)) does not depend on (vj  wj)H
takes the same value as LH0 (θ(H0)) with θ(H0) = (vi  wi)H0
i=1. We have thus the following theorem.
Theorem 9. Assume that θ(H0)
is a minimum of LH0. Then  the embedded point ˆθ(H) deﬁned by
Eq. (10) (inactive units) is a minimum of LH.
Theorem 9 and Proposition 7 imply that there is an (H − H0) × (M + D) dimensional afﬁne subset
that gives local minima  and in those directions LH is ﬂat.
Next  we consider the embedding by unit replication  which needs further restriction on γ and β. Let
θ(H0) be a parameter of NH0  and γ = (γj)H
by replacing
H0∗xν (cid:54)= 0 (∀ν) 
the function LH is differentiable on ηc  ξc  and for the same reason as Theorem 5  the derivatives are
zero. By restricting the function on those directions around θ(H)
= 0  we

satisfy(cid:80)H
(cid:1)  which includes a positive and negative eigenvalue

γj > 0. Deﬁne θ(H)
γk (H0 ≤ j ≤ H). If we assume uT

wj = βjuj in Eq. (9) with wj = uH0/(cid:80)H
can see that the Hessian has the form(cid:0) O ˜F
that(cid:80)H

H0∗xν (cid:54)= 0 for any
Theorem 10. Suppose that θ(H0)
ν = 1  . . .   n  and that F (cid:54)= O where F is given by Lemma 4. Then  for any γ ∈ RH−H0+1 such

is a minimum point of LH0. Assume that uT

unless F = O. This derives the following theorem. (See Sec. F.2 for a complete proof.)

  from the fact ∂2ϕ(xν ;uH0 )
∂uH0 ∂uH0

γj > 0  the embedded parameter θ(H)

is a saddle point of LH.

˜F T O

k=H0

j=H0

j=H0

∗

j=H0

γ

γ

γ

6

5 Discussions

5.1 Minimum of zero error

∗

In using a very large network with more parameters than the data size  the training error may reach
zero. Assume (cid:96)(y  z) ≥ 0 and that a narrower model attains LH0(θ(H0)
) = 0 without redundant
units  i.e.  any deletion of a unit will increase the training error. We investigate overparameterized
realization of such a global minimum by embedding in a wider network NH. Note that by any methods
the embedded parameter is a minimum. This causes special local properties on the embedded point.
For simplicity  we assume three-layer networks and (cid:107)xν(cid:107) ≤ 1 (∀ν). First  consider the unit replication
for the smooth activation. As discussed in the last part of Sec. 3.2  the Hessian takes the form


(cid:1). The case of inactive propagation is similar.
units  the lower-right four blocks take the form(cid:0) O O

where ˜G is non-negative deﬁnite. It is not difﬁcult to see (Sec. G.2.2) that  in the case of inactive

Smooth: ∇2LH (θ(H)

) O O
O O
O ˜G

 

∇2LH0 (θ(H0)

∗

λ ) =

(11)

O
O

θ(H0)

ηc

ξc

For ReLU activation  assume θ(H0)
7  the Hessian at the embedding ˆθ(H) by inactive units is given by

∗

is a differentiable point of LH0 for simplicity. From Proposition

ReLU: ∇2LH ( ˆθ(H)) =

θ(H0)
∗

∇2LH0(θ(H0)

O

(vj  wj )

)

O
O

(cid:21)

.

(12)

O S

(cid:20)

(cid:115)

Similarly to the smooth case  the Hessian for the unit replication θ(H)

γ

takes the same form as Eq. (12).

5.2 Generalization error bounds of embedded networks

Based on the results in Sec. 5.1  here we compare the embedding between ReLU and smooth
activation. The results suggest that the ReLU networks can have an advantage in generalization error
when zero training error is realized by some type of overparameterized models.
Suppose that the smooth model NH0 s and ReLU mdoel NH0 r attain zero training error without
redundant units. They are embedded by the method of inactive units into NHs and NHr  respectively 
so that Hs − H0 s = Hr − H0 r(=: E) (the same number of surplus units). The dimensionality of
the parameters of NH0 s and NH0 r are denoted by d0
The major difference of the local properties in Eqs. (11) and (12) is the existence of matrix S or ˜G in
the smooth case. The ReLU network has a ﬂat error surface LH in both the directions of wj and vj.
In this sense  the embedded minimum is ﬂatter in the ReLU network. We relate this difference of
semi-ﬂatness to the generalization ability of the networks through the PAC-Bayes bounds  which has
been already used for discussing deep learning [13]. Our motivation here is to consider the difference
of the activation functions. We give a summary here and defer the details in Sec. G  Supplements.
Let D be a probability distribution of (x  y) and LH (θ(H)) := ED[(cid:96)(y  f (x; θ(H)))] be the gener-
alization error (or risk). Training data (x1  y1)  . . .   (xn  yn) are i.i.d. sample with distribution D.
Then  with a trained parameter ˆθ  the PAC-Bayes bound tells

rl  respectively.

sm and d0

LH ( ˆθ) (cid:47) 1
n

LH ( ˆθ) + 2

2(KL(Q||P ) + ln 2δ
n )

n − 1

 

(13)

where P is a prior distribution which does not depend on the training data  and Q is any distribution
such that it distributes on parameters that do not change the value of LH so much from LH ( ˆθ).
We focus on the embedding by inactive units here. See Sec. G.2.3  Supplements  for the other
cases. The essential factor of the PAC-Bayes bound is the KL-divergence KL(Q||P )  which is to
be small. We use different choices of P and Q for the smooth and ReLU networks (see Sec. G for
details). For the smooth networks  Psm is a non-informative normal distribution N (0  σ2Idsm ) with

7

(a)

(b)

sm 0  τ 2H−1

sm) × N ( ˆθ(H)

Figure 2: (a) Data and ﬁtting by N5 with ReLU. (b) Ratio of generalization errors of NH and NH0.
sm 2  τ 2S−1) with τ (cid:28) 1  where
σ (cid:29) 1  and Qsm is N ( ˆθ(H)
j=H0+1. Hsm :=
the decomposition corresponds to the components θ(H0)  (vj)H
∇2LH0(θ(H0)
) ×
N (0  σ2Id1) × UnifBE
  where d1 =
E × M is dim(vj)H

∗ sm) is the Hessian. For ReLU  based on Proposition 7  Prl is given by N (0  σ2Id0

j=H0+1. For these choices  the major difference of the bounds is the term

sm 1  σ2Id1) × N ( ˆθ(H)

rl ) × N ( ˆθ(H)

rl 1   σ2Id1) × UnifBE

  while Qrl is N ( ˆθ(H)

K

rl 0   τ 2H−1

d1 log(cid:0)σ2/τ 2)

j=H0+1  and (wj)H

rl

K

in the KL divergence for the smooth model. We can argue that  in realizing perfect ﬁtting to
training data with an overparameterized network  the ReLU network achieves a better upper bound of
generalization than the smooth network  when the numbers of surplus units are the same.
Numerical experiments. We made experiments on the generalization errors of networks with ReLU
and tanh in overparameterization. The input and output dimension is 1. Training data of size 10
are given by N1 (one hidden unit) for the respective models with additive noise ε ∼ N (0  10−2) in
the output. We ﬁrst trained three-layer networks with each activation to achieve zero training error
(< 10−29 in squared errors) with minimum number of hidden units (H0 = 5 in both models). See
Figure 2(a) for an example of ﬁtting by the ReLU network. We used the method of inactive units for
embedding to NH  and perturb the whole parameters with N (0  ρ2)  where ρ is the 0.01 × (cid:107)θ(H0)
(cid:107).
The code is available in Supplements. Figure 2(b) shows the ratio of the generalization errors (average
and standard error for 1000 trials) of NH over NH0 as increasing H. We can see that  as more surplus
units are added  the generalization errors increase for the tanh networks  while the ReLU networks
do not show such increase. This accords with the theoretical considerations in Sec. 5.2: adding
surplus units in tanh activation makes sharp directions  which degrade the generalization.

∗

5.3 Additional remarks

Regularization. In training of a large network  one often regularizes parameters based on the norm
such as (cid:96)2 or (cid:96)1. Consider  for example  the inactive method of embedding for tanh or ReLU by
setting vj = 0 and wj = 0 (H0 + 1 ≤ j ≤ H). Then the norm of the embedded parameter is smaller
than that of unit replication. This implies that if norm regularization is applied during training  the
embedding by inactive units and propagation is to be promoted in overparameterized realization.
Abundance of semi-ﬂat minima in ReLU networks. Theorems 9 and 10 discuss three layer models
for simplicity  but they can be easily extended to networks of any number of layers. Given a minimum
of LH0  it can be embedded to a wider network by making inactive units in any layers. Thus  in
a very large (deep and wide) network with overparameterization  there are many afﬁne subsets of
parameters to realize the same function  which consist of semi-ﬂat minima of the training error.

6 Conclusions

For a better theoretical understanding of the error landscape  this paper has discussed three methods
for embedding a network to a wider model  and studied overparameterized realization of a function
and its local properties. From the difference of the properties between smooth and ReLU networks  our
results suggest that ReLU may have an advantage in realizing zero errors with better generalization.
The current analysis reveals some nontrivial geometry of the error landscape  and its implications to
dynamics of learning will be within important future works.

8

1.00.50.00.51.0x1.01.52.02.5yfitting curvetrue curvetraining data0.02.55.07.510.0Number of added nodes123Rates of Generalization ErrorsReLUTanhReferences
[1] Z. Allen-Zhu  Y. Li  and Y. Liang. Learning and generalization in overparameterized neural
networks  going beyond two layers. CoRR  abs/1811.04918  2018. URL http://arxiv.org/
abs/1811.04918.

[2] S. Arora  N. Cohen  and E. Hazan. On the optimization of deep networks: Implicit acceleration
by overparameterization. In J. Dy and A. Krause  editors  Proceedings of the 35th International
Conference on Machine Learning  volume 80 of Proceedings of Machine Learning Research 
pages 244–253  2018.

[3] P. Chaudhari  A. Choromanska  S. Soatto  Y. LeCun  C. Baldassi  C. Borgs  J. T. Chayes 
L. Sagun  and R. Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. CoRR 
abs/1611.01838  2017.

[4] K. Fukumizu and S. Amari. Local minima and plateaus in hierarchical structures of multilayer

perceptrons. Neural Networks  13(3):317–327  2000.

[5] X. Glorot  A. Bordes  and Y. Bengio. Deep sparse rectiﬁer neural networks. In G. Gordon 
D. Dunson  and M. Dudík  editors  Proceedings of the 14th International Conference on
Artiﬁcial Intelligence and Statistics  volume 15 of Proceedings of Machine Learning Research 
pages 315–323  Fort Lauderdale  FL  USA  11–13 Apr 2011.

[6] S. Hochreiter and J. Schmidhuber. Simplifying neural nets by discovering ﬂat minima. In

Advances in Neural Information Processing Systems 7  pages 529–536. MIT Press  1995.

[7] S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation  9(1):1–42  1997. doi:

10.1162/neco.1997.9.1.1.

[8] N. S. Keskar  D. Mudigere  J. Nocedal  M. Smelyanskiy  and P. T. P. Tang. On large-batch
training for deep learning: Generalization gap and sharp minima. CoRR  abs/1609.04836  2017.
[9] B. Kleinberg  Y. Li  and Y. Yuan. An alternative view: When does SGD escape local minima?
In Proceedings of the 35th International Conference on Machine Learning  pages 2698–2707 
2018.

[10] V. K˚urková and P. C. Kainen. Functionally equivalent feedforward neural networks. Neural

Computation  6(3):543–558  1994. doi: 10.1162/neco.1994.6.3.543.

[11] D. A. McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3):355–363  Dec

1999.

[12] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
Proceedings of the 27th International Conference on International Conference on Machine
Learning  ICML 2010  pages 807–814  2010.

[13] B. Neyshabur  S. Bhojanapalli  D. McAllester  and N. Srebro. Exploring generalization in deep
learning. In Advances in Neural Information Processing Systems 30  pages 5947–5956  2017.
[14] Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proceedings of
the 34th International Conference on Machine Learning - Volume 70  ICML’17  pages 2603–
2612. JMLR.org  2017. URL http://dl.acm.org/citation.cfm?id=3305890.3305950.
[15] A. Rangamani  N. H. Nguyen  A. Kumar  D. Phan  S. H. Chin  and T. D. Tran. A Scale Invariant

Flatness Measure for Deep Network Minima. arXiv:1902.02434 [stat.ML]  Feb 2019.

[16] H. J. Sussmann. Uniqueness of the weights for minimal feedforward nets with a given input-

output map. Neural Networks  5(4):589 – 593  1992.

[17] Y. Tsuzuku  I. Sato  and M. Sugiyama. Normalized Flat Minima: Exploring Scale Invariant
Deﬁnition of Flat Minima for Neural Networks using PAC-Bayesian Analysis. arXiv e-prints 
art. arXiv:1901.04653  Jan 2019.

9

,Kenji Fukumizu
Shoichiro Yamaguchi
Yoh-ichi Mototake
Mirai Tanaka