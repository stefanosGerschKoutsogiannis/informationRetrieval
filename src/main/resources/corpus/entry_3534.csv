2012,Monte Carlo Methods for Maximum Margin Supervised Topic Models,An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models  such as MedLDA  which employs max-margin posterior constraints. However  unlike the likelihood-based supervised topic models  of which posterior inference can be carried out using the Bayes' rule  the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable  thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper  we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler  respectively  in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency.,MonteCarloMethodsforMaximumMarginSupervisedTopicModelsQixiaJiang†‡ JunZhu†‡ MaosongSun† andEricP.Xing∗∗†DepartmentofComputerScience&Technology TsinghuaNationalTNListLab †StateKeyLabofIntelligentTech.&Sys. TsinghuaUniversity Beijing100084 China∗SchoolofComputerScience CarnegieMellonUniversity Pittsburgh PA15213{qixia dcszj sms}@mail.tsinghua.edu.cn;epxing@cs.cmu.eduAbstractAneffectivestrategytoexploitthesupervisingsideinformationfordiscoveringpredictivetopicrepresentationsistoimposediscriminativeconstraintsinducedbysuchinformationontheposteriordistributionsunderatopicmodel.Thisstrate-gyhasbeenadoptedbyanumberofsupervisedtopicmodels suchasMedLDA whichemploysmax-marginposteriorconstraints.However unlikethelikelihood-basedsupervisedtopicmodels ofwhichposteriorinferencecanbecarriedoutus-ingtheBayes’rule themax-marginposteriorconstraintshavemadeMonteCarlomethodsinfeasibleoratleastnotdirectlyapplicable therebylimitedthechoiceofinferencealgorithmstobebasedonvariationalapproximationwithstrictmeanﬁeldassumptions.Inthispaper wedeveloptwoefﬁcientMonteCarlomethodsundermuchweakerassumptionsformax-marginsupervisedtopicmodelsbasedonanimportancesamplerandacollapsedGibbssampler respectively inacon-vexdualformulation.Wereportthoroughexperimentalresultsthatcompareourapproachfavorablyagainstexistingalternativesinbothaccuracyandefﬁciency.1IntroductionTopicmodels suchasLatentDirichletAllocation(LDA)[3] haveshowngreatpromiseindiscover-inglatentsemanticrepresentationsoflargecollectionsoftextdocuments.Inordertoﬁtdatabetter LDAhasbeensuccessfullyextendedinvariousways.Onenotableextensionissupervisedtopicmodels whichweredevelopedtoincorporatesupervisingsideinformationfordiscoveringpredic-tivelatenttopicrepresentations.RepresentativemethodsincludesupervisedLDA(sLDA)[2 12] discriminativeLDA(DiscLDA)[8] andmax-entropydiscriminationLDA(MedLDA)[16].MedLDAdiffersfromitscounterpartsupervisedtopicmodelsbyimposingdiscriminativecon-straints(i.e. max-marginconstraints)directlyonthedesiredposteriordistributions insteadofdeﬁn-inganormalizedlikelihoodmodelasinsLDAandDiscLDA.Suchtopicmodelswithmax-marginposteriorconstraintshaveshownsuperiorperformanceinvarioussettings[16 14 13 9].However theirconstrainedformulations especiallywhenusingsoftmarginconstraintsforinseparablepracti-calproblems makeitinfeasibleoratleasthardifpossibleatall1todirectlyapplyMonteCarlo(MC)methods[10] whichhavebeenwidelyusedintheposteriorinferenceoflikelihoodbasedmodels suchasthecollapsedGibbssamplingmethodsforLDA[5].Previousinferencemethodsforsuchmodelswithmax-marginposteriorconstraintshavebeenexclusivelyonthevariationalmethods[7]usuallywithastrictmean-ﬁeldassumption.Althoughfactorizedvariationalmethodsoftenseekfasterapproximationsolutions theycouldbeinaccurateorobtaintoocompactresults[1].∗‡indicatesequalcontributionsfromtheseauthors.1Rejectionsamplingcanbeappliedwhentheconstraintsarehard e.g. forseparableproblems.Butitwouldbeinefﬁcientwhenthesamplespaceislarge.1Inthispaper wedevelopefﬁcientMonteCarlomethodsformax-marginsupervisedtopicmodels whichwebelieveiscrucialforhighlyscalableimplementation andfurtherperformanceenhance-mentofthisclassofmodels.Speciﬁcally weﬁrstprovideanewandequivalentformulationoftheMedLDAasaregularizedBayesianmodelwithmax-marginposteriorconstraints basedonZell-ner’sinterpretationofBayes’ruleasalearningmodel[15]andtherecentdevelopmentofregularizedBayesianinference[17].ThisinterpretationisarguablymorenaturalthantheoriginalformulationofMedLDAasahybridmax-likelihoodandmax-marginlearning wherethelog-likelihoodisap-proximatedbyavariationalupperboundforcomputationaltractability.Then wedealwiththesetofsoftmax-marginconstraintswithconvexdualitymethodsandderivetheoptimalsolutionsofthedesiredposteriordistributions.Toeffectivelyreducethesizeofthesamplingspace wedeveloptwosamplers namely animportancesamplerandacollapsedGibbssampler[4 1] withamuchweakerassumptiononthedesiredposteriordistributioncomparedtothemeanﬁeldmethodsin[16].Wenotethatthework[11]presentsadualitymethodtohandlemomentmatchingconstraintsinmax-imumentropymodels.Ourworkisanextensionoftheirresultstolearntopicmodels whichhavenontriviallystructuredlatentvariablesandalsousethegeneralsoftmarginconstraints.2LatentDirichletAllocationLDA[3]isahierarchicalBayesianmodelthatpositseachdocumentasanadmixtureofKtopics whereeachtopicΦkisamultinomialdistributionoveraV-wordvocabulary.Fordocumentd itstopicproportionθdisamultinomialdistributiondrawnfromaDirichletprior.Letwd={wdn}Nn=1denotethewordsappearingindocumentd.Forthen-thwordwdn atopicassignmentzdn=kisdrawnfromθdandwdnisdrawnfromΦk.Inshort thegenerativeprocessofdisθd∼Dir(α) zdn=k∼Mult(θd) wdn∼Mult(Φk) (1)whereDir(·)isaDirichlet Mult(·)isamultinomial.Forfully-BayesianLDA thetopicsarealsorandomsamplesdrawnfromaDirichletprior i.e. Φk∼Dir(β).LetW={wd}Dd=1denoteallthewordsinacorpuswithDdocuments anddeﬁnezd={zdn}Nn=1 Z={zd}Dd=1 Θ={θd}Dd=1.ThegoalofLDAistoinfertheposteriordistributionp(Θ Z Φ|W α β)=p0(Θ Z Φ|α β)p(W|Θ Z Φ)p(W|α β).(2)Sinceinferringthetrueposteriordistributionisintractable researchersmustresorttovariational[3]orMonteCarlo[5]approximatemethods.Althoughbothmethodshaveshownsuccessinvariousscenarios.Theyhavecomplementaryadvantages.Forexample variationalmethods(e.g. mean-ﬁeld)canbegenerallymoreefﬁcient whileMCmethodscanobtainmoreaccurateestimates.3MedLDA:asupervisedtopicmodelwithmax-marginconstraintsMedLDAextendsLDAbyintegratingthemax-marginlearningintotheprocedureofdiscoveringlatenttopicrepresentationstolearnlatentrepresentationsthataregoodforpredictingclasslabelsorratingscoresofadocument.Empirically MedLDAanditsvariousextensions[14 13 9]havedemonstratedpromiseinlearningmorediscriminativetopicrepresentations.TheoriginalMedL-DAwasdesignedasahybridmaxlikelihoodandmax-marginlearning wheretheintractablelog-likelihoodisapproximatedbyavariationalbound.Toderiveoursamplingmethods wepresentanewinterpretationofMedLDAfromtheperspectiveofregularizedBayesianinference[17].3.1BayesianinferenceasalearningmodelAsshowninEq.(2) Bayesianinferenceisaninformationprocessingrulethatprojectsthepriorp0andempiricalevidencetoapost-dataposteriordistributionviatheBayes’rule.Itisthecoreforlikelihood-basedsupervisedtopicmodels[2 12].AfreshinterpretationofBayesianinferencewasgivenbyZellner[15] whichleadstoournovelinterpretationofMedLDA.Speciﬁcally ZellnershowedthattheposteriordistributionbyBayes’ruleisthesolutionofanoptimizationproblem.Forinstance theposteriorp(Θ Z Φ|W)ofLDAisequivalenttotheoptimumsolutionofminp(Θ Z Φ)∈PKL[p(Θ Z Φ)∥p0(Θ Z Φ)]−Ep[logp(W|Θ Z Φ)] (3)whereKL(q||p)istheKullback-Leiblerdivergencefromqtop andPisthespaceofprobabilitydistributions.WewilluseL(p(Θ Z Φ))todenotetheobjectivefunction.23.2MedLDA:aregularizedBayesianmodelForbrevity weconsidertheclassiﬁcationmodel.LetD={(wd yd)}Dd=1beagivenfully-labeledtrainingset wheretheresponsevariableYtakesvaluesfromaﬁnitesetY={1 ... M}.MedLDAconsistsoftwoparts.TheﬁrstpartisanLDAlikelihoodmodelfordescribinginputdocuments.Asinpreviouswork weusethepartial2likelihoodmodelforW.Thesecondpartisamechanismtoconsidersupervisingsignal.SinceourgoalistodiscoverlatentrepresentationsZthataregoodforclassiﬁcation onenaturalsolutionistoconnectZdirectlytoourultimategoal.MedLDAobtainssuchagoalbybuildingaclassiﬁcationmodelonZ.Onegoodcandidateoftheclassiﬁcationmodelisthemax-marginmethods whichavoiddeﬁninganormalizedlikelihoodmodel[12].Formally letηdenotetheparametersoftheclassiﬁcationmodel.Tomakethemodelfully-Bayesian wealsotreatηrandom.Then wewanttoinferthejointposteriordistributionp(η Θ Z Φ|D).Forclassiﬁcation MedLDAdeﬁnesthefollowingdiscriminationfunctionF(y η z;w)=η⊤f(y ¯z) F(y;w)=Ep(η z|w)[F(y η z;w)] (4)where¯zisaK-dimvectorwhoseelement¯zkequalsto1N∑Nn=1I(zn=k) andI(x)isanindicatorfunctionwhichequalsto1whenxistrueotherwise0;f(y ¯z)isanMK-dimvectorwhoseelementsfrom(y−1)KtoyKare¯zandallothersarezero;andηisanMK-dimensionalvectorconcatenatingMclass-speciﬁcsub-vectors.Withtheabovedeﬁnitions anaturalpredictionruleisˆy=argmaxyF(y;w) (5)andwewouldliketo“regularize”thepropertiesofthelatenttopicrepresentationstomakethemsuitableforaclassiﬁcationtask.OnewaytoachievethatgoalistotaketheoptimizationviewofBayes’theoremandimposethefollowingmax-marginconstraintstoproblem(3)F(yd;wd)−F(y;wd)≥ℓd(y)−ξd ∀y∈Y ∀d (6)whereℓd(y)isanon-negativefunctionthatpenalizesthewrongpredictions;ξ={ξd}Dd=1arenon-negativeslackvariablesforinseparablecases.LetL(p)=KL(p||p0(η Θ Z Φ))−Ep[logp(W|Z Φ)]and∆f(y ¯zd)=f(yd ¯zd)−f(y ¯zd).Then wedeﬁnethesoft-marginMedLDAassolvingminp(η Θ Z Φ)∈P ξL(p(η Θ Z Φ))+CDD∑d=1ξds.t.:Ep[η⊤∆f(y ¯zd)]≥ℓd(y)−ξd ξd≥0 ∀d ∀y (7)wherethepriorisp0(η Θ Z Φ)=p0(η)p0(Θ Z Φ).Withtheabovediscussions wecanseethatMedLDAisaninstanceofregularizedBayesianmodels[17].Also problem(7)canbeequivalentlywrittenasminp(η Θ Z Φ)∈PL(p(η Θ Z Φ))+CR(p(η Θ Z Φ))(8)whereR=1D∑dargmaxy(ℓd(y)−Ep[η⊤∆f(y ¯zd)])isthehingeloss anupperboundofthepredictionerrorontrainingdata.4MonteCarlomethodsforMedLDAAsinothervariantsoftopicmodels itisintractabletosolveproblem(7)ortheequivalentproblem(8)directly.Previoussolutionsresorttovariationalmean-ﬁeldapproximationmethods.ItiseasytoshowthatthevariationalEMmethodin[16]isacoordinatedescentalgorithmtosolveproblem(7) withtheadditionalfully-factorizedmean-ﬁeldconstraint p(η Θ Z Φ)=p(η)(∏dp(θd)∏np(zdn))∏kp(Φk).(9)Below wepresenttwoMCsamplingmethodstosolvetheMedLDAproblem withmuchweakerconstraintsonp andthustheycouldbeexpectedtoproducemoreaccuratesolutions.Speciﬁcally weassumep(η Θ Z Φ)=p(η)p(Θ Z Φ).Then thegeneralprocedureistoalter-natelysolveproblem(8)byperformingthefollowingtwosteps.2AfulllikelihoodmodelonbothWandYcanbedeﬁnedasin[12].Butitsnormalizationconstant(afunctionofZ)couldmaketheproblemhardtosolve.3Estimatep(η):Givenp(Θ Z Φ) thesubproblem(inanequivalentconstrainedform)istosolveminp(η) ξKL(p(η)∥p0(η))+CDD∑d=1ξds.t.:Ep[η]⊤∆f(y E[¯zd])≥ℓd(y)−ξd ξd≥0 ∀d ∀y.(10)ByusingtheLagrangianmethodswithmultipliersλ wehavetheoptimumposteriordistributionp(η)∝p0(η)eη⊤·∑Dd=1∑yλyd∆f(y E[¯zd]).(11)Forthepriorp0 forsimplicity wechoosethestandardnormalprior i.e. p0(η)=N(0 I).Inthiscase p(η)=N(κ I)andthedualproblemismaxλ−12κ⊤κ+D∑d=1∑yλydℓd(y)s.t.:∑yλyd∈[0 CD] ∀d.(12)whereκ=∑Dd=1∑yλyd∆f(y E[¯zd]).Notethatκistheposteriormeanofclassiﬁerparametersη andtheelementκykrepresentsthecontributionoftopickinclassifyingadatapointtocategoryy.Thisproblemisthedualproblemofamulti-classSVM[6]andwecansolveit(oritsprimalform)efﬁcientlyusingexistinghigh-performanceSVMlearners.Wedenotetheoptimumsolutionofthisproblemby(p∗(η) κ∗ ξ∗ λ∗).Estimatep(Θ Z Φ):Givenp(η) thesubproblem(inanequivalentconstrainedform)istosolveminp(Θ Z Φ) ξL(p(Θ Z Φ))+CDD∑d=1ξds.t.:(κ∗)⊤∆f(y Ep[¯zd])≥ℓd(y)−ξd ξd≥0 ∀d ∀y.(13)AlthoughintheorywecansolvethissubproblemagainusingLagrangiandualmethods itwouldbehardtoderivethedualobjectivefunction(ifpossibleatall).Here weusethesamestrategyasin[16] thatis toupdatep(Θ Z Φ)foronlyonestepwithξbeingﬁxedatξ∗(theoptimumsolutionofthepreviousstep).Itiseasytoshowthatbyﬁxingξatξ∗ wewillhavetheoptimumsolutionp(Θ Z Φ)∝p(W Z Θ Φ)e(κ∗)⊤∑dy(λyd)∗∆f(y ¯zd) (14)ThedifferencesbetweenMedLDAandLDAlieintheaboveposteriordistribution.TheﬁrsttermisthesameastheposteriorofLDA(theevidencep(W)canbeabsorbedintothenormalizationconstant).Thesecondtermindicatestheregularizationeffectsduetothemax-marginposteriorcon-straints whichisconsistentwithourintuition.Speciﬁcally forthosedatawithnon-zeroLagrangemultipliers(i.e. thedataarearoundthedecisionboundaryormisclassiﬁed) thesecondtermwillbiasthemodeltowardsanewposteriordistributionthatfavorsmorediscriminativerepresentationsonthese“hard”datapoints.Now theremainingproblemishowtoefﬁcientlydrawsamplesfromp(Θ Z Φ)andestimatetheexpectationsE[¯z]asaccurateaspossible whichareneededinlearningclassiﬁcationmodels.Below wepresenttworepresentativesamplers–animportancesamplerandacollapsedGibbssampler.4.1ImportancesamplerToavoiddealingwiththeintractablenormalizationconstantofp(Θ Z Φ) onenaturalchoiceistouseimportancesampling.Importancesamplingaimsatdrawingsomesamplesfroma“simple”distributionandtheexpectationisestimatedasaweightedaverageoverthesesamples.However directlyapplyingimportancesamplingtop(Θ Z Φ)maycausesomeissuessinceimportancesam-plingsuffersfromseverelimitationsinlargesamplespaces.Alternatively sincethedistributionp(Θ Z Φ)inEq.(14)hasthefactorizationformp(Θ Z Φ)=p0(Θ Φ)p(Z|Θ Φ) anotherpos-siblemethodistoadopttheancestralsamplingstrategytodrawsample(ˆΘ ˆΦ)fromp0(Θ Φ)andthendrawsamplesfromp(Z|ˆΘ ˆΦ).AlthoughitiseasytodrawasamplefromtheDirichletpriorp0(Θ Φ)=Dir(α)Dir(β) itwouldrequirealargenumberofsamplestogetarobustestimateoftheexpectationsE[Z].Below wepresentonesolutiontoreducesamplespace.4Onefeasiblemethodtoreducethesamplespaceistocollapse(Θ Φ)outanddirectlydrawsamplesfromthemarginaldistributionp(Z).However thiswillcausetightcouplingsbetweenZandmakethenumberofsamplesneededtoestimatetheexpectationgrowexponentiallywiththedimensionalityofZforimportancesampler.ApracticalsamplerforthiscollapseddistributionwouldbeaMarkovchain aswewillpresentinnextsection.Here weproposetousetheMAPestimateof(Θ Φ)astheir“singlesample”3andproceedtodrawsamplesofZ.Speciﬁcally given(ˆΘ ˆΦ) wehavetheconditionaldistributionp(Z|ˆΘ ˆΦ)∝p(W Z|ˆΘ ˆΦ)e(κ∗)⊤∑dy(λyd)∗∆f(y ¯zd)=D∏d=1Nd∏n=1p(zdn|ˆθd ˆΦ) (15)wherep(zdn=k|ˆθd ˆΦ wdn=t)=1Zdnˆϕktˆθdke1Nd∑y(λyd)∗(κ∗ydk−κ∗yk)(16)andZdnisanormalizationconstant andκ∗ykisthe[(y−1)K+k]-thelementofκ∗.Thedifference(κ∗ydk−κ∗yk)representsthedifferentcontributionoftopickinclassifyingdtothetruecategoryydandawrongcategoryy.Ifthedifferenceispositive topickcontributestomakeacorrectpredictionford;otherwise itcontributestomakeawrongprediction.Then wedrawJsamples{z(j)dn}Jj=1fromaproposaldistributiong(z)andcomputetheexpectationsE[¯zdk]=1NdNd∑n=1E[zdn] ∀¯zdk∈¯zdandE[zdn]≈J∑j=1γjdn∑Jj=1γjdnz(j)dn (17)wheretheimportanceweightγjdnisγjdn=K∏k=1(ˆθdkˆϕkwdng(k)e1Nd∑y(λyd)∗(κ∗ydk−κ∗yk))I(z(j)dn=k)(18)WiththeJsamples weupdatetheMAPestimate(ˆΘ ˆΦ)ˆθdk∝1J∑Ndn=1∑Jj=1γjdn∑Jj=1γjdnI(z(j)dn=k)+αkˆϕkt∝1J∑Dd=1∑Ndn=1∑Jj=1γjdn∑Jj=1γjdnI(z(j)dn=k)I(wdn=t)+βt.(19)Theabovetwostepsarerepeateduntilconvergence initializing(ˆΘ ˆΦ)tobeuniform andthesamplesfromthelastiterationareusedtoestimatetheexpectationstatisticsneededintheproblemofinferringp(η).4.2CollapsedGibbssamplerAswehavestated anotherwaytoeffectivelyreducethesamplespaceistointegrateouttheintermediatevariables(Θ Φ)andbuildaMarkovchainwhoseequilibriumdistributionistheresultingmarginaldistributionp(Z).WeproposetousecollapsedGibbssampling whichhasbeensuccessfullyusedforLDA[5].ForMedLDA weintegrateout(Θ Φ)andgetthemarginalizedposteriordistributionp(Z)=p(W Z|α β)Zqe(κ∗)⊤∑d∑y(λyd)∗∆f(y ¯zd)=1Z[∏Dd=1δ(Cd+α)δ(α)e(κ∗)⊤∑y(λyd)∗∆f(y ¯zd)][∏Kk=1δ(Ck+β)δ(β)] (20)whereδ(x)=∏dim(x)i=1Γ(xi)Γ(∑dim(x)i=1xi) CtkisthenumberoftimesthetermtbeingassignedtotopickoverthewholecorpusandCk={Ctk}Vt=1;Ckdisthenumberoftimesthattermsbeingassociatedwithtopickwithinthed-thdocumentandCd={Ckd}Kk=1.WecanalsoderivethetransitionprobabilityofonevariablezdngivenotherswhichwedenotebyZ¬as:p(zdn=k|Z¬ W¬ wdn=t)∝Ctk ¬n+βt∑tCtk ¬n+∑Vt=1βt(Ckd ¬n+αk)e1Nd∑y(λyd)∗(κ∗ydk−κ∗yk)(21)whereC·· ¬nindicatesthattermnisexcludedfromthecorrespondingdocumentortopic.Again wecanseethedifferencebetweenMedLDAandLDA(usingcollapsedGibbssampling)fromtheadditionallastterminEq.(21) whichisduetothemax-marginposteriorconstraints.3Thiscollapsesthesamplespaceof(Θ Φ)toasinglepoint.5Forthosedataonthemarginormisclassiﬁed(withnon-zeroLagrangemultipliers) thelasttermisnon-zeroandactsasaregularizerdirectlyaffectingthetopicassignmentsofthesedifﬁcultdata.Then weusethetransitiondistributioninEq.(21)toconstructaMarkovchain.AfterthisMarkovchainhasconverged(i.e. ﬁnishedtheburn-instage) wedrawJsamples{Z(j)}andestimatetheexpectationstatisticsE[¯zdk]=1NdNd∑n=1E[zdn] ∀¯zdk∈¯zd andE[zdn]=1JJ∑j=1z(j)dn.(22)4.3PredictionTomakepredictiononunlabeledtestingdatausingthepredictionrule(5) wetaketheapproachthathasbeenadoptedforthevariationalMedLDA whichusesapointestimateoftopicsΦfromtrainingdataandmakespredictionbasedonthem.Speciﬁcally weusetheMAPestimateˆΦtoreplacetheprobabilitydistributionp(Φ).Fortheimportancesampler ˆΦiscomputedasinEq.(19).ForthecollapsedGibbssampler anestimateofˆΦusingthesamplesisˆϕkt∝1J∑Jj=1Ctk(j)+βt whereCtk(j)isthetimesthattermtisassignedtotopickinthej-thsample.Givenanewdocumentwtobepredicted forimportancesampler theimportanceweightshouldbealteredasγjn=∏Kk=1(θkˆϕkwn/g(k))I(z(j)n=k).Then weapproximatetheexpectationofzasinEq.(17).ForGibbssampler weinferitslatentcomponentszusingtheobtainedˆΦasp(zn=k|z¬n)∝ˆϕkwn(Ck¬n+αk) whereCk¬nisthetimesthatthetermsinthisdocumentwassignedtotopickwiththen-thtermexcluded.Then weapproximatetheE[¯z]asinEq.(22).5ExperimentsWeempiricallyevaluatetheimportancesamplerandtheGibbssamplerforMedLDA(denotedbyiMedLDAandgMedLDArespectively)onthe20Newsgroupsdatasetwithastandardlistofstopwords4removed.Thisdatasetcontainsabout20Kpostingswithin20groups.Duetospacelimita-tion wefocusonthemulti-classsetting.Weusethecutting-planealgorithm[6]tosolvethemulti-classSVMtoinferp(η)andsolveforthelagrangemultipliersλinMedLDA.Forsimplicity weusetheuniformproposaldistributionginiMedLDA.Inthiscase wecangloballydrawJ(e.g. =3×K)samples{Z(j)}Jj=1fromg(z)outsidetheiterationloopandonlyupdatetheimportanceweightstosavetime.ForgMedLDA wekeepJ(e.g. 20)adjacentsamplesaftergMedLDAhasconvergedtoestimatetheexpectationstatistics.Tobefair weusethesameCfordifferentMedLDAmethods.TheoptimumCischosenvia5-foldcrossvalidationduringthetrainingprocedureoffMedLDAfrom{a2:a=1 ... 8}.WeusesymmetricDirichletpriorsforallLDAtopicmodels i.e. α=αeKandβ=βeV whereenisan-dimvectorwitheveryentrybeing1.WeassesstheconvergenceofaMarkovchainwhen(1)ithasrunforamaximumnumberofiterations(e.g. 100) or(2)therelativechangeinitsobjective i.e. |Lt+1−Lt|Lt islessthanatolerancethresholdϵ(e.g. ϵ=10−4).Weusethesamestrategytojudgewhethertheoverallinferencealgorithmconverges.Werandomlyselect7 505documentsfromthewholesetasthetestsetandtherestasthetrainingdata.Wesetthecostparameterℓd(y)inproblem(7)tobe16 whichproducesbetterclassiﬁcationperformancethanthestandard0/1cost[16].Tomeasurethesparsityofthelatentrepresentationsofdocuments wecomputetheaverageentropyovertestdocuments:1|Dt|∑d∈DtH(θd).WealsomeasurethesparsityoftheinferredtopicdistributionsΦintermsoftheaverageentropyovertopics i.e. 1K∑Kk=1H(Φk).AllexperimentsarecarriedoutonaPCwith2.2GHzCPUand3.6GRAM.Wereportthemeanandstandarddeviationforeachmodelwith4timesrandomlyinitializedruns.5.1PerformancewithdifferenttopicnumbersThissectioncomparesgMedLDAandiMedLDAwithbaselinemethods.MedLDAwasshowntooutperformsLDAfordocumentclassiﬁcation.Here wefocusoncomparingtheperformanceofMedLDAandLDAwhenusingdifferentinferencealgorithms.Speciﬁcally wecomparewiththe4http://mallet.cs.umass.edu/6204060801001200.40.50.60.70.8# TopicsAccuracy  iMedLDAgMedLDAfMedLDAgLDAfLDA(a)2040608010012012345# TopicsAverage Entropy over Docs  iMedLDAgMedLDAfMedLDAgLDAfLDA(b)20406080100120456789# TopicsAverage Entropy over Topics  iMedLDAgMedLDAfMedLDAgLDAfLDA(c)Figure1:Performanceofmulti-classclassiﬁcationofdifferenttopicmodelswithdifferenttopicnumberson20-Newsgroupsdataset:(a)classiﬁcationaccuracy (b)theaverageentropyofΘovertestdocuments and(c)TheaverageentropyoftopicdistributionsΦ.LDAmodelthatusescollapsedGibbssampling[5](denotedbygLDA)andtheLDAmodelthatusesfully-factorizedvariationalmethods[3](denotedbyfLDA).ForLDAmodels wediscoverthelatentrepresentationsofthetrainingdocumentsandusethemtobuildamulti-classSVMclassiﬁer.ForMedLDA wereporttheresultswhenusingfully-factorizedvariationalmethods(denotedbyfMedLDA)asin[16].Furthermore fMedLDAandfLDAoptimizethehyper-parameterαusingtheNewton-Rampionmethod[3] whilegMedLDA iMedLDAandgLDAdetermineαby5-foldcross-validation.Wehavetestedawiderangeofvaluesofβ(e.g. 10−16∼103)andfoundthattheperformanceofiMedLDAdegradesseriouslywhenβislargerthan10−3.Therefore wesetβtobe10−5foriMedLDAwhile0.01fortheothertopicmodelsjustasintheliterature[5].Fig.1(a)showstheaccuracy.WecanseethatMonteCarlomethodsgenerallyoutperformthefully-factorizedmean-ﬁeldmethods mainlybecauseoftheirweakerfactorizationassumptions.Therea-sonforthesuperiorperformanceofiMedLDAovergMedLDAisprobablybecauseiMedLDAismoreeffectiveindealingwithsamplesparsityissues.MoreinsightswillbeprovidedinSection5.2.Fig.1(b)showstheaverageentropyoflatentrepresentationsΘovertestdocuments.WeﬁndthattheentropyofgMedLDAandiMedLDAaresmallerthanthoseofgLDAandfLDA especiallyfor(relatively)largeK.ThisimpliesthatsamplingmethodsforMedLDAcaneffectivelyconcentratetheprobabilitymassonjustseveraltopicsthusdiscovermorepredictivetopicrepresentations.How-ever fMedLDAyieldsthesmallestentropy whichismainlybecausethefully-factorizedvariationalmethodstendtogettoocompactresults e.g. sparselocaloptimums.Fig.1(c)showstheaverageentropyoftopicdistributionsΦovertopics.WecanseethatgMedLDAimprovesthesparsityofΦthanfMedLDA.However gMedLDA’sentropyislargerthangLDA’s.Thisisbecauseforthose“hard”documents theexponentialcomponentinEq.(21)“regularizes”theconditionalprobabilityp(zdn|Z¬)andleadstoasmootherestimateofΦ.Ontheotherhand weﬁndthatiMedLDAhasthelargestentropy.Thisisprobablybecausemanyofthesamples(topicassignments)generatedbytheproposaldistributionare“incorrect”butimportancesamplerstillassignsweightstothesesamples.Asaresult theinferredtopicdistributionsareverydenseandthushavealargeentropy.Moreover intheaboveexperiments wefoundthatthelagrangemultipliersinMedLDAareverysparse(about1%non-zerosforbothiMedLDAandgMedLDA;about1.5%forfMedLDA) muchsparserthanthoseofSVMbuiltonrawinputdata(about8%non-zeros).5.2SensitivityanalysiswithrespecttokeyparametersSensitivitytoα.Fig.2(a)showstheclassiﬁcationperformanceofgMedLDAandiMedLDAwithdifferentvaluesofα.WecanseethattheperformanceofgMedLDAincreasesasαbecomeslargeandretainsstablewhenαislargerthan0.1.Incontrast theaccuracyofiMedLDAdecreasesabit(especiallyforsmallK)whenαbecomeslarge butisrelativestablewhenαissmall(e.g. ≤0.01).Thisisprobablybecausewithaﬁnitenumberofsamples GibbssamplertendstoproduceatoosparseestimateofE[Z] andaslightlystrongerpriorishelpfultodealwiththesamplesparsityissue.Incontrast theimportancesampleravoidssuchsparsityissuebyusingauniformproposaldistribution whichcouldmakethesampleswellcoveralltopicdimensions.Thus asmallpriorissufﬁcienttogetgoodperformance andincreasingtheprior’sstrengthcouldpotentiallyhurt.SensitivitytosamplesizeJ.Forsamplingmethods wealwaysneedtodecidehowmanysamples(samplesizeJ)tokeeptoensuresufﬁcientstatisticspower.Fig.2(b)showstheclassiﬁcationaccu-racyofbothgMedLDAandiMedLDAwithdifferentsamplesizeJwhenα=10−2/KandC=16.710−410−310−210−11000.50.60.70.8αAccuracy  iMedLDAK=30iMedLDAK=60iMedLDAK=90gMedLDAK=30gMedLDAK=60gMedLDAK=90(a)510100100000.20.40.60.8Sample SizeAccuracy  iMedLDAK=30iMedLDAK=60iMedLDAK=90gMedLDAK=30gMedLDAK=60gMedLDAK=90(b)10−410−310−20.70.750.80.85εAccuracy  K=30K=60K=90(c)15105010000.20.40.60.81# iterationAccuracy  iMedLDAgMedLDAfMedLDA(d)Figure2:SensitivitystudyofiMedLDAandgMedLDA:(a)classiﬁcationaccuracywithdifferentαfordifferenttopicnumbers (b)classiﬁcationaccuracywithdifferentsamplesizeJ (c)classiﬁcationaccuracywithdifferentconvergencecriterionϵforgMedLDA and(d)classiﬁcationaccuracyofdifferentmethodsvariesasafunctionofiterationswhenthetopicnumberis30.ForgMedLDA wehavetesteddifferentvaluesofJfortrainingandprediction.Wefoundthatthesamplesizeinthetrainingprocesshasalmostnoinﬂuenceonthepredictionaccuracyevenwhenitequalsto1.Hence forefﬁciency wesetJtobe1duringthetraining.ItshowsthatgMedLDAisrelativelystablewhenJislargerthanabout20atprediction.ForiMedLDA Fig.2(b)showsthatitbecomesstablewhenthepredictionsamplesizeJislargerthan3×K.Sensitivitytoconvergencecriterionϵ.ForgMedLDA wehavetojudgewhetheraMarkovchainhasreacheditsstationarity.Relativechangeintheobjectiveisacommonlyuseddiagnostictojustifytheconvergence.Westudytheinﬂuenceofϵ.Inthisexperiment wedon’tboundthemaximumnumberofiterationsandallowtheGibbssamplertorununtilthetoleranceϵisreached.Fig.2(c)showstheaccuracyofgMedLDAwithdifferentvaluesofϵ.WecanseethatgMedLDAisrelativelyinsensitivetoϵ.ThisismainlybecausegMedLDAalternatelyupdatesposteriordistributionandLagrangianmultipliers.Thus itdoesGibbssamplingformanytimes whichcompensatesfortheinﬂuencethateachMarkovchainhasnotreacheditsstationarity.Ontheotherhand smallϵvaluescangreatlyslowtheconvergence.Forinstance whenthetopicnumberis90 gMedLDAtakes11 986secondsontrainingwhenϵ=10−4but1 795secondswhenϵ=10−2.Theseresultsimplythatwecanloosetheconvergencecriteriontospeeduptrainingwhilestillobtainagoodmodel.Sensitivitytoiteration.Fig.2(d)showsthetheclassiﬁcationaccuracyofMedLDAwithvariousinferencemethodsasafunctionofiterationwhenthetopicnumberissetat30.WecanseethatallthevariousMedLDAmodelsconvergequitequicklytogetgoodaccuracy.ComparedtofMedLDA whichusesmean-ﬁeldvariationalinference thetwoMedLDAmodelsusingMonteCarlomethods(i.e. iMedLDAandgMedLDA)areslightlyfastertogetstablepredictionperformance.5.3Timeefﬁciency20406080100120102103104# TopicsCPU−Seconds  iMedLDAgMedLDAfMedLDAgLDAfLDAFigure3:Trainingtime.AlthoughgMedLDAcangetgoodresultsevenforaloosenconver-gencecriterionϵasdiscussedinSec.5.2 wesetϵtobe10−4forallthemethodsinordertogetamoreobjectivecomparison.Fig.3reportsthetotaltrainingtimeofdifferentmodels whichincludestwophases–inferringthelatenttopicrepresentationsandtrainingSVMs.WeﬁndiMedLDAisthemostefﬁcient whichbeneﬁtsfrom(1)generateingsamplesoutsidetheiterationloopandusesthemforalliterations;and(2)usingtheMAPestimatestocollapsethesamplespaceof(Θ Φ)toa“singlesample”forefﬁciency.Incontrast bothgMedLDAandfMedLDAhavetoiterativelyupdatethevariablesorvariationalparameters.gMedLDArequiresmoretimethanfMedLDAbutiscompara-blewhenϵissettobe0.01.Byusingtheequivalent1-slackformulation about76%ofthetrainingtimespentoninferenceforiMedLDAand90%forgMedLDA.Forprediction bothiMedLDAandgMedLDAareslightlyslowerthanfMedLDA.6ConclusionsWehavepresentedtwoMonteCarlomethodsforMedLDA asupervisedtopicmodelusingmax-marginconstraintsdirectlyonthedesiredposteriordistributionsfordiscoveringpredictivelatenttopicrepresentations.OurmethodsarebasedonanovelinterpretationofMedLDAasaregular-izedBayesianmodelandtheaconvexdualformulationtodealwithsoft-marginconstraints.Ex-perimentalresultsonthe20NewsgroupsdatasetshowthatMonteCarlomethodsarerobusttohyper-parametersandcouldyieldverycompetitiveresultsforsuchmax-margintopicmodels.8AcknowledgementsPartoftheworkwasdonewhenQJwasvisitingCMU.JZandMSaresupportedbytheNationalBasicResearchProgramofChina(No.2013CB329403and2012CB316301) NationalNaturalScienceFoundationofChina(No.91120011 61273023and61170196)andTsinghuaInitiativeScientiﬁcResearchProgramNo.20121088071.EXissupportedbyAFOSRFA95501010247 ONRN000140910758 NSFCareerDBI-0546594andAlfredP.SloanResearchFellowship.References[1]C.M.Bishop.Patternrecognitionandmachinelearning volume4.springerNewYork 2006.[2]D.M.BleiandJ.D.McAuliffe.Supervisedtopicmodels.NIPS pages121–128 2007.[3]D.M.Blei A.Y.Ng andM.I.Jordan.LatentDirichletallocation.JMLR 3:993–1022 2003.[4]A.Gelman J.B.Carlin H.S.Stern andD.B.Rubin.Bayesiandataanalysis.BocaRaton FL:ChapmanandHall/CRC 2004.[5]T.L.GrifﬁthsandM.Steyvers.Findingscientiﬁctopics.Proc.ofNationalAcademyofSci. pages5228–5235 2004.[6]T.Joachims T.Finley andC.N.J.Yu.Cutting-planetrainingofstructuralSVMs.MachineLearning 77(1):27–59 2009.[7]M.I.Jordan Z.Ghahramani T.S.Jaakkola andL.K.Saul.Anintroductiontovariationalmethodsforgraphicalmodels.Machinelearning 37(2):183–233 1999.[8]S.Lacoste-Jullien F.Sha andM.I.Jordan.DiscLDA:Discriminativelearningfordimension-alityreductionandclassiﬁcation.NIPS pages897–904 2009.[9]D.Li S.Somasundaran andA.Chakraborty.Acombinationoftopicmodelswithmax-marginlearningforrelationdetection.InACLTextGraphs-6Workshop 2011.[10]R.Y.RubinsteinandD.P.Kroese.SimulationandtheMonteCarlomethod volume707.Wiley-interscience 2008.[11]E.Schoﬁeld.Fittingmaximum-entropymodelsonlargesamplespaces.PhDthesis Depart-mentofComputing ImperialCollegeLondon 2006.[12]C.Wang D.M.Blei andLiF.F.Simultaneousimageclassiﬁcationandannotation.CVPR 2009.[13]Y.WangandG.Mori.Max-marginlatentDirichletallocationforimageclassiﬁcationandannotation.InBMVC 2011.[14]S.Yang J.Bian andH.Zha.Hybridgenerative/discriminativelearningforautomaticimageannotation.InUAI 2010.[15]A.Zellner.OptimalinformationprocessingandBayes’stheorem.AmericanStatistician pages278–280 1988.[16]J.Zhu A.Ahmed andE.P.Xing.MedLDA:maximummarginsupervisedtopicmodelsforregressionandclassiﬁcation.InICML pages1257–1264 2009.[17]J.Zhu N.Chen andE.P.Xing.InﬁnitelatentSVMforclassiﬁcationandmulti-tasklearning.InNIPS 2011.9,Adams Wei Yu
Wanli Ma
Yaoliang Yu
Jaime Carbonell
Suvrit Sra
Jack Serrino
Max Kleiman-Weiner
David Parkes
Josh Tenenbaum