2011,Shallow vs. Deep Sum-Product Networks,We investigate the representational power of sum-product networks  (computation networks analogous to neural networks   but whose individual units compute either products  or weighted sums)  through a theoretical analysis that compares  deep (multiple hidden layers) vs. shallow (one hidden layer) architectures.  We prove there exist families of functions that can be represented  much more efficiently with a deep network than with a shallow one  i.e.  with substantially fewer hidden units.  Such results were not available until now  and  contribute to motivate recent research involving learning of deep  sum-product networks  and more generally motivate research in Deep  Learning.,Shallow vs. Deep Sum-Product Networks

Olivier Delalleau

Department of Computer Science and Operation Research

Universit´e de Montr´eal

delallea@iro.umontreal.ca

Department of Computer Science and Operation Research

Yoshua Bengio

Universit´e de Montr´eal

yoshua.bengio@umontreal.ca

Abstract

We investigate the representational power of sum-product networks (computation
networks analogous to neural networks  but whose individual units compute either
products or weighted sums)  through a theoretical analysis that compares deep
(multiple hidden layers) vs. shallow (one hidden layer) architectures. We prove
there exist families of functions that can be represented much more efﬁciently
with a deep network than with a shallow one  i.e. with substantially fewer hidden
units. Such results were not available until now  and contribute to motivate recent
research involving learning of deep sum-product networks  and more generally
motivate research in Deep Learning.

Introduction and prior work

1
Many learning algorithms are based on searching a family of functions so as to identify one member
of said family which minimizes a training criterion. The choice of this family of functions and how
members of that family are parameterized can be a crucial one. Although there is no universally
optimal choice of parameterization or family of functions (or “architecture”)  as demonstrated by
the no-free-lunch results [37]  it may be the case that some architectures are appropriate (or inap-
propriate) for a large class of learning tasks and data distributions  such as those related to Artiﬁcial
Intelligence (AI) tasks [4]. Different families of functions have different characteristics that can be
appropriate or not depending on the learning task of interest. One of the characteristics that has
spurred much interest and research in recent years is depth of the architecture. In the case of a
multi-layer neural network  depth corresponds to the number of (hidden and output) layers. A ﬁxed-
kernel Support Vector Machine is considered to have depth 2 [4] and boosted decision trees to have
depth 3 [7]. Here we use the word circuit or network to talk about a directed acyclic graph  where
each node is associated with some output value which can be computed based on the values associ-
ated with its predecessor nodes. The arguments of the learned function are set at the input nodes of
the circuit (which have no predecessor) and the outputs of the function are read off the output nodes
of the circuit. Different families of functions correspond to different circuits and allowed choices
of computations in each node. Learning can be performed by changing the computation associated
with a node  or rewiring the circuit (possibly changing the number of nodes). The depth of the circuit
is the length of the longest path in the graph from an input node to an output node.
Deep Learning algorithms [3] are tailored to learning circuits with variable depth  typically greater
than depth 2. They are based on the idea of multiple levels of representation  with the intuition that
the raw input can be represented at different levels of abstraction  with more abstract features of
the input or more abstract explanatory factors represented by deeper circuits. These algorithms are
often based on unsupervised learning  opening the door to semi-supervised learning and efﬁcient

1

use of large quantities of unlabeled data [3]. Analogies with the structure of the cerebral cortex (in
particular the visual cortex) [31] and similarities between features learned with some Deep Learning
algorithms and those hypothesized in the visual cortex [17] further motivate investigations into deep
architectures. It has been suggested that deep architectures are more powerful in the sense of being
able to more efﬁciently represent highly-varying functions [4  3]. In this paper  we measure “efﬁ-
ciency” in terms of the number of computational units in the network. An efﬁcient representation
is important mainly because: (i) it uses less memory and is faster to compute  and (ii) given a ﬁxed
amount of training samples and computational power  better generalization is expected.
The ﬁrst successful algorithms for training deep architectures appeared in 2006  with efﬁcient train-
ing procedures for Deep Belief Networks [14] and deep auto-encoders [13  27  6]  both exploiting
the general idea of greedy layer-wise pre-training [6]. Since then  these ideas have been inves-
tigated further and applied in many settings  demonstrating state-of-the-art learning performance
in object recognition [16  28  18  15] and segmentation [20]  audio classiﬁcation [19  10]  natural
language processing [9  36  21  32]  collaborative ﬁltering [30]  modeling textures [24]  modeling
motion [34  33]  information retrieval [29  26]  and semi-supervised learning [36  22].
Poon and Domingos [25] introduced deep sum-product networks as a method to compute partition
functions of tractable graphical models. These networks are analogous to traditional artiﬁcial neural
networks but with nodes that compute either products or weighted sums of their inputs. Analo-
gously to neural networks  we deﬁne “hidden” nodes as those nodes that are neither input nodes nor
output nodes. If the nodes are organized in layers  we deﬁne the “hidden” layers to be those that
are neither the input layer nor the output layer. Poon and Domingos [25] report experiments with
networks much deeper (30+ hidden layers) than those typically used until now  e.g. in Deep Belief
Networks [14  3]  where the number of hidden layers is usually on the order of three to ﬁve.
Whether such deep architectures have theoretical advantages compared to so-called “shallow” archi-
tectures (i.e. those with a single hidden layer) remains an open question. After all  in the case of a
sum-product network  the output value can always be written as a sum of products of input variables
(possibly raised to some power by allowing multiple connections from the same input)  and conse-
quently it is easily rewritten as a shallow network with a sum output unit and product hidden units.
The argument supported by our theoretical analysis is that a deep architecture is able to compute
some functions much more efﬁciently than a shallow one.
Until recently  very few theoretical results supported the idea that deep architectures could present
an advantage in terms of representing some functions more efﬁciently. Most related results originate
from the analysis of boolean circuits (see e.g. [2] for a review). Well-known results include the
proof that solving the n-bit parity task with a depth-2 circuit requires an exponential number of
gates [1  38]  and more generally that there exist functions computable with a polynomial-size depth-
k circuit that would require exponential size when restricted to depth k − 1 [11]. Another recent
result on boolean circuits by Braverman [8] offers proof of a longstanding conjecture  showing that
bounded-depth boolean circuits are unable to distinguish some (non-uniform) input distributions
from the uniform distribution (i.e.
In particular 
Braverman’s result suggests that shallow circuits can in general be fooled more easily than deep
ones  i.e.  that they would have more difﬁculty efﬁciently representing high-order dependencies
(those involving many input variables).
It is not obvious that circuit complexity results (that typically consider only boolean or at least dis-
crete nodes) are directly applicable in the context of typical machine learning algorithms such as
neural networks (that compute continuous representations of their input). Orponen [23] surveys the-
oretical results in computational complexity that are relevant to learning algorithms. For instance 
H˚astad and Goldmann [12] extended some results to the case of networks of linear threshold units
with positivity constraints on the weights. Bengio et al. [5  7] investigate  respectively  complexity
issues in networks of Gaussian radial basis functions and decision trees  showing intrinsic limitations
of these architectures e.g. on tasks similar to the parity problem. Utgoff and Stracuzzi [35] infor-
mally discuss the advantages of depth in boolean circuit in the context of learning architectures.
Bengio [3] suggests that some polynomials could be represented more efﬁciently by deep sum-
product networks  but without providing any formal statement or proofs. This work partly addresses
this void by demonstrating families of circuits for which a deep architecture can be exponentially
more efﬁcient than a shallow one in the context of real-valued polynomials.
Note that we do not address in this paper the problem of learning these parameters: even if an
efﬁcient deep representation exists for the function we seek to approximate  in general there is no

they are “fooled” by such input distributions).

2

guarantee for standard optimization algorithms to easily converge to this representation. This paper
focuses on the representational power of deep sum-product circuits compared to shallow ones  and
studies it by considering particular families of target functions (to be represented by the learner).
We ﬁrst formally deﬁne sum-product networks. We consider two families of functions represented
by deep sum-product networks (families F and G). For each family  we establish a lower bound on
the minimal number of hidden units a depth-2 sum-product network would require to represent a
function of this family  showing it is much less efﬁcient than the deep representation.

2 Sum-product networks
Deﬁnition 1. A sum-product network is a network composed of units that either compute the product
of their inputs or a weighted sum of their inputs (where weights are strictly positive).

Here  we restrict our deﬁnition of the generic term “sum-product network” to networks whose sum-
mation units have positive incoming weights1  while others are called “negative-weight” networks.
Deﬁnition 2. A “negative-weight“ sum-product network may contain summation units whose
weights are non-positive (i.e. less than or equal to zero).

Finally  we formally deﬁne what we mean by deep vs. shallow networks in the rest of the paper.
Deﬁnition 3. A “shallow“ sum-product network contains a single hidden layer (i.e. a total of three
layers when counting the input and output layers  and a depth equal to two).
Deﬁnition 4. A “deep“ sum-product network contains more than one hidden layer (i.e. a total of at
least four layers  and a depth at least three).

3 The family F
3.1 Deﬁnition
The ﬁrst family of functions we study  denoted by F  is made of functions built from deep sum-
product networks that alternate layers of product and sum units with two inputs each (details are
provided below). The basic idea we use here is that composing layers (i.e. using a deep architec-
ture) is equivalent to using a factorized representation of the polynomial function computed by the
network. Such a factorized representation can be exponentially more compact than its expansion as
a sum of products (which can be associated to a shallow network with product units in its hidden
layer and a sum unit as output). This is what we formally show in what follows.

+

1 = λ11ℓ1
ℓ2

1 + µ11ℓ1

2 = x1x2 + x3x4 = f (x1  x2  x3  x4)

λ11 = 1

µ11 = 1

ℓ1
1 = x1x2

××

ℓ1
2 = x3x4

×

x1

x2

x3

x4

Figure 1: Sum-product network computing the function f ∈ F such that i = λ11 = µ11 = 1.

Let n = 4i  with i a positive integer value. Denote by ℓ0 the input layer containing scalar variables
{x1  . . .   xn}  such that ℓ0
j = xj for 1 ≤ j ≤ n. Now deﬁne f ∈ F as any function computed by a
sum-product network (deep for i ≥ 2) composed of alternating product and sum layers:

= ℓ2k

j

2j

j = λjkℓ2k−1

• ℓ2k+1
• ℓ2k

2j−1 · ℓ2k
2j−1 + µjkℓ2k−1

2j for 0 ≤ k ≤ i − 1 and 1 ≤ j ≤ 22(i−k)−1
for 1 ≤ k ≤ i and 1 ≤ j ≤ 22(i−k)
where the weights λjk and µjk of the summation units are strictly positive.
1 ∈ R  the unique unit in the last layer.
The output of the network is given by f (x1  . . .   xn) = ℓ2i
The corresponding (shallow) network for i = 1 and additive weights set to one is shown in Figure 1

1This condition is required by some of the proofs presented here.

3

(this architecture is also the basic building block of bigger networks for i > 1). Note that both the
input size n = 4i and the network’s depth 2i increase with parameter i.

3.2 Theoretical results
The main result of this section is presented below in Corollary 1  providing a lower bound on the
minimum number of hidden units required by a shallow sum-product network to represent a function
f ∈ F. The high-level proof sketch consists in the following steps:
(1) Count the number of unique products found in the polynomial representation of f (Lemma 1 and
Proposition 1).
(2) Show that the only possible architecture for a shallow sum-product network to compute f is to
have a hidden layer made of product units  with a sum unit as output (Lemmas 2 to 5).
(3) Conclude that the number of hidden units must be at least the number of unique products com-
puted in step 3.2 (Lemma 6 and Corollary 1).
Lemma 1. Any element ℓk
j can be written as a (positively) weighted sum of products of input vari-
ables  such that each input variable xt is used in exactly one unit of ℓk. Moreover  the number mk of
products found in the sum computed by ℓk
j does not depend on j and obeys the following recurrence
rule for k ≥ 0: if k + 1 is odd  then mk+1 = m2
Proof. We prove the lemma by induction on k.
Assuming this is true for some k ≥ 0  we consider two cases:

It is obviously true for k = 0 since ℓ0

k  otherwise mk+1 = 2mk.

j = xj.

j = ℓk

2j−1 · ℓk

• If k + 1 is odd  then ℓk+1
2j−1 and ℓk

2j. By the inductive hypothesis  it is the product of
two (positively) weighted sums of products of input variables  and no input variable can
appear in both ℓk
2j  so the result is also a (positively) weighted sum of products
2j is mk  then
of input variables. Additionally  if the number of products in ℓk
k  since all products involved in the multiplication of the two units are different
mk+1 = m2
(since they use disjoint subsets of input variables)  and the sums have positive weights.
Finally  by the induction assumption  an input variable appears in exactly one unit of ℓk.
This unit is an input to a single unit of ℓk+1  that will thus be the only unit of ℓk+1 where
this input variable appears.
• If k + 1 is even  then ℓk+1

2j. Again  from the induction assumption  it
must be a (positively) weighted sum of products of input variables  but with mk+1 = 2mk
such products. As in the previous case  an input variable will appear in the single unit of
ℓk+1 that has as input the single unit of ℓk in which this variable must appear.

2j−1 + µjkℓk

2j−1 and ℓk

j = λjkℓk

Proposition 1. The number of products in the sum computed in the output unit l2i

1 of a network

computing a function in F is m2i = 2√n−1.
Proof. We ﬁrst prove by induction on k ≥ 1 that for odd k  mk = 22
mk = 22
single products of the form xrxs. Assuming this is true for some k ≥ 1  then:

2 −1. This is obviously true for k = 1 since 22

1+1

k

k+1

2 −2  and for even k 
2 −2 = 20 = 1  and all units in ℓ1 are

• if k + 1 is odd  then from Lemma 1 and the induction assumption  we have:

k = (cid:18)22
• if k + 1 is even  then instead we have:

mk+1 = m2

k

2 −1(cid:19)2

k
2

= 22

+1−2 = 22

(k+1)+1

2

−2

mk+1 = 2mk = 2 · 22

k+1

2 −2 = 22

(k+1)

2 −1

which shows the desired result for k + 1  and thus concludes the induction proof. Applying this
result with k = 2i (which is even) yields

2i

m2i = 22

2 −1 = 2

√22i−1 = 2

√n−1.

4

Lemma 2. The products computed in the output unit l2i
containing only variables x1  . . .   x n

and one containing only variables x n

1 can be split in two groups  one with products

2

2 +1  . . .   xn.

Proof. This is obvious since the last unit is a “sum“ unit that adds two terms whose inputs are these
two groups of variables (see e.g. Fig. 1).

Lemma 3. The products computed in the output unit l2i

1 involve more than one input variable.

Proof. It is straightforward to show by induction on k ≥ 1 that the products computed by lk
involve more than one input variable  thus it is true in particular for the output layer (k = 2i).
Lemma 4. Any shallow sum-product network computing f ∈ F must have a “sum” unit as output.

j all

Proof. By contradiction  suppose the output unit of such a shallow sum-product network is multi-
plicative. This unit must have more than one input  because in the case that it has only one input 
the output would be either a (weighted) sum of input variables (which would violate Lemma 3)  or
a single product of input variables (which would violate Proposition 1)  depending on the type (sum
or product) of the single input hidden unit. Thus the last unit must compute a product of two or
more hidden units. It can be re-written as a product of two factors  where each factor corresponds to
either one hidden unit  or a product of multiple hidden units (it does not matter here which speciﬁc
factorization is chosen among all possible ones). Regardless of the type (sum or product) of the
hidden units involved  those two factors can thus be written as weighted sums of products of vari-
ables xt (with positive weights  and input variables potentially raised to powers above one). From
Lemma 1  both x1 and xn must be present in the ﬁnal output  and thus they must appear in at least
one of these two factors. Without loss of generality  assume x1 appears in the ﬁrst factor. Variables
2 +1  . . .   xn then cannot be present in the second factor  since otherwise one product in the output
x n
would contain both x1 and one of these variables (this product cannot cancel out since weights must
be positive)  violating Lemma 2. But with a similar reasoning  since as a result xn must appear in
the ﬁrst factor  variables x1  . . .   x n
cannot be present in the second factor either. Consequently  no
input variable can be present in the second factor  leading to the desired contradiction.

2

Lemma 5. Any shallow sum-product network computing f ∈ F must have only multiplicative units
in its hidden layer.

Proof. By contradiction  suppose there exists a “sum“ unit in the hidden layer  written s =

Pt∈S αtxt with S the set of input indices appearing in this sum  and αt > 0 for all t ∈ S. Since
according to Lemma 4 the output unit must also be a sum (and have positive weights according to
Deﬁnition 1)  then the ﬁnal output will also contain terms of the form βtxt for t ∈ S  with βt > 0.
This violates Lemma 3  establishing the contradiction.

Lemma 6. Any shallow negative-weight sum-product network (see Deﬁnition 2) computing f ∈ F
must have at least 2√n−1 hidden units  if its output unit is a sum and its hidden units are products.

its output can be written as ΣjwjΠtx

Proof. Such a network computes a weighted sum of its hidden units  where each hidden unit is a
t with wj ∈ R and γjt ∈
product of input variables  i.e.
{0  1}. In order to compute a function in F  this shallow network thus needs a number of hidden
units at least equal to the number of unique products in that function. From Proposition 1  this
number is equal to 2√n−1.
Corollary 1. Any shallow sum-product network computing f ∈ F must have at least 2√n−1 hidden

γjt

units.

Proof. This is a direct corollary of Lemmas 4 (showing the output unit is a sum)  5 (showing that
hidden units are products)  and 6 (showing the desired result for any shallow network with this
speciﬁc structure – regardless of the sign of weights).

5

3.3 Discussion
Corollary 1 above shows that in order to compute some function in F with n inputs  the number of
units in a shallow network has to be at least 2√n−1  (i.e. grows exponentially in √n). On another
hand  the total number of units in the deep (for i > 1) network computing the same function  as
described in Section 3.1  is equal to 1 + 2 + 4 + 8 + . . . + 22i−1 (since all units are binary)  which is

also equal to 22i − 1 = n − 1 (i.e. grows only quadratically in √n). It shows that some deep sum-
product network with n inputs and depth O(log n) can represent with O(n) units what would
require O(2√n) units for a depth-2 network. Lemma 6 also shows a similar result regardless
of the sign of the weights in the summation units of the depth-2 network  but assumes a speciﬁc
architecture for this network (products in the hidden layer with a sum as output).

4 The family G
In this section we present similar results with a different family of functions  denoted by G. Com-
pared to F  one important difference of deep sum-product networks built to deﬁne functions in G
is that they can vary their input size independently of their depth. Their analysis thus provides ad-
ditional insight when comparing the representational efﬁciency of deep vs. shallow sum-product
networks in the case of a ﬁxed dataset.

4.1 Deﬁnition
Networks in family G also alternate sum and product layers  but their units have as inputs all units
from the previous layer except one. More formally  deﬁne the family G = ∪n≥2 i≥0Gin of func-
tions represented by sum-product networks  where the sub-family Gin is made of all sum-product
networks with n input variables and 2i + 2 layers (including the input layer ℓ0)  such that:

1. ℓ1 contains summation units; further layers alternate multiplicative and summation units.
2. Summation units have positive weights.
3. All layers are of size n  except the last layer ℓ2i+1 that contains a single sum unit that sums

all units in the previous layer ℓ2i.

4. In each layer ℓk for 1 ≤ k ≤ 2i  each unit ℓk

j takes as inputs {ℓk−1

m |m 6= j}.

An example of a network belonging to G1 3 (i.e. with three layers and three input variables) is shown
in Figure 2.

1 = x2
ℓ3

1 + x2

2 + x2

3 + 3(x1x2 + x1x3 + x2x3) = g(x1  x2  x3)

1 = x2
ℓ2

1 + x1x2

+x1x3 + x2x3

ℓ1
1 = x2 + x3

×

+

+

×

ℓ2
2 = . . .

×

3 = x2
ℓ2

3 + x1x2

+x1x3 + x2x3

+

ℓ1
2 = x1 + x3

+

ℓ1
3 = x1 + x2

x1

x2

x3

Figure 2: Sum-product network computing a function of G1 3 (summation units’ weights are all 1’s).
4.2 Theoretical results
The main result is stated in Proposition 3 below  establishing a lower bound on the number of hidden
units of a shallow sum-product network computing g ∈ G. The proof sketch is as follows:

1. We show that the polynomial expansion of g must contain a large set of products (Proposi-

tion 2 and Corollary 2).

2. We use both the number of products in that set as well as their degree to establish the

desired lower bound (Proposition 3).

6

We will also need the following lemma  which states that when n − 1 items each belong to n − 1
sets among a total of n sets  then we can associate to each item one of the sets it belongs to without
using the same set for different items.
Lemma 7. Let S1  . . .   Sn be n sets (n ≥ 2) containing elements of {P1  . . .   Pn−1}  such that for
any q  r  |{r|Pq ∈ Sr}| ≥ n − 1 (i.e. each element Pq belongs to at least n − 1 sets). Then there
exist r1  . . .   rn−1 different indices such that Pq ∈ Srq for 1 ≤ q ≤ n − 1.

Proof. Omitted due to lack of space (very easy to prove by construction).

Proposition 2. For any 0 ≤ j ≤ i  and any product of variables P = Πn
such that αt ∈ N and
Pt αt = (n − 1)j  there exists a unit in ℓ2j whose computed value  when expanded as a weighted
sum of products  contains P among these products.

t=1xαt

t

t=1xαt

t

Proof. We prove this proposition by induction on j.
First  for j = 0  this is obvious since any P of this form must be made of a single input variable xt 
that appears in ℓ0
Suppose now the proposition is true for some j < i. Consider a product P = Πn

such that

t = xt.

βqt
t=1x
t

αt ∈ N and Pt αt = (n − 1)j+1. P can be factored in n − 1 sub-products of degree (n − 1)j 
  βqt ∈ N and Pt βqt = (n − 1)j for all q. By
i.e. written P = P1 . . . Pn−1 with Pq = Πn
the induction hypothesis  each Pq can be found in at least one unit ℓ2j
. As a result  by property 4
kq
(in the deﬁnition of family G)  each Pq will also appear in the additive layer ℓ2j+1  in at least n − 1
different units (the only sum unit that may not contain Pq is the one that does not have ℓ2j
as input).
kq
By Lemma 7  we can thus ﬁnd a set of units ℓ2j+1
such that for any 1 ≤ q ≤ n − 1  the product
Pq appears in ℓ2j+1
  with indices rq being different from each other. Let 1 ≤ s ≤ n be such that
s 6= rq for all q. Then  from property 4 of family G  the multiplicative unit ℓ2(j+1)
computes the
product Πn−1
  and as a result  when expanded as a sum of products  it contains in particular
P1 . . . Pn−1 = P . The proposition is thus true for j + 1  and by induction  is true for all j ≤ i.
Corollary 2. The output gin of a sum-product network in Gin  when expanded as a sum of products 
contains all products of variables of the form Πn
such that αt ∈ N and Pt αt = (n − 1)i.

q=1 ℓ2j+1

t=1xαt

rq

rq

t

rq

s

Proof. Applying Proposition 2 with j = i  we obtain that all products of this form can be found in
the multiplicative units of ℓ2i. Since the output unit ℓ2i+1
computes a sum of these multiplicative
units (weighted with positive weights)  those products are also present in the output.

1

Proposition 3. A shallow negative-weight sum-product network computing gin ∈ Gin must have at
least (n − 1)i hidden units.
Proof. First suppose the output unit of the shallow network is a sum. Then it may be able to compute
gin  assuming we allow multiplicative units in the hidden layer in the hidden layer to use powers
of their inputs in the product they compute (which we allow here for the proof to be more generic).
However  it will require at least as many of these units as the number of unique products that can
be found in the expansion of gin. In particular  from Corollary 2  it will require at least the number
t=1 αt = (n − 1)i. Denoting
(cid:1)  and it is easy to verify it is higher

of unique tuples of the form (α1  . . .   αn) such that αt ∈ N and Pn
dni = (n − 1)i  this number is known to be equal to (cid:0)n+dni−1
than (or equal to) dni for any n ≥ 2 and i ≥ 0.
Now suppose the output unit is multiplicative. Then there can be no multiplicative hidden unit 
otherwise it would mean one could factor some input variable xt in the computed function output:
this is not possible since by Corollary 2  for any variable xt there exist products in the output function
that do not involve xt. So all hidden units must be additive  and since the computed function contains
products of degree dni  there must be at least dni such hidden units.

dni

7

4.3 Discussion
Proposition 3 shows that in order to compute the same function as gin ∈ Gin  the number of units
in the shallow network has to grow exponentially in i  i.e. in the network’s depth (while the deep
network’s size grows linearly in i). The shallow network also needs to grow polynomially in the
number of input variables n (with a degree equal to i)  while the deep network grows only linearly in
n. It means that some deep sum-product network with n inputs and depth O(i) can represent
with O(ni) units what would require O((n − 1)i) units for a depth-2 network.
Note that in the similar results found for family F  the depth-2 network computing the same function
as a function in F had to be constrained to either have a speciﬁc combination of sum and hidden
units (in Lemma 6) or to have non-negative weights (in Corollary 1). On the contrary  the result
presented here for family G holds without requiring any of these assumptions.
5 Conclusion

We compared a deep sum-product network and a shallow sum-product network representing the
same function  taken from two families of functions F and G. For both families  we have shown that
the number of units in the shallow network has to grow exponentially  compared to a linear growth
in the deep network  so as to represent the same functions. The deep version thus offers a much
more compact representation of the same functions.
This work focuses on two speciﬁc families of functions: ﬁnding more general parameterization of
functions leading to similar results would be an interesting topic for future research. Another open
question is whether it is possible to represent such functions only approximately (e.g. up to an
error bound ǫ) with a much smaller shallow network. Results by Braverman [8] on boolean circuits
suggest that similar results as those presented in this paper may still hold  but this topic has yet to be
formally investigated in the context of sum-product networks. A related problem is also to look into
functions deﬁned only on discrete input variables: our proofs do not trivially extend to this situation
because we cannot assume anymore that two polynomials yielding the same output values must have
the same expansion coefﬁcients (since the number of input combinations becomes ﬁnite).

Acknowledgments

The authors would like to thank Razvan Pascanu and David Warde-Farley for their help in improv-
ing this manuscript  as well as the anonymous reviewers for their careful reviews. This work was
partially funded by NSERC  CIFAR  and the Canada Research Chairs.

References
[1] Ajtai  M. (1983). P1
[2] Allender  E. (1996). Circuit complexity before the dawn of the new millennium. In 16th Annual Conference
on Foundations of Software Technology and Theoretical Computer Science  pages 1–18. Lecture Notes in
Computer Science 1180  Springer Verlag.

1-formulae on ﬁnite structures. Annals of Pure and Applied Logic  24(1)  1–48.

[3] Bengio  Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine Learning  2(1) 

1–127. Also published as a book. Now Publishers  2009.

[4] Bengio  Y. and LeCun  Y. (2007). Scaling learning algorithms towards AI. In L. Bottou  O. Chapelle 

D. DeCoste  and J. Weston  editors  Large Scale Kernel Machines. MIT Press.

[5] Bengio  Y.  Delalleau  O.  and Le Roux  N. (2006). The curse of highly variable functions for local kernel

machines. In NIPS’05  pages 107–114. MIT Press  Cambridge  MA.

[6] Bengio  Y.  Lamblin  P.  Popovici  D.  and Larochelle  H. (2007). Greedy layer-wise training of deep

networks. In NIPS 19  pages 153–160. MIT Press.

[7] Bengio  Y.  Delalleau  O.  and Simard  C. (2010). Decision trees do not generalize to new variations.

Computational Intelligence  26(4)  449–467.

[8] Braverman  M. (2011). Poly-logarithmic independence fools bounded-depth boolean circuits. Communi-

cations of the ACM  54(4)  108–115.

[9] Collobert  R. and Weston  J. (2008). A uniﬁed architecture for natural language processing: Deep neural

networks with multitask learning. In ICML 2008  pages 160–167.

[10] Dahl  G. E.  Ranzato  M.  Mohamed  A.  and Hinton  G. E. (2010). Phone recognition with the mean-

covariance restricted boltzmann machine. In Advances in Neural Information Processing Systems (NIPS).

8

[11] H˚astad  J. (1986). Almost optimal lower bounds for small depth circuits.

In Proceedings of the 18th

annual ACM Symposium on Theory of Computing  pages 6–20  Berkeley  California. ACM Press.

[12] H˚astad  J. and Goldmann  M. (1991). On the power of small-depth threshold circuits. Computational

Complexity  1  113–129.

[13] Hinton  G. E. and Salakhutdinov  R. (2006). Reducing the dimensionality of data with neural networks.

Science  313(5786)  504–507.

[14] Hinton  G. E.  Osindero  S.  and Teh  Y. (2006). A fast learning algorithm for deep belief nets. Neural

Computation  18  1527–1554.

[15] Kavukcuoglu  K.  Sermanet  P.  Boureau  Y.-L.  Gregor  K.  Mathieu  M.  and LeCun  Y. (2010). Learning

convolutional feature hierarchies for visual recognition. In NIPS’10.

[16] Larochelle  H.  Erhan  D.  Courville  A.  Bergstra  J.  and Bengio  Y. (2007). An empirical evaluation of

deep architectures on problems with many factors of variation. In ICML’07  pages 473–480. ACM.

[17] Lee  H.  Ekanadham  C.  and Ng  A. (2008). Sparse deep belief net model for visual area V2. In NIPS’07 

pages 873–880. MIT Press  Cambridge  MA.

[18] Lee  H.  Grosse  R.  Ranganath  R.  and Ng  A. Y. (2009a). Convolutional deep belief networks for

scalable unsupervised learning of hierarchical representations. In ICML 2009. Montreal (Qc)  Canada.

[19] Lee  H.  Pham  P.  Largman  Y.  and Ng  A. (2009b). Unsupervised feature learning for audio classiﬁcation

using convolutional deep belief networks. In NIPS’09  pages 1096–1104.

[20] Levner  I. (2008). Data Driven Object Segmentation. Ph.D. thesis  Department of Computer Science 

University of Alberta.

[21] Mnih  A. and Hinton  G. E. (2009). A scalable hierarchical distributed language model.

pages 1081–1088.

In NIPS’08 

[22] Mobahi  H.  Collobert  R.  and Weston  J. (2009). Deep learning from temporal coherence in video. In

ICML’2009  pages 737–744.

[23] Orponen  P. (1994). Computational complexity of neural networks: a survey. Nordic Journal of Comput-

ing  1(1)  94–110.

[24] Osindero  S. and Hinton  G. E. (2008). Modeling image patches with a directed hierarchy of markov

random ﬁeld. In NIPS’07  pages 1121–1128  Cambridge  MA. MIT Press.

[25] Poon  H. and Domingos  P. (2011). Sum-product networks: A new deep architecture.

Barcelona  Spain.

In UAI’2011 

[26] Ranzato  M. and Szummer  M. (2008). Semi-supervised learning of compact document representations

with deep networks. In ICML.

[27] Ranzato  M.  Poultney  C.  Chopra  S.  and LeCun  Y. (2007). Efﬁcient learning of sparse representations

with an energy-based model. In NIPS’06  pages 1137–1144. MIT Press.

[28] Ranzato  M.  Boureau  Y.-L.  and LeCun  Y. (2008). Sparse feature learning for deep belief networks. In

NIPS’07  pages 1185–1192  Cambridge  MA. MIT Press.

[29] Salakhutdinov  R. and Hinton  G. E. (2007). Semantic hashing. In Proceedings of the 2007 Workshop on

Information Retrieval and applications of Graphical Models (SIGIR 2007)  Amsterdam. Elsevier.

[30] Salakhutdinov  R.  Mnih  A.  and Hinton  G. E. (2007). Restricted Boltzmann machines for collaborative

ﬁltering. In ICML 2007  pages 791–798  New York  NY  USA.

[31] Serre  T.  Kreiman  G.  Kouh  M.  Cadieu  C.  Knoblich  U.  and Poggio  T. (2007). A quantitative theory
of immediate visual recognition. Progress in Brain Research  Computational Neuroscience: Theoretical
Insights into Brain Function  165  33–56.

[32] Socher  R.  Lin  C.  Ng  A. Y.  and Manning  C. (2011). Learning continuous phrase representations and

syntactic parsing with recursive neural networks. In ICML’2011.

[33] Taylor  G. and Hinton  G. (2009). Factored conditional restricted Boltzmann machines for modeling

motion style. In ICML 2009  pages 1025–1032.

[34] Taylor  G.  Hinton  G. E.  and Roweis  S. (2007). Modeling human motion using binary latent variables.

In NIPS’06  pages 1345–1352. MIT Press  Cambridge  MA.

[35] Utgoff  P. E. and Stracuzzi  D. J. (2002). Many-layered learning. Neural Computation  14  2497–2539.
[36] Weston  J.  Ratle  F.  and Collobert  R. (2008). Deep learning via semi-supervised embedding. In ICML

2008  pages 1168–1175  New York  NY  USA.

[37] Wolpert  D. H. (1996). The lack of a priori distinction between learning algorithms. Neural Computation 

8(7)  1341–1390.

[38] Yao  A. (1985). Separating the polynomial-time hierarchy by oracles. In Proceedings of the 26th Annual

IEEE Symposium on Foundations of Computer Science  pages 1–10.

9

,Yichao Zhou
Haozhi Qi
Jingwei Huang
Yi Ma