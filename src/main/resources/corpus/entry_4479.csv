2019,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers,Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to $\ell_2$-norm adversarial perturbations. In this paper  we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers  and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably $\ell_2$-robust classifiers by a significant margin on ImageNet and CIFAR-10  establishing the state-of-the-art for provable $\ell_2$-defenses. Moreover  we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further. Our code and trained models are available at http://github.com/Hadisalman/smoothing-adversarial.,Provably Robust Deep Learning via Adversarially

Trained Smoothed Classiﬁers

Pengchuan Zhang∗  Huan Zhang∗  Ilya Razenshteyn∗  Sébastien Bubeck∗

Hadi Salman†  Greg Yang§  Jerry Li 

Microsoft Research AI

{hadi.salman  gregyang  jerrl 

penzhan  t-huzhan  ilyaraz  sebubeck }@microsoft.com

Abstract

Recent works have shown the effectiveness of randomized smoothing as a scalable
technique for building neural network-based classiﬁers that are provably robust to
(cid:96)2-norm adversarial perturbations. In this paper  we employ adversarial training
to improve the performance of randomized smoothing. We design an adapted
attack for smoothed classiﬁers  and we show how this attack can be used in an
adversarial training setting to boost the provable robustness of smoothed classiﬁers.
We demonstrate through extensive experimentation that our method consistently
outperforms all existing provably (cid:96)2-robust classiﬁers by a signiﬁcant margin on
ImageNet and CIFAR-10  establishing the state-of-the-art for provable (cid:96)2-defenses.
Moreover  we ﬁnd that pre-training and semi-supervised learning boost adversar-
ially trained smoothed classiﬁers even further. Our code and trained models are
available at http://github.com/Hadisalman/smoothing-adversarial2.

Introduction

1
Neural networks have been very successful in tasks such as image classiﬁcation and speech recogni-
tion  but have been shown to be extremely brittle to small  adversarially-chosen perturbations of their
inputs [33  14]. A classiﬁer (e.g.  a neural network)  which correctly classiﬁes an image x  can be
fooled by an adversary to misclassify x + δ where δ is an adversarial perturbation so small that x
and x + δ are indistinguishable for the human eye. Recently  many works have proposed heuristic
defenses intended to train models robust to such adversarial perturbations. However  most of these
defenses were broken using more powerful adversaries [4  2  35]. This encouraged researchers to
develop defenses that lead to certiﬁably robust classiﬁers  i.e.  whose predictions for most of the test
examples x can be veriﬁed to be constant within a neighborhood of x [39  27]. Unfortunately  these
techniques do not immediately scale to large neural networks that are used in practice.
To mitigate this limitation of prior certiﬁable defenses  a number of papers [21  22  6] consider the
randomized smoothing approach  which transforms any classiﬁer f (e.g.  a neural network) into a
new smoothed classiﬁer g that has certiﬁable (cid:96)2-norm robustness guarantees. This transformation
works as follows.
Let f be an arbitrary base classiﬁer which maps inputs in Rd to classes in Y. Given an input x  the
smoothed classiﬁer g(x) labels x as having class c which is the most likely to be returned by the base
classiﬁer f when fed a noisy corruption x + δ  where δ ∼ N (x  σ2I) is a vector sampled according
to an isotropic Gaussian distribution.
As shown in [6]  one can derive certiﬁable robustness for such smoothed classiﬁers via the Neyman-
Pearson lemma. They demonstrate that for (cid:96)2 perturbations  randomized smoothing outperforms
∗Reverse alphabetical order. †Work done as part of the Microsoft AI Residency Program. §Primary mentor.
2Please see http://arxiv.org/abs/1906.04584 for the full and most recent version of this paper.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Certiﬁed top-1 accuracy of our best ImageNet classiﬁers at various (cid:96)2 radii.

(cid:96)2 RADIUS (IMAGENET)
COHEN ET AL. [6] (%)
OURS (%)

0.5
49
56

1.0
37
45

1.5
29
38

2.0
19
28

2.5
15
26

3.0
12
20

3.5
9
17

Table 2: Certiﬁed top-1 accuracy of our best CIFAR-10 classiﬁers at various (cid:96)2 radii.

(cid:96)2 RADIUS (CIFAR-10)
COHEN ET AL. [6] (%)
OURS (%)
+ PRE-TRAINING (%)
+ SEMI-SUPERVISION (%)
+ BOTH(%)

0.25
61
73
80
80
81

0.5
43
58
62
63
63

0.75
32
48
52
52
52

1.0
22
38
38
40
37

1.25
17
33
34
34
33

1.5
13
29
30
29
29

1.75
10
24
25
25
25

2.0
7
18
19
19
18

2.25

4
16
16
17
16

other certiﬁably robust classiﬁers that have been previously proposed. It is scalable to networks with
any architecture and size  which makes it suitable for building robust real-world neural networks.

Our contributions
In this paper  we employ adversarial training to substantially improve on the
previous certiﬁed robustness results3 of randomized smoothing [21  22  6]. We present  for the
ﬁrst time  a direct attack for smoothed classiﬁers. We then demonstrate how to use this attack to
adversarially train smoothed models with not only boosted empirical robustness but also substantially
improved certiﬁable robustness using the certiﬁcation method of Cohen et al. [6].
We demonstrate that our method outperforms all existing provably (cid:96)2-robust classiﬁers by a signiﬁcant
margin on ImageNet and CIFAR-10  establishing the state-of-the-art for provable (cid:96)2-defenses. For
instance  our Resnet-50 ImageNet classiﬁer achieves 56% provable top-1 accuracy (compared to
the best previous provable accuracy of 49%) under adversarial perturbations with (cid:96)2 norm less than
127/255. Similarly  our Resnet-110 CIFAR-10 smoothed classiﬁer achieves up to 16% improvement
over previous state-of-the-art  and by combining our technique with pre-training [17] and semi-
supervised learning [5]  we boost our results to up to 22% improvement over previous state-of-the-art.
Our main results are reported in Tables 1 and 2 for ImageNet and CIFAR-10. See Tables 16 and 17 in
Appendix G for the standard accuracies corresponding to these results.
Finally  we provide an alternative  but more concise  proof of the tight robustness guarantee of Cohen
et al. [6] by casting this as a nonlinear Lipschitz property of the smoothed classiﬁer. See appendix A
for the complete proof.

2 Our techniques

Here we describe our techniques for adversarial attacks and training on smoothed classiﬁers. We ﬁrst
require some background on randomized smoothing classiﬁers. For a more detailed description of
randomized smoothing  see Cohen et al. [6].

2.1 Background on randomized smoothing
Consider a classiﬁer f from Rd to classes Y. Randomized smoothing is a method that constructs a
new  smoothed classiﬁer g from the base classiﬁer f. The smoothed classiﬁer g assigns to a query
point x the class which is most likely to be returned by the base classiﬁer f under isotropic Gaussian
noise perturbation of x  i.e. 

g(x) = arg max

c∈Y

P(f (x + δ) = c) where δ ∼ N (0  σ2I) .

(1)

The noise level σ2 is a hyperparameter of the smoothed classiﬁer g which controls a robust-
ness/accuracy tradeoff. Equivalently  this means that g(x) returns the class c whose decision region
{x(cid:48) ∈ Rd : f (x(cid:48)) = c} has the largest measure under the distribution N (x  σ2I). Cohen et al. [6]
3Note that we do not provide a new certiﬁcation method incorporating adversarial training; the improvements

that we get are due to the higher quality of our base classiﬁers as a result of adversarial training.

2

recently presented a tight robustness guarantee for the smoothed classiﬁer g and gave Monte Carlo
algorithms for certifying the robustness of g around x or predicting the class of x using g  that succeed
with high probability.

Robustness guarantee for smoothed classiﬁers The robustness guarantee presented by [6] uses
the Neyman-Pearson lemma  and is as follows: suppose that when the base classiﬁer f classiﬁes
N (x  σ2I)  the class cA is returned with probability pA = P(f (x + δ) = cA)  and the “runner-up”
P(f (x + δ) = c). The smoothed classiﬁer g is
class cB is returned with probability pB = maxc(cid:54)=cA
robust around x within the radius

R =

(2)
where Φ−1 is the inverse of the standard Gaussian CDF. It is not clear how to compute pA and pB
exactly (if f is given by a deep neural network for example). Monte Carlo sampling is used to
estimate some pA and pB for which pA ≤ pA and pB ≥ pB with arbitrarily high probability over the
samples. The result of (2) still holds if we replace pA with pA and pB with pB.
This guarantee can in fact be obtained alternatively by explicitly computing the Lipschitz constant of
the smoothed classiﬁer  as we do in Appendix A.

(cid:0)Φ−1(pA) − Φ−1(pB)(cid:1)  

σ
2

2.2 SMOOTHADV: Attacking smoothed classiﬁers
We now describe our attack against smoothed classiﬁers. To do so  it will ﬁrst be useful to describe
smoothed classiﬁers in a more general setting. Speciﬁcally  we consider a generalization of (1) to soft
classiﬁers  namely  functions F : Rd → P (Y)  where P (Y) is the set of probability distributions over
Y. Neural networks typically learn such soft classiﬁers  then use the argmax of the soft classiﬁer as the
ﬁnal hard classiﬁer. Given a soft classiﬁer F   its associated smoothed soft classiﬁer G : Rn → P (Y)
is deﬁned as

G(x) =(cid:0)F ∗ N (0  σ2I)(cid:1) (x) =

[F (x + δ)] .

E

(3)

δ∼N (0 σ2I)

Let f (x) and F (x) denote the hard and soft classiﬁers learned by the neural network  respectively 
and let g and G denote the associated smoothed hard and smoothed soft classiﬁers. Directly ﬁnding
adversarial examples for the smoothed hard classiﬁer g is a somewhat ill-behaved problem because
of the argmax  so we instead propose to ﬁnd adversarial examples for the smoothed soft classiﬁer
G. Empirically we found that doing so will also ﬁnd good adversarial examples for the smoothed
hard classiﬁer. More concretely  given a labeled data point (x  y)  we wish to ﬁnd a point ˆx which
maximizes the loss of G in an (cid:96)2 ball around x for some choice of loss function. As is canonical in
the literature  we focus on the cross entropy loss (cid:96)CE. Thus  given a labeled data point (x  y) our
(ideal) adversarial perturbation is given by the formula:

(cid:18)

ˆx = arg max
(cid:107)x(cid:48)−x(cid:107)2≤

= arg max
(cid:107)x(cid:48)−x(cid:107)2≤

(cid:96)CE(G(x(cid:48))  y)

− log

E

δ∼N (0 σ2I)

(cid:104)
(F (x(cid:48) + δ))y

(cid:105)(cid:19)

.

(S)

We will refer to (S) as the SMOOTHADV objective. The SMOOTHADV objective is highly non-convex 
so as is common in the literature  we will optimize it via projected gradient descent (PGD)  and
variants thereof. It is hard to ﬁnd exact gradients for (S)  so in practice we must use some estimator
based on random Gaussian samples. There are a number of different natural estimators for the
derivative of the objective function in (S)  and the choice of estimator can dramatically change the
performance of the attack. For more details  see Section 3.
We note that (S) should not be confused with the similar-looking objective
[(cid:96)CE(F (x(cid:48) + δ)  y)]

E

(cid:18)
(cid:18)

(cid:19)
(cid:105)(cid:19)
(cid:104)− log (F (x(cid:48) + δ))y

ˆxwrong = arg max
(cid:107)x(cid:48)−x(cid:107)2≤

= arg max
(cid:107)x(cid:48)−x(cid:107)2≤

δ∼N (0 σ2I)

E

δ∼N (0 σ2I)

as suggested in section G.3 of [6]. There is a subtle  but very important  distinction between (S) and
(4). Conceptually  solving (4) corresponds to ﬁnding an adversarial example of F that is robust to

3

 

(4)

Gaussian noise. In contrast  (S) is directly attacking the smoothed model i.e. trying to ﬁnd adversarial
examples that decrease the probability of correct classiﬁcation of the smoothed soft classiﬁer G.
From this point of view  (S) is the right optimization problem that should be used to ﬁnd adversarial
examples of G. This distinction turns out to be crucial in practice: empirically  Cohen et al. [6] found
attacks based on (4) not to be effective.
Interestingly  for a large class of classiﬁers  including neural networks  one can alternatively derive the
objective (S) from an optimization perspective  by attempting to directly ﬁnd adversarial examples to
the smoothed hard classiﬁer that the neural network provides. While they ultimately yield the same
objective  this perspective may also be enlightening  and so we include it in Appendix B.

2.3 Adversarial training using SMOOTHADV
We now wish to use our new attack to boost the adversarial robustness of smoothed classiﬁers. We do
so using the well-studied adversarial training framework [20  25]. In adversarial training  given a
current set of model weights wt and a labeled data point (xt  yt)  one ﬁnds an adversarial perturbation
ˆxt of xt for the current model wt  and then takes a gradient step for the model parameters  evaluated
at the point (ˆxt  yt). Intuitively  this encourages the network to learn to minimize the worst-case loss
over a neighborhood around the input.
At a high level  we propose to instead do adversarial training using an adversarial example for the
smoothed classiﬁer. We combine this with the approach suggested in Cohen et al. [6]  and train at
Gaussian perturbations of this adversarial example. That is  given current set of weights wt and
a labeled data point (xt  yt)  we ﬁnd ˆxt as a solution to (S)  and then take a gradient step for wt
based at gaussian perturbations of ˆxt. In contrast to standard adversarial training  we are training the
base classiﬁer so that its associated smoothed classiﬁer minimizes worst-case loss in a neighborhood
around the current point. For more details of our implementation  see Section 3.2. We emphasize that
although we are training using adversarial examples for the smoothed soft classiﬁer  in the end we
certify the robustness of the smoothed hard classiﬁer we obtain after training.
We make two important observations about our method. First  adversarial training is an empirical
defense  and typically offers no provable guarantees. However  we demonstrate that by combining
our formulation of adversarial training with randomized smoothing  we are able to substantially boost
the certiﬁable robust accuracy of our smoothed classiﬁers. Thus  while adversarial training using
SMOOTHADV is still ultimately a heuristic  and offers no provable robustness by itself  the smoothed
classiﬁer that we obtain using this heuristic has strong certiﬁable guarantees.
Second  we found empirically that to obtain strong certiﬁable numbers using randomized smoothing 
it is insufﬁcient to use standard adversarial training on the base classiﬁer. While such adversarial
training does indeed offer good empirical robust accuracy  the resulting classiﬁer is not optimized for
randomized smoothing. In contrast  our method speciﬁcally ﬁnds base classiﬁers whose smoothed
counterparts are robust. As a result  the certiﬁable numbers for standard adversarial training are
noticeably worse than those obtained using our method. See Appendix C.1 for an in-depth comparison.

3

Implementing SMOOTHADV via ﬁrst order methods

As mentioned above  it is difﬁcult to optimize the SMOOTHADV objective  so we will approximate it
via ﬁrst order methods. We focus on two such methods: the well-studied projected gradient descent
(PGD) method [20  25]  and the recently proposed decoupled direction and norm (DDN) method [29]
which achieves (cid:96)2 robust accuracy competitive with PGD on CIFAR-10.
The main task when implementing these methods is to  given a data point (x  y)  compute the gradient
of the objective function in (S) with respect to x(cid:48). If we let J(x(cid:48)) = (cid:96)CE(G(x(cid:48))  y) denote the
objective function in (S)  we have

∇x(cid:48)J(x(cid:48)) = ∇x(cid:48)

− log

E

δ∼N (0 σ2I)

[F (x(cid:48) + δ)y]

.

(5)

However  it is not clear how to evaluate (5) exactly  as it takes the form of a complicated high
dimensional integral. Therefore  we will use Monte Carlo approximations. We sample i.i.d. Gaussians
δ1  . . .   δm ∼ N (0  σ2I)  and use the plug-in estimator for the expectation:

∇x(cid:48)J(x(cid:48)) ≈ ∇x(cid:48)

− log

F (x(cid:48) + δi)y

.

(6)

(cid:19)

(cid:33)(cid:33)

(cid:18)

(cid:32)

m(cid:88)

i=1

1
m

(cid:32)

4

Pseudocode 1: SMOOTHADV-ersarial Training

function TRAINMINIBATCH((x(1)  y(1))  (x(2)  y(2))  . . .   (x(B)  y(B)))
i ∼ N (0  σ2I) for 1 ≤ i ≤ m  1 ≤ j ≤ B

ATTACKER ← (SMOOTHADVPGD or SMOOTHADVDDN)
Generate noise samples δ(j)
L ← []
for 1 ≤ j ≤ B do

# List of adversarial examples for training

ˆx(j) ← x(j)
for 1 ≤ k ≤ T do

# Adversarial example

Update ˆx(j) according to the k-th step of ATTACKER  where we use
the noise samples δ(j)
model according to (6)
# We are reusing the same noise samples between different steps of the attack

m to estimate a gradient of the loss of the smoothed

1   δ(j)

2   . . .   δ(j)

end
Append ((ˆx(j) + δ(j)
# Again  we are reusing the same noise samples for the augmentation

2   y(j))  . . .   (ˆx(j) + δ(j)

1   y(j))  (ˆx(j) + δ(j)

m   y(j))) to L

end
Run backpropagation on L with an appropriate learning rate

It is not hard to see that if F is smooth  this estimator will converge to (5) as we take more samples. In
practice  if we take m samples  then to evaluate (6) on all m samples requires evaluating the network
m times. This becomes expensive for large m  especially if we want to plug this into the adversarial
training framework  which is already slow. Thus  when we use this for adversarial training  we
use mtrain ∈ {1  2  4  8}. When we run this attack to evaluate the empirical adversarial accuracy
of our models  we use substantially larger choices of m  speciﬁcally  mtest ∈ {1  4  8  16  64  128}.
Empirically we found that increasing m beyond 128 did not substantially improve performance.
While this estimator does converge to the true gradient given enough samples  note that it is not
an unbiased estimator for the gradient. Despite this  we found that using (6) performs very well in
practice. Indeed  using (6) yields our strongest empirical attacks  as well as our strongest certiﬁable
defenses when we use this attack in adversarial training. In the remainder of the paper  we let
SMOOTHADVPGD denote the PGD attack with gradient steps given by (6)  and similarly we let
SMOOTHADVDDN denote the DDN attack with gradient steps given by (6).

3.1 An unbiased  gradient free method
We note that there is an alternative way to optimize (S) using ﬁrst order methods. Notice that the
logarithm in (S) does not change the argmax  and so it sufﬁces to ﬁnd a minimizer of G(x(cid:48))y subject
to the (cid:96)2 constraint. We then observe that

∇x(cid:48)(G(x(cid:48))y) =

E

δ∼N (0 σ2I)

[∇x(cid:48)F (x(cid:48) + δ)y]

(a)
=

E

δ∼N (0 σ2I)

.

(7)

(cid:21)

(cid:20) δ
σ2 · F (x(cid:48) + δ)y
(cid:80)m

The equality (a) is known as Stein’s lemma [32]  although we note that something similar can be
derived for more general distributions. There is a natural unbiased estimator for (7): sample i.i.d.
gaussians δ1  . . .   δm ∼ N (0  σ2I)  and form the estimator ∇x(cid:48)(G(x(cid:48))y) ≈ 1
σ2 · F (x(cid:48) +
δi)y . This estimator has a number of nice properties. As mentioned previously  it is an unbiased
estimator for (7)  in contrast to (6). It also requires no computations of the gradient of F ; if F is a
neural network  this saves both time and memory by not storing preactivations during the forward
pass. Finally  it is very general: the derivation of (7) actually holds even if F is a hard classiﬁer
(or more precisely  the one-hot embedding of a hard classiﬁer). In particular  this implies that this
technique can even be used to directly ﬁnd adversarial examples of the smoothed hard classiﬁer.
Despite these appealing features  in practice we ﬁnd that this attack is quite weak. We speculate that
this is because the variance of the gradient estimator is too high. For this reason  in the empirical
evaluation we focus on attacks using (6)  but we believe that investigating this attack in practice is an
interesting direction for future work. See Appendix C.6 for more details.

i=1

m

δi

Implementing adversarial training for smoothed classiﬁers

3.2
We incorporate adversarial training into the approach of Cohen et al. [6] changing as few moving
parts as possible in order to enable a direct comparison. In particular  we use the same network
architectures  batch size  and learning rate schedule. For CIFAR-10  we change the number of epochs 

5

Figure 1: Comparing our SMOOTHADV-ersarially trained CIFAR-10 classiﬁers vs Cohen et al. [6].
(Left) Upper envelopes of certiﬁed accuracies over all experiments. (Middle) Upper envelopes of
certiﬁed accuracies per σ. (Right) Certiﬁed accuracies of one representative model per σ. Details of
each model used to generate these plots and their certiﬁed accuracies are in Tables 7-15 in Appendix G.

but for ImageNet  we leave it the same. We discuss more of these speciﬁcs in Appendix D  and here
we describe how to perform adversarial training on a single mini-batch. The algorithm is shown in
Pseudocode 1  with the following parameters: B is the mini-batch size  m is the number of noise
samples used for gradient estimation in (6) as well as for Gaussian noise data augmentation  and T is
the number of steps of an attack4.

4 Experiments
We primarily compare with Cohen et al. [6] as it was shown to outperform all other scalable provable
(cid:96)2 defenses by a wide margin. As our experiments will demonstrate  our method consistently and
signiﬁcantly outperforms Cohen et al. [6] even further  establishing the state-of-the-art for provable
(cid:96)2-defenses. We run experiments on ImageNet [8] and CIFAR-10 [19]. We use the same base
classiﬁers f as Cohen et al. [6]: a ResNet-50 [16] on ImageNet  and ResNet-110 on CIFAR-10.
Other than the choice of attack (SMOOTHADVPGD or SMOOTHADVDDN) for adversarial training 
our experiments are distinguished based on ﬁve main hyperparameters:

 = maximum allowed (cid:96)2 perturbation of the input
T = number of steps of the attack
σ = std. of Gaussian noise data augmentation during training and certiﬁcation

mtrain = number of noise samples used to estimate (6) during training
mtest = number of noise samples used to estimate (6) during evaluation

(♦)

Given a smoothed classiﬁer g  we use the same prediction and certiﬁcation algorithms  PREDICT and
CERTIFY  as [6]. Both algorithms sample base classiﬁer predictions under Gaussian noise. PREDICT
outputs the majority vote if the vote count passes a binomial hypothesis test  and abstains otherwise.
CERTIFY certiﬁes the majority vote is robust if the fraction of such votes is higher by a calculated
margin than the fraction of the next most popular votes  and abstains otherwise. For details of these
algorithms  we refer the reader to [6].
The certiﬁed accuracy at radius r is deﬁned as the fraction of the test set which g classiﬁes correctly
(without abstaining) and certiﬁes robust at an (cid:96)2 radius r. Unless otherwise speciﬁed  we use the
same σ for certiﬁcation as the one used for training the base classiﬁer f. Note that g is a randomized
smoothing classiﬁer  so this reported accuracy is approximate  but can get arbitrarily close to the
true certiﬁed accuracy as the number of samples of g increases (see [6] for more details). Similarly 
the empirical accuracy is deﬁned as the fraction of the (cid:96)2 SMOOTHADV-ersarially attacked test set
which g classiﬁes correctly (without abstaining). Both PREDICT and CERTIFY have a parameter α
deﬁning the failure rate of these algorithms. Throughout the paper  we set α = 0.001 (similar to [6]) 
which means there is at most a 0.1% chance that PREDICT does not return the most probable class
under the smoothed classiﬁer g  or that CERTIFY falsely certiﬁes a non-robust input.

4.1 SMOOTHADV-ersarial training
To assess the effectiveness of our method  we learn a smoothed classiﬁer g that is adversarial trained
using (S). Then we compute the certiﬁed accuracies5 over a range of (cid:96)2 radii r. Tables 1 and 2
4Note that we are reusing the same noise samples during every step of our attack as well as during augmenta-

tion. Intuitively  this helps to stabilize the attack process.

5Similar to Cohen et al. [6]  we certiﬁed the full CIFAR-10 test set and a subsampled ImageNet test set of

500 samples.

6

0.000.250.500.751.001.251.501.752.002.252 radius0.00.20.40.60.81.0AccuracyOurs certifiedCohen et al. certifiedOurs empiricalCohen et. al empirical0.00.51.01.52.02.53.03.54.02 radius0.00.20.40.60.81.0Certified AccuracyOurs|=0.12Ours|=0.25Ours|=0.50Ours|=1.00Cohen et al.|=0.12Cohen et al.|=0.25Cohen et al.|=0.50Cohen et al.|=1.000.00.51.01.52.02.53.03.54.02 radius0.00.20.40.60.81.0Certified AccuracyOurs|=0.12Ours|=0.25Ours|=0.50Ours|=1.00Cohen et al.|=0.12Cohen et al.|=0.25Cohen et al.|=0.50Cohen et al.|=1.00Figure 2: Comparing our SMOOTHADV-ersarially trained ImageNet classiﬁers vs Cohen et al. [6].
Subﬁgure captions are same as Fig. 1. Details of each model used to generate these plots and their
certiﬁed accuracies are in Table 6 in Appendix G.

Table 3: Certiﬁed (cid:96)∞ robustness at a radius of 2
et al. [5]’s give accuracies with high probability (W.H.P).

255 on CIFAR-10. Note that our models and Carmon

MODEL
OURS (%)
CARMON ET AL. [5] (%)
WONG AND KOLTER [39] (SINGLE) (%)
WONG AND KOLTER [39] (ENSEMBLE) (%)
IBP [15] (%)

(cid:96)∞ ACC. AT 2/255

STANDARD ACC.

68.2 (W.H.P)

63.8 ± 0.5 (W.H.P)

53.9
63.6
50.0

86.2 (W.H.P)

80.7 ± 0.3 (W.H.P)

68.3
64.1
70.2

report the certiﬁed accuracies using our method compared to [6]. For all radii  we outperform the
certiﬁed accuracies of [6] by a signiﬁcant margin on both ImageNet and CIFAR-10. These results are
elaborated below.

For CIFAR-10 Fig. 1(left) plots the upper envelope of the certiﬁed accuracies that we get by
choosing the best model for each radius over a grid of hyperparameters. This grid consists of
mtrain ∈ {1  2  4  8}  σ ∈ {0.12  0.25  0.5  1.0}   ∈ {0.25  0.5  1.0  2.0} (see ♦ for explanation) 
and one of the following attacks {SMOOTHADVPGD  SMOOTHADVDDN} with T ∈ {2  4  6  8  10}
steps. The certiﬁed accuracies of each model can be found in Tables 7-15 in Appendix G. These results
are compared to those of Cohen et al. [6] by plotting their reported certiﬁed accuracies. Fig. 1(left)
also plots the corresponding empirical accuracies using SMOOTHADVPGD with mtest = 128. Note
that our certiﬁed accuracies are higher than the empirical accuracies of Cohen et al. [6].
Fig. 1(middle) plots our vs [6]’s best models for varying noise level σ. Fig. 1(right) plots a represen-
tative model for each σ from our adversarially trained models. Observe that we outperform [6] in all
three plots.

For ImageNet The results are summarized in Fig. 2  which is similar to Fig. 1 for CIFAR-10  with
the difference being the set of smoothed models we certify. This set includes smoothed models
trained using mtrain = 1  σ ∈ {0.25  0.5  1.0}   ∈ {0.5  1.0  2.0  4.0}  and one of the following
attacks {1-step SMOOTHADVPGD  2-step SMOOTHADVDDN}. Again  our models outperform those
of Cohen et al. [6] overall and per σ as well. The certiﬁed accuracies of each model can be found in
Table 6 in Appendix G.
We point out  as mentioned by Cohen et al. [6]  that σ controls a robustness/accuracy trade-off. When
σ is low  small radii can be certiﬁed with high accuracy  but large radii cannot be certiﬁed at all.
When σ is high  larger radii can be certiﬁed  but smaller radii are certiﬁed at a lower accuracy. This
can be observed in the middle and the right plots of Fig. 1 and 2.

Effect on clean accuracy Training smoothed classifers using SMOOTHADV as shown improves
upon the certiﬁed accuracy of Cohen et al. [6] for each σ  although this comes with the well-known
effect of adversarial training in decreasing the standard accuracy  so we sometimes see small drops in
the accuracy at r = 0  as observed in Fig. 1(right) and 2(right).

d contains the (cid:96)∞ unit ball in Rd  a model
√
(cid:96)2 to (cid:96)∞ certiﬁed defense Since the (cid:96)2 ball of radius
robust against (cid:96)2 perturbation of radius r is also robust against (cid:96)∞ perturbation of norm r/
d.
Via this naive conversion  we ﬁnd our (cid:96)2-robust models enjoy non-trivial (cid:96)∞ certiﬁed robustness.

√

7

0.00.51.01.52.02.53.03.52 radius0.00.20.40.60.81.0AccuracyOurs certifiedCohen et al. certified0.00.51.01.52.02.53.03.54.02 radius0.00.20.40.60.81.0Certified AccuracyOurs|=0.25Ours|=0.50Ours|=1.00Cohen et al.|=0.25Cohen et al.|=0.50Cohen et al.|=1.000.00.51.01.52.02.53.03.54.02 radius0.00.20.40.60.81.0Certified AccuracyOurs|=0.25Ours|=0.50Ours|=1.00Cohen et al.|=0.25Cohen et al.|=0.50Cohen et al.|=1.00√
In Table 3  we report the best6 (cid:96)∞ certiﬁed accuracy that we get on CIFAR-10 at a radius of 2/255
(implied by the (cid:96)2 certiﬁed accuracy at a radius of 0.435 ≈ 2
3 × 322/255). We exceed previous
state-of-the-art in certiﬁed (cid:96)∞ defenses by at least 3.9%. We obtain similar results for ImageNet
certiﬁed (cid:96)∞ defenses at a radius of 1/255 where we exceed the previous state-of-the-art by 8.2%;
details are in appendix F.

Additional experiments and observations We compare the effectiveness of smoothed classiﬁers
when they are trained SMOOTHADV-versarially vs. when their base classiﬁer is trained via standard
adversarial training (we will refer to the latter as vanilla adversarial training). As expected  because
the training objective of SMOOTHADV-models aligns with the actual certiﬁcation objective  those
models achieve noticeably more certiﬁed robustness over all radii compared to smoothed classiﬁers
resulting from vanilla adversarial training. We defer the results and details to Appendix C.1.
Furthermore  SMOOTHADV requires the evaluation of (6) as discussed in Section 3. We analyze
in Appendix C.2 how the number of Gaussian noise samples mtrain  used in (6) to ﬁnd adversarial
examples  affects the robustness of the resulting smoothed models. As expected  we observe that
models trained with higher mtrain tend to have higher certiﬁed accuracies.
Finally  we analyze the effect of the maximum allowed (cid:96)2 perturbation  used in SMOOTHADV on
the robustness of smoothed models in Appendix C.3. We observe that as  increases  the certiﬁed
accuracies for small (cid:96)2 radii decrease  but those for large (cid:96)2 radii increase  which is expected.

4.2 More Data for Better Provable Robustness

We explore using more data to improve the robustness of smoothed classiﬁers. Speciﬁcally  we pursue
two ideas: 1) pre-training similar to [17]  and 2) semi-supervised learning as in [5].

Pre-training Hendrycks et al. [17] recently showed that using pre-training can improve the adver-
sarial robustness of classiﬁers  and achieved state-of-the-art results for empirical l∞ defenses on
CIFAR-10 and CIFAR-100. We employ this within our framework; we pretrain smoothed classiﬁers
on ImageNet  then ﬁne-tune them on CIFAR-10. Details can be found in Appendix E.1.

Semi-supervised learning Carmon et al. [5] recently showed that using unlabelled data can im-
prove the adversarial robustness as well. They employ a simple  yet effective  semi-supervised
learning technique called self-training to improve the robustness of CIFAR-10 classiﬁers. We employ
this idea in our framework and we train our CIFAR-10 smoothed classiﬁers via self-training using the
unlabelled dataset used in Carmon et al. [5]. Details can be found in Appendix E.2.
We further experiment with combining semi-supervised learning and pre-training  and the details are
in Appendix E.3. We observe consistent improvement in the certiﬁed robustness of our smoothed
models when we employ pre-training or semi-supervision. The results are summarized in Table 2.

4.3 Attacking trained models with SMOOTHADV
In this section  we assess the performance of our attack  particularly SMOOTHADVPGD  for ﬁnding
adversarial examples for the CIFAR-10 randomized smoothing models of Cohen et al. [6].
SMOOTHADVPGD requires the evaluation of (6) as discussed in Section 3. Here  we analyze how
sensitive our attack is to the number of samples mtest used in (6) for estimating the gradient of the
adversarial objective. Fig. 3 shows the empirical accuracies for various values of mtest. Lower
accuracies corresponds to stronger attack. SMOOTHADV with mtest = 1 sample performs worse
than the vanilla PGD attack on the base classiﬁer  but as mtest increases  our attack becomes stronger 
decreasing the gap between certiﬁed and empirical accuracies. We did not observe any noticeable
improvement beyond mtest = 128. More details are in Appendix C.4.
While as discussed here  the success rate of the attack is affected by the number of Gaussian noise
samples mtest used by the attacker  it is also affected by the number of Gaussian noise samples n in
PREDICT used by the classiﬁer. Indeed  as n increases  abstention due to low conﬁdence becomes
more rare  increasing the prediction quality of the smoothed classiﬁer. See a detailed analysis in
Appendix C.5.

6We report the model with the highest certiﬁed (cid:96)2 accuracy on CIFAR-10 at a radius of 0.435  amongst all

our models trained in this paper.

8

Figure 3: Certiﬁed and empirical robust accuracy of Cohen et al. [6]’s models on CIFAR-10. For
each (cid:96)2 radius r  the certiﬁed/empirical accuracy is the maximum over randomized smoothing
models trained using σ ∈ {0.12  0.25  0.5  1.0}. The empirical accuracies are found using 20 steps
of SMOOTHADVPGD. The closer an empirical curve is to the certiﬁed curve  the stronger the
corresponding attack is (the lower the better).

5 Related Work
Recently  many approaches (defenses) have been proposed to build adversarially robust classiﬁers 
and these approaches can be broadly divided into empirical defenses and certiﬁed defenses.
Empirical defenses are empirically robust to existing adversarial attacks  and the best empirical
defense so far is adversarial training [20  25]. In this kind of defense  a neural network is trained to
minimize the worst-case loss over a neighborhood around the input. Although such defenses seem
powerful  nothing guarantees that a more powerful  not yet known  attack would not break them; the
most that can be said is that known attacks are unable to ﬁnd adversarial examples around the data
points. In fact  most empirical defenses proposed in the literature were later “broken” by stronger
adversaries [4  2  35  1]. To stop this arms race between defenders and attackers  a number of work
tried to focus on building certiﬁed defenses which enjoy formal robustness guarantees.
Certiﬁed defenses are provably robust to a speciﬁc class of adversarial perturbation  and can guaran-
tee that for any input x  the classiﬁer’s prediction is constant within a neighborhood of x. These are
typically based on certiﬁcation methods which are either exact (a.k.a “complete”) or conservative
(a.k.a “sound but incomplete”). Exact methods  usually based on Satisﬁability Modulo Theories
solvers [18  11] or mixed integer linear programming [34  24  12]  are guaranteed to ﬁnd an adversar-
ial example around a datapoint if it exists. Unfortunately  they are computationally inefﬁcient and
difﬁcult to scale up to large neural networks. Conservative methods are also guaranteed to detect an
adversarial example if exists  but they might mistakenly ﬂag a safe data point as vulnerable to adversar-
ial examples. On the bright side  these methods are more scalable and efﬁcient which makes some of
them useful for building certiﬁed defenses [39  36  37  27  28  40  10  9  7  30  13  26  31  15  38  41].
However  none of them have yet been shown to scale to practical networks that are large and expres-
sive enough to perform well on ImageNet  for example. To scale up to practical networks  randomized
smoothing has been proposed as a probabilistically certiﬁed defense.
Randomized smoothing A randomized smoothing classiﬁer is not itself a neural network  but
uses a neural network as its base for classiﬁcation. Randomized smoothing was proposed by several
works [23  3] as a heuristic defense without proving any guarantees. Lecuyer et al. [21] ﬁrst proved
robustness guarantees for randomized smoothing classiﬁer  utilizing inequalities from the differential
privacy literature. Subsequently  Li et al. [22] gave a stronger robustness guarantee using tools from
information theory. Recently  Cohen et al. [6] provided a tight robustness guarantee for randomized
smoothing and consequently achieved the state of the art in (cid:96)2-norm certiﬁed defense.

6 Conclusions
In this paper  we designed an adapted attack for smoothed classiﬁers  and we showed how this attack
can be used in an adversarial training setting to substantially improve the provable robustness of
smoothed classiﬁers. We demonstrated through extensive experimentation that our adversarially
trained smooth classiﬁers consistently outperforms all existing provably (cid:96)2-robust classiﬁers by
a signiﬁcant margin on ImageNet and CIFAR-10  establishing the state of the art for provable
(cid:96)2-defenses.

9

0.00.51.01.52.02 radius0.000.250.500.751.00AccuracyCohen et al. certifiedEmpirical vanilla PGD Empirical  mtest=1Empirical  mtest=4Empirical  mtest=8Empirical  mtest=16Empirical  mtest=64Empirical  mtest=128Acknowledgements

We would like to thank Zico Kolter  Jeremy Cohen  Elan Rosenfeld  Aleksander Madry  Andrew
Ilyas  Dimitris Tsipras  Shibani Santurkar  and Jacob Steinhardt for comments and discussions.

References
[1] Anish Athalye and Nicholas Carlini. On the robustness of the cvpr 2018 white-box adversarial

example defenses. arXiv preprint arXiv:1804.03286  2018.

[2] Anish Athalye  Nicholas Carlini  and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420 
2018.

[3] Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion attacks to deep neural networks via
region-based classiﬁcation. In Proceedings of the 33rd Annual Computer Security Applications
Conference  pages 278–287. ACM  2017.

[4] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing
ten detection methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security  pages 3–14. ACM  2017.

[5] Yair Carmon  Aditi Raghunathan  Ludwig Schmidt  Percy Liang  and John C Duchi. Unlabeled

data improves adversarial robustness. arXiv preprint arXiv:1905.13736  2019.

[6] Jeremy M Cohen  Elan Rosenfeld  and J Zico Kolter. Certiﬁed adversarial robustness via

randomized smoothing. arXiv preprint arXiv:1902.02918  2019.

[7] Francesco Croce  Maksym Andriushchenko  and Matthias Hein. Provable robustness of relu

networks via maximization of linear regions. arXiv preprint arXiv:1810.07481  2018.

[8] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition  pages 248–255. Ieee  2009.

[9] Krishnamurthy Dvijotham  Sven Gowal  Robert Stanforth  Relja Arandjelovic  Brendan
O’Donoghue  Jonathan Uesato  and Pushmeet Kohli. Training veriﬁed learners with learned
veriﬁers. arXiv preprint arXiv:1805.10265  2018.

[10] Krishnamurthy Dvijotham  Robert Stanforth  Sven Gowal  Timothy Mann  and Pushmeet Kohli.

A dual approach to scalable veriﬁcation of deep networks. UAI  2018.

[11] Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In
International Symposium on Automated Technology for Veriﬁcation and Analysis  pages 269–
286. Springer  2017.

[12] Matteo Fischetti and Jason Jo. Deep neural networks as 0-1 mixed integer linear programs: A

feasibility study. arXiv preprint arXiv:1712.06174  2017.

[13] Timon Gehr  Matthew Mirman  Dana Drachsler-Cohen  Petar Tsankov  Swarat Chaudhuri 
and Martin Vechev. Ai2: Safety and robustness certiﬁcation of neural networks with abstract
interpretation. In 2018 IEEE Symposium on Security and Privacy (SP)  pages 3–18. IEEE  2018.

[14] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. ICLR  2015.

[15] Sven Gowal  Krishnamurthy Dvijotham  Robert Stanforth  Rudy Bunel  Chongli Qin  Jonathan
Uesato  Timothy Mann  and Pushmeet Kohli. On the effectiveness of interval bound propagation
for training veriﬁably robust models. arXiv preprint arXiv:1810.12715  2018.

[16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

10

[17] Dan Hendrycks  Kimin Lee  and Mantas Mazeika. Using pre-training can improve model

robustness and uncertainty. arXiv preprint arXiv:1901.09960  2019.

[18] Guy Katz  Clark Barrett  David L Dill  Kyle Julian  and Mykel J Kochenderfer. Reluplex:
An efﬁcient smt solver for verifying deep neural networks. In International Conference on
Computer Aided Veriﬁcation  pages 97–117. Springer  2017.

[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

[20] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial machine learning at scale.

arXiv preprint arXiv:1611.01236  2016.

[21] Mathias Lecuyer  Vaggelis Atlidakis  Roxana Geambasu  Daniel Hsu  and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471 
2018.

[22] Bai Li  Changyou Chen  Wenlin Wang  and Lawrence Carin. Second-order adversarial attack

and certiﬁable robustness. arXiv preprint arXiv:1809.03113  2018.

[23] Xuanqing Liu  Minhao Cheng  Huan Zhang  and Cho-Jui Hsieh. Towards robust neural networks
via random self-ensemble. In Proceedings of the European Conference on Computer Vision
(ECCV)  pages 369–385  2018.

[24] Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward

relu neural networks. arXiv preprint arXiv:1706.07351  2017.

[25] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 
2017.

[26] Matthew Mirman  Timon Gehr  and Martin Vechev. Differentiable abstract interpretation for
provably robust neural networks. In International Conference on Machine Learning  pages
3575–3583  2018.

[27] Aditi Raghunathan  Jacob Steinhardt  and Percy Liang. Certiﬁed defenses against adversarial
International Conference on Learning Representations (ICLR)  arXiv preprint

examples.
arXiv:1801.09344  2018.

[28] Aditi Raghunathan  Jacob Steinhardt  and Percy S Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems 
pages 10877–10887  2018.

[29] Jérôme Rony  Luiz G Hafemann  Luis S Oliveira  Ismail Ben Ayed  Robert Sabourin  and Eric
Granger. Decoupling direction and norm for efﬁcient gradient-based l2 adversarial attacks and
defenses. arXiv preprint arXiv:1811.09600  2018.

[30] Hadi Salman  Greg Yang  Huan Zhang  Cho-Jui Hsieh  and Pengchuan Zhang. A convex
relaxation barrier to tight robustness veriﬁcation of neural networks. In Advances in Neural
Information Processing Systems  pages 9832–9842  2019.

[31] Gagandeep Singh  Timon Gehr  Matthew Mirman  Markus Püschel  and Martin Vechev. Fast
and effective robustness certiﬁcation. In Advances in Neural Information Processing Systems 
pages 10825–10836  2018.

[32] Charles M Stein. Estimation of the mean of a multivariate normal distribution. The annals of

Statistics  pages 1135–1151  1981.

[33] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 
2013.

[34] Vincent Tjeng  Kai Y. Xiao  and Russ Tedrake. Evaluating robustness of neural networks with
mixed integer programming. In International Conference on Learning Representations  2019.
URL https://openreview.net/forum?id=HyGIdiRqtm.

11

[35] Jonathan Uesato  Brendan O’Donoghue  Aaron van den Oord  and Pushmeet Kohli. Adversarial
risk and the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666  2018.

[36] Shiqi Wang  Yizheng Chen  Ahmed Abdou  and Suman Jana. Mixtrain: Scalable training of

formally robust neural networks. arXiv preprint arXiv:1811.02625  2018.

[37] Shiqi Wang  Kexin Pei  Justin Whitehouse  Junfeng Yang  and Suman Jana. Efﬁcient formal
safety analysis of neural networks. In Advances in Neural Information Processing Systems 
pages 6369–6379  2018.

[38] Tsui-Wei Weng  Huan Zhang  Hongge Chen  Zhao Song  Cho-Jui Hsieh  Duane Boning 
Inderjit S Dhillon  and Luca Daniel. Towards fast computation of certiﬁed robustness for ReLU
networks. In International Conference on Machine Learning  2018.

[39] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex
outer adversarial polytope. In International Conference on Machine Learning (ICML)  pages
5283–5292  2018.

[40] Eric Wong  Frank Schmidt  Jan Hendrik Metzen  and J Zico Kolter. Scaling provable adversarial

defenses. Advances in Neural Information Processing Systems (NIPS)  2018.

[41] Huan Zhang  Tsui-Wei Weng  Pin-Yu Chen  Cho-Jui Hsieh  and Luca Daniel. Efﬁcient neural
In Advances in Neural

network robustness certiﬁcation with general activation functions.
Information Processing Systems  pages 4939–4948  2018.

12

,Hadi Salman
Jerry Li
Ilya Razenshteyn
Pengchuan Zhang
Huan Zhang
Sebastien Bubeck
Greg Yang