2016,Flexible Models for Microclustering with Application to Entity Resolution,Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models  Dirichlet process mixture models  and Pitman--Yor process mixture models make this assumption  as do all other infinitely exchangeable clustering models. However  for some applications  this assumption is inappropriate. For example  when performing entity resolution  the size of each cluster should be unrelated to the size of the data set  and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.,Flexible Models for Microclustering with

Application to Entity Resolution

Giacomo Zanella∗

Department of Decision Sciences

Bocconi University

giacomo.zanella@unibocconi.it

Brenda Betancourt∗

Department of Statistical Science

Duke University

bb222@stat.duke.edu

Hanna Wallach
Microsoft Research
hanna@dirichlet.net

Jeffrey Miller

Abbas Zaidi

Department of Biostatistics

Department of Statistical Science

Harvard University

jwmiller@hsph.harvard.edu

Duke University

amz19@stat.duke.edu

Departments of Statistical Science and Computer Science

Rebecca C. Steorts

Duke University

beka@stat.duke.edu

Abstract

Most generative models for clustering implicitly assume that the number of data
points in each cluster grows linearly with the total number of data points. Finite
mixture models  Dirichlet process mixture models  and Pitman–Yor process mixture
models make this assumption  as do all other inﬁnitely exchangeable clustering
models. However  for some applications  this assumption is inappropriate. For
example  when performing entity resolution  the size of each cluster should be
unrelated to the size of the data set  and each cluster should contain a negligible
fraction of the total number of data points. These applications require models that
yield clusters whose sizes grow sublinearly with the size of the data set. We address
this requirement by deﬁning the microclustering property and introducing a new
class of models that can exhibit this property. We compare models within this class
to two commonly used clustering models using four entity-resolution data sets.

1

Introduction

Many clustering applications require models that assume cluster sizes grow linearly with the size of the
data set. These applications include topic modeling  inferring population structure  and discriminating
among cancer subtypes. Inﬁnitely exchangeable clustering models  including ﬁnite mixture models 
Dirichlet process mixture models  and Pitman–Yor process mixture models  all make this linear-
growth assumption  and have seen numerous successes when used in these contexts. For other cluster-
ing applications  such as entity resolution  this assumption is inappropriate. Entity resolution (includ-
ing record linkage and de-duplication) involves identifying duplicate2 records in noisy databases [1  2] 
traditionally by directly linking records to one another. Unfortunately  this traditional approach is
computationally infeasible for large data sets—a serious limitation in “the age of big data” [1  3]. As a

∗Giacomo Zanella and Brenda Betancourt are joint ﬁrst authors.
2In the entity resolution literature  the term “duplicate records” does not mean that the records are identical 

but rather that the records are corrupted  degraded  or otherwise noisy representations of the same entity.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

result  researchers increasingly treat entity resolution as a clustering problem  where each entity is im-
plicitly associated with one or more records and the inference goal is to recover the latent entities (clus-
ters) that correspond to the observed records (data points) [4  5  6]. In contrast to other clustering appli-
cations  the number of data points in each cluster should remain small  even for large data sets. Appli-
cations like this require models that yield clusters whose sizes grow sublinearly with the total number
of data points [7]. To address this requirement  we deﬁne the microclustering property in section 2 and 
in section 3  introduce a new class of models that can exhibit this property. In section 4  we compare
two models within this class to two commonly used inﬁnitely exchangeable clustering models.

2 The Microclustering Property

To cluster N data points x1  . . .   xN using a partition-based Bayesian clustering model  one ﬁrst
places a prior over partitions of [N ] = {1  . . .   N}. Then  given a partition CN of [N ]  one models the
data points in each part c ∈ CN as jointly distributed according to some chosen distribution. Finally 
one computes the posterior distribution over partitions and  e.g.  uses it to identify probable partitions
of [N ]. Mixture models are a well-known type of partition-based Bayesian clustering model  in which
CN is implicitly represented by a set of cluster assignments z1  . . .   zN . These cluster assignments
can be regarded as the ﬁrst N elements of an inﬁnite sequence z1  z2  . . .  drawn a priori from

π ∼ H and z1  z2  . . . | π iid∼ π 

where H is a prior over π and π is a vector of mixture weights with(cid:80)

(1)
l πl = 1 and πl ≥ 0 for
all l. Commonly used mixture models include (a) ﬁnite mixtures where the dimensionality of π
is ﬁxed and H is usually a Dirichlet distribution; (b) ﬁnite mixtures where the dimensionality of
π is a random variable [8  9]; (c) Dirichlet process (DP) mixtures where the dimensionality of π
is inﬁnite [10]; and (d) Pitman–Yor process (PYP) mixtures  which generalize DP mixtures [11].
Equation 1 implicitly deﬁnes a prior over partitions of N = {1  2  . . .}. Any random partition CN of
(cid:80)N
N induces a sequence of random partitions (CN : N = 1  2  . . .)  where CN is a partition of [N ]. Via
the strong law of large numbers  the cluster sizes in any such sequence obtained via equation 1 grow
n=1 I(zn = l) → πl as N → ∞  where
linearly with N because  with probability one  for all l  1
I(·) denotes the indicator function. Unfortunately  this linear growth assumption is not appropriate
N
for entity resolution and other applications that require clusters whose sizes grow sublinearly with N.
To address this requirement  we therefore deﬁne the microclustering property: A sequence of random
partitions (CN : N = 1  2  . . .) exhibits the microclustering property if MN is op(N )  where MN is
the size of the largest cluster in CN   or  equivalently  if MN / N → 0 in probability as N → ∞.
A clustering model exhibits the microclustering property if the sequence of random partitions implied
by that model satisﬁes the above deﬁnition. No mixture model can exhibit the microclustering property
(unless its parameters are allowed to vary with N). In fact  Kingman’s paintbox theorem [12  13] im-
plies that any exchangeable partition of N  such as a partition obtained using equation 1  is either equal
to the trivial partition in which each part contains one element or satisﬁes lim infN→∞ MN / N > 0
with positive probability. By Kolmogorov’s extension theorem  a sequence of random partitions
(CN : N = 1  2  . . .) corresponds to an exchangeable random partition of N whenever (a) each CN
is ﬁnitely exchangeable (i.e.  its probability is invariant under permutations of {1  . . .   N}) and (b)
the sequence is projective (also known as consistent in distribution)—i.e.  if N(cid:48) < N  the distribution
over CN(cid:48) coincides with the marginal distribution over partitions of [N(cid:48)] induced by the distribution
over CN . Therefore  to obtain a nontrivial model that exhibits the microclustering property  we must
sacriﬁce either (a) or (b). Previous work [14] sacriﬁced (a); in this paper  we instead sacriﬁce (b).
Sacriﬁcing ﬁnite exchangeability and sacriﬁcing projectivity have very different consequences. If a
partition-based Bayesian clustering model is not ﬁnitely exchangeable  then inference will depend on
the order of the data points. For most applications  this consequence is undesirable—there is no reason
to believe that the order of the data points is meaningful. In contrast  if a model lacks projectivity 
then the implied joint distribution over a subset of the data points in a data set will not be the same as
the joint distribution obtained by modeling the subset directly. In the context of entity resolution  sac-
riﬁcing projectivity is a more natural and less restrictive choice than sacriﬁcing ﬁnite exchangeability.

2

3 Kolchin Partition Models for Microclustering

We introduce a new class of Bayesian models for microclustering by placing a prior on the number of
clusters K and  given K  modeling the cluster sizes N1  . . .   NK directly. We start by deﬁning

(cid:123)(cid:122)

  2  . . .   2

K ∼ κ and N1  . . .   NK | K iid∼ µ 

  . . . . . .   K  . . .   K

(cid:124) (cid:123)(cid:122) (cid:125)

(2)
where κ = (κ1  κ2  . . . ) and µ = (µ1  µ2  . . . ) are probability distributions over N = {1  2  . . .}.
k=1 Nk and  given N1  . . .   NK  generate a set of cluster assign-
ments z1  . . .   zN by drawing a vector uniformly at random from the set of permutations of
). The cluster assignments z1  . . .   zN induce a random par-
(1  . . .   1

We then deﬁne N = (cid:80)K
(cid:124)
(cid:124) (cid:123)(cid:122) (cid:125)

N1 times
tition CN of [N ]  where N is itself a random variable—i.e.  CN is a random partition of a random
number of elements. We refer to the resulting class of marginal distributions over CN as Kolchin
partition (KP) models [15  16] because the form of equation 2 is closely related to Kolchin’s repre-
sentation theorem for Gibbs-type partitions (see  e.g.  16  theorem 1.2). For appropriate choices of κ
and µ  KP models can exhibit the microclustering property (see appendix B for an example).

If CN denotes the set of all possible partitions of [N ]  then(cid:83)∞
partitions of [N ] for all N ∈ N. The probability of any given partition CN ∈(cid:83)∞

CN is the set of all possible

CN is

NK times

N2 times

(cid:125)

N =1

N =1

(cid:32) (cid:89)
N  a KP model implies that P (CN | N ) ∝ |CN|! κ|CN|(cid:0)(cid:81)

|CN|! κ|CN|

P (CN ) =

c∈CN

N !

where | · | denotes the cardinality of a set  |CN| is the number of clusters in CN   and |c| is the
number of elements in cluster c. In practice  however  N is usually observed. Conditioned on

|c|! µ|c|(cid:1). Equation 3 leads to a

 

(3)

“reseating algorithm”—much like the Chinese restaurant process (CRP)—derived by sampling from
P (CN | N  CN \n)  where CN \n is the partition obtained by removing element n from CN :

|c|! µ|c|

(cid:33)

c∈CN

• for n = 1  . . .   N  reassign element n to

– an existing cluster c ∈ CN \n with probability ∝ (|c| + 1) µ(|c|+1)
µ|c|
– or a new cluster with probability ∝ (|CN \n| + 1)
κ(|CN\n|+1)
κ|CN\n| µ1.

We can use this reseating algorithm to draw samples from P (CN | N ); however  unlike the CRP  it
does not produce an exact sample if it is used to incrementally construct a partition from the empty
set. In practice  this limitation does not lead to any negative consequences because standard posterior
inference sampling methods do not rely on this property. When a KP model is used as the prior in a
partition-based clustering model—e.g.  as an alternative to equation 1—the resulting Gibbs sampling
algorithm for CN is similar to this reseating algorithm  but accompanied by likelihood terms. Unfor-
tunately  this algorithm is slow for large data sets. In appendix C  we therefore propose a faster Gibbs
sampling algorithm—the chaperones algorithm—that is particularly well suited to microclustering.
In sections 3.1 and 3.2  we introduce two related KP models for microclustering  and in section 3.4
we explain how KP models can be applied in the context of entity resolution with categorical data.

3.1 The NBNB Model

We start with equation 3 and deﬁne

κ = NegBin (a  q)

(4)
where NegBin(a  q) and NegBin(r  p) are negative binomial distributions truncated to N =
{1  2  . . .}. We assume that a > 0 and q ∈ (0  1) are ﬁxed hyperparameters  while r and p are
distributed as r ∼ Gam(ηr  sr) and p ∼ Beta(up  vp) for ﬁxed ηr  sr  up and vp.3 We refer to the
resulting marginal distribution over CN as the negative binomial–negative binomial (NBNB) model.

and µ = NegBin (r  p)  

3We use the shape-and-rate parameterization of the gamma distribution.

3

κ = NegBin (a  q)

and µ| α  µ(0) ∼ Dir

with(cid:80)∞

where α > 0 is a ﬁxed concentration parameter and µ(0) = (µ(0)

1   µ(0)

m=1 µ(0)

m = 1 and µ(0)

m ≥ 0 for all m. The probability of CN conditioned on N and µ is

P (CN | N  a  q  µ) ∝ Γ (|CN| + a) q|CN| (cid:89)

|c|! µ|c|.

(7)

α  µ(0)(cid:17)

(cid:16)
(6)
2  ··· ) is a ﬁxed base measure

 

Figure 1: The NBNB (left) and NBD (right) models appear to exhibit the microclustering property.

By substituting equation 4 into equation 3  we obtain the probability of CN conditioned N:

P (CN | N  a  q  r  p) ∝ Γ (|CN| + a) β|CN| (cid:89)

Γ (|c| + r)

Γ (r)

c∈CN

 

(5)

where β = q (1−p)r
1−(1−p)r . We provide the complete derivation of equation 5  along with the conditional
posterior distributions over r and p  in appendix A.2. Posterior inference for the NBNB model involves
alternating between (a) sampling CN from P (CN | N  a  q  r  p) using the chaperones algorithm and
(b) sampling r and p from their respective conditional posteriors using  e.g.  slice sampling [17].

3.2 The NBD Model

Although κ = NegBin (a  q) will yield plausible values of K  µ = NegBin (r  p) may not be
sufﬁciently ﬂexible to capture realistic properties of N1  . . .   NK  especially when K is large. For
example  in a record-linkage application involving two otherwise noise-free databases containing
thousands of records  K will be large and each Nk will be at most two. A negative binomial
distribution cannot capture this property. We therefore deﬁne a second KP model—the negative
binomial–Dirichlet (NBD) model—by taking a nonparametric approach to modeling N1  . . .   NK
and drawing µ from an inﬁnite-dimensional Dirichlet distribution over the positive integers:

c∈CN

(cid:17)

(cid:16)

Posterior inference for the NBD model involves alternating between (a) sampling CN from
P (CN | N  a  q  µ) using the chaperones algorithm and (b) sampling µ from its conditional posterior:

µ| CN   α  µ(0) ∼ Dir

α µ(0)

1 + L1  α µ(0)

2 + L2  . . .

 

(8)

(N + 1)-dimensional vector (µ1  . . .   µN   1 −(cid:80)N

where Lm is the number of clusters of size m in CN . Although µ is an inﬁnite-dimensional
vector  only the ﬁrst N elements affect P (CN | a  q  µ). Therefore  it is sufﬁcient to sample the
m=1 µm) from equation 8  modiﬁed accordingly 
and retain only µ1  . . .   µN . We provide complete derivations of equations 7 and 8 in appendix A.3.

3.3 The Microclustering Property for the NBNB and NBD Models

Figure 1 contains empirical evidence suggesting that the NBNB and NBD models both exhibit the mi-
croclustering property. For each model  we generated samples of MN / N for N = 100  . . .   104. For
the NBNB model  we set a = 1  q = 0.5  r = 1  and p = 0.5 and generated the samples using rejec-
tion sampling. For the NBD model  we set a = 1  q = 0.5  and α = 1 and set µ(0) to be a geometric
distribution over N = {1  2  . . .} with a parameter of 0.5. We generated the samples using MCMC
methods. For both models  MN / N appears to converge to zero in probability as N → ∞  as desired.
In appendix B  we also prove that a variant of the NBNB model exhibits the microclustering property.

4

56789−6−4log(N)log(M N / N)56789−6−4−2log(N)log(M N / N)3.4 Application to Entity Resolution

KP models can be used to perform entity resolution. In this context  the data points x1  . . .   xN are ob-
served records and the K clusters are latent entities. If each record consists of F categorical ﬁelds  then

(9)
(10)
(11)
(12)

θf k | δf   γf ∼ Dir(cid:0)δf   γf

CN ∼ KP model

(cid:1)

zn ∼ ζ(CN   n)
xf n | zn  θf 1  . . .   θf K ∼ Cat (θf zn )

for f = 1  . . .   F   k = 1  . . .   K  and n = 1  . . .   N  where ζ(CN   n) maps the nth record to a latent
cluster assignment zn according to CN . We assume that δf > 0 is distributed as δf ∼ Gam (1  1) 
while γf is ﬁxed. Via Dirichlet–multinomial conjugacy  we can marginalize over θ11  . . .   θF K to
obtain a closed-form expression for P (x1  . . .   xN | z1  . . .   zN   δf   γf ). Posterior inference involves
alternating between (a) sampling CN from P (CN | x1  . . .   xN   δf ) using the chaperones algorithm
accompanied by appropriate likelihood terms  (b) sampling the parameters of the KP model from
their conditional posteriors  and (c) sampling δf from its conditional posterior using slice sampling.

4 Experiments

we set a and q to reﬂect a weakly informative prior belief that E[K] = (cid:112)Var[K] = N

In this section  we compare two entity resolution models based on the NBNB model and the NBD
model to two similar models based on the DP mixture model [10] and the PYP mixture model [11].
All four models use the likelihood in equations 10 and 12. For the NBNB model and the NBD model 
2 . For the
NBNB model  we set ηr = sr = 1 and up = vp = 2.4 For the NBD model  we set α = 1 and set µ(0)
to be a geometric distribution over N = {1  2  . . .} with a parameter of 0.5. This base measure reﬂects
a prior belief that E[Nk] = 2. Finally  to ensure a fair comparison between the two different classes
of model  we set the DP and PYP concentration parameters to reﬂect a prior belief that E[K] = N
2 .
We assess how well each model “ﬁts” four data sets typical of those arising in real-world entity reso-
lution applications. For each data set  we consider four statistics: (a) the number of singleton clusters 
(b) the maximum cluster size  (c) the mean cluster size  and (d) the 90th percentile of cluster sizes.
We compare each statistic’s true value to its posterior distribution according to each of the models.
For each model and data set combination  we also consider ﬁve entity-resolution summary statistics:
(a) the posterior expected number of clusters  (b) the posterior standard error  (c) the false negative
rate  (d) the false discovery rate  and (e) the posterior expected value of δf = δ for f = 1  . . .   F .
The false negative and false discovery rates are both invariant under permutations of 1  . . .   K [5  18].

4.1 Data Sets

We constructed four realistic data sets  each consisting of N records associated with K entities.
Italy: We derived this data set from the Survey on Household Income and Wealth  conducted by the
Bank of Italy every two years. There are nine categorical ﬁelds  including year of birth  employment
status  and highest level of education attained. Ground truth is available via unique identiﬁers based
upon social security numbers; roughly 74% of the clusters are singletons. We used the 2008 and 2010
databases from the Fruili region to create a record-linkage data set consisting of N = 789 records;
each Nk is at most two. We discarded the records themselves  but preserved the number of ﬁelds  the
empirical distribution of categories for each ﬁeld  the number of clusters  and the cluster sizes. We
then generated synthetic records using equations 10 and 12. We created three variants of this data set 
corresponding to δ = 0.02  0.05  0.1. For all three  we used the empirical distribution of categories for
ﬁeld f as γf . By generating synthetic records in this fashion  we preserve the pertinent characteristics
of the original data  while making it easy to isolate the impacts of the different priors over partitions.
NLTCS5000: We derived this data set from the National Long Term Care Survey (NLTCS)5—a
longitudinal survey of older Americans  conducted roughly every six years. We used four of the

4We used p ∼ Beta (2  2) because a uniform prior implies an unrealistic prior belief that E[Nk] = ∞.
5http://www.nltcs.aas.duke.edu/

5

available ﬁelds: date of birth  sex  state of residence  and regional ofﬁce. We split date of birth into
three separate ﬁelds: day  month  and year. Ground truth is available via social security numbers;
roughly 68% of the clusters are singletons. We used the 1982  1989  and 1994 databases and
down-sampled the records  preserving the proportion of clusters of each size and the maximum
cluster size  to create a record-linkage data set of N = 5  000 records; each Nk is at most three. We
then generated synthetic records using the same approach that we used to create the Italy data set.
Syria2000 and SyriaSizes: We constructed these data sets from data collected by four human-rights
groups between 2011 and 2014 on people killed in the Syrian conﬂict [19  20]. Hand-matched
ground truth is available from the Human Rights Data Analysis Group. Because the records were
hand matched  the data are noisy and potentially biased. Performing entity resolution is non-trivial
because there are only three categorical ﬁelds: gender  governorate  and date of death. We split date
of death  which is present for most records  into three separate ﬁelds: day  month  and year. However 
because the records only span four years  the year ﬁeld conveys little information. In addition  most
records are male  and there are only fourteen governorates. We created the Syria2000 data set by
down-sampling the records  preserving the proportion of clusters of each size  to create a data set
of N = 2  000 records; the maximum cluster size is ﬁve. We created the SyriaSizes data set by
down-sampling the records  preserving some of the larger clusters (which necessarily contain within-
database duplications)  to create a data set of N = 6  700 records; the maximum cluster size is ten.
We provide the empirical distribution over cluster sizes for each data set in appendix D. We generated
synthetic records for both data sets using the same approach that we used to create the Italy data set.

4.2 Results

We report the results of our experiments in table 1 and ﬁgure 2. The NBNB and NBD models
outperformed the DP and PYP models for almost all variants of the Italy and NLTCS5000 data sets.
In general  the NBD model performed the best of the four  and the differences between the models’
performance grew as the value of δ increased. For the Syria2000 and SyriaSizes data sets  we see no
consistent pattern to the models’ abilities to recover the true values of the data-set statistics. Moreover 
all four models had poor false negative rates  and false discovery rates—most likely because these
data sets are extremely noisy and contain very few ﬁelds. We suspect that no entity resolution model
would perform well for these data sets. For three of the four data sets  the exception being the
Syria2000 data set  the DP model and the PYP model both greatly overestimated the number of
clusters for larger values of δ. Taken together  these results suggest that the ﬂexibility of the NBNB
and NBD models make them more appropriate choices for most entity resolution applications.

5 Summary

Inﬁnitely exchangeable clustering models assume that cluster sizes grow linearly with the size of the
data set. Although this assumption is reasonable for some applications  it is inappropriate for others.
For example  when entity resolution is treated as a clustering problem  the number of data points in
each cluster should remain small  even for large data sets. Applications like this require models that
yield clusters whose sizes grow sublinearly with the size of the data set. We introduced the microclus-
tering property as one way to characterize models that address this requirement. We then introduced a
highly ﬂexible class of models—KP models—that can exhibit this property. We presented two models
within this class—the NBNB model and the NBD model—and showed that they are better suited
to entity resolution applications than two inﬁnitely exchangeable clustering models. We therefore
recommend KP models for applications where the size of each cluster should be unrelated to the size
of the data set  and each cluster should contain a negligible fraction of the total number of data points.

Acknowledgments

We thank Tamara Broderick  David Dunson  Merlise Clyde  and Abel Rodriguez for conversations
that helped form the ideas in this paper. In particular  Tamara Broderick played a key role in develop-
ing the idea of microclustering. We also thank the Human Rights Data Analysis Group for providing
us with data. This work was supported in part by NSF grants SBE-0965436  DMS-1045153  and
IIS-1320219; NIH grant 5R01ES017436-05; the John Templeton Foundation; the Foerster-Bernstein
Postdoctoral Fellowship; the UMass Amherst CIIR; and an EPSRC Doctoral Prize Fellowship.

6

(a) Italy: NBD model > NBNB model > PYP mixture model > DP mixture model.

(b) NLTCS5000: NBD model > NBNB model > PYP mixture model > DP mixture model.

(c) Syria2000: the models perform similarly because there are so few ﬁelds.

(d) SyriaSizes: the models perform similarly because there are so few ﬁelds.

Figure 2: Box plots depicting the true value (dashed line) of each data-set statistic for each variant of
each data set  as well as its posterior distribution according to each of the four entity resolution models.

7

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)350400450500Singleton ClusterslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)2345678Maximum Cluster SizellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)1.251.301.351.40Mean Cluster SizeDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)1.52.02.590th Percentile of Cluster SizeslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)160017001800Singleton ClusterslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)3456789Maximum Cluster SizellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)1.581.601.621.641.661.681.70Mean Cluster SizeDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)2.02.53.03.54.090th Percentile of Cluster SizeslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)1000120014001600Singleton ClusterslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)246810Maximum Cluster SizellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)1.101.151.201.251.301.35Mean Cluster SizelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)1.01.21.41.61.82.090th Percentile of Cluster SizeslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)200025003000Singleton ClustersllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)681012141618Maximum Cluster SizellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)1.41.51.61.71.8Mean Cluster SizelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllDP(0.02)PYP(0.02)NBNB(0.02)NDB(0.02)DP(0.05)PYP(0.05)NBNB(0.05)NDB(0.05)DP(0.1)PYP(0.1)NBNB(0.1)NDB(0.1)2.02.22.42.62.83.090th Percentile of Cluster SizesTable 1: Entity-resolution summary statistics—the posterior expected number of clusters  the posterior
standard error  the false negative rate (lower is better)  the false discovery rate (lower is better)  and
the posterior expected value of δ—for each variant of each data set and each of the four models.

Data Set

Italy

NLTCS5000

587

δ = 0.05

δ = 0.02

True K Variant Model
DP
PYP
NBNB
NBD
DP
PYP
NBNB
NBD
DP
PYP
NBNB
NBD
DP
PYP

δ = 0.02

δ = 0.1

3 061

δ = 0.05

δ = 0.1

Syria2000

1 725

δ = 0.02

δ = 0.05

δ = 0.1

SyriaSizes

4 075

δ = 0.02

δ = 0.05

δ = 0.1

DP
PYP

DP
PYP

E[K]
594.00
593.90
591.00
590.50
601.60
601.50
596.40
592.60
617.40
617.40
610.90
596.60
3021.70
3018.70
NBNB 3037.80
NBD 3028.20
3024.00
3045.80
NBNB 3040.90
NBD 3039.30
3130.50
3115.10
NBNB 3067.30
NBD 3049.10
1695.20
1719.70
NBNB 1726.80
NBD 1715.20
1701.80
1742.90
NBNB 1738.30
NBD 1711.40
1678.10
1761.20
NBNB 1779.40
NBD 1757.30
4175.70
4234.30
NBNB 4108.70
NBD 3979.50
4260.00
4139.10
NBNB 4047.10
NBD 3863.90
4507.40
4540.30
NBNB 4400.60
NBD 4251.90

DP
PYP

DP
PYP

DP
PYP

DP
PYP

DP
PYP

DP
PYP

Std. Err.

4.51
4.52
4.43
3.64
5.89
5.90
5.79
5.20
7.23
7.22
7.81
9.37
24.96
25.69
25.18
5.65
26.15
23.66
24.86
10.17
21.44
25.73
25.31
16.48
25.40
36.10
27.96
51.56
31.15
24.33
25.48
47.10
40.56
39.38
29.84
73.60
66.04
68.55
70.56
70.85
77.18
104.22
55.18
68.05
82.27
100.53
111.91
203.23

FNR FDR E[δ]
0.02
0.07
0.02
0.07
0.04
0.02
0.02
0.03
0.03
0.13
0.04
0.13
0.11
0.04
0.04
0.09
0.07
0.27
0.07
0.27
0.08
0.24
0.18
0.10
0.03
0.02
0.03
0.03
0.02
0.02
0.01
0.03
0.06
0.05
0.05
0.05
0.05
0.04
0.03
0.06
0.10
0.12
0.10
0.13
0.11
0.11
0.12
0.09
0.70
0.07
0.04
0.71
0.05
0.70
0.02
0.67
0.77
0.07
0.04
0.75
0.04
0.74
0.03
0.69
0.18
0.81
0.81
0.08
0.04
0.77
0.03
0.74
0.01
0.65
0.64
0.01
0.01
0.65
0.03
0.68
0.02
0.71
0.04
0.75
0.73
0.04
0.07
0.75
0.03
0.80
0.03
0.80
0.80
0.03
0.04
0.82

0.03
0.03
0.03
0.00
0.03
0.03
0.04
0.04
0.06
0.05
0.06
0.05
0.11
0.11
0.07
0.09
0.13
0.10
0.06
0.07
0.09
0.10
0.08
0.08
0.27
0.26
0.28
0.28
0.31
0.32
0.31
0.32
0.19
0.22
0.26
0.25
0.17
0.19
0.19
0.20
0.21
0.18
0.20
0.22
0.19
0.20
0.23
0.25

8

References
[1] P. Christen. Data Matching: Concepts and Techniques for Record Linkage  Entity Resolution 

and Duplicate Detection. Springer  2012.

[2] P. Christen. A survey of indexing techniques for scalable record linkage and deduplication.

IEEE Transactions on Knowledge and Data Engineering  24(9)  2012.

[3] W. E. Winkler. Overview of record linkage and current research directions. Technical report 

U.S. Bureau of the Census Statistical Research Division  2006.

[4] R. C. Steorts  R. Hall  and S. E. Fienberg. A Bayesian approach to graphical record linkage and

de-duplication. Journal of the American Statistical Society  In press.

[5] R. C. Steorts. Entity resolution with empirically motivated priors. Bayesian Analysis  10(4):849–

875  2015.

[6] R. C. Steorts  R. Hall  and S. E. Fienberg. SMERED: A Bayesian approach to graphical record

linkage and de-duplication. Journal of Machine Learning Research  33:922–930  2014.

[7] T. Broderick and R. C. Steorts. Variational bayes for merging noisy databases. In NIPS 2014

Workshop on Advances in Variational Inference  2014. arXiv:1410.4792.

[8] S. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number of

components. Journal of the Royal Statistical Society Series B  pages 731–792  1997.

[9] J. W. Miller and M. T. Harrison. Mixture models with a prior on the number of components.

arXiv:1502.06241  2015.

[10] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica  4:639–650  1994.

[11] H. Ishwaran and L. F. James. Generalized weighted Chinese restaurant processes for species

sampling mixture models. Statistica Sinica  13(4):1211–1236  2003.

[12] J. F. .C Kingman. The representation of partition structures. Journal of the London Mathematical

Society  2(2):374–380  1978.

[13] D. Aldous. Exchangeability and related topics. École d’Été de Probabilités de Saint-Flour

XIII—1983  pages 1–198  1985.

[14] H. M. Wallach  S. Jensen  L. Dicker  and K. A. Heller. An alternative prior process for
nonparametric Bayesian clustering. In Proceedings of the 13th International Conference on
Artiﬁcial Intelligence and Statistics  2010.

[15] V. F. Kolchin. A problem of the allocation of particles in cells and cycles of random permutations.

Theory of Probability & Its Applications  16(1):74–90  1971.

[16] J. Pitman. Combinatorial stochastic processes. École d’Été de Probabilités de Saint-Flour

XXXII—2002  2006.

[17] R. M. Neal. Slice sampling. Annals of Statistics  31:705–767  2003.

[18] R. C. Steorts  S. L. Ventura  M. Sadinle  and S. E. Fienberg. A comparison of blocking methods
for record linkage. In International Conference on Privacy in Statistical Databases  pages
253–268  2014.

[19] M. Price  J. Klingner  A. Qtiesh  and P. Ball. Updated statistical analysis of documentation of
killings in the Syrian Arab Republic  2013. United Nations Ofﬁce of the UN High Commissioner
for Human Rights.

[20] M. Price  J. Klingner  A. Qtiesh  and P. Ball. Updated statistical analysis of documentation of

killings in the Syrian Arab Republic. Human Rights Data Analysis Group  Geneva  2014.

9

,Nan Li
Rong Jin
Zhi-Hua Zhou
Brenda Betancourt
Giacomo Zanella
Jeffrey Miller
Hanna Wallach
Abbas Zaidi
Rebecca Steorts