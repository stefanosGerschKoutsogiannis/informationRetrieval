2018,Link Prediction Based on Graph Neural Networks,Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions  such as common neighbors and Katz index  to measure the likelihood of links. They have obtained wide practical uses due to their simplicity  interpretability  and for some of them  scalability. However  every heuristic has a strong assumption on when two nodes are likely to link  which limits their effectiveness on networks where these assumptions fail. In this regard  a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link  we aim to learn a function mapping the subgraph patterns to link existence  thus automatically learning a ``heuristic'' that suits the current network. In this paper  we study this heuristic learning paradigm for link prediction. First  we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework  and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second  based on the $\gamma$-decaying theory  we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance  working consistently well on a wide range of problems.,Link Prediction Based on Graph Neural Networks

Muhan Zhang

Department of CSE

Washington University in St. Louis

muhan@wustl.edu

Yixin Chen

Department of CSE

Washington University in St. Louis

chen@cse.wustl.edu

Abstract

Link prediction is a key problem for network-structured data. Link prediction
heuristics use some score functions  such as common neighbors and Katz index 
to measure the likelihood of links. They have obtained wide practical uses due to
their simplicity  interpretability  and for some of them  scalability. However  every
heuristic has a strong assumption on when two nodes are likely to link  which
limits their effectiveness on networks where these assumptions fail. In this regard 
a more reasonable way should be learning a suitable heuristic from a given network
instead of using predeﬁned ones. By extracting a local subgraph around each target
link  we aim to learn a function mapping the subgraph patterns to link existence 
thus automatically learning a “heuristic” that suits the current network. In this
paper  we study this heuristic learning paradigm for link prediction. First  we
develop a novel -decaying heuristic theory. The theory uniﬁes a wide range of
heuristics in a single framework  and proves that all these heuristics can be well
approximated from local subgraphs. Our results show that local subgraphs reserve
rich information related to link existence. Second  based on the -decaying theory 
we propose a new method to learn heuristics from local subgraphs using a graph
neural network (GNN). Its experimental results show unprecedented performance 
working consistently well on a wide range of problems.

1

Introduction

Link prediction is to predict whether two nodes in a network are likely to have a link [1]. Given the
ubiquitous existence of networks  it has many applications such as friend recommendation [2]  movie
recommendation [3]  knowledge graph completion [4]  and metabolic network reconstruction [5].
One class of simple yet effective approaches for link prediction is called heuristic methods. Heuristic
methods compute some heuristic node similarity scores as the likelihood of links [1  6]. Existing
heuristics can be categorized based on the maximum hop of neighbors needed to calculate the
score. For example  common neighbors (CN) and preferential attachment (PA) [7] are ﬁrst-order
heuristics  since they only involve the one-hop neighbors of two target nodes. Adamic-Adar (AA) and
resource allocation (RA) [8] are second-order heuristics  as they are calculated from up to two-hop
neighborhood of the target nodes. We deﬁne h-order heuristics to be those heuristics which require
knowing up to h-hop neighborhood of the target nodes. There are also some high-order heuristics
which require knowing the entire network. Examples include Katz  rooted PageRank (PR) [9]  and
SimRank (SR) [10]. Table 3 in Appendix A summarizes eight popular heuristics.
Although working well in practice  heuristic methods have strong assumptions on when links may
exist. For example  the common neighbor heuristic assumes that two nodes are more likely to connect
if they have many common neighbors. This assumption may be correct in social networks  but is
shown to fail in protein-protein interaction (PPI) networks – two proteins sharing many common
neighbors are actually less likely to interact [11].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

B

A

D

C

B

?

A

Extract enclosing 

subgraphs

D

?

C

Graph neural network

common neighbors = 3
Jaccard = 0.6
preferential attachment = 16

Katz ≈	0.03
……
Katz ≈	0.001
……

Learn graph structure features

common neighbors = 0
Jaccard = 0
preferential attachment = 8

1 (link)

Predict links

0 (non-link)

Figure 1: The SEAL framework. For each target link  SEAL extracts a local enclosing subgraph around it  and
uses a GNN to learn general graph structure features for link prediction. Note that the heuristics listed inside the
box are just for illustration – the learned features may be completely different from existing heuristics.

In fact  the heuristics belong to a more generic class  namely graph structure features. Graph structure
features are those features located inside the observed node and edge structures of the network  which
can be calculated directly from the graph. Since heuristics can be viewed as predeﬁned graph structure
features  a natural idea is to automatically learn such features from the network. Zhang and Chen
[12] ﬁrst studied this problem. They extract local enclosing subgraphs around links as the training
data  and use a fully-connected neural network to learn which enclosing subgraphs correspond to
link existence. Their method called Weisfeiler-Lehman Neural Machine (WLNM) has achieved
state-of-the-art link prediction performance. The enclosing subgraph for a node pair (x  y) is the
subgraph induced from the network by the union of x and y’s neighbors up to h hops. Figure 1
illustrates the 1-hop enclosing subgraphs for (A  B) and (C  D). These enclosing subgraphs are very
informative for link prediction – all ﬁrst-order heuristics such as common neighbors can be directly
calculated from the 1-hop enclosing subgraphs.
However  it is shown that high-order heuristics such as rooted PageRank and Katz often have much
better performance than ﬁrst and second-order ones [6]. To effectively learn good high-order features 
it seems that we need a very large hop number h so that the enclosing subgraph becomes the entire
network. This results in unaffordable time and memory consumption for most practical networks.
But do we really need such a large h to learn high-order heuristics?
Fortunately  as our ﬁrst contribution  we show that we do not necessarily need a very large h to
learn high-order graph structure features. We dive into the inherent mechanisms of link prediction
heuristics  and ﬁnd that most high-order heuristics can be uniﬁed by a -decaying theory. We prove
that  under mild conditions  any -decaying heuristic can be effectively approximated from an h-hop
enclosing subgraph  where the approximation error decreases at least exponentially with h. This
means that we can safely use even a small h to learn good high-order features. It also implies that the
“effective order” of these high-order heuristics is not that high.
Based on our theoretical results  we propose a novel link prediction framework  SEAL  to learn general
graph structure features from local enclosing subgraphs. SEAL ﬁxes multiple drawbacks of WLNM.
First  a graph neural network (GNN) [13  14  15  16  17] is used to replace the fully-connected neural
network in WLNM  which enables better graph feature learning ability. Second  SEAL permits
learning from not only subgraph structures  but also latent and explicit node features  thus absorbing
multiple types of information. We empirically veriﬁed its much improved performance.
Our contributions are summarized as follows. 1) We present a new theory for learning link prediction
heuristics  justifying learning from local subgraphs instead of entire networks. 2) We propose SEAL 
a novel link prediction framework based on GNN (illustrated in Figure 1). SEAL outperforms all
heuristic methods  latent feature methods  and recent network embedding methods by large margins.
SEAL also outperforms the previous state-of-the-art method  WLNM.

2 Preliminaries

Notations Let G = (V  E) be an undirected graph  where V is the set of vertices and E ✓ V ⇥ V
is the set of observed links. Its adjacency matrix is A  where Ai j = 1 if (i  j) 2 E and Ai j = 0

2

otherwise. For any nodes x  y 2 V   let (x) be the 1-hop neighbors of x  and d(x  y) be the shortest
path distance between x and y. A walk w = hv0 ···   vki is a sequence of nodes with (vi  vi+1) 2 E.
We use |hv0 ···   vki| to denote the length of the walk w  which is k here.
Latent features and explicit features Besides graph structure features  latent features and explicit
features are also studied for link prediction. Latent feature methods [3  18  19  20] factorize some
matrix representations of the network to learn a low-dimensional latent representation/embedding for
each node. Examples include matrix factorization [3] and stochastic block model [18] etc. Recently 
a number of network embedding techniques have been proposed  such as DeepWalk [19]  LINE
[21] and node2vec [20]  which are also latent feature methods since they implicitly factorize some
matrices too [22]. Explicit features are often available in the form of node attributes  describing all
kinds of side information about individual nodes. It is shown that combining graph structure features
with latent features and explicit features can improve the performance [23  24].
Graph neural networks Graph neural network (GNN) is a new type of neural network for learning
over graphs [13  14  15  16  25  26]). Here  we only brieﬂy introduce the components of a GNN since
this paper is not about GNN innovations but is a novel application of GNN. A GNN usually consists
of 1) graph convolution layers which extract local substructure features for individual nodes  and 2) a
graph aggregation layer which aggregates node-level features into a graph-level feature vector. Many
graph convolution layers can be uniﬁed into a message passing framework [27].
Supervised heuristic learning There are some previous attempts to learn supervised heuristics
for link prediction. The closest work to ours is the Weisfeiler-Lehman Neural Machine (WLNM)
[12]  which also learns from local subgraphs. However  WLNM has several drawbacks. Firstly 
WLNM trains a fully-connected neural network on the subgraphs’ adjacency matrices. Since fully-
connected neural networks only accept ﬁxed-size tensors as input  WLNM requires truncating
different subgraphs to the same size  which may lose much structural information. Secondly  due
to the limitation of adjacency matrix representations  WLNM cannot learn from latent or explicit
features. Thirdly  theoretical justiﬁcations are also missing. We include more discussion on WLNM
in Appendix D. Another related line of research is to train a supervised learning model on different
heuristics’ combination. For example  the path ranking algorithm [28] trains logistic regression on
different path types’ probabilities to predict relations in knowledge graphs. Nickel et al. [23] propose
to incorporate heuristic features into tensor factorization models. However  these models still rely on
predeﬁned heuristics – they cannot learn general graph structure features.

3 A theory for unifying link prediction heuristics

In this section  we aim to understand deeper the mechanisms behind various link prediction heuristics 
and thus motivating the idea of learning heuristics from local subgraphs. Due to the large number of
graph learning techniques  note that we are not concerned with the generalization error of a particular
method  but focus on the information reserved in the subgraphs for calculating existing heuristics.
Deﬁnition 1. (Enclosing subgraph) For a graph G = (V  E)  given two nodes x  y 2 V   the
h-hop enclosing subgraph for (x  y) is the subgraph Gh
x y induced from G by the set of nodes
{ i | d(i  x)  h or d(i  y)  h }.
The enclosing subgraph describes the “h-hop surrounding environment" of (x  y). Since Gh
x y
contains all h-hop neighbors of x and y  we naturally have the following theorem.
Theorem 1. Any h-order heuristic for (x  y) can be accurately calculated from Gh
For example  a 2-hop enclosing subgraph will contain all the information needed to calculate any ﬁrst
and second-order heuristics. However  although ﬁrst and second-order heuristics are well covered
by local enclosing subgraphs  an extremely large h seems to be still needed for learning high-order
heuristics. Surprisingly  our following analysis shows that learning high-order heuristics is also
feasible with a small h. We support this ﬁrst by deﬁning the -decaying heuristic. We will show
that under certain conditions  a -decaying heuristic can be very well approximated from the h-hop
enclosing subgraph. Moreover  we will show that almost all well-known high-order heuristics can be
uniﬁed into this -decaying heuristic framework.
Deﬁnition 2. (-decaying heuristic) A -decaying heuristic for (x  y) has the following form:

x y.

H(x  y) = ⌘

lf (x  y  l) 

(1)

1Xl=1

3

where  is a decaying factor between 0 and 1  ⌘ is a positive constant or a positive function of  that
is upper bounded by a constant  f is a nonnegative function of x  y  l under the the given network.

Next  we will show that under certain conditions  a -decaying heuristic can be approximated from
an h-hop enclosing subgraph  and the approximation error decreases at least exponentially with h.

Theorem 2. Given a -decaying heuristic H(x  y) = ⌘P1l=1 lf (x  y  l)  if f (x  y  l) satisﬁes:
• (property 1) f (x  y  l)  l where  < 1
 ; and
x y for l = 1  2 ···   g(h)  where g(h) = ah+b with
• (property 2) f (x  y  l) is calculable from Gh
a  b 2 N and a > 0 
x y and the approximation error decreases at least expo-
then H(x  y) can be approximated from Gh
nentially with h.
Proof. We can approximate such a -decaying heuristic by summing over its ﬁrst g(h) terms.

The approximation error can be bounded as follows.

|H(x  y)  eH(x  y)| = ⌘

1Xl=g(h)+1

g(h)Xl=1

eH(x  y) := ⌘
lf (x  y  l)  ⌘

lf (x  y  l).

(2)

1Xl=ah+b+1

ll = ⌘()ah+b+1(1  )1.

In practice  a small  and a large a lead to a faster decreasing speed. Next we will prove that three
popular high-order heuristics: Katz  rooted PageRank and SimRank  are all -decaying heuristics
which satisfy the properties in Theorem 2. First  we need the following lemma.
Lemma 1. Any walk between x and y with length l  2h + 1 is included in Gh
x y.
Proof. Given any walk w = hx  v1 ···   vl1  yi with length l  we will show that every node vi
is included in Gh
x y. Consider any vi. Assume d(vi  x)  h + 1 and d(vi  y)  h + 1. Then 
2h + 1  l = |hx  v1 ···   vii| +|hvi ···   vl1  yi|  d(vi  x) + d(vi  y)  2h + 2  a contradiction.
Thus  d(vi  x)  h or d(vi  y)  h. By the deﬁnition of Gh
Next we will analyze Katz  rooted PageRank and SimRank one by one.

x y  vi must be included in Gh

x y.

3.1 Katz index
The Katz index [29] for (x  y) is deﬁned as

Katzx y =

l|walkshli(x  y)| =

l[Al]x y 

(3)

1Xl=1

1Xl=1

where walkshli(x  y) is the set of length-l walks between x and y  and Al is the lth power of the
adjacency matrix of the network. Katz index sums over the collection of all walks between x and y
where a walk of length l is damped by l (0 <  < 1)  giving more weight to shorter walks.
Katz index is directly deﬁned in the form of a -decaying heuristic with ⌘ = 1   =   and
f (x  y  l) = |walkshli(x  y)|. According to Lemma 1  |walkshli(x  y)| is calculable from Gh
x y for
l  2h + 1  thus property 2 in Theorem 2 is satisﬁed. Now we show when property 1 is satisﬁed.
Proposition 1. For any nodes i  j  [Al]i j is bounded by dl  where d is the maximum node degree of
the network.
Proof. We prove it by induction. When l = 1  Ai j  d for any (i  j). Thus the base case is correct.
Now  assume by induction that [Al]i j  dl for any (i  j)  we have

[Al+1]i j =

[Al]i kAk j  dl

Ak j  dld = dl+1.

|V |Xk=1

|V |Xk=1

Taking  = d  we can see that whenever d < 1/  the Katz index will satisfy property 1 in Theorem
2. In practice  the damping factor  is often set to very small values like 5E-4 [1]  which implies that
Katz can be very well approximated from the h-hop enclosing subgraph.

4

3.2 PageRank
The rooted PageRank for node x calculates the stationary distribution of a random walker starting at
x  who iteratively moves to a random neighbor of its current position with probability ↵ or returns
to x with probability 1  ↵. Let ⇡x denote the stationary distribution vector. Let [⇡x]i denote the
probability that the random walker is at node i under the stationary distribution.
Let P be the transition matrix with Pi j = 1
if (i  j) 2 E and Pi j = 0 otherwise. Let ex be a
vector with the xth element being 1 and others being 0. The stationary distribution satisﬁes
(4)

|(vj )|

⇡x = ↵P ⇡x + (1  ↵)ex.

When used for link prediction  the score for (x  y) is given by [⇡x]y (or [⇡x]y + [⇡y]x for symmetry).
To show that rooted PageRank is a -decaying heuristic  we introduce the inverse P-distance theory
[30]  which states that [⇡x]y can be equivalently written as follows:
P [w]↵len(w) 

(5)

[⇡x]y = (1  ↵) Xw:x y

where the summation is taken over all walks w starting at x and ending at y (possibly touching x
and y multiple times). For a walk w = hv0  v1 ···   vki  len(w) := |hv0  v1 ···   vki| is the length
of the walk. The term P [w] is deﬁned asQk1
  which can be interpreted as the probability of
traveling w. Now we have the following theorem.
Theorem 3. The rooted PageRank heuristic is a -decaying heuristic which satisﬁes the properties
in Theorem 2.
Proof. We ﬁrst write [⇡x]y in the following form.

|(vi)|

i=0

1

[⇡x]y = (1  ↵)

1Xl=1 Xw:x y

len(w)=l

P [w]↵l.

(6)

P [w] leads to the form of a -decaying heuristic. Note that f (x  y  l)
is the probability that a random walker starting at x stops at y with exactly l steps  which satisﬁes
↵ (property 1). According to Lemma 1  f (x  y  l) is

Deﬁning f (x  y  l) :=P w:x y
Pz2V f (x  z  l) = 1. Thus  f (x  y  l)  1 < 1

x y for l  2h + 1 (property 2).

also calculable from Gh

len(w)=l

3.3 SimRank
The SimRank score [10] is motivated by the intuition that two nodes are similar if their neighbors are
also similar. It is deﬁned in the following recursive way: if x = y  then s(x  y) := 1; otherwise 

s(x  y) := Pa2(x)Pb2(y) s(a  b)
s(x  y) = Xw:(x y)((z z)

|(x)| · |(y)|

P [w]len(w) 

(7)

(8)

where  is a constant between 0 and 1. According to [10]  SimRank has an equivalent deﬁnition:

where w : (x  y) ( (z  z) denotes all simultaneous walks such that one walk starts at x  the other walk
starts at y  and they ﬁrst meet at any vertex z. For a simultaneous walk w = h(v0  u0) ···   (vk  uk)i 
len(w) = k is the length of the walk. The term P [w] is similarly deﬁned asQk1
 
|(vi)||(ui)|
describing the probability of this walk. Now we have the following theorem.
Theorem 4. SimRank is a -decaying heuristic which satisﬁes the properties in Theorem 2.
Proof. We write s(x  y) as follows.

i=0

1

s(x  y) =

1Xl=1 Xw:(x y)((z z)

len(w)=l

P [w]l 

(9)

P [w] reveals that SimRank is a -decaying heuristic. Note

Deﬁning f (x  y  l) :=Pw:(x y)((z z)
that f (x  y  l)  1 < 1

len(w)=l

 . It is easy to see that f (x  y  l) is also calculable from Gh

x y for l  h.

5

Discussion There exist several other high-order heuristics based on path counting or random walk
[6] which can be as well incorporated into the -decaying heuristic framework. We omit the analysis
here. Our results reveal that most high-order heuristics inherently share the same -decaying heuristic
form  and thus can be effectively approximated from an h-hop enclosing subgraph with exponentially
smaller approximation error. We believe the ubiquity of -decaying heuristics is not by accident –
it implies that a successful link prediction heuristic is better to put exponentially smaller weight on
structures far away from the target  as remote parts of the network intuitively make little contribution
to link existence. Our results build the foundation for learning heuristics from local subgraphs  as they
imply that local enclosing subgraphs already contain enough information to learn good graph
structure features for link prediction which is much desired considering learning from the entire
network is often infeasible. To summarize  from the small enclosing subgraphs extracted around
links  we are able to accurately calculate ﬁrst and second-order heuristics  and approximate a wide
range of high-order heuristics with small errors. Therefore  given adequate feature learning ability of
the model used  learning from such enclosing subgraphs is expected to achieve performance at least
as good as a wide range of heuristics. There is some related work which empirically veriﬁes that local
methods can often estimate PageRank and SimRank well [31  32]. Another related theoretical work
[33] establishes a condition of h to achieve some ﬁxed approximation error for ordinary PageRank.

4 SEAL: An implemetation of the theory using GNN

In this section  we describe our SEAL framework for link prediction. SEAL does not restrict the
learned features to be in some particular forms such as -decaying heuristics  but instead learns
general graph structure features for link prediction. It contains three steps: 1) enclosing subgraph
extraction  2) node information matrix construction  and 3) GNN learning. Given a network  we aim
to learn automatically a “heuristic” that best explains the link formations. Motivated by the theoretical
results  this function takes local enclosing subgraphs around links as input  and output how likely
the links exist. To learn such a function  we train a graph neural network (GNN) over the enclosing
subgraphs. Thus  the ﬁrst step in SEAL is to extract enclosing subgraphs for a set of sampled positive
links (observed) and a set of sampled negative links (unobserved) to construct the training data.
A GNN typically takes (A  X) as input  where A (with slight abuse of notation) is the adjacency matrix
of the input enclosing subgraph  X is the node information matrix each row of which corresponds
to a node’s feature vector. The second step in SEAL is to construct the node information matrix
X for each enclosing subgraph. This step is crucial for training a successful GNN link prediction
model. In the following  we discuss this key step. The node information matrix X in SEAL has three
components: structural node labels  node embeddings and node attributes.

4.1 Node labeling
The ﬁrst component in X is each node’s structural label. A node labeling is function fl : V ! N
which assigns an integer label fl(i) to every node i in the enclosing subgraph. The purpose is to use
different labels to mark nodes’ different roles in an enclosing subgraph: 1) The center nodes x and
y are the target nodes between which the link is located. 2) Nodes with different relative positions to
the center have different structural importance to the link. A proper node labeling should mark such
differences. If we do not mark such differences  GNNs will not be able to tell where are the target
nodes between which a link existence should be predicted  and lose structural information.
Our node labeling method is derived from the following criteria: 1) The two target nodes x and y
always have the distinctive label “1”. 2) Nodes i and j have the same label if d(i  x) = d(j  x) and
d(i  y) = d(j  y). The second criterion is because  intuitively  a node i’s topological position within
an enclosing subgraph can be described by its radius with respect to the two center nodes  namely
(d(i  x)  d(i  y)). Thus  we let nodes on the same orbit have the same label  so that the node labels
can reﬂect nodes’ relative positions and structural importance within subgraphs.
Based on the above criteria  we propose a Double-Radius Node Labeling (DRNL) as follows. First 
assign label 1 to x and y. Then  for any node i with (d(i  x)  d(i  y)) = (1  1)  assign label fl(i) = 2.
Nodes with radius (1  2) or (2  1) get label 3. Nodes with radius (1  3) or (3  1) get 4. Nodes with
(2  2) get 5. Nodes with (1  4) or (4  1) get 6. Nodes with (2  3) or (3  2) get 7. So on and so forth. In
other words  we iteratively assign larger labels to nodes with a larger radius w.r.t. both center nodes 
where the label fl(i) and the double-radius (d(i  x)  d(i  y)) satisfy

6

1) if d(i  x) + d(i  y) 6= d(j  x) + d(j  y)  then d(i  x) + d(i  y) < d(j  x) + d(j  y)   fl(i) < fl(j);
2) if d(i  x) + d(i  y) = d(j  x) + d(j  y)  then d(i  x)d(i  y) < d(j  x)d(j  y)   fl(i) < fl(j).
One advantage of DRNL is that it has a perfect hashing function

fl(i) = 1 + min(dx  dy) + (d/2)[(d/2) + (d%2)  1] 

(10)
where dx := d(i  x)  dy := d(i  y)  d := dx + dy  (d/2) and (d%2) are the integer quotient and
remainder of d divided by 2  respectively. This perfect hashing allows fast closed-form computations.
For nodes with d(i  x) = 1 or d(i  y) = 1  we give them a null label 0. Note that DRNL is not
the only possible way of node labeling  but we empirically veriﬁed its better performance than no
labeling and other naive labelings. We discuss more about node labeling in Appendix B. After getting
the labels  we use their one-hot encoding vectors to construct X.

Incorporating latent and explicit features

4.2
Other than the structural node labels  the node information matrix X also provides an opportunity to
include latent and explicit features. By concatenating each node’s embedding/attribute vector to its
corresponding row in X  we can make SEAL simultaneously learn from all three types of features.
Generating the node embeddings for SEAL is nontrivial. Suppose we are given the observed network
G = (V  E)  a set of sampled positive training links Ep ✓ E  and a set of sampled negative training
links En with En \ E = ?. If we directly generate node embeddings on G  the node embeddings
will record the link existence information of the training links (since Ep ✓ E). We observed that
GNNs can quickly ﬁnd out such link existence information and optimize by only ﬁtting this part
of information. This results in bad generalization performance in our experiments. Our trick is to
temporally add En into E  and generate the embeddings on G0 = (V  E [ En). This way  the positive
and negative training links will have the same link existence information recorded in the embeddings 
so that GNN cannot classify links by only ﬁtting this part of information. We empirically veriﬁed the
much improved performance of this trick to SEAL. We name this trick negative injection.
We name our proposed framework SEAL (learning from Subgraphs  Embeddings and Attributes for
Link prediction)  emphasizing its ability to jointly learn from three types of features.

5 Experimental results

We conduct extensive experiments to evaluate SEAL. Our results show that SEAL is a superb and
robust framework for link prediction  achieving unprecedentedly strong performance on various
networks. We use AUC and average precision (AP) as evaluation metrics. We run all experiments for
10 times and report the average AUC results and standard deviations. We leave the the AP and time
results in Appendix F. SEAL is ﬂexible with what GNN or node embeddings to use. Thus  we choose
a recent architecture DGCNN [17] as the default GNN  and node2vec [20] as the default embeddings.
The code and data are available at https://github.com/muhanzhang/SEAL.
Datasets The eight datasets used are: USAir  NS  PB  Yeast  C.ele  Power  Router  and E.coli (please
see Appendix C for details). We randomly remove 10% existing links from each dataset as positive
testing data. Following a standard manner of learning-based link prediction  we randomly sample
the same number of nonexistent links (unconnected node pairs) as negative testing data. We use the
remaining 90% existing links as well as the same number of additionally sampled nonexistent links
to construct the training data.
Comparison to heuristic methods We ﬁrst compare SEAL with methods that only use graph
structure features. We include eight popular heuristics (shown in Appendix A  Table 3): common
neighbors (CN)  Jaccard  preferential attachment (PA)  Adamic-Adar (AA)  resource allocation (RA) 
Katz  PageRank (PR)  and SimRank (SR). We additionally include Ensemble (ENS) which trains
a logistic regression classiﬁer on the eight heuristic scores. We also include two heuristic learning
methods: Weisfeiler-Lehman graph kernel (WLK) [34] and WLNM [12]  which also learn from
(truncated) enclosing subgraphs. We omit path ranking methods [28] as well as other recent methods
which are speciﬁcally designed for knowledge graphs or recommender systems [23  35]. As all the
baselines only use graph structure features  we restrict SEAL to not include any latent or explicit
features. In SEAL  the hop number h is an important hyperparameter. Here  we select h only from
{1  2}  since on one hand we empirically veriﬁed that the performance typically does not increase

7

after h  3  which validates our theoretical results that the most useful information is within local
structures. On the other hand  even h = 3 sometimes results in very large subgraphs if a hub node
is included. This raises the idea of sampling nodes in subgraphs  which we leave to future work.
The selection principle is very simple: If the second-order heuristic AA outperforms the ﬁrst-order
heuristic CN on 10% validation data  then we choose h = 2; otherwise we choose h = 1. For datasets
PB and E.coli  we consistently use h = 1 to ﬁt into the memory. We include more details about the
baselines and hyperparameters in Appendix D.

Table 1: Comparison with heuristic methods (AUC).

Data
USAir
NS
PB
Yeast
C.ele
Power
Router
E.coli

CN

93.80±1.22
94.42±0.95
92.04±0.35
89.37±0.61
85.13±1.61
58.80±0.88
56.43±0.52
93.71±0.39

Jaccard
89.79±1.61
94.43±0.93
87.41±0.39
89.32±0.60
80.19±1.64
58.79±0.88
56.40±0.52
81.31±0.61

PA

AA

RA

88.84±1.45
68.65±2.03
90.14±0.45
82.20±1.02
74.79±2.04
44.33±1.02
47.58±1.47
91.82±0.58

95.06±1.03
94.45±0.93
92.36±0.34
89.43±0.62
86.95±1.40
58.79±0.88
56.43±0.51
95.36±0.34

95.77±0.92
94.45±0.93
92.46±0.37
89.45±0.62
87.49±1.41
58.79±0.88
56.43±0.51
95.95±0.35

Katz

92.88±1.42
94.85±1.10
92.92±0.35
92.24±0.61
86.34±1.89
65.39±1.59
38.62±1.35
93.50±0.44

PR

SR

94.67±1.08
94.89±1.08
93.54±0.41
92.76±0.55
90.32±1.49
66.00±1.59
38.76±1.39
95.57±0.44

78.89±2.31
94.79±1.08
77.08±0.80
91.49±0.57
77.07±2.00
76.15±1.06
37.40±1.27
62.49±1.43

ENS

88.96±1.44
97.64±0.25
90.15±0.45
82.36±1.02
74.94±2.04
79.52±1.78
47.58±1.48
91.89±0.58

WLK

96.63±0.73
98.57±0.51
93.83±0.59
95.86±0.54
89.72±1.67
82.41±3.43
87.42±2.08
96.94±0.29

WLNM
95.95±1.10
98.61±0.49
93.49±0.47
95.62±0.52
86.18±1.72
84.76±0.98
94.41±0.88
97.21±0.27

SEAL

96.62±0.72
98.85±0.47
94.72±0.46
97.91±0.52
90.30±1.35
87.61±1.57
96.38±1.45
97.64±0.22

Table 1 shows the results. Firstly  we observe that methods which learn from enclosing subgraphs
(WLK  WLNM and SEAL) generally perform much better than predeﬁned heuristics. This indicates
that the learned “heuristics” are better at capturing the network properties than manually designed
ones. Among learning-based methods  SEAL has the best performance  demonstrating GNN’s
superior graph feature learning ability over graph kernels and fully-connected neural networks. From
the results on Power and Router  we can see that although existing heuristics perform similarly to
random guess  learning-based methods still maintain high performance. This suggests that we can
even discover new “heuristics” for networks where no existing heuristics work.

LINE

SPC

Data
USAir
NS
PB
Yeast
C.ele
Power
Router
E.coli

MF

94.08±0.80
74.55±4.34
94.30±0.53
90.28±0.69
85.90±1.74
50.63±1.10
78.03±1.63
93.76±0.56

74.22±3.11
89.94±2.39
83.96±0.86
93.25±0.40
51.90±2.57
91.78±0.61
68.79±2.42
94.92±0.32

81.47±10.71
80.63±1.90
76.95±2.76
87.45±3.33
69.21±3.14
55.63±1.47
67.15±2.10
82.38±2.19

N2V

91.44±1.78
91.52±1.28
85.79±0.78
93.67±0.46
84.11±1.27
76.22±0.92
65.46±0.86
90.82±1.49

SBM

94.85±1.14
92.30±2.26
93.90±0.42
91.41±0.60
86.48±2.60
66.57±2.05
85.65±1.93
93.82±0.41

VGAE

89.28±1.99
94.04±1.64
90.70±0.53
93.88±0.21
81.80±2.18
71.20±1.65
61.51±1.22
90.81±0.63

SEAL
97.09±0.70
97.71±0.93
95.01±0.34
97.20±0.64
89.54±2.04
84.18±1.82
95.68±1.22
97.22±0.28

Table 2: Comparison with latent feature methods (AUC).

Comparison to latent
feature
methods Next we compare SEAL
with six state-of-the-art latent feature
methods: matrix factorization (MF) 
stochastic block model
(SBM)
[18]  node2vec (N2V) [20]  LINE
[21] 
spectral clustering (SPC) 
and variational graph auto-encoder
(VGAE) [36]. Among them  VGAE
uses a GNN too. Please note the difference between VGAE and SEAL: VGAE uses a node-level
GNN to learn node embeddings that best reconstruct the network  while SEAL uses a graph-level
GNN to classify enclosing subgraphs. Therefore  VGAE still belongs to latent feature methods. For
SEAL  we additionally include the 128-dimensional node2vec embeddings in the node information
matrix X. Since the datasets do not have node attributes  explicit features are not included.
Table 2 shows the results. As we can see  SEAL shows signiﬁcant improvement over latent feature
methods. One reason is that SEAL learns from both graph structures and latent features simulta-
neously  thus augmenting those methods that only use latent features. We observe that SEAL with
node2vec embeddings outperforms pure node2vec by large margins. This implies that network
embeddings alone may not be able to capture the most useful link prediction information located
in the local structures. It is also interesting that compared to SEAL without node2vec embeddings
(Table 1)  joint learning does not always improve the performance. More experiments and discussion
are included in Appendix F.

6 Conclusions

Learning link prediction heuristics automatically is a new ﬁeld. In this paper  we presented theoretical
justiﬁcations for learning from local enclosing subgraphs. In particular  we proposed a -decaying
theory to unify a wide range of high-order heuristics and prove their approximability from local
subgraphs. Motivated by the theory  we proposed a novel link prediction framework  SEAL  to
simultaneously learn from local enclosing subgraphs  embeddings and attributes based on graph
neural networks. Experimentally we showed that SEAL achieved unprecedentedly strong performance
by comparing to various heuristics  latent feature methods  and network embedding algorithms. We
hope SEAL can not only inspire link prediction research  but also open up new directions for other
relational machine learning problems such as knowledge graph completion and recommender systems.

8

Acknowledgments
The work is supported in part by the III-1526012 and SCH-1622678 grants from the National Science
Foundation and grant 1R21HS024581 from the National Institute of Health.

References

[1] David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal of the

American society for information science and technology  58(7):1019–1031  2007.

[2] Lada A Adamic and Eytan Adar. Friends and neighbors on the web. Social networks  25(3):211–230 

2003.

[3] Yehuda Koren  Robert Bell  and Chris Volinsky. Matrix factorization techniques for recommender systems.

Computer  (8):30–37  2009.

[4] Maximilian Nickel  Kevin Murphy  Volker Tresp  and Evgeniy Gabrilovich. A review of relational machine

learning for knowledge graphs. Proceedings of the IEEE  104(1):11–33  2016.

[5] Tolutola Oyetunde  Muhan Zhang  Yixin Chen  Yinjie Tang  and Cynthia Lo. Boostgapﬁll: Improving
the ﬁdelity of metabolic network reconstructions through integrated constraint and pattern-based methods.
Bioinformatics  2016.

[6] Linyuan Lü and Tao Zhou. Link prediction in complex networks: A survey. Physica A: Statistical

Mechanics and its Applications  390(6):1150–1170  2011.

[7] Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. Science  286(5439):

509–512  1999.

[8] Tao Zhou  Linyuan Lü  and Yi-Cheng Zhang. Predicting missing links via local information. The European

Physical Journal B  71(4):623–630  2009.

[9] Sergey Brin and Lawrence Page. Reprint of: The anatomy of a large-scale hypertextual web search engine.

Computer networks  56(18):3825–3833  2012.

[10] Glen Jeh and Jennifer Widom. Simrank: a measure of structural-context similarity. In Proceedings of the
eighth ACM SIGKDD international conference on Knowledge discovery and data mining  pages 538–543.
ACM  2002.

[11] István A Kovács  Katja Luck  Kerstin Spirohn  Yang Wang  Carl Pollis  Sadie Schlabach  Wenting Bian 
Dae-Kyum Kim  Nishka Kishore  Tong Hao  et al. Network-based prediction of protein interactions.
bioRxiv  page 275529  2018.

[12] Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages
575–583. ACM  2017.

[13] Joan Bruna  Wojciech Zaremba  Arthur Szlam  and Yann LeCun. Spectral networks and locally connected

networks on graphs. arXiv preprint arXiv:1312.6203  2013.

[14] David K Duvenaud  Dougal Maclaurin  Jorge Iparraguirre  Rafael Bombarell  Timothy Hirzel  Alán
Aspuru-Guzik  and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints.
In Advances in neural information processing systems  pages 2224–2232  2015.

[15] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907  2016.

[16] Mathias Niepert  Mohamed Ahmed  and Konstantin Kutzkov. Learning convolutional neural networks for

graphs. In International conference on machine learning  pages 2014–2023  2016.

[17] Muhan Zhang  Zhicheng Cui  Marion Neumann  and Yixin Chen. An end-to-end deep learning architecture

for graph classiﬁcation. In AAAI  pages 4438–4445  2018.

[18] Edoardo M Airoldi  David M Blei  Stephen E Fienberg  and Eric P Xing. Mixed membership stochastic

blockmodels. Journal of Machine Learning Research  9(Sep):1981–2014  2008.

9

[19] Bryan Perozzi  Rami Al-Rfou  and Steven Skiena. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining 
pages 701–710. ACM  2014.

[20] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the
22nd ACM SIGKDD international conference on Knowledge discovery and data mining  pages 855–864.
ACM  2016.

[21] Jian Tang  Meng Qu  Mingzhe Wang  Ming Zhang  Jun Yan  and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th International Conference on World Wide Web 
pages 1067–1077. International World Wide Web Conferences Steering Committee  2015.

[22] Jiezhong Qiu  Yuxiao Dong  Hao Ma  Jian Li  Kuansan Wang  and Jie Tang. Network embedding as matrix

factorization: Unifyingdeepwalk  line  pte  and node2vec. arXiv preprint arXiv:1710.02971  2017.

[23] Maximilian Nickel  Xueyan Jiang  and Volker Tresp. Reducing the rank in relational factorization models
by including observable patterns. In Advances in Neural Information Processing Systems  pages 1179–1187 
2014.

[24] He Zhao  Lan Du  and Wray Buntine. Leveraging node attributes for incomplete relational data. In

International Conference on Machine Learning  pages 4072–4081  2017.

[25] Yujia Li  Daniel Tarlow  Marc Brockschmidt  and Richard Zemel. Gated graph sequence neural networks.

arXiv preprint arXiv:1511.05493  2015.

[26] Hanjun Dai  Bo Dai  and Le Song. Discriminative embeddings of latent variable models for structured
data. In Proceedings of The 33rd International Conference on Machine Learning  pages 2702–2711  2016.

[27] Justin Gilmer  Samuel S Schoenholz  Patrick F Riley  Oriol Vinyals  and George E Dahl. Neural message

passing for quantum chemistry. arXiv preprint arXiv:1704.01212  2017.

[28] Ni Lao and William W Cohen. Relational retrieval using a combination of path-constrained random walks.

Machine learning  81(1):53–67  2010.

[29] Leo Katz. A new status index derived from sociometric analysis. Psychometrika  18(1):39–43  1953.

[30] Glen Jeh and Jennifer Widom. Scaling personalized web search. In Proceedings of the 12th international

conference on World Wide Web  pages 271–279. Acm  2003.

[31] Yen-Yu Chen  Qingqing Gan  and Torsten Suel. Local methods for estimating pagerank values.

In
Proceedings of the thirteenth ACM international conference on Information and knowledge management 
pages 381–389. ACM  2004.

[32] Xu Jia  Hongyan Liu  Li Zou  Jun He  Xiaoyong Du  and Yuanzhe Cai. Local methods for estimating
simrank score. In Web Conference (APWEB)  2010 12th International Asia-Paciﬁc  pages 157–163. IEEE 
2010.

[33] Ziv Bar-Yossef and Li-Tal Mashiach. Local approximation of pagerank and reverse pagerank. In Pro-
ceedings of the 17th ACM conference on Information and knowledge management  pages 279–288. ACM 
2008.

[34] Nino Shervashidze  Pascal Schweitzer  Erik Jan van Leeuwen  Kurt Mehlhorn  and Karsten M Borgwardt.

Weisfeiler-lehman graph kernels. Journal of Machine Learning Research  12(Sep):2539–2561  2011.

[35] Federico Monti  Michael Bronstein  and Xavier Bresson. Geometric matrix completion with recurrent
multi-graph neural networks. In Advances in Neural Information Processing Systems  pages 3700–3710 
2017.

[36] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308 

2016.

[37] Ulrike V Luxburg  Agnes Radl  and Matthias Hein. Getting lost in space: Large sample analysis of the

resistance distance. In Advances in Neural Information Processing Systems  pages 2622–2630  2010.

[38] Leonardo FR Ribeiro  Pedro HP Saverese  and Daniel R Figueiredo. struc2vec: Learning node represen-
tations from structural identity. In Proceedings of the 23rd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  pages 385–394. ACM  2017.

[39] Will Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on large graphs. In

Advances in Neural Information Processing Systems  pages 1025–1035  2017.

10

[40] Yi-An Lai  Chin-Chi Hsu  Wen Hao Chen  Mi-Yen Yeh  and Shou-De Lin. Prune: Preserving proximity
and global ranking for network embedding. In Advances in Neural Information Processing Systems  pages
5263–5272  2017.

[41] Alberto Garcia Duran and Mathias Niepert. Learning graph representations with embedding propagation.

In Advances in Neural Information Processing Systems  pages 5125–5136  2017.

[42] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. In
Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining 
pages 426–434. ACM  2008.

[43] Steffen Rendle. Factorization machines. In 10th IEEE International Conference on Data Mining (ICDM) 

pages 995–1000. IEEE  2010.

[44] Vladimir Batagelj and Andrej Mrvar. http://vlado.fmf.uni-lj.si/pub/networks/data/  2006.
[45] Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices. Physical

review E  74(3):036104  2006.

[46] Robert Ackland et al. Mapping the us political blogosphere: Are conservative bloggers more prominent?
In BlogTalk Downunder 2005 Conference  Sydney. BlogTalk Downunder 2005 Conference  Sydney  2005.
[47] Christian Von Mering  Roland Krause  Berend Snel  Michael Cornell  Stephen G Oliver  Stanley Fields 
and Peer Bork. Comparative assessment of large-scale data sets of protein–protein interactions. Nature 
417(6887):399–403  2002.

[48] Duncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’networks. Nature  393(6684):

440–442  1998.

[49] Neil Spring  Ratul Mahajan  David Wetherall  and Thomas Anderson. Measuring isp topologies with

rocketfuel. IEEE/ACM Transactions on networking  12(1):2–16  2004.

[50] Muhan Zhang  Zhicheng Cui  Shali Jiang  and Yixin Chen. Beyond link prediction: Predicting hyperlinks

in adjacency space. In AAAI  pages 4430–4437  2018.

[51] Christopher Aicher  Abigail Z Jacobs  and Aaron Clauset. Learning latent block structure in weighted

networks. Journal of Complex Networks  3(2):221–248  2015.

[52] Steffen Rendle. Factorization machines with libfm. ACM Transactions on Intelligent Systems and

Technology (TIST)  3(3):57  2012.

[53] Rong-En Fan  Kai-Wei Chang  Cho-Jui Hsieh  Xiang-Rui Wang  and Chih-Jen Lin. Liblinear: A library

for large linear classiﬁcation. Journal of machine learning research  9(Aug):1871–1874  2008.

[54] S Vichy N Vishwanathan  Nicol N Schraudolph  Risi Kondor  and Karsten M Borgwardt. Graph kernels.

Journal of Machine Learning Research  11(Apr):1201–1242  2010.

[55] Mahito Sugiyama and Karsten Borgwardt. Halting in random walk kernels.

information processing systems  pages 1639–1647  2015.

In Advances in neural

[56] Fabrizio Costa and Kurt De Grave. Fast neighborhood subgraph pairwise distance kernel. In Proceedings

of the 26th International Conference on Machine Learning  pages 255–262. Omnipress  2010.

[57] Nils Kriege and Petra Mutzel. Subgraph matching kernels for attributed graphs. In Proceedings of the 29th

International Conference on Machine Learning (ICML-12)  pages 1015–1022  2012.

[58] Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Data Mining  Fifth

IEEE International Conference on  pages 8–pp. IEEE  2005.

[59] Marion Neumann  Roman Garnett  Christian Bauckhage  and Kristian Kersting. Propagation kernels:

efﬁcient graph kernels from propagated information. Machine Learning  102(2):209–245  2016.

[60] Jure Leskovec and Andrej Krevl. {SNAP Datasets}:{Stanford} large network dataset collection. 2015.
[61] Reza Zafarani and Huan Liu. Social computing data repository at asu  2009. URL http://socialcomputing.

asu. edu.

[62] Matt Mahoney. Large text compression benchmark  2011.
[63] Chris Stark  Bobby-Joe Breitkreutz  Teresa Reguly  Lorrie Boucher  Ashton Breitkreutz  and Mike Tyers.
Biogrid: a general repository for interaction datasets. Nucleic acids research  34(suppl_1):D535–D539 
2006.

11

,Muhan Zhang
Yixin Chen