2014,Global Sensitivity Analysis for MAP Inference in Graphical Models,We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global  in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself  so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration  and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.,Global Sensitivity Analysis

for MAP Inference in Graphical Models

Jasper De Bock

Ghent University  SYSTeMS

Ghent (Belgium)

Cassio P. de Campos
Queen’s University

Belfast (UK)

Alessandro Antonucci

IDSIA

Lugano (Switzerland)

jasper.debock@ugent.be

c.decampos@qub.ac.uk

alessandro@idsia.ch

Abstract

We study the sensitivity of a MAP conﬁguration of a discrete probabilistic graph-
ical model with respect to perturbations of its parameters. These perturbations are
global  in the sense that simultaneous perturbations of all the parameters (or any
chosen subset of them) are allowed. Our main contribution is an exact algorithm
that can check whether the MAP conﬁguration is robust with respect to given per-
turbations. Its complexity is essentially the same as that of obtaining the MAP
conﬁguration itself  so it can be promptly used with minimal effort. We use our
algorithm to identify the largest global perturbation that does not induce a change
in the MAP conﬁguration  and we successfully apply this robustness measure in
two practical scenarios: the prediction of facial action units with posed images and
the classiﬁcation of multiple real public data sets. A strong correlation between
the proposed robustness measure and accuracy is veriﬁed in both scenarios.

1

Introduction

Probabilistic graphical models (PGMs) such as Markov random ﬁelds (MRFs) and Bayesian net-
works (BNs) are widely used as a knowledge representation tool for reasoning under uncertainty.
When coping with such a PGM  it is not always practical to obtain numerical estimates of the
parameters—the local probabilities of a BN or the factors of an MRF—with sufﬁcient precision.
This is true even for quantiﬁcations based on data  but it becomes especially important when elic-
iting the parameters from experts. An important question is therefore how precise these estimates
should be to avoid a degradation in the diagnostic performance of the model. This remains impor-
tant even if the accuracy can be arbitrarily reﬁned in order to trade it off with the relative costs. This
paper is an attempt to systematically answer this question.
More speciﬁcally  we address sensitivity analysis (SA) of discrete PGMs in the case of maximum a
posteriori (MAP) inferences  by which we mean the computation of the most probable conﬁguration
of some variables given an observation of all others.1
Let us clarify the way we intend SA here  while giving a short overview of previous work on SA
in PGMs. First of all  a distinction should be made between quantitative and qualitative SA. Quan-
titative approaches are supposed to evaluate the effect of a perturbation of the parameters on the
numerical value of a particular inference. Qualitative SA is concerned with deciding whether or not
the perturbed values are leading to a different decision  e.g.  about the most probable conﬁguration of
the queried variable(s). Most of the previous work in SA is quantitative  being in particular focused
on updating  i.e.  the computation of the posterior probability of a single variable given some evi-
dence  and mostly focus on BNs. After a ﬁrst attempt based on a purely empirical investigation [17] 
a number of analytical methods based on the derivatives of the updated probability with respect to

1Some authors refer to this problem as MPE (most probable explanation) rather than MAP.

1

the perturbed parameters have been proposed [3  4  5  11  14]. Something similar has been done for
MRFs as well [6]. To the best of our knowledge  qualitative SA received almost no attention  with
few exceptions [7  18].
Secondly  we distinguish between local and global SA. The former considers the effect of the per-
turbation of a single parameter (and of possible additional perturbations that are induced by nor-
malization constraints)  while the latter aims at more general perturbations possibly affecting all the
parameters of the PGM. Initial work on SA in PGMs considered the local approach [4  14]  while
later work considered global SA as well [3  5  11]. Yet  for BNs  global SA has been tackled by
methods whose time complexity is exponential in the number of perturbed conditional probability
tables (CPTs)  as they basically require the computation of all the mixed derivatives. For qualita-
tive SA  as far as we know  only the local approach has been studied [7  18]. This is unfortunate 
as global SA might reveal stronger effects of perturbations due to synergetic effects  which might
remain hidden in a local analysis.
In this paper  we study global qualitative SA in discrete PGMs for MAP inferences  thereby intend-
ing to ﬁll the existing gap in this topic. Let us introduce it by a simple example.
Example 1. Let X1 and X2 be two Boolean variables. For each i ∈ {1  2}  Xi takes values in
{xi ¬xi}. The following probabilistic assessments are available: P (x1) = .45  P (x2|x1) = .2 
and P (x2|¬x1) = .9. This induces a complete speciﬁcation of the joint probability mass func-
tion P (X1  X2). If no evidence is present  the MAP joint state is (¬x1  x2)  its probability being
.495. The second most probable joint state is (x1 ¬x2)  whose probability is .36. We perturb
the above three parameters. Given x1 ≥ 0  we consider any assessment of P (x1) such that
|P (x1) − .45| ≤ x1. We similarly perturb P (x2|x1) with x2|x1 and P (x2|¬x1) with x2|¬x1.
The goal is to investigate whether or not (¬x1  x2) is also the unique MAP instantiation for each
P (X1  X2) consistent with the above constraints  given a maximum perturbation level of  = .06
for each parameter. Straightforward calculations show that this is true if only one parameter is
perturbed at each time. The state (¬x1  x2) remains the most probable even if two parameters are
perturbed (for any pair of them). The situation is different if the perturbation level  = .06 is applied
to all three parameters simultaneously. There is a speciﬁcation of the parameters consistent with
the perturbations and such that the MAP instantiation is (x1 ¬x2) and achieves probability .4386 
corresponding to P (x1) = .51  P (x2|x1) = .14  and P (x2|¬x1) = .84. The minimum perturbation
level for which this behaviour is observed is ∗ = .05. For this value  there is a single speciﬁcation
of the model for which (x1 ¬x2) has the same probability as (¬x1  x2)  which—for this value—is
the single most probable instantiation for any other speciﬁcation of the model that is consistent with
the perturbations.

The above example can be regarded as a qualitative SA for which the local approach is unable to
identify a lack of robustness in the MAP solution  which is revealed instead by the global analysis.
In the rest of the paper we develop an algorithm to efﬁciently detect the minimum perturbation level
∗ leading to a different MAP solution. The time complexity of the algorithm is equal to that of the
MAP inference in the PGM times the number of variables in the domain  that is  exponential in the
treewidth of the graph in the worst case. The approach can be specialized to local SA or any other
choice of parameters to perform SA  thus reproducing and extending existing results. The paper
is organized as follows: the problem of checking the robustness of a MAP inference is introduced
in its general formulation in Section 2. The discussion is then specialized to the case of PGMs in
Section 3 and applied to global SA in Section 4. Experiments with real data sets are reported in
Section 5  while conclusions and outlooks are given in Section 6.

2 MAP Inference and its Robustness

We start by explaining how we intend SA for MAP inference and how this problem can be translated
into an optimisation problem very similar to that used for the computation of MAP itself. For the
sake of readibility  but without any lack of generality  we begin by considering a single variable
only; the multivariate and the conditional cases are dicussed in Section 3. Consider a single variable
X taking its values in a ﬁnite set Val(X). Given a probability mass function P over X  ˜x ∈ Val(X)
is said to be a MAP instantiation for P if

P (x) 

(1)

˜x ∈ arg max
x∈Val(X)

2

which means that ˜x is the most likely value of X according to P . In principle a mass function P can
have multiple (equally probable) MAP instantiations. However  in practice there will often be only
one  and we then call it the unique MAP instantiation for P .
As we did in Example 1  SA can be achieved by modeling perturbations of the parameters in terms
of (linear) constraints over them  which are used to deﬁne the set of all perturbed models whose
mass function is consistent with these constraints. Generally speaking  we consider an arbitrary set
P of candidate mass functions  one of which is the original unperturbed mass function P . The only
imposed restriction is that P must be compact. This way of deﬁning candidate models establishes
a link between SA and the theory of imprecise probability  which extends the Bayesian theory of
probability to cope with compact (and often convex) sets of mass functions [19].
For the MAP inference in Eq. (1)  performing SA with respect to a set of candidate models P requires
the identiﬁcation of the instantiations that are MAP for at least one perturbed mass function  that is 

(cid:12)(cid:12)(cid:12)(cid:12) ∃P (cid:48) ∈ P : ˜x ∈ arg max

x∈Val(X)

(cid:27)

P (cid:48)(x)

.

(2)

∗
Val

(X) :=

˜x ∈ Val(X)

(cid:26)

These instantiations are called E-admissible [15].
If the above set contains only a single MAP
instantiation ˜x (which is then necessarily the unique solution of Eq. (1) as well)  then we say that
the model P is robust with respect to the perturbation P.
Example 2. Let X take values in Val(X) := {a  b  c  d}. Consider a perturbation P := {P1  P2}
that contains only two candidate mass functions over X. Let P1 be deﬁned by P1(a) = .5  P1(b) =
P1(c) = .2 and P1(d) = .1 and let P2 be deﬁned by P2(b) = .35  P2(a) = P2(c) = .3 and
P2(d) = .05. Then a and b are the unique MAP instantiations of P1 and P2  respectively. This
∗
implies that Val

(X) = {a  b} and that neither P1 nor P2 is robust with respect to P.

∗
(X) is a time con-
For large domains Val(X)  for instance in the multivariate case  evaluating Val
∗
suming task that is often intractable. However  if we are not interested in evaluating Val
(X)  but
only want to decide whether or not P is robust with respect to the perturbation described by P 
more efﬁcient methods can be used. The following theorem establishes how this decision can be
reformulated as an optimisation problem that  as we are about to show in Section 3  can be solved
efﬁciently for PGMs. Due to space constraints  the proofs are provided as supplementary material.
Theorem 1. Let X be a variable taking values in a ﬁnite set Val(X) and let P be a set of candidate
mass functions over X. Let ˜x be a MAP instantiation for a mass funtion P ∈ P. Then ˜x is the
unique MAP instantiation for every P (cid:48) ∈ P  that is  Val
∗

(X) has cardinality one  if and only if

P (cid:48)∈P P (cid:48)(˜x) > 0 and

min

max

x∈Val(X)\{˜x} max
P (cid:48)∈P

P (cid:48)(x)
P (cid:48)(˜x)

< 1 

(3)

where the ﬁrst inequality should be checked ﬁrst because if it fails  then the left-hand side of the
second inequality is ill-deﬁned.

3 PGMs and Efﬁcient Robustness Veriﬁcation

Let X = (X1  . . .   Xn) be a vector of variables taking values in their respective ﬁnite domains
Val(X1)  . . .   Val(Xn). We will use [n] a shorthand notation for {1  . . .   n}  and similarly for other
natural numbers. For every non-empty C ⊆ [n]  XC is a vector that consists of the variables Xi 
i ∈ C  that takes values in Val(XC) := ×i∈C Val(Xi). For C = [n] and C = {i}  we obtain
X = X[n] and Xi = X{i} as important special cases. A factor φ over a vector XC is a real-valued
map on Val(XC). If for all xC ∈ XC  φ(xC) ≥ 0  then φ is said to be nonnegative.
Let I1  . . .   Im be a collection of index sets such that I1 ∪···∪ Im = [n] and Φ = {φ1  . . .   φm} be
a set of nonnegative factors over the vectors XI1  . . .   XIm  respectively. We say that Φ is a PGM if
it induces a joint probability mass function PΦ over Val(X)  deﬁned by
φk(xIk ) for all x ∈ Val(X) 

m(cid:89)

PΦ(x) :=

(4)

where ZΦ :=(cid:80)

1
ZΦ

k=1

(cid:81)m

Val(X) is ﬁnite  Φ is a PGM if and only if ZΦ > 0.

x∈Val(X)

k=1 φk(xIk ) is the normalising constant called partition function. Since

3

3.1 MAP and Second Best MAP Inference for PGMs
If Φ is a PGM then  by merging Eqs. (1) and (4)  we see that ˜x ∈ Val(X) is a MAP instantiation for
PΦ if and only if

φk(˜xIk ) for all x ∈ Val(X) 

m(cid:89)

φk(xIk ) ≤ m(cid:89)

k=1

k=1

where ˜xIk is the unique element of Val(XIk ) that is consistent with ˜x  and likewise for xIk and x.
Similarly  x(2) ∈ Val(X) is said to be a second best MAP instantiation for PΦ if and only if there is
a MAP instantiation x(1) for PΦ such that x(1) (cid:54)= x(2) and

φk(x(2)
Ik

) for all x ∈ Val(X) \ {x(1)}.

(5)

m(cid:89)

φk(xIk ) ≤ m(cid:89)

k=1

k=1

MAP inference in PGMs is an NP-hard task (see [12] for details). The task can be solved exactly by
junction tree algorithms in time exponential in the treewidth of the network’s moral graph. While
ﬁnding the k-th best instantiation might be an even harder task [13] for general k  the second best
MAP instantiation can be found by a sequence of MAP queries: (i) compute a ﬁrst best MAP
instantiation ˜x(1); (ii) for each queried variable Xi  take the original PGM and add an extra factor
for Xi that equals 1 minus the indicator of the value that Xi has in ˜x(1)  and run the MAP inference;
(iii) report the instantiation with highest probability among all these runs. Because the second best
has to differ from the ﬁrst best in at least one Xi (and this is ensured by that extra factor)  this
procedure is correct and in worst case it spends time equal to a single MAP inference multiplied
by the number of variables. Faster approaches to directly compute the second best MAP  without
reduction to standard MAP queries  have been also proposed (see [8] for an overview).

3.2 Evaluating the Robustness of MAP Inference With Respect to a Family of PGMs
For every k ∈ [m]  let ψk be a set of nonnegative factors over the vector XIk. Every combination
of factors Φ = {φ1  . . .   φm} from the sets ψ1  . . .   ψm  respectively  is called a selection. Let
Ψ := ×m
k=1ψk be the set consisting of all these selections. If every selection Φ ∈ Ψ is a PGM 
then Ψ is said to be a family of PGMs. We then denote the corresponding set of distributions by
PΨ := {PΦ : Φ ∈ Ψ}. In the following theorem  we establish that evaluating the robustness of MAP
inference with respect to this set PΨ can be reduced to a second best MAP instantiation problem.
Theorem 2. Let X = (X1  . . .   Xn) be a vector of variables taking values in their respective ﬁnite
domains Val(X1)  . . .   Val(Xn)  let I1  . . .   Im be a collection of index sets such that I1∪···∪Im =
[n] and  for every k ∈ [m]  let ψk be a compact set of nonnegative factors over XIk such that
Ψ = ×m
Consider now a PGM Φ ∈ Ψ and a MAP instantiation ˜x for PΦ and deﬁne  for every k ∈ [m] and
every xIk ∈ Val(XIk ):

k=1ψk is a family of PGMs.

αk := min
k∈ψk
φ(cid:48)

φ(cid:48)
k(˜xk) and βk(xIk ) := max
k∈ψk
φ(cid:48)

φ(cid:48)
k(xIk )
φ(cid:48)
k(˜xIk )

.

(6)

Then ˜x is the unique MAP instantiation for every P (cid:48) ∈ PΨ if and only if

(∀k ∈ [m]) αk > 0 and

βk(x(2)
Ik

) < 1 

(RMAP)

k=1

where x(2) is an arbitrary second best MAP instantiation for the distribution P ˜Φ that corresponds
to the PGM ˜Φ := {β1  . . .   βm}. The ﬁrst criterion in (RMAP) should be checked ﬁrst because
βk(x(2)
Ik

) is ill-deﬁned if αk = 0.

Theorem 2 provides an algorithm to test the robustness of MAP in PGMs. From a computational
point of view  checking (RMAP) can be done as described in the previous subsection  apart from
the local computations appearing in Eq. (6). These local computations will depend on the particular
choice of perturbation. As we will see further on  many natural perturbations induce very efﬁcient
local computations (usually because they are related somehow to simple linear or convex program-
ming problems).

4

m(cid:89)

In most practical situations  some variables XO  with O ⊂ [n]  are observed and therefore known
to be in a given conﬁguration y ∈ Val(XO). In this case  the MAP inference for the conditional
mass function PΦ(XQ|y) should be considered  where XQ := X[n]\O are the queried variables.
While we have avoided the discussion about the conditional case and considered only the MAP
inference (and its robustness check) for the whole set of variables of the PGM  the standard technique
employed with MRFs of including additional identity functions to encode observations sufﬁces  as
the probability of the observation (and therefore also the partition function value) does not inﬂuence
the result of MAP inferences. Hence  one can run the MAP inference for the PGM Φ(cid:48) augmented
with local identity functions that yield y  such that ZΦ(cid:48)PΦ(cid:48)(XQ) = ZΦPΦ(XQ  y) (that is  the
unnormalized probabilities are equal  so MAP instantiations are equal too) and hence the very same
techniques explained for the unconditional case are applicable to conditional MAP inference (and
its robustness check) as well.

4 Global SA in PGMs
The most natural way to perform global SA in a PGM Φ = {φ1  . . .   φm} is by perturbing all its
factors. Following the ideas introduced in Section 2 and 3  we model the effect of the perturbation
by replacing the factor φk with a compact set ψk of factors  for each k ∈ [m]. This induces a
family Ψ of PGMs. The condition (RMAP) can be therefore used to decide whether or not the MAP
instantiation for PΦ is the unique MAP instantiation for every P (cid:48) ∈ PΨ. In other words  we have an
algorithm to test the robustness of PΦ with respect to the perturbation PΨ.
To characterize the perturbation level we introduce the notion of a parametrized perturbation ψ
k of
a factor φk  deﬁned by requiring that: (i) for each  ∈ [0  1]  ψ
k is a compact set of factors  each of
which has the same domain as φk; (ii) if 2 ≥ 1  then ψ2
k = {φk}. Given a
parametrized perturbation for each factor of the PGM Φ  we denote by Ψ the corresponding family
of PGMs and by PΨ the relative set of joint mass functions.
We deﬁne the critical perturbation threshold ∗ as the supremum value of  ∈ [0  1] such that PΦ
is robust with respect to the perturbation PΨ  i.e.  such that the condition (RMAP) is still satisﬁed.
Because of the property (ii) of parametrized perturbations  we know that if (RMAP) is not satisﬁed
for a particular value of  then it cannot be satisﬁed for a larger value and  vice versa  if the criterion
is satisﬁed for a particular value than it will also be satisﬁed for every smaller value. An algorithm
to evaluate ∗ can therefore be obtained by iteratively checking (RMAP) according to a bracketing
scheme (e.g.  bisection) over . Local SA  as well as SA of only a selective collection of parameters 
come as a byproduct  as one can perturb only some factors and our results and algorithm still apply.

k ; and (iii) ψ0

k ⊇ ψ1

4.1 Global SA in Markov Random Fields (MRFs)

MRFs are PGMs based on undirected graphs. The factors are associated to cliques of the graph. The
specialization of the technique outlined by Theorem 2 is straightforward. A possible perturbation
technique is the rectangular one. Given a factor φk  its rectangular parametric perturbation ψ

k is:

k = {φ(cid:48)
ψ

k ≥ 0 : |φ(cid:48)

k(xIk ) − φk(xIk )| ≤ ∆ for all xIk ∈ Val(XIk )}  

(7)

where ∆ > 0 is a chosen maximum perturbation level  achieved for  = 1.
For this kind of perturbation  the optimization in Eq. (6) is trivial: αk = max{0  φk(˜xk) − ∆}
and  if αk > 0  then βk(˜xIk ) = 1 and  for all xIk ∈ Val(XIk ) \ {˜xIk}  βk(xIk ) = φk(xIk )+∆
φk(˜xIk )−∆. If
αk = 0  even for a single k  the criterion (RMAP) is not satisﬁed and βk should not be computed.

4.2 Global SA in Bayesian Networks (BNs)

BNs are PGMs based on directed graphs. The factors are CPTs  one for each variable  each con-
ditioned on the parents of the variable. Each CPT contains a conditional mass function for each
joint state of the parents. Perturbations in BNs can take this into consideration and use perturbations
with a direct probabilistic interpretation. Consider an unconditional mass function P over X. A
parametrized perturbation P  of P can be achieved by -contamination [2]:

P  := {(1 − )P (X) + P ∗(X) : P ∗(X) any mass function on X}.

(8)

5

It is a trivial exercise to check that this is a proper parametric perturbation of P (X) and that P 1 is
the whole probabilistic simplex.
We perturb the CPTs of a BN by applying this parametric perturbation to every conditional mass
function. Let P (X|Y) =: ψ(X  Y) be a CPT. The optimization in Eq. (6) is trivial also in this case.
We have αk = (1−)P (˜x|˜y) and  if αk > 0  then βk(˜xIk ) = 1 and  for all xIk ∈ Val(XIk )\{˜xIk} 
βk(xIk ) = (1−)P (x|y)+
More general perturbations can also be considered  and the efﬁciency of their computation relates to
the optimization in Eq. (6). Because of that  we are sure that at least any linear or convex perturbation
can be solved efﬁciently and in polynomial time by convex programming methods  while other
more sophisticated perturbations might demand general non-linear optimization and hence cannot
anymore ensure that computations are exact and quick.

(1−)P (˜x|˜y)   where ˜x and ˜y are consistent with ˜xIk and similarly for x  y and xIk.

5 Experiments

5.1 Facial Action Unit Recognition

We consider the problem of recognizing facial action units from real image data using the CK+ data
set [10  16]. Based on the Facial Action Coding System [9]  facial behaviors can be decomposed
into a set of 45 action units (AUs)  which are related to contractions of speciﬁc sets of facial muscles.
We work with 23 recurrent AUs (for a complete description  see [9]). Some AUs happen together
to show a meaningful facial expression: AU6 (cheek raiser) tends to occur together with AU12 (lip
corner puller) when someone is smiling. On the other hand  some AUs may be mutually exclusive:
AU25 (lips part) never happens simultaneously with AU24 (lip presser) since they are activated by
the same muscles but with opposite motions. The data set contains 68 landmark positions (given
by coordinates x and y) of the face of 589 posed individuals (after ﬁltering out cases with missing
data)  as well as the labels for the AUs. Our goal is to predict all the AUs happening in a given
image. In this work  we do not aim to outperform other methods designed for this particular task 
but to analyse the robustness of a model when applied in this context. In spite of that  we expected
to obtain a reasonably good accuracy by using an MRF.
One third of the posed faces are selected for testing  and two thirds for training the model. The
labels of the testing data are not available during training and are used only to compute the accuracy
of the predictions. Using the training data and following the ideas in [16]  we build a linear support
vector machine (SVM) separately for each one of the 23 AUs  using the image landmarks to predict
that given AU. With these SVMs  we create new variables o1 . . .  o45  one for each selected AU 
containing the predicted value from the SVM. This is performed for all the data  including training
and testing data. After that  landmarks are discarded and the data is considered to have 46 variables
(true values and SVM predicted ones). At this point  the accuracy of the SVM measurements on the
testing data  if one considers the average Hamming distance between the vector of 23 true values
and the vector of 23 predicted ones (that is  the sum of the number of times AUi equals oi over all i
and all instances in the testing data divided by 23 times the number of instances)  is about 87%. We
now use these 46 variables to build an MRF (we use a very simplistic penalized likelihood approach
for learning the MRF  as the goal is not to obtain state-of-the-art classiﬁcation but to analyse robust-
ness)  as shown in Fig. 1(a)  where SVM-built variables are treated as observational/measurement
nodes and relations are learned between the AUs (non displayed AU variables in the ﬁgure are only
connected to their corresponding measurements).
Using the MRF  we predict the AU conﬁguration using a MAP algorithm  where all AUs are queried
and all measurement nodes are observed. As before  we characterise the accuracy of this model
by the average Hamming distance between predicted vectors and true vectors  obtaining about 89%
accuracy. That is  the inclusion of the relations between AUs by means of the MRF was able to
slightly improve the accuracy obtained independently for each AU from the SVM. For our present
purposes  we are however more interested in the associated perturbation thresholds ∗. For each
instance of the testing data (that is  for each vector of 23 measurements)  we compute it using the
rectangular perturbations of Section 4.1. The higher ∗ is  the more robust is the issued vector 
because it represents the single optimal MAP instantiation even if one varied all the parameters of
the MRF by ∗. To understand the relation between ∗ and the accuracy of predictions  we have
split the testing instances into bins  according to the Hamming distance between true and predicted

6

(a) MRF used in the computations.

(b) Robustness split by Hamming distances.

Figure 1: On the left  the graph of the MRF used to compute MAP. On the right  boxplots for the
robustness measure ∗ of MAP solutions  for different values of the Hamming distance to the truth.

vectors. Figure 1(b) shows the boxplot of ∗ for each value of the Hamming distance between 0 and
4 (lower ∗ of a MAP instantiation means lower robustness). As we can see in the ﬁgure  the median
robustness ∗ decreases monotonically with the distance  indicating that this measure is correlated
with the accuracy of the issued predictions  and hence can be used as a second order information
about the obtained MAP instantiation for each instance.
The data set also contains information about the emotion expressed in the posed faces (at least for
part of the images)  which are shown in Figure 2(b): anger  disgust  fear  happy  sadness and sur-
prise. We have partitioned the testing data according to these six emotions and plotted the robustness
measure ∗ of them (Figure 2(a)). It is interesting to see the relation between robustness and emo-
tions. Arguably  it is much easier to identify surprise (because of the stretched face and open mouth)
than anger (because of the more restricted muscle movements deﬁning it). Figure 2 corroborates
with this statement  and suggests that the robustness measure ∗ can have further applications.

(a) Robustness split by emotions.

(b) Examples of emotions.

Figure 2: On the left  box plots for the robustness measure ∗ of the MAP solutions  split according
to the emotion that was presented in the instance were MAP was computed. On the right  examples
of emotions encoded in the data set [10  16]. Each row is a different emotion.

7

0.0000.0050.0100.0150.0200.0250.030012340.0000.0050.0100.0150.0200.0250.030angerdisgustfearhappysadnesssurprisey
c
a
r
u
c
c
A

1

0.8

0.6

0.4

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

∗

audiology

autos

breast-cancer
horse-colic
german-credit
pima-diabetes
hypothyroid
ionosphere

lymphography

mfeat

optdigits
segment
solar-ﬂare

sonar
soybean
sponge

zoo
vowel

Figure 3: Average accuracy of a classiﬁer over 10 times 5-fold cross-validation. Each instance is
classiﬁed by a MAP inference. Instances are categorized by their ∗  which indicates their robustness
(or amount of perturbation up to which the MAP instantiation remains unique).

5.2 Robustness of Classiﬁcation

In this second experiment  we turn our attention to the classiﬁcation problem using data sets from
the UCI machine learning repository [1]. Data sets with many different characteristics have been
used. Continuous variables have been discretized by their median before any other use of the data.
Our empirical results are obtained out of 10 runs of 5-fold cross-validation (each run splits the data
into folds randomly and in a stratiﬁed way)  so the learning procedure of each classiﬁer is called 50
times per data set. In all tests we have employed a Naive Bayes classiﬁer with equivalent sample size
equal to one. After the classiﬁer is learned using 4 out of 5 folds  predictions for the other fold are
issued based on the MAP solution  and the computation of the robustness measure ∗ is done. Here 
the value ∗ is related to the size of the contamination of the model for which the classiﬁcation result
of a given test instance remains unique and unchanged (as described in Section 4.2). Figure 3 shows
the classiﬁcation accuracy for varying values of ∗ that were used to perturb the model (in order to
obtain the curves  the technicality was to split the test instances into bins according to the computed
value ∗  using intervals of length 10−2  that is  accuracy was calculated for every instance with ∗
between 0 and 0.01  then between 0.01 and 0.02  and so on). We can see a clear relation between
accuracy and predicted robustness ∗. We remind that the computation of ∗ does not depend on the
true MAP instantiation  which is only used to verify the accuracy. Again  the robustness measure
provides a valuable information about the quality of the obtained MAP results.

6 Conclusions

We consider the sensitivity of the MAP instantiations of discrete PGMs with respect to perturbations
of the parameters. Simultaneous perturbations of all the parameters (or any chosen subset of them)
are allowed. An exact algorithm to check the robustness of the MAP instantiation with respect to
the perturbations is derived. The worst-case time complexity is that of the original MAP inference
times the number of variables in the domain. The algorithm is used to compute a robustness measure 
related to changes in the MAP instantiation  which is applied to the prediction of facial action units
and to classiﬁcation problems. A strong association between that measure and accuracy is veriﬁed.
As future work  we want to develop efﬁcient algorithms to determine  if the result is not robust  what
deﬁnes such instances and how this robustness can be used to improve classiﬁcation accuracy.

Acknowledgements

J. De Bock is a PhD Fellow of the Research Foundation Flanders (FWO) and he wishes to acknowl-
edge its ﬁnancial support. The work of C. P. de Campos has been mostly performed while he was
with IDSIA and has been partially supported by the Swiss NSF grant 200021 146606 / 1.

8

References
[1] A. Asuncion

http://www.ics.uci.edu/∼mlearn/MLRepository.html  2007.

and D.J. Newman.

UCI machine

learning

repository.

[2] J. Berger. Statistical decision theory and Bayesian analysis. Springer Series in Statistics.

Springer  New York  NY  1985.

[3] E.F. Castillo  J.M. Gutierrez  and A.S. Hadi. Sensitivity analysis in discrete Bayesian networks.

IEEE Transactions on Systems  Man  and Cybernetics  Part A  27(4):412–423  1997.

[4] H. Chan and A. Darwiche. When do numbers really matter? Journal of Artiﬁcial Intelligence

Research  17:265–287  2002.

[5] H. Chan and A. Darwiche. Sensitivity analysis in Bayesian networks: from single to multiple

parameters. In Proceedings of UAI 2004  pages 67–75  2004.

[6] H. Chan and A. Darwiche. Sensitivity analysis in Markov networks. In Proceedings of IJCAI

2005  pages 1300–1305  2005.

[7] H. Chan and A. Darwiche. On the robustness of most probable explanations. In Proceedings

of UAI 2006  pages 63–71  2006.

[8] R. Dechter  N. Flerova  and R. Marinescu. Search algorithms for m best solutions for graphical

models. In Proceedings of AAAI 2012  2012.

[9] P. Ekman and W. V. Friesen. Facial action coding system: A technique for the measurement of

facial movement. Consulting Psychologists Press  Palo Alto  CA  1978.

[10] T. Kanade  J. F. Cohn  and Y. Tian. Comprehensive database for facial expression analysis.
In Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture
Recognition  pages 46–53  Grenoble  2000.

[11] U. Kjaerulff and L.C. van der Gaag. Making sensitivity analysis computationally efﬁcient. In

Proceedings of UAI 2000  pages 317–325  2000.

[12] J. Kwisthout. Most probable explanations in Bayesian networks: complexity and tractability.

International Journal of Approximate Reasoning  52(9):1452–1469  2011.

[13] J. Kwisthout  H. L. Bodlaender  and L. C. van der Gaag. The complexity of ﬁnding k-th most
probable explanations in probabilistic networks. In Proceedings of SOFSEM 2011  pages 356–
367  2011.

[14] K. B. Laskey. Sensitivity analysis for probability assessments in Bayesian networks. IEEE

Transactions on Systems  Man  and Cybernetics  25(6):901–909  1995.

[15] I. Levi. The Enterprise of Knowledge. MIT Press  London  1980.
[16] P. Lucey  J. F. Cohn  T. Kanade  J. Saragih  Z. Ambadar  and I. Matthews. The Extended
Cohn-Kanade Dataset (CK+): A complete expression dataset for action unit and emotion-
speciﬁed expression. In Proceedings of the Third International Workshop on CVPR for Human
Communicative Behavior Analysis  pages 94–101  San Francisco  2010.

[17] M. Pradhan  M. Henrion  G.M. Provan  B.D. Favero  and K. Huang. The sensitivity of be-
lief networks to imprecise probabilities: an experimental investigation. Artiﬁcial Intelligence 
85(1-2):363–397  1996.

[18] S. Renooij and L.C. van der Gaag. Evidence and scenario sensitivities in naive Bayesian

classiﬁers. International Journal of Approximate Reasoning  49(2):398–416  2008.

[19] P. Walley. Statistical Reasoning with Imprecise Probabilities. Chapman and Hall  London 

1991.

9

,Jasper De Bock
Cassio de Campos
Alessandro Antonucci
Ian En-Hsu Yen
Kai Zhong
Cho-Jui Hsieh
Pradeep Ravikumar
Inderjit Dhillon
Hossein Esfandiari
Nitish Korula
Vahab Mirrokni