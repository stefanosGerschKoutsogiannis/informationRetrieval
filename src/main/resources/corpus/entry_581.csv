2018,Approximating Real-Time Recurrent Learning with Random Kronecker Factors,Despite all the impressive advances of recurrent neural networks  sequential data is still in need of better modelling. Truncated backpropagation through time (TBPTT)  the learning algorithm most widely used in practice  suffers from the truncation bias  which drastically limits its ability to learn long-term dependencies.The Real Time Recurrent Learning algorithm (RTRL) addresses this issue   but its high computational requirements  make it infeasible in practice. The Unbiased Online Recurrent Optimization algorithm (UORO) approximates RTRL with a smaller runtime and memory cost  but with the disadvantage  of obtaining noisy gradients that also limit its practical applicability. In this paper we propose the Kronecker Factored RTRL (KF-RTRL) algorithm that uses a Kronecker product decomposition to approximate the gradients for a large class of RNNs. We show that KF-RTRL is an unbiased and memory efficient online learning algorithm. Our theoretical analysis shows that  under reasonable assumptions  the noise introduced by our algorithm is not only stable over time but also asymptotically much smaller than the one of the UORO algorithm. We also confirm these theoretical results experimentally. Further  we show empirically that the KF-RTRL algorithm captures long-term dependencies and almost matches the performance of TBPTT on real world tasks by training Recurrent Highway Networks on a synthetic string memorization task and on the Penn TreeBank task  respectively. These results indicate that RTRL based approaches might be a promising future alternative to TBPTT.,Approximating Real-Time Recurrent Learning with

Random Kronecker Factors

Asier Mujika ∗

Florian Meier

Department of Computer Science

Department of Computer Science

ETH Zürich  Switzerland
asierm@inf.ethz.ch

ETH Zürich  Switzerland
meierflo@inf.ethz.ch

Angelika Steger

Department of Computer Science

ETH Zürich  Switzerland
steger@inf.ethz.ch

Abstract

Despite all the impressive advances of recurrent neural networks  sequential data is
still in need of better modelling. Truncated backpropagation through time (TBPTT) 
the learning algorithm most widely used in practice  suffers from the truncation bias 
which drastically limits its ability to learn long-term dependencies.The Real-Time
Recurrent Learning algorithm (RTRL) addresses this issue  but its high computa-
tional requirements make it infeasible in practice. The Unbiased Online Recurrent
Optimization algorithm (UORO) approximates RTRL with a smaller runtime and
memory cost  but with the disadvantage of obtaining noisy gradients that also limit
its practical applicability. In this paper we propose the Kronecker Factored RTRL
(KF-RTRL) algorithm that uses a Kronecker product decomposition to approximate
the gradients for a large class of RNNs. We show that KF-RTRL is an unbiased and
memory efﬁcient online learning algorithm. Our theoretical analysis shows that 
under reasonable assumptions  the noise introduced by our algorithm is not only
stable over time but also asymptotically much smaller than the one of the UORO
algorithm. We also conﬁrm these theoretical results experimentally. Further  we
show empirically that the KF-RTRL algorithm captures long-term dependencies
and almost matches the performance of TBPTT on real world tasks by training
Recurrent Highway Networks on a synthetic string memorization task and on
the Penn TreeBank task  respectively. These results indicate that RTRL based
approaches might be a promising future alternative to TBPTT.

1

Introduction

Processing sequential data is a central problem in the ﬁeld of machine learning. In recent years 
Recurrent Neural Networks (RNN) have achieved great success  outperforming all other approaches in
many different sequential tasks like machine translation  language modeling  reinforcement learning
and more.

Despite this success  it remains unclear how to train such models. The standard algorithm  Truncated
Back Propagation Through Time (TBPTT) [18]  considers the RNN as a feed-forward model over
time with shared parameters. While this approach works extremely well in the range of a few hundred
time-steps  it scales very poorly to longer time dependencies. As the time horizon is increased  the

∗Author was supported by grant no. CRSII5_173721 of the Swiss National Science Foundation.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

parameters are updated less frequently and more memory is required to store all past states. This
makes TBPTT ill-suited for learning long-term dependencies in sequential tasks.

An appealing alternative to TBPTT is Real-Time Recurrent Learning (RTRL) [19]. This algorithm
allows online updates of the parameters and learning arbitrarily long-term dependencies by exploiting
the recurrent structure of the network for forward propagation of the gradient. Despite its impressive
theoretical properties  RTRL is impractical for decently sized RNNs because run-time and memory
costs scale poorly with network size.

As a remedy to this issue  Tallec and Ollivier [16] proposed the Unbiased Online Recurrent Learning
algorithm (UORO). This algorithm unbiasedly approximates the gradients  which reduces the run-
time and memory costs such that they are similar to the costs required to run the RNN forward.
Unbiasedness is of central importance since it guarantees convergence to a local optimum. Still  the
variance of the gradients slows down learning.

Here we propose the Kronecker Factored RTRL (KF-RTRL) algorithm. This algorithm builds up on
the ideas of the UORO algorithm  but uses Kronecker factors for the RTRL approximation. We show
both theoretically and empirically that this drastically reduces the noise in the approximation and
greatly improves learning. However  this comes at the cost of requiring more computation and only
being applicable to a class of RNNs. Still  this class of RNNs is very general and includes Tanh-RNN
and Recurrent Highway Networks [20] among others.

The main contributions of this paper are:

• We propose the KF-RTRL online learning algorithm.
• We theoretically prove that our algorithm is unbiased and under reasonable assumptions the
noise is stable over time and asymptotically by a factor n smaller than the noise of UORO.
• We test KF-RTRL on a binary string memorization task where our networks can learn

dependencies of up to 36 steps.

• We evaluate in character-level Penn TreeBank  where the performance of KF-RTRL almost

matches the one of TBPTT for 25 steps.

• We empirically conﬁrm that the variance of KF-RTRL is stable over time and that increasing

the number of units does not increase the noise signiﬁcantly.

2 Related Work

Training Recurrent Neural Networks for ﬁnite length sequences is currently almost exclusively done
using BackPropagation Through Time [15] (BPTT). The network is "unrolled" over time and is
considered as a feed-forward model with shared parameters (the same parameters are used at each
time step). Like this  it is easy to do backpropagation and exactly calculate the gradients in order to
do gradient descent.

However  this approach does not scale well to very long sequences  as the whole sequence needs to
be processed before calculating the gradients  which makes training extremely slow and very memory
intensive. In fact  BPTT cannot be applied to an online stream of data. In order to circumvent this
issue  Truncated BackPropagation Through Time [18] (TBPTT) is used generally. The RNN is only
"unrolled" for a ﬁxed number of steps (the truncation horizon) and gradients beyond these steps are
ignored. Therefore  if the truncation horizon is smaller than the length of the dependencies needed to
solve a task  the network cannot learn it.

Several approaches have been proposed to deal with the truncation horizon. Anticipated Reweighted
Truncated Backpropagation [17] samples different truncation horizons and weights the calculated
gradients such that the expected gradient is that of the whole sequence. Jaderberg et al. [5] proposed
Decoupled Neural Interfaces  where the network learns to predict incoming gradients from the future.
Then  it uses these predictions for learning. The main assumption of this model is that all future
gradients can be computed as a function of the current hidden state.

A more extreme proposal is calculating the gradients forward and not doing any kind of BPTT. This is
known as Real-Time Recurrent Learning [19] (RTRL). RTRL allows updating the model parameters
online after observing each input/output pair; we explain it in detail in Section 3. However  its large
running time of order O(n4) and memory requirements of order O(n3)  where n is the number of

2

units of a fully connected RNN  make it unpractical for large networks. To ﬁx this  Tallec and Ollivier
[16] presented the Unbiased Online Recurrent Optimization (UORO) algorithm. This algorithm
approximates RTRL using a low rank matrix. This makes the run-time of the algorithm of the same
order as a single forward pass in an RNN  O(n2). However  the low rank approximation introduces a
lot of variance  which negatively affects learning as we show in Section 5.

Other alternatives are Reservoir computing approaches [8] like Echo State Networks [6] or Liquid
State Machines [9]. In these approaches  the recurrent weights are ﬁxed and only the output con-
nections are learned. This allows online learning  as gradients do not need to be propagated back in
time. However  it prevents any kind of learning in the recurrent connections  which makes the RNN
computationally much less powerful.

3 Real-Time Recurrent Learning and UORO

RTRL [19] is an online learning algorithm for RNNs. Contrary to TBPPT  no previous inputs or
network states need to be stored. At any time-step t  RTRL only requires the hidden state ht  input
xt and dht−1
dht
dθ is obtained by applying the
chain rule. Thus  the parameters can be updated online  that is  for each observed input/output pair
one parameter update can be performed.

in order to compute dht

dθ at hand  dLt

dθ

dθ . With dht

dθ = dLt
dht

In order to present the RTRL update precisely  let us ﬁrst deﬁne an RNN formally. An RNN is a
differentiable function f   that maps an input xt  a hidden state ht−1 and parameters θ to the next
hidden state ht = f (xt  ht−1  θ). At any time-step t  RTRL computes dht
dθ by applying the chain rule:

dht
dθ

=

∂ht
∂ht−1

dht−1

dθ

+

∂ht
∂xt

dxt
dθ

+

∂ht
∂θ

=

∂ht
∂ht−1

dht−1

dθ

+

∂ht
∂θ

 

(1)

(2)

where the middle term vanishes because we assume that the inputs do not depend on the parameters.
For notational simplicity  deﬁne Gt := dht
∂θ   which reduces the above
equation to

dθ   Ht := ∂ht
∂ht−1

and Ft := ∂ht

dht
dθ

= Gt = HtGt−1 + Ft .

(3)

Both Ft and Ht are straight-forward to compute for RNNs. We assume h0 to be ﬁxed  which implies
G0 = 0. With all this  RTRL obtains the exact gradient Gt for each time-step and enables online
updates of the parameters. However  updating the parameters means that Gt is only exact in the limit
were the learning rate is arbitrarily small. This is because the θ that was used for computing Gt is
different from the θ after the parameter update. In practice learning rates are sufﬁciently small such
that this is not an issue.

The downside of RTRL is that for a fully connected RNN with n units the matrices Ht and Gt have
size n × n and n × n2  respectively. Therefore  computing Equation 3 takes O(n4) operations and
requires O(n3) storage  which makes RTRL impractical for large networks.

The UORO algorithm [16] addresses this issue and reduces run-time and memory requirements to
O(n2) at the cost of obtaining an unbiased but noisy estimate of Gt. More precisely  the UORO
algorithm keeps an unbiased rank-one estimate of Gt by approximating Gt as the outer product vwT
of two vectors of size n and size n2  respectively. At any time t  the UORO update consists of two
approximation steps. Assume that the unbiased approximation ˆGt−1 = vwT of Gt−1 is given. First 
Ft is approximated by a rank-one matrix. Second  the sum of two rank-one matrices Ht ˆGt−1 + Ft
is approximated by a rank-one matrix yielding the estimate ˆGt of Gt. The estimate ˆGt is provably
unbiased and the UORO update requires the same run-time and memory as updating the RNN [16].

3

4 Kronecker Factored RTRL

Our proposed Kronecker Factored RTRL algorithm (KF-RTRL) is an online learning algorithm for
RNNs  which does not require storing any previous inputs or network states. KF-RTRL approximates
Gt  which is the derivative of the internal state with respect to the parameters  see Section 3  by a
Kronecker product. The following theorem shows that the KF-RTRL algorithm satisﬁes various
desireable properties.

Theorem 1. For the class of RNNs deﬁned in Lemma 1  the estimate G′
algorithm satisﬁes

t obtained by the KF-RTRL

1. G′

t is an unbiased estimate of Gt  that is E[G′

t] = Gt  and

2. assuming that the spectral norm of Ht is at most 1 − ǫ for some arbitrary small ǫ > 0  then

at any time t  the mean of the variances of the entries of G′

t is of order O(n−1).

Moreover  one time-step of the KF-RTRL algorithm requires O(n3) operations and O(n2) memory.

We remark that the class of RNNs deﬁned in Lemma 1 contains many widely used RNN architectures
like Recurrent Highway Networks and Tanh-RNNs  and can be extended to include LSTMs [4]  see
Section A.6. Further  the assumption that the spectral norm of Ht is at most 1 − ǫ is reasonable  as
otherwise gradients might grow exponentially as noted by Bengio et al. [2]. Lastly  the bottleneck of
the algorithm is a matrix multiplication and thus for sufﬁciently large matrices an algorithm with a
better run time than O(n3) may be be practical.

In the remainder of this section  we explain the main ideas behind the KF-RTRL algorithm (formal
proofs are given in the appendix). In the subsequent Section 5 we show that these theoretical
properties carry over into practical application. KF-RTRL is well suited for learning long-term
dependencies (see Section 5.1) and almost matches the performance of TBPTT on a complex real
world task  that is  character level language modeling (see Section 5.2). Moreover  we conﬁrm
empirically that the variance of the KF-RTRL estimate is stable over time and scales well as the
network size increases (see Section 5.3).

Before giving the theoretical background and motivating the key ideas of KF-RTRL  we give a
brief overview of the KF-RTRL algorithm. At any time-step t  KF-RTRL maintains a vector ut
and a matrix At  such that G′
t−1 and Ft are factored
as a Kronecker product  and then the sum of these two Kronecker products is approximated by
one Kronecker product. This approximation step (see Lemma 2) works analogously to the second
approximation step of the UORO algorithm (see rank-one trick  Proposition 1 in [16]). The detailed
algorithmic steps of KF-RTRL are presented in Algorithm 1 and motivated below.

t = ut ⊗ At satisﬁes E[G′

t] = Gt. Both HtG′

Theoretical motivation of the KF-RTRL algorithm

The key observation that motivates our algorithm is that for many popular RNN architectures F can be
exactly decomposed as the Kronecker product of a vector and a diagonal matrix  see Lemma 1. In the
following Lemma  we show that this property is satisﬁed by a large class of RNNs that include many
popular RNN architectures like Tanh-RNN and Recurrent Highway Networks. Intuitively  an RNN of
this class computes several linear transformations (corresponding to the matrices W 1  . . .   W r) and
merges the resulting vectors through pointwise non-linearities. For example  in the case of RHNs 
there are two linear transformations that compute the new candidate cell and the forget gate  which
then are merged through pointwise non-linearities to generate the new hidden state.
Lemma 1. Assume the learnable parameters θ are a set of matrices W 1  . . .   W r  let ˆht−1 be the
hidden state ht−1 concatenated with the input xt and let zk = ˆht−1W k for k = 1  . . .   r. Assume
that ht is obtained by point-wise operations over the zk’s  that is  (ht)j = f (z1
j )  where
f is such that ∂f
is bounded by a constant. Let Dk ∈ Rn×n be the diagonal matrix deﬁned by
∂zk
j

j   . . .   zr

Dk

jj = ∂(ht)j
∂zk
j

  and let D = (cid:0)D1| . . . |Dr(cid:1). Then  it holds ∂ht

∂θ = ˆht−1 ⊗ D.

Further  we observe that the sum of two Kronecker products can be approximated by a single
Kronecker product in expectation. The following lemma  which is the analogue of Proposition 1 in
[14] for Kronecker products  states how this is achieved.

4

Algorithm 1 — One step of KF-RTRL (from time t − 1 to t)

Inputs:

– input xt  target yt  previous recurrent state ht−1 and parameters θ
– ut−1 and At−1 such that E [ut−1 ⊗ At−1] = Gt−1
– SGDopt and ηt+1: stochastic optimizer and its learning rate

Outputs:

– new recurrent state ht and updated parameters θ
– ut and At such that E [ut ⊗ At] = Gt

/* Run one step of the RNN and compute the necessary matrices*/
zk
j ← Compute linear transformations using xt  ht−1 and θ
ht ← Compute ht using using point-wise operations over the zk
j
ˆht−1 ← Concatenate ht−1 and xt
Dk

H ′ ← H · At−1

H ← ∂ht
∂ht−1

jj ← ∂(ht)j
∂zk
j

/* Compute variance minimization and random multipliers */

p2 ← qkDkHS/kˆht−1kHS

p1 ← pkH ′kHS/kut−1kHS

c1  c2 ← Independent random signs
/* Compute next approximation */
ut ← c1p1ut−1 + c2p2ˆht−1
/* Compute gradients and update parameters */

At ← c1

1
p1

H ′ + c2

1
p2

D

Lgrad ← ut ⊗(cid:16) ∂L(yt ht)

∂ht

· At(cid:17)

SGDopt(Lgrad  ηt+1  θ)

Lemma 2. Let C = A1 ⊗ B1 + A2 ⊗ B2  where the matrix A1 has the same size as the matrix
A2 and B1 has the same size as B2. Let c1 and c2 be chosen independently and uniformly at
random from {−1  1} and let p1  p2 > 0 be positive reals. Deﬁne A′ = c1p1A1 + c2p2A2 and
B ′ = c1
B2. Then  A′ ⊗ B ′ is an unbiased approximation of C  that is E [A′ ⊗ B ′] = C.
Moreover  the variance of this approximation is minimized by setting the free parameters pi =

B1 + c2

1
p2

1
p1

p||Bi||/||Ai||.

Lastly  we show by induction that random vectors ut and random matrices At exist  such that
G′
t−1] = Gt−1.
Equation 3 and Lemma 1 imply that

t−1 = ut−1 ⊗ At−1 satisﬁes E[G′

t = ut ⊗ At satisﬁes E[G′

t] = Gt. Assume that G′

Next  by linearity of expectation and since the ﬁrst dimension of ut−1 is 1  it follows

Gt = HtE(cid:2)G′

t−1(cid:3) + Ft = HtE [ut−1 ⊗ At−1] + ˆht ⊗ Dt .

Gt = EhHt(ut−1 ⊗ At−1) + ˆht ⊗ Dti = Ehut−1 ⊗ (HtAt−1) + ˆht ⊗ Dti .

Finally  we obtain by Lemma 2 for any p1  p2 > 0

Gt = E(cid:20)(c1p1ut−1 + c2p1ˆht) ⊗ (c1

1
p1

(HtAt−1) + c2

1
p2

Dt)(cid:21)  

(4)

(5)

(6)

where the expectation is taken over the probability distribution of ut−1  At−1  c1 and c2.

With these observations at hand  we are ready to present the KF-RTRL algorithm. At any time-step t
we receive the estimates ut−1 and At−1 from the previous time-step. First  compute ht  Dt and Ht.
Then  choose c1 and c2 uniformly at random from {−1  +1} and compute

ut = c1p1ut−1 + c2p2ˆht

At = c1

1
p1

(HtAt−1) + c2

1
p2

Dt  

(7)

(8)

where p1 = pkHtAt−1k/kut−1k and p2 = qkDtk/kˆhtk. Lastly  our algorithm computes dLt

· G′
t 
which is used for optimizing the parameters. For a detailed pseudo-code of the KF-RTRL algorithm

dht

5

see Algorithm 1. In order to see that dLt
dht

· G′

linearity of expectation: Eh dLt

dht

· G′

ti = dLt

dht

t is an unbiased estimate of dLt
· E [G′

· Gt = dLt
dθ .

t] = dLt
dht

dθ   we apply once more

One KF-RTRL update has run-time O(n3) and requires O(n2) memory. In order to see the statement
for the memory requirement  note that all involved matrices and vectors have O(n2) elements 
except G′
dθ   because
dLt
At) can be evaluated in this order. In order to see the statement
dht
for the run-time  note that Ht and At−1 have both size O(n) × O(n). Therefore  computing HtAt−1
requires O(n3) operations. All other arithmetic operations trivially require run-time O(n2).

t. However  we do not need to explicitly compute G′

t in order to obtain dLt

· ut ⊗ At = ut ⊗ ( dLt
dht

t = dLt
dht

· G′

The proofs of Lemmas 1 and 2 and of the second statement of Theorem 1 are given in the appendix.

Comparison of the KF-RTRL with the UORO algorithm

Since the variance of the gradient estimate is directly linked to convergence speed and performance 
let us ﬁrst compare the variance of the two algorithms. Theorem 1 states that the mean of the variances
t are of order O(n−1). In the appendix  we show a slightly stronger statement  that
of the entries of G′
is  if kFtk ≤ C for all t  then the mean of the variances of the entries is of order O( C 2
n3 )  where n3 is
the number of elements of Gt. The bound O(n−1) is obtained by a trivial bound on the size of the
entries of ht and Dt and using khtkkDtk = kFtk. In the appendix  we show further that already the
ﬁrst step of the UORO approximation  in which Ft is approximated by a rank-one matrix  introduces
noise of order (n − 1)kFtk. Assuming that all further approximations would not add any noise  then
the same trivial bounds on kFtk yield a mean variance of O(1). We conclude that the variance of
KF-RTRL is asymptotically by (at least) a factor n smaller than the variance of UORO.

Next  let us compare the generality of the algorithm when applying it to different network architectures.
The KF-RTRL algorithms requires that in one time-step each parameter only affects one element of
the next hidden state (see Lemma 1). Although many widely used RNN architectures satisfy this
requirement  seen from this angle  the UORO algorithm is favorable as it is applicable to arbitrary
RNN architectures.

Finally  let us compare memory requirements and runtime of KF-RTRL and UORO. In terms of
memory requirements  both algorithms require O(n2) storage and perform equally good. In terms of
run-time  KF-RTRL requires O(n3)  while UORO requires O(n2) operations. However  the faster
run-time of UORO comes at the cost of worse variance and therefore worse performance. In order
to reduce the variance of UORO by a factor n  one would need n independent samples of G′
t. This
could be achieved by reducing the learning rate by a factor of n  which would then require n times
more data  or by sampling G′
t n times in parallel  which would require n times more memory. Still 
our empirical investigation shows  that KF-RTRL outperforms UORO  even when taking n UORO
samples of Gt to reduce the variance (see Figure 3). Moreover  for sufﬁciently large networks the
scaling of the KF-RTRL run-time improves by using fast matrix multiplication algorithms.

5 Experiments

In this section  we quantify the effect on learning that the reduced variance of KF-RTRL compared
to the one of UORO has. First  we evaluate the ability of learning long-term dependencies on a
deterministic binary string memorization task. Since most real world problems are more complex and
of stochastic nature  we secondly evaluate the performance of the learning algorithms on a character
level language modeling task  which is a more realistic benchmark. For these two tasks  we also
compare to learning with Truncated BPTT and measure the performance of the considered algorithms
based on ’data time’  i.e. the amount of data observed by the algorithm. Finally  we investigate the
variance of KF-RTRL and UORO by comparing to their exact counterpart  RTRL. For all experiments 
we use a single-layer Recurrent Highway Network [20] 2.

2For implementation simplicity  we use 2 ∗ sigmoid(x) − 1 instead of T anh(x) as non-linearity function.

Both functions have very similar properties  and therefore  we do not believe this has any signiﬁcant effect.

6

Input: #01101––––––
Output: ––––––#01101

Input: #11010––––––
Output: ––––––#11010

Input: #00100––––––
Output: ––––––#00100

(a)

(b)

Figure 1: Copy Task: Figure 1(a) shows the learning curves of UORO  KF-RTRL and TBPTT with truncation
horizon of 25 steps. We plot the mean and standard deviation (shaded area) over 5 trials. Figure 1(b) shows
three input and output examples with T = 5.

5.1 Copy Task

In the copy task experiment  a binary string is presented sequentially to an RNN. Once the full
string has been presented  it should reconstruct the original string without any further information.
Figure 1(b) shows several input-output pairs. We refer to the length of the string as T . Figure 1(a)
summarizes the results. The smaller variance of KF-RTRL greatly helps learning faster and capturing
longer dependencies. KF-RTRL and UORO manage to solve the task on average up to T = 36
and T = 13  respectively. As expected  TBPTT cannot learn dependencies that are longer than the
truncation horizon.

In this experiment  we start with T = 1 and when the RNN error drops below 0.15 bits/char  we
increase T by one. After each sequence  the hidden state is reset to all zeros. To improve performance 
the length of each sample is picked uniformly at random from T to T − 5. This forces the network to
learn a general algorithm for the task  rather than just learning to solve sequences of length T . We
use a RHN with 256 units and a batch size of 256. We optimize the log-likelihood using the Adam
optimizer [7] with default Tensorﬂow [1] parameters  β1 = 0.9 and β2 = 0.999. For each model
we pick the optimal learning rate from {10−2.5  10−3  10−3.5  10−4}. We repeat each experiment 5
times.

5.2 Character level language modeling on the Penn Treebank dataset

A standard test for RNNs is character level language modeling. The network receives a text sequen-
tially  character by character  and at each time-step it must predict the next character. This task is very
challenging  as it requires both long and short term dependencies. Additionally  it is highly stochastic 
i.e. for the same input sequence there are many possible continuations  but only one is observed
at each training step. Figure 2 and Table 1 summarize the results. Truncated BPTT outperforms
both online learning algorithms  but KF-RTRL almost reaches its performance and considerably
outperforms UORO. For this task  the noise introduced in the approximation is more harmful than the
truncation bias. This is probably the case because the short term dependencies dominate the loss  as
indicated by the small difference between TBPTT with truncation horizon 5 and 25.

For this experiment we use the Penn TreeBank [10] dataset  which is a collection of Wall Street
Journal articles. The text is lower cased and the vocabulary is restricted to 10K words. Out of
vocabulary words are replaced by "<unk>" and numbers by "N". We split the data following Mikolov
et al. [13]. The experimental setup is the same as in the Copy task  and we pick the optimal learning
rate from the same range. Apart from that  we reset the hidden state to the all zeros state with
probability 0.01 at each time step. This technique was introduced by Melis et al. [11] to improve
the performance on the validation set  as the initial state for the validation is the all zeros state.
Additionally  this helps the online learning algorithms  as it resets the gradient approximation  getting
rid of stale gradients. Similar techniques have been shown [3] to also improve RTRL.

7

0500000100000015000002000000data time01020304050TKF-RTRLUOROTBPTT-25Table 1: Results on Penn TreeBank. Merity et al. [12] is
currently the state of the art (trained with TBPTT). For
simplicity we do not report standard deviations  as all of
them are smaller than 0.03.

Name

Validation Test

#params

KF-RTRL

UORO

TBPTT-5
TBPTT-25

Merity et al. [12]

1.77
2.63
1.64
1.61

-

1.72
2.61
1.58
1.56
1.18

133K
133K
133K
133K
13.8M

Figure 2: Validation performance on Penn
TreeBank in bits per character (BPC). The
small variance of the KF-RTRL approxima-
tion considerably improves the performance
compared to UORO.

(a)

(b)

Figure 3: Variance analysis: We compare the cosine of the angle between the approximated and the true value
of dL
dθ . A cosine of 1 implies that the approximation and the true value are exactly aligned  while a random
vector gets a cosine of 0 in expectation. Figure 3(a) shows that the variance is stable over time for the three
algorithms. Figure 3(b) shows that the variance of KF-RTRL and UORO-AVG are almost unaffected by the
number of units  while UORO degrades more quickly as the network size increases.

5.3 Variance Analysis

With our ﬁnal set of experiments  we empirically measure how the noise evolves over time and how
it is affected by the number of units n. Here  we also compare to UORO-AVG that computes n
independent samples of UORO and averages them to reduce the variance. The computation costs of
UORO-AVG are on par with those of KF-RTRL  O(n3)  however the memory costs of O(n3) are
higher than the ones of KF-RTRL of O(n2). For each experiment  we compute the angle φ between
the gradient estimate and the exact gradient of the loss with respect to the parameters. Intuitively  φ
measures how aligned the gradients are  even if the magnitude is different. Figure 3(a) shows that φ
is stable over time and the noise does not accumulate for any of the three algorithms. Figure 3(b)
shows that KF-RTRL and UORO-AVG have similar performance as the number of units increases.
This observation is in line with the theoretical prediction in Section A.5 that the variance of UORO
is by a factor n larger than the KF-RTRL variance (averaging n samples as done in AVG-UORO
reduces the variance by a factor n).

In the ﬁrst experiment  we run several untrained RHNs with 256 units over the ﬁrst 10000 characters
of Penn TreeBank. In the second experiment  we compute φ after running RHNs with different
number of units for 100 steps on Penn TreeBank. We perform 100 repetitions per experiment and
plot the mean and standard deviation.

6 Conclusion

In this paper  we have presented the KF-RTRL online learning algorithm. We have proven that it
approximates RTRL in an unbiased way  and that under reasonable assumptions the noise is stable
over time and much smaller than the one of UORO  the only other previously known unbiased RTRL

8

2000004000006000008000001000000data time1.52.02.53.03.5BPCKF-RTRLUOROTBPTT-25TBPTT-50200040006000800010000timesteps0.20.00.20.40.60.81.01.2cos()KF-RTRLUOROUORO-AVG200400units0.00.51.0cos()approximation algorithm. Additionally  we have empirically veriﬁed that the reduced variance of
our algorithm greatly improves learning for the two tested tasks. In the ﬁrst task  an RHN trained
with KF-RTRL effectively captures long-term dependencies (it learns to memorize binary strings of
length up to 36). In the second task  it almost matches the performance of TBPTT in a standard RNN
benchmark  character level language modeling on Penn TreeBank.

More importantly  our work opens up interesting directions for future work  as even minor reductions
of the noise could make the approach a viable alternative to TBPTT  especially for tasks with inherent
long-term dependencies. For example constraining the weights  constraining the activations or using
some form of regularization could reduce the noise. Further  it may be possible to design architectures
that make the approximation less noisy. Moreover  one might attempt to improve the run-time of
KF-RTRL by using approximate matrix multiplication algorithms or inducing properties on the Ht
matrix that allow for fast matrix multiplications  like sparsity or low-rank.

This work advances the understanding of how unbiased gradients can be computed  which is of
central importance as unbiasedness is essential for theoretical convergence guarantees. Since RTRL
based approaches satisfy this key assumption  it is of interest to further progress them.

References

[1] M. Abadi  P. Barham  J. Chen  Z. Chen  A. Davis  J. Dean  M. Devin  S. Ghemawat  G. Irving 
M. Isard  et al. Tensorﬂow: A system for large-scale machine learning. In OSDI  volume 16 
pages 265–283  2016.

[2] Y. Bengio  P. Simard  and P. Frasconi. Learning long-term dependencies with gradient descent

is difﬁcult. IEEE transactions on neural networks  5(2):157–166  1994.

[3] T. Catfolis. A method for improving the real-time recurrent learning algorithm. Neural Networks 

6(6):807–821  1993.

[4] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):

1735–1780  1997.

[5] M. Jaderberg  W. M. Czarnecki  S. Osindero  O. Vinyals  A. Graves  D. Silver  and
K. Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv preprint
arXiv:1608.05343  2016.

[6] H. Jaeger. The “echo state” approach to analysing and training recurrent neural networks-with an
erratum note. Bonn  Germany: German National Research Center for Information Technology
GMD Technical Report  148(34):13  2001.

[7] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[8] M. Lukoševiˇcius and H. Jaeger. Reservoir computing approaches to recurrent neural network

training. Computer Science Review  3(3):127–149  2009.

[9] W. Maass  T. Natschläger  and H. Markram. Real-time computing without stable states: A
new framework for neural computation based on perturbations. Neural computation  14(11):
2531–2560  2002.

[10] M. P. Marcus  M. A. Marcinkiewicz  and B. Santorini. Building a large annotated corpus of

english: The penn treebank. Computational linguistics  19(2):313–330  1993.

[11] G. Melis  C. Dyer  and P. Blunsom. On the state of the art of evaluation in neural language

models. arXiv preprint arXiv:1707.05589  2017.

[12] S. Merity  N. S. Keskar  and R. Socher. An analysis of neural language modeling at multiple

scales. arXiv preprint arXiv:1803.08240  2018.

[13] T. Mikolov  I. Sutskever  A. Deoras  H.-S. Le  S. Kombrink  and J. Cernocky. Subword language
modeling with neural networks. preprint (http://www. ﬁt. vutbr. cz/imikolov/rnnlm/char. pdf) 
2012.

[14] Y. Ollivier  C. Tallec  and G. Charpiat. Training recurrent networks online without backtracking.

arXiv preprint arXiv:1507.07680  2015.

[15] D. E. Rumelhart  G. E. Hinton  and R. J. Williams. Learning representations by back-propagating

errors. nature  323(6088):533  1986.

9

[16] C. Tallec and Y. Ollivier. Unbiased online recurrent optimization.

arXiv preprint

arXiv:1702.05043  2017.

[17] C. Tallec and Y. Ollivier. Unbiasing truncated backpropagation through time. arXiv preprint

arXiv:1705.08209  2017.

[18] R. J. Williams and J. Peng. An efﬁcient gradient-based algorithm for on-line training of recurrent

network trajectories. Neural Computation  2:490–501  1990.

[19] R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural

networks. Neural computation  1(2):270–280  1989.

[20] J. G. Zilly  R. K. Srivastava  J. Koutník  and J. Schmidhuber. Recurrent highway networks.

arXiv preprint arXiv:1607.03474  2016.

10

,Asier Mujika
Florian Meier
Angelika Steger