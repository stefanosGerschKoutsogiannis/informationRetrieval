2019,Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games,We study the global convergence of policy optimization for finding the Nash equilibria (NE) in zero-sum linear quadratic (LQ) games. To this end  we first investigate the landscape of LQ games  viewing it as a nonconvex-nonconcave saddle-point problem in the policy space. Specifically  we show that despite its nonconvexity and nonconcavity  zero-sum LQ games have the property that the stationary point of the objective function with respect to the linear feedback control policies constitutes the NE of the game. Building upon this  we develop three projected nested-gradient methods that are guaranteed to converge to the NE of the game. Moreover  we show that all these algorithms enjoy both globally sublinear and locally linear convergence rates. Simulation results are also provided to illustrate the satisfactory convergence properties of the algorithms. To the best of our knowledge  this work appears to be the first one to investigate the optimization landscape of LQ games  and provably show the convergence of policy optimization methods to the NE. Our work serves as an initial step toward understanding the theoretical aspects of policy-based reinforcement learning algorithms for zero-sum Markov games in general.,Policy Optimization Provably Converges to Nash
Equilibria in Zero-Sum Linear Quadratic Games

Kaiqing Zhang
ECE and CSL

University of Illinois at Urbana-Champaign

kzhang66@illinois.edu

Zhuoran Yang

ORFE

Princeton University
zy6@princeton.edu

Tamer Ba¸sar
ECE and CSL

University of Illinois at Urbana-Champaign

basar1@illinois.edu

Abstract

We study the global convergence of policy optimization for ﬁnding the Nash equi-
libria (NE) in zero-sum linear quadratic (LQ) games. To this end  we ﬁrst inves-
tigate the landscape of LQ games  viewing it as a nonconvex-nonconcave saddle-
point problem in the policy space. Speciﬁcally  we show that despite its noncon-
vexity and nonconcavity  zero-sum LQ games have the property that the stationary
point of the objective function with respect to the linear feedback control policies
constitutes the NE of the game. Building upon this  we develop three projected
nested-gradient methods that are guaranteed to converge to the NE of the game.
Moreover  we show that all these algorithms enjoy both globally sublinear and lo-
cally linear convergence rates. Simulation results are also provided to illustrate the
satisfactory convergence properties of the algorithms. To the best of our knowl-
edge  this work appears to be the ﬁrst one to investigate the optimization landscape
of LQ games  and provably show the convergence of policy optimization methods
to the NE. Our work serves as an initial step toward understanding the theoretical
aspects of policy-based reinforcement learning algorithms for zero-sum Markov
games in general.

1

Introduction

Reinforcement learning [1] has achieved sensational progress recently in several prominent decision-
making problems  e.g.  playing the game of Go [2  3]  and playing real-time strategy games [4 
5]. Interestingly  all of these problems can be formulated as zero-sum Markov games involving
two opposing players or teams. Moreover  their algorithmic frameworks are all based upon policy
optimization (PO) methods such as actor-critic [6] and proximal policy optimization (PPO) [7] 
where the policies are parametrized and iteratively updated. Such popularity of PO methods are
mainly attributed to the facts that: (i) they are easy to implement and can handle high-dimensional
and continuous action spaces; (ii) they can readily incorporate advanced optimization results to
facilitate the algorithm design [7  8  9]. Moreover  empirically  some observations have shown that
PO methods usually converge faster than value-based ones [9  10].
In contrast to the tremendous empirical success  theoretical understanding of policy optimization
methods for zero-sum Markov games lags behind. Although the convergence of policy optimization
algorithms to locally optimal policies has been established in the classical reinforcement learning
setting with a single agent/player [11  6  12  7  13  14]  extending those theoretical guarantees to

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Nash equilibrium (NE) policies  a common solution concept in game theory also known as saddle-
point equilibrium (SPE) in the zero-sum setting [15]  suffers from the following two caveats.
First  since the players simultaneously determine their actions in the games  the decision-making
problem faced by each player becomes non-stationary. As a result  single-agent algorithms fail to
work due to lack of Markov property [16]. Second  with parametrized policies  the policy optimiza-
tion for ﬁnding NE in a function space is reduced to solving for NE in the policy parameter space 
where the underlying game is in general nonconvex-nonconcave. Since nonconvex optimization
problems are NP-hard [17] in the worst case  so is ﬁnding an NE in nonconvex-nonconcave saddle-
point problems [18]. In fact  it has been showcased recently that vanilla gradient-based algorithms
might have cyclic behaviors and fail to converge to any NE [19  20  21] in zero-sum games.
As an initial attempt in merging the gap between theory and practice  we study the performance of
PO methods on a simple but quintessential example of zero-sum Markov games  namely  zero-sum
linear quadratic (LQ) games.
In LQ games  the system evolves following linear dynamics con-
trolled by both players  while the cost function is quadratically dependent on the states and joint
control actions. Zero-sum LQ games ﬁnd broad applications in H∞-control for robust control syn-
thesis [15  22]  and risk-sensitive control [23  24].
In fact  such an LQ setting can be used for
studying general continuous control problems with adversarial disturbances/opponents  by lineariz-
ing the system of interest around the operational point [15]. Therefore  developing theory for the
LQ setting may provide some insights into the local property of the general control settings. Our
study is pertinent to the recent efforts on policy optimization for linear quadratic regulator (LQR)
problems [25  26  27]  a single-player counterpart of LQ games. As to be shown later  LQ games
are more challenging to solve using PO methods  since they are not only nonconvex in the pol-
icy space for one player (as LQR)  but also nonconcave for the other. Compared to PO for LQR 
such nonconvexity-nonconcavity has caused technical difﬁculties in showing the stabilizing prop-
erties along the iterations  an essential requirement for the iterative PO algorithms to be feasible.
Additionally  in contrast to the recent non-asymptotic analyses on gradient methods for nonconvex-
nonconcave saddle-point problems [28]  the objective function lacks smoothness in LQ games  as
the main challenge identiﬁed in [25] for LQR.
To address these technical challenges  we ﬁrst investigate the optimization landscape of LQ games 
showing that the stationary point of the objective function constitutes the NE of the game  despite its
nonconvexity and nonconcavity. We then propose three projected nested-gradient methods  which
separate the updates into two loops with both gradient-based iterations. Such a nested-loop update
mitigates the inherent non-stationarity of learning in games. The projection ensures the stabilizing
property of the control along the iterations. The algorithms are guaranteed to converge to the NE 
with globally sublinear and locally linear rates. Our results set theoretical foundations for developing
model-free policy-based reinforcement learning algorithms for zero-sum LQ games.
Related Work. There is a huge body of literature on value-based methods for zero-sum Markov
games; see  e.g  [29  30  31  32  33  34] and the references therein. Specially  for the linear quadratic
setting  [35] proposed a Q-learning approximate dynamic programming approach. In contrast  the
study of PO methods for zero-sum Markov games is limited  which are either empirical without any
theoretical guarantees [36]  or developed only for the tabular setting [37  38  39  40]. Within the
LQ setting  our work is related to the recent one on the global convergence of policy gradient (PG)
methods for LQR [25  26]. However  our setting is more challenging since it concerns a saddle-point
problem with not only nonconvexity on the minimizer  but also nonconcavity on the maximizer.
Our work also falls into the realm of solving nonconvex-(non)concave saddle-point problems
[41  42  43  44  45  46]  which has recently drawn great attention due to the popularity of train-
ing generative adversarial networks (GANs) [47  48  42  49]. However  most of the existing results
are either for the nonconvex but concave minimax setting [50  42  49]  or only have asymptotic con-
vergence results [41  47  48  43  44]. Two recent pieces of results on non-asymptotic analyses for
solving this problem have been established under strong assumptions that the objective function is
either weakly-convex and weakly-concave [51]  or smooth [28]. However  LQ games satisfy neither
of these assumptions. In addition  even asymptotically  basic gradient-based approaches may not
converge to (local) Nash equilibria [45  46]  not even to stationary points  due to the oscillatory be-
haviors [20]. In contrast to [45  46]  our results show the global convergence to actual NE (instead
of any surrogate as local minimax in [46]) of the game.

2

Contribution. Our contribution is two-fold: i) we investigate the optimization landscape of zero-
sum LQ games in the parametrized feedback control policy space  showing its desired property that
stationary points constitute the Nash equilibria; ii) we develop projected nested-gradient methods
that are proved to converge to the NE with globally sublinear and locally linear rates. We also pro-
vide several interesting simulation ﬁndings on solving this problem with PO methods  even beyond
the settings we consider that enjoy theoretical guarantees. To the best of our knowledge  for the ﬁrst
time  policy-based methods with function approximation are shown to converge to the global Nash
equilibria in a class of zero-sum Markov games  and also with convergence rate guarantees.

2 Background

Consider a zero-sum LQ game model  where the dynamics are characterized by a linear system

xt+1 = Axt + But + Cvt 

(2.1)
where the system state is xt ∈ Rd  the control inputs of players 1 and 2 are ut ∈ Rm1 and vt ∈ Rm2 
respectively. The matrices satisfy A ∈ Rd×d  B ∈ Rd×m1  and C ∈ Rd×m2. The objective of player
1 (player 2) is to minimize (maximize) the inﬁnite-horizon value function 
t Qxt + u(cid:62)

t Ruut − v(cid:62)

(cid:20) ∞(cid:88)

(cid:20) ∞(cid:88)

= Ex0∼D

ct(xt  ut  vt)

Ex0∼D

t Rvvt)

(x(cid:62)

(cid:21)

(cid:21)

 

t=0

t=0

(2.2)
where x0 ∼ D is the initial state drawn from a distribution D  the matrices Q ∈ Rd×d  Ru ∈
Rm1×m1  and Rv ∈ Rm2×m2 are all positive deﬁnite. If the solution to (2.2) exists and the inf and
sup can be interchanged  we refer to the solution value in (2.2) as the value of the game.
To investigate the property of the solution to (2.2)  we ﬁrst introduce the generalized algebraic
Riccati equation (GARE) as follows

P ∗ = A(cid:62)P ∗A + Q −(cid:2)A(cid:62)P ∗B A(cid:62)P ∗C(cid:3)(cid:20)Ru + B(cid:62)P ∗B

(cid:21)−1(cid:20)B(cid:62)P ∗A

(cid:21)

 

B(cid:62)P ∗C

−Rv + C(cid:62)P ∗C

C(cid:62)P ∗B

C(cid:62)P ∗A

(2.3)
where P ∗ denotes the minimal non-negative deﬁnite solution to (2.3). Under standard assumptions
to be speciﬁed shortly  the value exists and can be characterized by P ∗ [15]  i.e.  for any x0 ∈ Rd 

inf

{ut}t≥0

sup
{vt}t≥0

x(cid:62)
0 P ∗x0 = inf
{ut}t≥0

sup
{vt}t≥0

ct(xt  ut  vt) = sup
{vt}t≥0

inf

{ut}t≥0

ct(xt  ut  vt) 

(2.4)

and there exists a pair of linear feedback stabilizing polices to make the equality in (2.4) hold  i.e. 
(2.5)
with K∗ ∈ Rm1×d and L∗ ∈ Rm2×d being the control gain matrices for the minimizer and the
maximizer  respectively. The values of K∗ and L∗ can be given by

t = −K∗xt 
u∗

t = −L∗xt 
v∗

∞(cid:88)

t=0

∞(cid:88)

t=0

K∗ =[Ru + B(cid:62)P ∗B − B(cid:62)P ∗C(−Rv + C(cid:62)P ∗C)−1C(cid:62)P ∗B]−1

× [B(cid:62)P ∗A − B(cid:62)P ∗C(−Rv + C(cid:62)P ∗C)−1C(cid:62)P ∗A] 

L∗ =[−Rv + C(cid:62)P ∗C − C(cid:62)P ∗B(Ru + B(cid:62)P ∗B)−1B(cid:62)P ∗C]−1

(2.6)

(2.7)
Since the controller pair (K∗  L∗) achieves the value (2.4) for any x0  the value of the game is thus

(cid:1). Now we introduce the assumption that guarantees the arguments above to hold.

× [C(cid:62)P ∗A − C(cid:62)P ∗B(Ru + B(cid:62)P ∗B)−1B(cid:62)P ∗A].

Ex0∼D(cid:0)x(cid:62)

0 P ∗x0

Assumption 2.1. The following conditions hold: i) there exists a minimal positive deﬁnite solution
P ∗ to the GARE (2.3) that satisﬁes Rv − C(cid:62)P ∗C > 0; ii) L∗ satisﬁes Q − (L∗)(cid:62)RvL∗ > 0.
The condition i) in Assumption 2.1 is a standard sufﬁcient condition that ensures the existence of the
value of the game [15  35  52]. The additional condition ii) leads to the saddle-point property of the
control pair (K∗  L∗)  i.e.  the controller sequence ({u∗
t}t≥0 {v∗
t }t≥0) generated from (2.5) con-
stitutes the NE of the game (2.2)  which is also unique. We formally state the arguments regarding
(2.3)-(2.7) in the following lemma  whose proof is deferred to §C.1.

3

∞(cid:88)

Lemma 2.2. Under Assumption 2.1 i)  for any x0 ∈ Rd  the value of the minimax game

inf

{ut}t≥0

sup
{vt}t≥0

ct(xt  ut  vt)

(2.8)
exists  i.e.  (2.4) holds  and (K∗  L∗) is stabilizing. Furthermore  under Assumption 2.1 ii)  the
t }t≥0) generated from (2.5) constitutes the saddle-point of (2.8) 
controller sequence ({u∗
i.e.  the NE of the game  and it is unique.
Lemma 2.2 implies that the solution to (2.2) can be found by searching for (K∗  L∗) in the matrix
space Rm1×d × Rm2×d  given by (2.6)-(2.7) for some P ∗ > 0 satisfying (2.3). Next  we aim to
develop policy optimization methods that are guaranteed to converge to such a (K∗  L∗).

t}t≥0 {v∗

t=0

3 Policy Gradient and Landscape

(cid:26) ∞(cid:88)

C(K  L) := Ex0∼D

By Lemma 2.2  we focus on ﬁnding the state feedback policies of players parameterized by ut =
−Kxt and vt = −Lxt  such that ρ(A− BK − CL) < 1. Accordingly  we denote the corresponding
expected cost in (2.2) as

t Qxt + (Kxt)(cid:62)Ru(Kxt) − (Lxt)(cid:62)Rv(Lxt)(cid:3)(cid:27)
(cid:2)x(cid:62)
Then for any stablilizing control pair (K  L)  it follows that C(K  L) = Ex0∼D(cid:0)x(cid:62)
we deﬁne ΣK L as the state correlation matrix  i.e.  ΣK L := Ex0∼D(cid:80)∞

PK L = Q + K(cid:62)RuK − L(cid:62)RvL + (A − BK − CL)(cid:62)PK L(A − BK − CL).

Also  deﬁne PK L as the unique solution to the Lyapunov equation

the NE (K∗  L∗) using policy optimization methods that solve the following minimax problem

(cid:1). Also 

(3.2)

t . Our goal is to ﬁnd

t=0 xtx(cid:62)

.

(3.1)

0 PK Lx0

t=0

C(K  L) 

(3.3)

max
such that for any K ∈ Rm1×d and L ∈ Rm2×d 

min
K

L

C(K∗  L) ≤ C(K∗  L∗) ≤ C(K  L∗).

As has been recognized in [25] that the LQR problem is nonconvex with respect to (w.r.t.) the control
gain K  we note that in general  for some given L (or K)  the minimization (or maximization)
problem is not convex (or concave). This has in fact caused the main challenge for the design of
equilibrium-seeking algorithms for zero-sum LQ games. We formally state this in the following
lemma  which is proved in §C.2.
Lemma 3.1 (Nonconvexity-Nonconcavity of C(K  L)). Deﬁne a subset Ω ⊂ Rm2×d as

Ω :=(cid:8)L ∈ Rm2×d | Q − L(cid:62)RvL > 0(cid:9).

(3.4)
Then there exists L ∈ Ω such that minK C(K  L) is a nonconvex minimization problem; there exists
K such that maxL∈Ω C(K  L) is a nonconcave maximization problem.
To facilitate the algorithm design  we establish the explicit expression of the policy gradient w.r.t.
the parameters K and L in the following lemma  with a proof provided in §C.3.
Lemma 3.2 (Policy Gradient Expression). The policy gradients of C(K  L) have the form

∇KC(K  L) = 2[(Ru + B(cid:62)PK LB)K − B(cid:62)PK L(A − CL)]ΣK L
∇LC(K  L) = 2[(−Rv + C(cid:62)PK LC)L − C(cid:62)PK L(A − BK)]ΣK L.

(3.5)
(3.6)

To study the landscape of this nonconvex-nonconcave problem  we ﬁrst examine the property of the
stationary points of C(K  L)  which are the points that gradient-based methods converge to.
Lemma 3.3 (Stationary Point Property). For a stabilizing control pair (K  L)  i.e.  ρ(A − BK −
CL) < 1  suppose ΣK L is full-rank and (−Rv + C(cid:62)PK LC) is invertible.
If ∇KC(K  L) =
∇LC(K  L) = 0  and the induced matrix PK L deﬁned in (3.2) is positive deﬁnite  then (K  L)
constitutes the control gain pair at the Nash equilibrium.
Lemma 3.3  proved in §C.4  shows that the stationary point of C(K  L) sufﬁces to characterize
the NE of the game under certain conditions. In fact  for ΣK L to be full-rank  it sufﬁces to let
Ex0∼Dx0x(cid:62)
0 be full-rank  i.e.  to use a random initial state x0. This can be easily satisﬁed in practice.

4

4 Policy Optimization Algorithms

In this section  we propose three PO methods  based on policy gradients  to ﬁnd the global NE of
the LQ game. In particular  we develop nested-gradient (NG) methods  which ﬁrst solve the inner
optimization by PG methods  and then use the stationary-point solution to perform gradient-update
for the outer optimization. One way to solve for the NE is to directly address the minimax problem
(2.2). Success of this procedure  as pointed out in [25] for LQR  requires the stability guarantee
of the system along the outer policy-gradient updates. However  unlike LQR  it is not clear so far
if there exists a stepsize and/or condition on K that ensures such stability of the system along the
outer-loop policy-gradient update. Instead  if we solve the maximin problem  which has the same
value as (2.2) (see Lemma 2.2)  then a simple projection step on the iterate L  as to be shown later 
can guarantee the stability of the updates. Therefore  we aim to solve maxL minK C(K  L).
For some given L  the inner minimization problem becomes an LQR problem with equivalent cost

matrix (cid:101)QL = Q − L(cid:62)RvL  and state transition matrix (cid:101)AL = A − CL. Motivated by [25]  we

propose to ﬁnd the stationary point of the inner problem  since the stationary point sufﬁces to be the
global optimum under certain conditions (see Corollary 4 in [25]). Let the stationary-point solution
be K(L). By setting ∇KC(K  L) = 0 and by Lemma 3.2  we have

K(L) = (Ru + B(cid:62)PK(L) LB)−1B(cid:62)PK(L) L(A − CL).

(4.1)

We then substitute (4.1) into (3.2) to obtain the Riccati equation for the inner problem:

L PK(L) L(cid:101)AL − (cid:101)A(cid:62)

L PK(L) LC(Ru + B(cid:62)PK(L) LB)−1C(cid:62)PK(L) L(cid:101)AL. (4.2)

PK(L) L = (cid:101)QL + (cid:101)A(cid:62)

Note that K(L) can be obtained using gradient-based algorithms as in [25]. For example  one can
use the basic policy gradient update in the inner-loop  i.e. 

K(cid:48) = K − α∇KC(K  L) = K − 2α[(Ru + B(cid:62)PK LB)K − B(cid:62)PK L(cid:101)AL]ΣK L 

(4.3)
where α > 0 denotes the stepsize  and PK L denotes the solution to (3.2) for given (K  L). Alterna-
tively  one can also use the following algorithms that use the approximate second-order information
to accelerate the update  i.e.  the natural policy gradient update:

K(cid:48) = K − α∇KC(K  L)Σ−1

K L = K − 2α[(Ru + B(cid:62)PK LB)K − B(cid:62)PK L(cid:101)AL] 
= K − 2α(Ru + B(cid:62)PK LB)−1[(Ru + B(cid:62)PK LB)K − B(cid:62)PK L(cid:101)AL].

K(cid:48) = K − α(Ru + B(cid:62)PK LB)−1∇KC(K  L)Σ−1

K L

and the Gauss-Newton update:

(4.4)

(4.5)

Suppose K(L) in (4.1) can be obtained  regardless of the algorithms used. Then  we substitute
K(L) back to ∇LC(K(L)  L) to obtain the nested-gradient w.r.t. L  which has the following form
(4.6)

∇L(cid:101)C(L) := ∇LC(K(L)  L)
(cid:110)(cid:2) − Rv + C(cid:62)PK(L) LC − C(cid:62)PK(L) LB(Ru + B(cid:62)PK(L) LB)−1B(cid:62)PK(L) LC(cid:3)L (4.7)
Note that the stationary-point condition of the outer-loop that ∇L(cid:101)C(L) = 0 is identical to that of

(cid:2)A − B(Ru + B(cid:62)PK(L) LB)−1B(cid:62)PK(L) LA(cid:3)(cid:111)

− C(cid:62)PK(L) L

ΣK(L) L.

(4.8)

= 2

∇LC(K(L)  L) = 0  since

∇L(cid:101)C(L) = ∇LC(K(L)  L) + ∇LK(L) · ∇KC(K(L)  L) = ∇LC(K(L)  L) 

where ∇KC(K(L)  L) = 0 by deﬁnition of K(L). Thus  the convergent point (K(L)  L) that makes

∇L(cid:101)C(L) = 0 satisfy both conditions ∇KC(K(L)  L) = 0 and ∇LC(K(L)  L) = 0  which implies

from Lemma 3.3 that the convergent control pair (K(L)  L) constitutes the Nash equilibrium.
Thus  we propose the projected nested-gradient update in the outer-loop to ﬁnd the pair (K(L)  L):

Projected Nested-Gradient:

(4.9)

Ω [L + η∇L(cid:101)C(L)] 

L(cid:48) = PGD

5

where Ω is some convex set in Rm2×d  and PGD

Ω [(cid:101)L] = argmin

PGD

Ω [·] is the projection operator onto Ω deﬁned as
Tr

(cid:104)(cid:0)L −(cid:101)L(cid:1)(cid:0)L −(cid:101)L(cid:1)(cid:62)(cid:105)

 

(4.10)

i.e.  the minimizer of the distance between (cid:101)L and L in Frobenius norm. It is assumed that the set
there always exists a constant ζ with 0 < ζ < σmin((cid:101)QL∗ )  with one example of Ω that serves the

Ω is large enough such that the Nash equilibrium (K∗  L∗) is contained in it. By Assumption 2.1 

L∈Ω

purpose is

Ω :=(cid:8)L ∈ Rm2×d | Q − L(cid:62)RvL ≥ ζ · I(cid:9) 

(4.11)
which contains L∗ at the NE. Thus  the projection does not exclude the convergence to the NE. The
following lemma  whose proof is in §C.5  shows that this set Ω is indeed convex and compact.
Lemma 4.1. The subset Ω ⊂ Rm2×d deﬁned in (4.11) is a convex and compact set.
Remark 4.2 (Constraint Set Ω). The projection is mainly for the purpose of theoretical analysis 
and is not necessarily used in the implementation of the algorithm in practice. In fact  the simulation
results in §6 show that the algorithms can converge without this projection in many cases. Such a
projection is also implementable  since the set to project on is convex  and the constraint is directly
imposed on the policy parameter iterate L (not on some derivative quantities  e.g.  PK(L) L).

Similarly  we develop the following projected natural nested-gradient update:
Projected Natural Nested-Gradient:
where the projection operator PN G

Ω [·] for natural nested-gradient is deﬁned as
.

(cid:104)(cid:0) ˇL −(cid:101)L(cid:1)ΣK(L) L

(cid:2)L + η∇L(cid:101)C(L)Σ−1
(cid:0) ˇL −(cid:101)L(cid:1)(cid:62)(cid:105)

Ω [(cid:101)L] = argmin

L(cid:48) = PN G

PN G

Tr

Ω

K(L) L

(cid:3) 

ˇL∈Ω

(4.12)

(4.13)

A weight matrix ΣK(L) L is added for the convenience of subsequent theoretical analysis. We note
that the weight matrix ΣK(L) L depends on the current iterate L in (4.12).
Moreover  we can develop the projected nested-gradient algorithm with preconditioning matrices.
For example  if we assume that Rv − C(cid:62)PK(L) LC is positive deﬁnite  and deﬁne

WL = Rv − C(cid:62)(cid:2)PK(L) L − PK(L) LB(Ru + B(cid:62)PK(L) LB)−1B(cid:62)PK(L) L

(4.14)
we have the following update that is referred to as a projected Gauss-Newton nested-gradient update
Projected Gauss-Newton Nested-Gradient:

(cid:3)C 

L(cid:48) = PGN
Ω [·] is deﬁned as

Ω

(cid:2)L + ηW −1
(cid:104)

L ∇L(cid:101)C(L)Σ−1
(cid:0) ˇL −(cid:101)L(cid:1)ΣK(L) L

(cid:3) 
(cid:0) ˇL −(cid:101)L(cid:1)(cid:62)

W 1/2

K(L) L

L

Tr

(cid:105)

.

W 1/2

L

(4.15)

(4.16)

where the projection operator PGN

Ω [(cid:101)L] = argmin

PGN

ˇL∈Ω

The weight matrices ΣK(L) L and WL both depend on the current iterate L in (4.15).
Based on the updates above  it is straightforward to develop model-free versions of NG algorithms
using sampled data. In particular  we propose to ﬁrst use zeroth-order optimization algorithms to ﬁnd
the stationary point of the inner LQR problem after a ﬁnite number of iterations. Since the Gauss-
Newton update cannot be estimated via sampling  only the PG and natural PG updates are converted
to model-free versions. The approximate stationary point is then substituted into the outer-loop to
perform the projected (natural) NG updates. Details of our model-free projected NG updates are
provided in §A. Note that building upon our theory next  high-probability convergence guarantees
for these model-free counterparts can be established as in the LQR setting in [25].

5 Convergence Results

We start by establishing the convergence for the inner optimization problem as follows  which shows
the globally linear convergence rates of the inner-loop policy gradient updates (4.3)-(4.5).

6

Proposition 5.1 (Global Convergence Rate of Inner-Loop Update). Suppose Ex0∼Dx0x(cid:62)
0 > 0 and
Assumption 2.1 holds. For any L ∈ Ω  where Ω is deﬁned in (3.4)  it follows that: i) the inner-loop
LQR problem always admits a solution  with a positive deﬁnite PK(L) L and a stabilizing control pair
(K(L)  L); ii) there exists a constant stepsize α > 0 for each of the updates (4.3)-(4.5) such that the
generated control pair sequences {(Kτ   L)}τ≥0 are always stabilizing; iii) the updates (4.3)-(4.5)
enables the convergence of the cost value sequence {C(Kτ   L)}τ≥0 to the optimum C(K(L)  L)
with linear rate.

Proof of Proposition 5.1  deferred to §B.2  primarily follows that for Theorem 7 in [25]. However 
we provide additional stability arguments for the control pair (Kτ   L) along the iteration of τ.
We then establish the global convergence of the projected NG updates (4.9)  (4.12)  and (4.15).
Before we state the results  we deﬁne the gradient mapping for all three projection operators
Ω   PN G
Ω   and PGD
PGN
PGN
Ω

Ω at any L ∈ Ω as follows
L ∇L(cid:101)C(L)Σ−1

(cid:2)L + η∇L(cid:101)C(L)Σ−1

(cid:2)L + ηW −1

(cid:3) − L

PN G
Ω

K(L) L

K(L) L

ˆG∗
L :=

(cid:3) − L
(cid:2)L + η∇L(cid:101)C(L)](cid:3) − L

(cid:101)G∗

L :=

2η

2η

PGD
Ω

ˇG∗
L :=

2η

.

(5.1)

Lτ

τ =0

Lτ

τ =0

Lτ

τ =0

(cid:13)(cid:13)2(cid:9)

(cid:13)(cid:13)2(cid:9)

(cid:13)(cid:13) ˆG∗

(cid:13)(cid:13) ˇG∗

(cid:13)(cid:13)(cid:101)G∗

Note that gradient mappings have been commonly adopted in the analysis of projected gradient
descent methods in constrained optimization [53].
Theorem 5.2 (Global Convergence Rate of Outer-Loop Update). Suppose Ex0∼Dx0x(cid:62)
0 > 0  As-
sumption 2.1 holds  and the initial maximizer control L0 ∈ Ω  where Ω is deﬁned in (4.11). Then it
follows that: i) at iteration t of the projected NG updates (4.9)  (4.12)  and (4.15)  the inner-loop up-
dates (4.3)-(4.5) converge to K(Lt) with linear rate; ii) the control pair sequences {(K(Lt)  Lt)}t≥0
generated from (4.9)  (4.12)  and (4.15) are always stabilizing (regardless of the stepsize choice η);
iii) with proper choices of the stepsize η  the updates (4.9)  (4.12)  and (4.15) all converge to the
Nash equilibrium (K∗  L∗) of the zero-sum LQ game (3.3) with O(1/t) rate  in the sense that the
t≥1 all

t≥1  and (cid:8)t−1(cid:80)t−1

sequences (cid:8)t−1(cid:80)t−1

(cid:13)(cid:13)2(cid:9)
t≥1  (cid:8)t−1(cid:80)t−1

converge to zero with O(1/t) rate.
Since Ω ⊂ Ω  the ﬁrst two arguments follow directly by applying Proposition 5.1. The last argument
shows that the iterate (K(Lt)  Lt) generated from the projected NG updates converges to the Nash
equilibrium with a sublinear rate. Detailed proof of Theorem 5.2 is provided in §B.3.
Due to the nonconvexity-nonconcavity of the problem (see Lemma 3.1)  our result is pertinent to
the recent work on ﬁnding a ﬁrst-order stationary point for nonconvex-nonconcave minimax games
under the Polyak-Łojasiewicz (PŁ)-condition for one of the players [28].
Interestingly  the LQ
games considered here also satisfy the one-sided PŁ-condition in [28]  since for a given L ∈ Ω  the
inner problem is an LQR  which enables the use of Lemma 11 in [25] to show this. However  as
recognized by [25] for LQR problems  the main challenge of the LQ games here in contrast to the
minimax game setting in [28] is coping with the lack of smoothness in the objective function.
This O(1/t) rate matches the sublinear convergence rate to ﬁrst-order stationary points  instead
of (local) Nash equilibrium  in [28]. In contrast  by the landscape of zero-sum LQ games shown in
Lemma 3.3  our convergence is to the global NE of the game  if the projection is not effective. In fact 
in this case  the convergence rate can be improved to be linear  as to be introduced next in Theorem
5.3. In addition  our rate also matches the (worst-case) global convergence rate of gradient descent
and second-order algorithms for nonconvex optimization  either under the smoothness assumption
of the objective [54  55]  or for a certain class of non-smooth objectives [56].
Compared to [25]  the nested-gradient algorithms cannot be shown to have globally linear conver-
gence rates so far  owing to the additional nonconcavity on L added to the standard LQR problems.
Nonetheless  the PŁ property of the LQ games still enables linear convergence rate near the Nash
equilibrium. We formally establish the local convergence results in the following theorem  the proof
of which is provided in §B.4.
Theorem 5.3 (Local Convergence Rate of Outer-Loop Update). Under the conditions of Theorem
5.2  the projected NG updates (4.9)  (4.12)  and (4.15) all have locally linear convergence rates
around the Nash equilibrium (K∗  L∗) of the LQ game (3.3)  in the sense that the cost value se-

7

quence {C(K(Lt)  Lt)}t≥0 converges to C(K∗  L∗)  and the nested gradient norm square sequence

{(cid:107)∇L(cid:101)C(Lt)(cid:107)2}t≥0 converges to zero  both with linear rates.

Theorem 5.3 shows that when the proposed NG updates (4.9)  (4.12)  and (4.15) get closer to the NE
(K∗  L∗)  the local convergence rates can be improved from sublinear (see Theorem 5.2) to linear.
This resembles the convergence property of (Quasi)-Newton methods for nonconvex optimization 
with globally sublinear and locally linear convergence rates. To the best of our knowledge  this ap-
pears to be the ﬁrst such result on equilibrium-seeking for nonconvex-nonconcave minimax games 
even with the smoothness assumption as in [28].
Remark 5.4. We note that for the class of zero-sum LQ games that Assumption 2.1 ii) fails to hold 
there may not exists a set Ω of the form (4.11) that contains the NE (K∗  L∗). Even then  our global
convergence results in Proposition 5.1 and Theorem 5.2 still hold. This is because the convergence
is established in the sense of gradient mappings. In this case  the statement should be changed from
global convergence to the NE  to global convergence to the projected point of NE onto Ω. . However 
this may invalidate the statements on local convergence in Theorem 5.3  as the proof relies on the
ineffectiveness of the projection operator around the NE.

6 Simulation Results

In this section  we provide some numerical results to show the superior convergence property of
several PO methods. We consider two settings referred to as Case 1 and Case 2  which are created
based on the simulations in [35]  with

(cid:34) 0.956488

A =

0.0741349

0.0816012
0.94121 −0.000708383

−0.0005

0

0

0.132655

  B = [−0.00550808 −0.096

(cid:62)

0.867345]

 

(cid:35)

.

Case 1: P ∗ =

  Case 2: P ∗ =

(cid:34) 6.0173

(cid:34)23.7658

16.8959 0.0937
16.8959 18.4645 0.1014
0.0937
1.0107

0.1014

5.6702
−0.0071 −0.0067

(cid:62)
and Ru = Rv = I  Σ0 = 0.03 · I. We choose Q = I and C = [0.00951892  0.0038373  0.001]
(cid:62) for Case 2. By direct
for Case 1; while Q = 0.01 · I and C = [0.00951892  0.0038373  0.2]
calculation  we have that
5.6702 −0.0071
5.4213 −0.0067
0.0102
Thus  one can easily check that Rv − C(cid:62)P ∗C > 0 is satisﬁed for both Case 1 and Case 2  i.e. 
Assumption 2.1 i) holds. However  for Case 1  λmin(Q − (L∗)(cid:62)RvL∗) = 0.8739 > 0 satisﬁes
Assumption 2.1 ii); for Case 2  λmin(Q − (L∗)(cid:62)RvL∗) = −0.0011 < 0 fails to satisfy it.
In both settings  we evaluate the convergence performance of not only our nested-gradient methods 
but also two types of their variants  alternating-gradient (AG) and gradient-descent-ascent (GDA)
methods. AG methods are based on the nested-gradient methods  but at each outer-loop iteration 
the inner-loop gradient-based updates only perform a ﬁnite number of iterations  instead of con-
verging to the exact solution K(Lt) as nested-gradient methods  which follows the idea in [28].
The GDA methods perform policy gradient descent for the minimizer and ascent for the maximizer
simultaneously. Detailed updates of these two types of methods are deferred to §D.
Figure 1 shows that for Case 1  our nested-gradient methods indeed enjoy the global convergence to
the NE. The cost C(K(L)  L) monotonically increases to that at the NE  and the convergence rate of
natural NG sits between that of the other two NG methods. Also  we note that the convergence rates

of gradient mapping square in (b) are linear  which are due to (c) that λ((cid:101)QL) is always positive along

the iteration  i.e.  the projection is not effective. This way  our convergence results follow from the
local convergence rates in Theorem 5.3  although the initialization is random (global). We have also
shown in Figure 2 that even without Assumption 2.1 ii)  i.e.  in Case 2  all the PO methods mentioned
successfully converge to the NE  although the cost sequences do not converge monotonically. This
motivates us to provide theory for other policy optimization methods  also for more general settings
of LQ games. We note that no projection was imposed when implementing these algorithms in all
our experiments  which justiﬁes that the projection here is just for the purpose of theoretical analysis.
In fact  we have not found an instance of zero-sum LQ games that makes the projections effective
as the algorithms proceed. This motivates the theoretical study of projection-free algorithms in our
future work. More simulation results can be found in §D.

(cid:35)

(cid:35)

8

(a) C(K(L)  L)

(b) Grad. Mapp. Norm Square

(c) λmin((cid:101)QL)

Figure 1: Performance of the three projected NG methods for Case 1 where Assumption 2.1 ii)
is satisﬁed. (a) shows the monotone convergence of the expected cost C(K(L)  L) to the NE cost
C(K∗  L∗); (b) shows the convergence of the gradient mapping norm square; (c) shows the change

of the smallest eigenvalue of (cid:101)QL = Q − L(cid:62)RvL.

(a) Nested-Gradient

(c) Gradient-Descent-Ascent
Figure 2: Convergence of the cost for Case 2 where Assumption 2.1 ii) is not satisﬁed. (a)  (b)  and
(c) show convergence of the NG  AG  and GDA methods  respectively.

(b) Alternating-Gradient

7 Concluding Remarks

This paper has developed policy optimization methods  speciﬁcally  projected nested-gradient meth-
ods  to solve for the Nash equilibria of zero-sum LQ games. Despite the nonconvexity-nonconcavity
of the problem  the gradient-based algorithms have been shown to converge to the NE with globally
sublinear and locally linear rates. This work appears to be the ﬁrst one showing that policy opti-
mization methods can converge to the NE of a class of zero-sum Markov games  with ﬁnite-iteration
analyses. Interesting simulation results have demonstrated the superior convergence property of our
algorithms  even without the projection operator  and that of the gradient-descent-ascent algorithms
with simultaneous updates of both players  even when Assumption 2.1 ii) is relaxed. Based on both
the theory and simulation  future directions include convergence analysis for the setting under a re-
laxed version of Assumption 2.1  and that for the projection-free versions of the algorithms  which
we believe can be done by the techniques in our recent work [22]. Besides  developing policy opti-
mization methods for general-sum LQ games is another interesting yet challenging future direction.

Acknowledgements
K. Zhang and T. Ba¸sar were supported in part by the US Army Research Laboratory (ARL) Coop-
erative Agreement W911NF-17-2-0196  and in part by the Ofﬁce of Naval Research (ONR) MURI
Grant N00014-16-1-2710. Z. Yang was supported by Tencent PhD Fellowship.

References

[1] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press 

2018.

9

0204060801.141.161.181.21.221.241.261.28Nash EquilibriumGauss-NewtonNaturalGradient02040608010-1510-1010-5100Gauss-NewtonNaturalGradient0204060800.880.90.920.940.960.98Gauss-NewtonNaturalGradient0204060800.330.3320.3340.3360.3380.340.342Nash EquilibriumGauss-NewtonNaturalGradient0204060800.340.360.380.40.420.440.46Nash EquilibriumGauss-NewtonNaturalGradient01020304050600.350.360.370.380.39Nash EquilibriumGauss-NewtonNaturalGradient[2] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van
Den Driessche  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanc-
tot  et al. Mastering the game of Go with deep neural networks and tree search. Nature 
529(7587):484–489  2016.

[3] David Silver  Julian Schrittwieser  Karen Simonyan  Ioannis Antonoglou  Aja Huang  Arthur
Guez  Thomas Hubert  Lucas Baker  Matthew Lai  Adrian Bolton  et al. Mastering the game
of Go without human knowledge. Nature  550(7676):354–359  2017.

[4] OpenAI. Openai ﬁve. https://blog.openai.com/openai-five/  2018.

[5] Oriol Vinyals  Igor Babuschkin  Junyoung Chung  Michael Mathieu  Max Jaderberg  Woj-
ciech M. Czarnecki  Andrew Dudzik  Aja Huang  Petko Georgiev  Richard Powell  Timo
Ewalds  Dan Horgan  Manuel Kroiss  Ivo Danihelka  John Agapiou  Junhyuk Oh  Valentin
Dalibard  David Choi  Laurent Sifre  Yury Sulsky  Sasha Vezhnevets  James Molloy  Trevor
Cai  David Budden  Tom Paine  Caglar Gulcehre  Ziyu Wang  Tobias Pfaff  Toby Pohlen 
Yuhuai Wu  Dani Yogatama  Julia Cohen  Katrina McKinney  Oliver Smith  Tom Schaul  Tim-
othy Lillicrap  Chris Apps  Koray Kavukcuoglu  Demis Hassabis  and David Silver. AlphaS-
tar: Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/
alphastar-mastering-real-time-strategy-game-starcraft-ii/  2019.

[6] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Informa-

tion Processing Systems  pages 1008–1014  2000.

[7] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

[8] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust
region policy optimization. In International Conference on Machine Learning  pages 1889–
1897  2015.

[9] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap 
Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In International conference on machine learning  pages 1928–1937  2016.

[10] Brendan O’Donoghue  Remi Munos  Koray Kavukcuoglu  and Volodymyr Mnih. Combining

policy gradient and Q-learning. arXiv preprint arXiv:1611.01626  2016.

[11] Richard S Sutton  David A McAllester  Satinder P Singh  and Yishay Mansour. Policy gradi-
ent methods for reinforcement learning with function approximation. In Advances in Neural
Information Processing Systems  pages 1057–1063  2000.

[12] Sham M Kakade. A natural policy gradient. In Advances in Neural Information Processing

Systems  pages 1531–1538  2002.

[13] Matteo Papini  Damiano Binaghi  Giuseppe Canonaco  Matteo Pirotta  and Marcello Restelli.

Stochastic variance-reduced policy gradient. arXiv preprint arXiv:1806.05618  2018.

[14] Kaiqing Zhang  Alec Koppel  Hao Zhu  and Tamer Ba¸sar. Global convergence of policy gra-

dient methods to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383  2019.

[15] Tamer Ba¸sar and Pierre Bernhard. H∞ Optimal Control and Related Minimax Design Prob-

lems: A Dynamic Game Approach. Springer Science & Business Media  2008.

[16] Pablo Hernandez-Leal  Michael Kaisers  Tim Baarslag  and Enrique Munoz de Cote. A sur-
vey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint
arXiv:1707.09183  2017.

[17] Katta G Murty and Santosh N Kabadi. Some NP-complete problems in quadratic and nonlinear

programming. Mathematical Programming  39(2):117–129  1987.

[18] Robert S Chen  Brendan Lucier  Yaron Singer  and Vasilis Syrgkanis. Robust optimization for
non-convex objectives. In Advances in Neural Information Processing Systems  pages 4705–
4714  2017.

10

[19] David Balduzzi  Sebastien Racaniere  James Martens  Jakob Foerster  Karl Tuyls  and Thore
In International Conference on

Graepel. The mechanics of n-player differentiable games.
Machine Learning  pages 363–372  2018.

[20] Eric Mazumdar and Lillian J Ratliff. On the convergence of competitive  multi-agent gradient-

based learning. arXiv preprint arXiv:1804.05464  2018.

[21] Leonard Adolphs  Hadi Daneshmand  Aurelien Lucchi  and Thomas Hofmann. Local saddle

point optimization: A curvature exploitation approach. 2019.

[22] Kaiqing Zhang  Bin Hu  and Tamer Ba¸sar. Policy optimization for H2 linear control with
H∞ robustness guarantee: Implicit regularization and global convergence. arXiv preprint
arXiv:1910.09496  2019.

[23] David Jacobson. Optimal stochastic linear systems with exponential performance criteria and
IEEE Transactions on Automatic control 

their relation to deterministic differential games.
18(2):124–131  1973.

[24] Peter Whittle. Risk-sensitive linear/quadratic/Gaussian control. Advances in Applied Proba-

bility  13(4):764–777  1981.

[25] Maryam Fazel  Rong Ge  Sham Kakade  and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. In International Conference on Machine
Learning  pages 1467–1476  2018.

[26] Dhruv Malik  Ashwin Pananjady  Kush Bhatia  Koulik Khamaru  Peter L Bartlett  and Martin J
Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic
systems. arXiv preprint arXiv:1812.08305  2018.

[27] Stephen Tu and Benjamin Recht. The gap between model-based and model-free methods on
the linear quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565 
2018.

[28] Maher Nouiehed  Maziar Sanjabi  Jason D Lee  and Meisam Razaviyayn.

class of non-convex min-max games using iterative ﬁrst order methods.
arXiv:1902.08297  2019.

Solving a
arXiv preprint

[29] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In

International Conference on Machine Learning  pages 157–163  1994.

[30] Michail G Lagoudakis and Ronald Parr. Value function approximation in zero-sum Markov

games. In Conference on Uncertainty in Artiﬁcial Intelligence  pages 283–292  2002.

[31] Vincent Conitzer and Tuomas Sandholm. Awesome: A general multiagent learning algorithm
that converges in self-play and learns a best response against stationary opponents. Machine
Learning  67(1-2):23–43  2007.

[32] Julien Pérolat  Bilal Piot  Bruno Scherrer  and Olivier Pietquin. On the use of non-stationary
strategies for solving two-player zero-sum markov games. In Conference on Artiﬁcial Intelli-
gence and Statistics  2016.

[33] Kaiqing Zhang  Zhuoran Yang  Han Liu  Tong Zhang  and Tamer Ba¸sar. Finite-sample analyses
for fully decentralized multi-agent reinforcement learning. arXiv preprint arXiv:1812.02783 
2018.

[34] Shaofeng Zou  Tengyu Xu  and Yingbin Liang. Finite-sample analysis for SARSA and Q-

learning with linear function approximation. arXiv preprint arXiv:1902.02234  2019.

[35] Asma Al-Tamimi  Frank L Lewis  and Murad Abu-Khalaf. Model-free Q-learning designs
for linear discrete-time zero-sum games with application to H-inﬁnity control. Automatica 
43(3):473–481  2007.

[36] Lerrel Pinto  James Davidson  Rahul Sukthankar  and Abhinav Gupta. Robust adversarial
reinforcement learning. In International Conference on Machine Learning  pages 2817–2826 
2017.

11

[37] Michael Bowling and Manuela Veloso. Rational and convergent learning in stochastic games.
In International Joint Conference on Artiﬁcial Intelligence  volume 17  pages 1021–1026 
2001.

[38] Bikramjit Banerjee and Jing Peng. Adaptive policy gradient in multiagent learning. In Confer-

ence on Autonomous Agents and Multiagent Systems  pages 686–692. ACM  2003.

[39] Julien Pérolat  Bilal Piot  and Olivier Pietquin. Actor-critic ﬁctitious play in simultaneous
move multistage games. In International Conference on Artiﬁcial Intelligence and Statistics 
pages 919–928  2018.

[40] Sriram Srinivasan  Marc Lanctot  Vinicius Zambaldi  Julien Pérolat  Karl Tuyls  Rémi Munos 
and Michael Bowling. Actor-critic policy optimization in partially observable multiagent en-
vironments. In Advances in Neural Information Processing Systems  pages 3422–3435  2018.

[41] Ashish Cherukuri  Bahman Gharesifard  and Jorge Cortes. Saddle-point dynamics: Condi-
tions for asymptotic stability of saddle points. SIAM Journal on Control and Optimization 
55(1):486–511  2017.

[42] Hassan Raﬁque  Mingrui Liu  Qihang Lin  and Tianbao Yang. Non-convex min-max
arXiv preprint

optimization: Provable algorithms and applications in machine learning.
arXiv:1810.02060  2018.

[43] Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient de-
scent in min-max optimization. In Advances in Neural Information Processing Systems  pages
9236–9246  2018.

[44] Panayotis Mertikopoulos  Houssam Zenati  Bruno Lecouat  Chuan-Sheng Foo  Vijay Chan-
drasekhar  and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. In International Conference on Learning Representations  2019.

[45] Eric V Mazumdar  Michael I Jordan  and S Shankar Sastry. On ﬁnding local Nash equilibria
(and only local Nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838  2019.

[46] Chi Jin  Praneeth Netrapalli  and Michael I Jordan. Minmax optimization: Stable limit points

of gradient descent ascent are locally optimal. arXiv preprint arXiv:1902.00618  2019.

[47] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochre-
iter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In
Advances in Neural Information Processing Systems  pages 6626–6637  2017.

[48] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable.

In Advances in Neural Information Processing Systems  pages 5585–5595  2017.

[49] Songtao Lu  Rahul Singh  Xiangyi Chen  Yongxin Chen  and Mingyi Hong. Understand the

dynamics of GANs via primal-dual optimization. 2018.

[50] Paulina Grnarova  Kﬁr Y Levy  Aurelien Lucchi  Thomas Hofmann  and Andreas Krause. An
online learning approach to generative adversarial networks. arXiv preprint arXiv:1706.03269 
2017.

[51] Qihang Lin  Mingrui Liu  Hassan Raﬁque  and Tianbao Yang. Solving weakly-convex-weakly-
concave saddle-point problems as weakly-monotone variational inequality. arXiv preprint
arXiv:1810.10207  2018.

[52] Anton A Stoorvogel and Arie JTM Weeren. The discrete-time Riccati equation related to the

H∞ control problem. IEEE Transactions on Automatic Control  39(3):686–691  1994.

[53] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course  volume 87.

Springer Science & Business Media  2013.

[54] Coralia Cartis  Nicholas IM Gould  and Ph L Toint. On the complexity of steepest descent 
Newton’s and regularized Newton’s methods for nonconvex unconstrained optimization prob-
lems. SIAM Journal on Optimization  20(6):2833–2852  2010.

12

[55] Coralia Cartis  Nick IM Gould  and Philippe L Toint. Worst-case evaluation complexity
and optimality of second-order methods for nonconvex smooth optimization. arXiv preprint
arXiv:1709.07180  2017.

[56] Koulik Khamaru and Martin J Wainwright. Convergence guarantees for a class of non-convex

and non-smooth optimization problems. arXiv preprint arXiv:1804.09629  2018.

[57] Huibert Kwakernaak and Raphael Sivan. Linear Optimal Control Systems  volume 1. Wiley-

Interscience New York  1972.

[58] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control  volume 1. Athena Scien-

tiﬁc Belmont  MA  2005.

[59] Steven G Krantz and Harold R Parks. The Implicit Function Theorem: History  Theory  and

Applications. Springer Science & Business Media  2012.

[60] Eugene E Tyrtyshnikov. A Brief Introduction to Numerical Analysis. Springer Science &

Business Media  2012.

[61] David Jacobson. On values and strategies for inﬁnite-time linear quadratic games.

Transactions on Automatic Control  22(3):490–491  1977.

IEEE

[62] Jan R Magnus and Heinz Neudecker. Matrix differential calculus with applications to simple 
Hadamard  and Kronecker products. Journal of Mathematical Psychology  29(4):474–492 
1985.

[63] Michail M Konstantinov  P Hr Petkov  and Nicolai D Christov. Perturbation analysis of the

discrete Riccati equation. Kybernetika  29(1):18–29  1993.

[64] Ji-Guang Sun. Perturbation theory for algebraic Riccati equations. SIAM Journal on Matrix

Analysis and Applications  19(1):39–65  1998.

[65] Alexander Graham. Kronecker Products and Matrix Calculus with Applications. Courier

Dover Publications  2018.

13

,Kaiqing Zhang
Zhuoran Yang