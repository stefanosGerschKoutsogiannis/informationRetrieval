2016,Learning Additive Exponential Family Graphical Models via $\ell_{2 1}$-norm Regularized M-Estimation,We investigate a subclass of exponential family graphical models of which the sufficient statistics are defined by arbitrary additive forms. We propose two $\ell_{2 1}$-norm regularized maximum likelihood estimators to learn the model parameters from i.i.d. samples. The first one is a joint MLE estimator which estimates all the parameters simultaneously. The second one is a node-wise conditional MLE estimator which estimates the parameters for each node individually. For both estimators  statistical analysis shows that under mild conditions the extra flexibility gained by the additive exponential family models comes at almost no cost of statistical efficiency. A Monte-Carlo approximation method is developed to efficiently optimize the proposed estimators. The advantages of our estimators over Gaussian graphical models and Nonparanormal estimators are demonstrated on synthetic and real data sets.,Learning Additive Exponential Family Graphical
Models via ℓ2 1-norm Regularized M-Estimation

Xiao-Tong Yuan† Ping Li‡§ Tong Zhang‡ Qingshan Liu† Guangcan Liu†

†B-DAT Lab  Nanjing University of Info. Sci.&Tech.

‡Depart. of Statistics and §Depart. of Computer Science  Rutgers University

Nanjing  Jiangsu  210044  China

Piscataway  NJ  08854  USA

{xtyuan qsliu  gcliu}@nuist.edu.cn  {pingli tzhang}@stat.rutgers.edu

Abstract

We investigate a subclass of exponential family graphical models of which the
sufﬁcient statistics are deﬁned by arbitrary additive forms. We propose two ℓ2;1-
norm regularized maximum likelihood estimators to learn the model parameters
from i.i.d. samples. The ﬁrst one is a joint MLE estimator which estimates all
the parameters simultaneously. The second one is a node-wise conditional MLE
estimator which estimates the parameters for each node individually. For both
estimators  statistical analysis shows that under mild conditions the extra ﬂexibil-
ity gained by the additive exponential family models comes at almost no cost of
statistical efﬁciency. A Monte-Carlo approximation method is developed to efﬁ-
ciently optimize the proposed estimators. The advantages of our estimators over
Gaussian graphical models and Nonparanormal estimators are demonstrated on
synthetic and real data sets.

1 Introduction

∑

s∈V

P(X; θ) ∝ exp

∑

 .

As an important class of statistical models for exploring the interrelationship among a large number
of random variables  undirected graphical models (UGMs) have enjoyed popularity in a wide range
of scientiﬁc and engineering domains  including statistical physics  computer vision  data mining 
⊤ be a p-dimensional random vector with each
and computational biology. Let X = [X1  ...  Xp]
variable Xi taking values in a set X . Suppose G = (V  E) is an undirected graph consists of a set
of vertices V = {1  ...  p} and a set of unordered pairs E representing edges between the vertices.
The pairwise UGMs over X corresponding to G can be written as the following exponential family
distribution:

θsφs(Xs) +

(s;t)∈E

θstϕst(Xs  Xt)

(1)

In such a pairwise model  (Xs  Xt) are conditionally independent (given the rest of the variables)
if and only if the weight θst is zero. The most popular instances of pairwise UGMs are Gaussian
graphical models (GGMs) [19  2] for real-valued random variables and Ising (or Potts) models [15]
for binary or ﬁnite nominal discrete random variables. More broadly  in order to derive multivari-
ate graphical models from univariate exponential family distributions (such as the Gaussian  bino-
mial/multinomial  Poisson  exponential distributions  etc.)  the exponential family graphical models
(EFGMs) [27  21] were proposed as a uniﬁed framework to learn UGMs with node-wise conditional
distributions arising from generalized linear models (GLMs).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

1.1 Overview of contribution

is

that

issue

arises

in UGMs

to specify sufﬁcient

i.e. 
A fundamental
{φs(Xs)  ϕst(Xs  Xt)}  for modeling the interactions among variables.
It is noteworthy that
most prior pairwise UGMs use pairwise product of variables (or properly transformed variables)
as pairwise sufﬁcient statistics [16  11  27]. This is clearly restrictive in modern data analysis
tasks where the underlying pairwise interactions among variables are more often than not highly
complex and unknown a prior. The goal of this work is to remove such a restriction and explore
the feasibility (in theory and practice) of deﬁning sufﬁcient statistics in an additive formulation to
approximate the underlying unknown sufﬁcient statistics. To this end  we consider the following
Additive Exponential Family Graphical Model (AdEFGM) distribution with joint density function:

statistics 

∑

s∈V

∑

  

(2)

(3)

P(X; f ) = exp

∫

{∑

fs(Xs) +

(s;t)∈E

∑

fst(Xs  Xt) − A(f )

}

X p exp

s∈V fs(Xs) +

where fs : X → R and fst(· ·) : X 2 → R are respectively node-wise and pairwise statistics  and
dX is the log-partition function.
A(f ) := log
We require the condition A(f ) < ∞ holds so that the deﬁnition of probability is valid. In this paper 
we assume the formulations of sufﬁcient statistics fs and fst are unknown but they admit linear
representations over two sets of pre-ﬁxed basis functions {φk(·)  k = 1  2  ...  q} and {ϕl(· ·)  l =
q∑
1  2  ...  r}  respectively. That is 

(s;t)∈E fst(Xs  Xt)

r∑

fs(Xs) =

θs;kφk(Xs) 

fst(Xs  Xt) =

θst;lϕl(Xs  Xt) 

k=1

l=1

where q and r are the truncation order parameters. In the formulation (3)  the choice of basis and
their sizes is ﬂexible and task-dependent. For instance  if the mapping functions fs and fst are
periodic  then we can choose {φk(·)} as 1-D Fourier basis and {ϕl(· ·)} as 2-D Fourier basis. As
another instance  the basis {ϕl} can be chosen as multiple kernels which are commonly used in
computer vision tasks. Specially  when q = r = 1  ϕl(Xs  Xt) = XsXt and φk(Xs) is ﬁxed as
certain parametric function  AdEFGM reduces to the standard EFGM [27  21]. In general cases  by
imposing an additive structure on the sufﬁcient statistics fs and fst  AdEFGM is expected to be able
to capture more complex interactions among variables beyond pairwise product.
As the core contribution of this paper  we propose two ℓ2;1-norm regularized maximum likelihood
estimation (MLE) estimators to learn the weights of AdEFGM in high dimensional settings. The
√
ﬁrst estimator is formulated as an ℓ2;1-norm regularized MLE to jointly estimate all the parameters
in the model. The second estimator is formulated as an ℓ2;1-norm regularized node-wise conditional
√
MLE to estimate the parameters associated with each node individually. Theoretically  we prove that
(2|E| + p) ln p/n)
under mild conditions the joint MLE estimator achieves convergence rate O((
where |E| while the node-wise conditional estimator achieves convergence rate O(
(d + 1) ln p/n)
in which d is the degree of the underlying graph G. Computationally  we propose a Monte-Carlo
approximation scheme to efﬁciently optimize the estimators via proximal gradient descent methods.
We conduct numerical studies on simulated and real data to support our claims. The simulation
results conﬁrm that  when the data are drawn from an underlying UGMs with highly nonlinear suf-
ﬁcient statistics  our estimators signiﬁcantly outperform GGMs and Nonparanormal [10] estimators
in most cases. The experimental results on a stock price data show that our estimators are able to
recover more accurate category links among stocks than GMMs and Nonparanormal estimators.

1.2 Related work

In order to model random variables beyond parametric UGMs such as GGMs and Ising models  re-
searchers recently investigated semi-parametric/nonparametric extensions of these parametric mod-
els. The Nonparanormal [11] and copula-based methods [5] are semi-parametric graphical models
which assume that data is Gaussian after applying a monotone transformation. More broadly  one
could learn transformations of the variables and then ﬁt any parametric UGMs (like EFGMs) over
the transformed variables. In [10  26]  two rank-based estimators were used to estimate correlation
matrix and then ﬁt the GGMs. In [24]  a semi-parametric method was proposed to ﬁt the conditional

2

means of the features with an arbitrary additive formulation. The Semi-EFGM proposed in [28] is
a semi-parametric rank-based conditional estimator for exponential family graphical models. In [1] 
a kernel method was proposed for learning the structure of graphical models by treating variables
as Gaussians in a mapped high-dimensional feature space. In [7]  Gu proposed a functional min-
imization framework to estimate the nonparametric model (1) over a Reproducing Hilbert Kernel
Space (RKHS). Nonparametric exponential family graphical models based on score matching loss
were investigated in [9  20]. The forest density estimation [8] is a fully nonparametric method for
estimating UGMs with structure restricted to be a forest.
In contrast to all these existing semi-
parametric/nonparametric models  our approach is novel in model deﬁnition and computation: we
impose a simple additive structure on sufﬁcient statistics to describe complex interactions between
variables and use Monte-Carlo approximation to estimate the intractable normalization constant for
efﬁcient optimization.

∑

1.3 Notation and organization
Notation Let θ = {θs;k  θst;l : s ∈ V  k = 1  2  ..    (s  t) ∈ V 2  s ̸= t  l = 1  2  ...} be a vector
of parameters associated with AdEFGM and G = {{(s  k)}k {(st  l)}l : s ∈ V  (s  t) ∈ V 2  s ̸=
t} be a group induced by the additive structures of nodes and edges. We conventionally deﬁne
the following grouped-norm related notations: ∥θ∥2;1 =
g∈G ∥θg∥  ∥θ∥2;∞ = maxg∈G ∥θg∥ 
supp(θ G) = {g ∈ G : ∥θg∥ ̸= 0} and ∥θ∥2;0 = |supp(θ G)|. For any S ⊆ G  these notations can
be deﬁned restrictively over θS. We denote ¯S = G \ S the complement of S in G.
Organization. The remaining of this paper is organized as follows: In §2  we present two maximum
likelihood estimators for learning the model parameters of AdEFGM. The statistical guarantees of
the proposed estimators are analyzed in §3. Monte-Carlo simulations and experimental results on
real stock price data are presented in §4. Finally  we conclude this paper in §5. Due to space limit 
all the technical proofs of theoretical results are deferred to an appendix section which is included
in the supplementary material.

2 ℓ2;1-norm Regularized MLE for AdEFGM

In this section  we investigate the problem of estimating the parameters of AdEFGM in high dimen-
sional settings. By substituting (3) into (2)  the distribution of an AdEFGM can be converted to the
following form:
where θ = {θs;k  θst;l}  and

P(X; θ) = exp{B(X; θ) − A(θ)}  
∑

∑

∫

(4)

B(X; θ) :=

θs;kφk(Xs) +

θst;lϕl(Xs  Xt)  A(θ) := log

exp{B(X; θ)} dX.

X p

s∈V;k

(s;t)∈E;l

Suppose we have n i.i.d. samples Xn = {X (i)}n
parameters θ

∗:

i=1 drawn from the following AdEFGM with true

P(X; θ

∗

) = exp{B(X; θ

∗

) − A(θ

∗

)} .

(5)
∗ from the ob-
An important goal of graphical model learning is to estimate the true parameters θ
served data Xn. The more accurate parameter estimation is  the more accurate we are able to recover
the underlying true graph structure. We next propose two ℓ2;1-norm regularized maximum likelihood
estimation (MLE) methods for joint and node-conditional learning of parameters  respectively.

2.1 Joint MLE estimation
Given the sample set Xn = {X (i)}n

i=1  the negative log-likelihood of the joint distribution (5) is:

It is trivial to verify L(θ; Xn) has the following ﬁrst order derivative (see  e.g.  [25]):

∂L
∂θs;k

= E(cid:18)[φk(Xs)] − 1

n

n∑

i=1

L(θ; Xn) = − 1
n∑

n

φk(X (i)

s ) 

i=1

B(X (i); θ) + A(θ).

∂L
∂θst;l

= E(cid:18)[ϕl(Xs  Xt)] − 1

n

3

n∑

i=1

ϕl(X (i)

s   X (i)

t )  (6)

where the expectation E(cid:18)[·] is taken over the joint distribution (2). Also  it is well known that
L(θ; Xn) is convex in θ.
In order to estimate the parameters which are expected to be sparse in edge level due to the potential
sparse structure of graph  we consider the following ℓ2;1-norm regularized MLE estimator:

∑

(∑

ˆθn = arg min

)1=2

(cid:18)

{L(θ; Xn) + λn∥θ∥2;1}  
∑

(∑

)1=2

where ∥θ∥2;1 =
is the ℓ2;1-norm with
respect to the basis statistics and λn > 0 is the regularization strength parameter dependent on n.
The ℓ2;1-norm penalty is used to promote edgewise sparsity as the graph structure is expected to be
sparse in high dimensional settings.

(s;t)∈V 2;s̸=t

q
k=1 θ2
s;k

r
l=1 θ2

s∈V

st;l

+

(7)

2.2 Node-conditional MLE estimation

Recent state of the art methods for learning UGMs suggest a natural procedure deriving multivariate
graphical models from univariate distributions [12  15  27]. The common idea in these methods is
to learn the graph structure by estimating node-neighborhoods  or by ﬁtting the node-conditional
distribution of each individual node conditioned on the rest of the nodes. Indeed  these node-wise
ﬁtting methods have been shown to have strong statistical guarantees and attractive computational
performance.
Inspired by these approaches  we propose an alternative estimator to estimate the
weights of sufﬁcient statistics associated with each individual node. With a slight abuse of notation 
we denote θs a subvector of θ associated with node s  i.e. 

where N (s) is the neighborhood of s. Given the joint distribution (4)  it is easy to show that the
conditional distribution of Xs given the rest variables  X\s  is written by:

θs := {θs;k | k = 1  ...  q} ∪ {θst;l | t ∈ N (s)  l = 1  ...  r} 
}
C(Xs | X\s; θs) − D(X\s; θs)

P(Xs | X\s; θs) = exp

{

 

log

{

X exp

}
∫
where C(Xs | X\s; θs) :=
C(Xs | X\s; θs)
note that the condition A(θ) < ∞ for the joint log-partition function implies D(X\s; θs) < ∞.
In order to estimate the parameters associated with a node  we consider using the sparsity regularized
conditional maximum likelihood estimation. Given n independent samples Xn drawn from (5)  we
can write the negative log-likelihood of the conditional distribution as:

t∈N (s);l θst;lϕl(Xs  Xt)  and D(X\s; θs) :=
dXs is the log-partition function which ensures normalization. We

k θs;kφk(Xs) +

(8)

∑

∑

It is standard that ˜L(θs; Xn) is convex with respect to θs and it has the following ﬁrst-order derivative:

}

.

| X (i)\s ; θs) + D(X (i)\s ; θs)
}

s ) + E(cid:18)s[φk(Xs) | X (i)\s ]

s   X (i)

t ) + E(cid:18)s [ϕl(Xs  X (i)

 

}
t ) | X (i)\s ]

(9)

 

{
−C(X (i)

s

n∑

i=1

˜L(θs; Xn) =

1
n

n∑
n∑

i=1

i=1

{
−φk(X (i)
{
−ϕl(X (i)
{
(∑

∑

(cid:18)s

∂ ˜L(θs; Xn)

∂θs;k

∂ ˜L(θs; Xn)

∂θst;l

=

=

1
n

1
n

(∑

)1=2

where the expectation E(cid:18)s[· | X\s] is taken over the node-wise conditional distribution (8).
Let us consider the following ℓ2;1-norm regularized conditional MLE formulation associated with
the variable Xs:

}

ˆθn
s = arg min

˜L(θs; Xn) + λn∥θs∥2;1

 

(10)

where ∥θs∥2;1 =
is the grouped ℓ2;1-norm with respect to
the node-wise and pairwise basis associated with s and λn > 0 controls the regularization strength.

q
k=1 θ2
s;k

r
l=1 θ2

t̸=s

st;l

+

)1=2

4

2.3 Computation via Monte-Carlo approximation

We consider using proximal gradient descent methods [22] to solve the composite optimization
problems in (7) and (10). For both estimators  the major computational overhead is to iteratively
calculate the expectation terms involved in the gradients ∇L(θ; Xn) and ∇ ˜L(θs; Xn). In general 
these expectation terms have no close-form for exact calculation and sampling methods such as im-
portance sampling and MCMC are usually needed for approximate estimation. There are  however 
two challenging issues with such a sampling based optimization procedure: (1) the multivariate
sampling methods typically suffer from high computational cost even when the dimensionality p
is moderately large; and (2) the non-vanishing sampling error of gradient will accumulate during
the iteration which according to the results in [18] will deteriorate the overall convergence perfor-
mance. Obviously  the main source of these challenges is caused by the intractable log-partition
terms appeared in the estimators.
To more efﬁciently apply the ﬁrst-order methods without suffering from iterative sampling and error
accumulation  it is a natural idea to replace the log-partition terms by a Monte-Carlo approximation
and minimize the resulting approximated formulation. Taking the joint estimator (7) as an example 
we resort to the basic importance sampling method to approximately estimate the log-partition term
j=1 drawn
A(θ) = log
from a random vector Y ∈ X p with known probability density P(Y ). Given θ  an importance
sampling estimate of exp{A(θ)} is given by
exp{ ˆA(θ; Ym)} =
{

∫
X p exp{B(X; θ)} dX. Assume we have m i.i.d. samples Ym = {Y (j)}m

We consider the following Monte-Carlo approximation to the estimator (7):

B(Y (j); θ)
P(Y (j))

m∑

}

{

}

1
m

exp

j=1

.

ˆL(θ; Xn  Ym) + λn∥θ∥2;1

 

(11)

ˆˆθn = arg min

(cid:18)

∑

n

n

where ˆL(θ; Xn  Ym) = − 1
i=1 B(X (i); θ) + ˆA(θ; Ym). Since the random samples Ym are ﬁxed
in (11)  the sampling operation can be avoided in the computation of ∇ ˆL(θ; Xn  Ym). Concerning
the accuracy of the approximate estimator (11)  the following result guarantees that  with high prob-
√
ability  the minimizer of the approximate estimator (11) is suboptimal to the population estimator (7)
with suboptimality O(1/
m). A proof of this proposition is provided in A.1 (see the supplementary
)
material).
Proposition 1. Assume that P(Y ) > 0. Then the following inequality holds with high probability:
ˆˆθn} + exp{− ˆA(ˆθn; Ym)}

exp{−A(

2.58ˆσ

ˆˆθn; Xn)+λn∥ˆˆθn∥2;1 ≤ L(ˆθn; Xn)+λn∥ˆθn∥2;1 +

L(

 

√
m

(
)2

where ˆσn = 1
m

m
j=1

exp{B(Y (j);^(cid:18)n)}

P(Y (j))

− exp{ ˆA(ˆθn; Ym)}

.

(

∑

A similar Monte-Carlo approximation strategy can be applied to the node-wise MLE estimator (10).

3 Statistical Analysis

In this section  we provide statistical guarantees on parameter estimation error for the joint MLE
estimator (7) and the node-conditional estimator (10).
In large picture  our analysis follows the
techniques presented in [13  30] by specifying the conditions under which these techniques can be
applied to our setting.

3.1 Analysis of the joint estimator

We are interested in the concentration bounds of the random variables deﬁned by

Zs;k := φk(Xs) − E(cid:18)(cid:3)[φk(Xs)]  Zst;l := ϕl(Xs  Xt) − E(cid:18)(cid:3)[ϕl(Xs  Xt)] 

5

where the expectation E(cid:18)(cid:3) [·] is taken over the underlying true distribution (5). By the “law of the
unconscious statistician” we have E[Zs;k] = E[Zst;l] = 0. That is  {Zs;k} and {Zst;l} are zero-
mean random variables. We introduce the following technical condition on {Zs;k  Zst;l} which we
will show to guarantee the gradient ∇L(θ
; Xn) vanishes exponentially fast  with high probability 
as sample size increases.
{
Assumption 1. For all (s  k) and all (s  t  l)  we assume that there exist constants σ > 0 and ζ > 0
such that for all |η| ≤ ζ 

}

{

}

∗

σ2η2/2

.

E[exp{ηZs;k}] ≤ exp

σ2η2/2

  E[exp{ηZst;l}] ≤ exp

This assumption essentially imposes an exponential-type bound on the moment generating function
of the random variables Zs;k  Zst;l.
It is well known that the Hessian ∇2L(θ; Xn) is positive semideﬁnite at any θ and it is independent
on the sample set Xn. We also need the following condition which guarantees the restricted positive
deﬁniteness of ∇2L(θ; Xn) over certain low dimensional subspace when θ is in the vicinity of θ
∗.
;G). There exist
∗
Assumption 2 (Locally Restricted Positive Deﬁnite Hessian). Let S = supp(θ
constants δ > 0 and β > 0 such that for any θ ∈ {∥θ− θ
⊤∇2L(θ; Xn)ϑ ≥
∗∥ ≤ δ}  the inequality ϑ
β∥ϑ∥2 holds for any ϑ ∈ CS := {∥θ (cid:22)S
∥2;1 ≤ 3∥θS∥2;1}.
Assumption 2 requires that the Hessian ∇2L(θ; Xn) is positive deﬁnite in the cone CS when θ lies
∗. This condition is a speciﬁcation of the concept restricted strong
in a local ball centered at θ
convexity [30] to AdEFGM.
Remark 1 (Minimal Representation). We say an AdEFGM has minimal representation if there is a
unique parameter vector θ associated with the distribution (4). This condition equivalently requires
that there exists no non-zero ϑ such that B(X; ϑ) is equal to an absolute constant. This implies that
for any θ and for all non-zero ϑ 

Var(cid:18) [B(X; ϑ)] = ϑ
If AdEFGM has minimal representation at θ
δ > 0 and β > 0 such that for any θ ∈ {∥θ − θ
Assumption 2 holds true when AdEFGM has minimal representation at θ

⊤∇2L(θ; Xn)ϑ > 0.
∗∥ ≤ δ}  ϑ

∗  then there must exist sufﬁciently small constants
⊤∇2L(θ; Xn)ϑ ≥ β∥ϑ∥2. Therefore 

∗.

{

n > max

The following theorem is our main result on the estimation error of the joint MLE estimator (7). A
proof of this result is provided in Appendix A.2 in the supplementary material.
Theorem 1. Assume that the conditions in Assumption 1 and Assumption 2 hold. If sample size n
satisﬁes

6 max{q  r} ln p
 
 
√
then with probability at least 1 − 2 max{q  r}p
−1  the following inequality holds:
6 max{q  r}∥θ∗∥2;0 ln p/n.
−1σ
√
∗∥ vanishes at the order of O(

Remark 2. The main message Theorem 1 conveys is that when n is sufﬁciently large  the estimation
max{q  r}(2|E| + p) ln p/n) with high probability.
error ∥ˆθn − θ
This convergence rate matches the results obtained in [17  16] for GGMs and the results in [10  26]
for Nonparanormal.

0σ2 max{q  r}∥θ

∗∥ ≤ 3c0β

∗∥2;0 ln p

∥ˆθn − θ

}

σ2ζ 2

54c2

δ2β2

3.2 Analysis of the node-conditional estimator

s

∗
s

− θ

For the node-conditional estimator (10)  we study the rate of convergence of the parameter estima-
tion error ∥ˆθn
∥ as a function of sample size n. We need Assumption 1 and the following
assumption in our analysis.
s ;G). There exist constants ˜δ > 0 and ˜β > 0 such
∗
Assumption 3. For any node s  let S = supp(θ
∇2 ˜L(θs; Xn)ϑs ≥ ˜β∥ϑs∥2 holds for any
that for any θs ∈ {∥θs − θ
ϑs ∈ ˜CS := {∥(θs) (cid:22)S

∥ < ˜δ}  the inequality ϑ

∥2;1 ≤ 3∥(θs)S∥2;1}.

⊤
s

∗
s

6

216˜c2

∗
s

}

{

s

n > max

σ2ζ 2

∥. A proof of this result is provided in Appendix A.3 in the supplementary material.

The following is our main result on the convergence rate of node-conditional estimation error ∥ˆθn
∗
θ
s
Theorem 2. Assume that the conditions in Assumption 1 and Assumption 3 hold. If sample size n
satisﬁes

−

s

∥2;0 ln p

0 ˜σ2 max{q  r}∥θ

6 max{q  r} ln p
 
 
√
then with probability at least 1 − 4 max{q  r}p
−2  the following inequality holds:
6 max{q  r}∥θ∗
−1σ
˜β

∥2;0 ln p/n.
√
Remark 3. Theorem 2 indicates that with overwhelming probability 
the estimation error
∥ˆθn
∥ = O(
∗
(d + 1) ln p)/n) where d is the degree of the underlying graph  i.e.  d =
∥2;0 − 1. We may combine the parameter estimation errors from all the nodes as
maxs∈V ∥θ
∗
s
s
Indeed  by Theorem 2 and union of probability we get that
a global measurement of accuracy.
−θ
maxs∈V ∥ˆθn
−1. This
estimation error bound matches those for GGMs with neighborhood-selection-type estimators [29].

∥ˆθn
− θ
√
(d + 1) ln p/n) holds with probability at least 1−4 max{q  r}p

∥ ≤ 6˜c0

∗
s

∥ = O(

∗
s

− θ

s

δ2 ˜β2

s

s

4 Experiments

This section is devoted to showing the actual learning performance of AdEFGM. We ﬁrst investigate
graph structure recovery accuracy using simulation data (for which we know the ground truth)  and
then we apply our method to a stock price data for inferring the statistical dependency among stocks.

4.1 Monte-Carlo simulation

This is a proof-of-concept experiment. The purpose is to conﬁrm that when the pairwise interactions
of the underlying graphical models are highly nonlinear and unknown a prior  our additive estimator
will be signiﬁcantly superior to existing parametric/semi-parametric graphical models for inferring
the structure of graphs. The numerical results of AdEFGM reported in this experiment are obtained
by the joint MLE estimator in (7).
Simulated data Our simulation study employs a graphical model of which the edges are generated
independently with probability P . We will consider the model under different levels of sparsity
by adjusting the probability P . For simplicity purpose  we assume fs(Xs) ≡ 1 and consider a
nonlinear pairwise interaction function fst(Xs  Xt) = cos(π(Xs − Xt)/5). We ﬁt the data to
the additive model (4) with a 2-D Fourier basis of size 8. Using Gibbs sampling  we generate a
training sample of size n from the true graphical model  and an independent sample of the same
size from the same distribution for tuning the strength parameter λn. We compare performance for
n = 200  varying values of p ∈ {50  100  150  200  250  300}  and different sparsity levels under
P = {0.02  0.05  0.1}  replicated 10 times for each conﬁguration.
Baselines We compare the performance of our estimator to Graphical Lasso [6] as a GGM estimator
and SKEPTIC [10] as a Nonparanormal estimator.
In our implementation  we use a version of
SKEPTIC with Kendall’s tau to infer the correlation.
Evaluation metric To evaluate the support recovery performance  we use the standard F-score from
the information retrieval literature. The larger F-score is  the better the support recovery perfor-
mance. The numerical values over 10
Results Figure 1 shows the support recovery F-scores of the considered methods on the synthetic
data. From this group of results we can observe that by using 2-D Fourier basis to approximate the
unknown cosine distance function  AdEFGM is able to more accurately recover the underlying graph
structure than the other two considered methods. The advantage of AdEFGM illustrated here is as
expected because it is designed to automatically learn the unknown complex pairwise interactions
while GGM and Nonparanormal are restrictive to certain UGMs with known sufﬁcient statistics.

−3 in magnitude are considered to be nonzero.

4.2 Stock price data

We further study the performance of AdEFGM on a stock price data. This data contains the historical
prices of S&P500 stocks over 5 years  from January 1  2008 to January 1  2013. By taking out the

7

Figure 1: Simulated data: Support recovery F-score curves. Left panels: P = 0.02  Middle panels:
P = 0.05  Right panels: P = 0.1.

Figure 2: Stock price data S&P500: Category link precision  recall and F-score curves.

stocks with less than 5 years of history  we end up with 465 stocks  each having daily closing
prices over 1 260 trading days. The prices are ﬁrst adjusted for dividends and splits and the used
to calculate daily log returns. Each day’s return can be represented as a point in R465. To apply
AdEFGM to this data  we consider the general model (4) with the 2-D Fourier basis being used to
approximate the pairwise interaction between stocks Xs and Xt. Since the category information of
S&P500 is available  we measure the performance by Precision  Recall and F-score of the top k
links (edges) on the constructed graph. A link is regarded as true if and only if it connects two nodes
belonging to the same category. We use the joint MLE estimator for this experiment. Figure 2 shows
the curves of precision  recall and F-score as functions of k in a wide range [103  105]. It is apparent
that AdEFGM signiﬁcantly outperforms GGM and Nonparanormal for identifying correct category
links. This result suggests that the interactions among the S&P500 stocks are highly nonlinear.

5 Conclusions

In this paper  we proposed and analyzed AdEFGMs as a generic class of additive undirected graphi-
cal models. By expressing node-wise and pairwise sufﬁcient statistics as linear representations over
a set of basis statistics  AdEFGM is able to capture complex interactions among variables which
are not uncommon in modern engineering applications. We investigated two types of ℓ2;1-norm
regularized MLE estimators for joint and node-conditional high dimensional estimation. Based on
our theoretical justiﬁcation and empirical observation  we can draw the following two conclusions:
1) the ℓ2;1-norm regularized AdEFGM learning is a powerful tool for inferring pairwise exponen-
tial family graphical models with unknown arbitrary sufﬁcient statistics; and 2) the extra ﬂexibility
gained by AdEFGM comes at almost no cost of statistical and computational efﬁciency.

Acknowledgments

Xiao-Tong Yuan and Ping Li were partially supported by NSF-Bigdata-1419210  NSF-III-1360971 
ONR-N00014-13-1-0764  and AFOSR-FA9550-13-1-0137. Xiao-Tong Yuan is also partially sup-
ported by NSFC-61402232  NSFC-61522308  and NSFJP-BK20141003. Tong Zhang is supported
by NSF-IIS-1407939 and NSF-IIS-1250985. Qingshan Liu is supported by NSFC-61532009.
Guangcan Liu is supported by NSFC-61622305  NSFC-61502238 and NSFJP-BK20160040.

8

5010015020025030000.20.40.60.81Dimension pRecovery F−scoreCosine Distance AdEFGMGGMNonparanormal5010015020025030000.20.40.60.81Dimension pRecovery F−scoreCosine Distance AdEFGMGGMNonparanormal5010015020025030000.20.40.60.81Dimension pRecovery F−scoreCosine Distance AdEFGMGGMNonparanormal246810x 10400.20.40.60.81Number of linksLink Precision AdEFGMGGMNonparanormal246810x 10400.20.40.60.81Number of linksLink Recall AdEFGMGGMNonparanormal246810x 10400.20.40.6Number of linksLink F−score AdEFGMGGMNonparanormalReferences
[1] F. Bach and M. Jordan. Learning graphical models with mercer kernels.

Annual Conference on Neural Information Processing Systems (NIPS’02)  2002.

In Proceedings of the 16th

[2] O. Banerjee  L. E. Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood

estimation for multivariate gaussian or binary data. JMLR  9:485–516  2008.

[3] R. G. Baraniuk  M. Davenport  R. A. DeVore  and M. Wakin. A simple proof of the restricted isometry

property for random matrices. Constructive Approximation  28(3):253–263  2008.

[4] E. J. Candès  Y. C. Eldarb  D. Needella  and P. Randallc. Compressedsensing with coherent and redun-

dantdictionarie. Applied and Computational Harmonic Analysis  31(1):59–73  2011.

[5] A. Dobra and A. Lenkoski. Copula gaussian graphical models and their application to modeling functional

disability data. The Annals of Applied Statistics  5:969–993  2011.

[6] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.

[7] C. Gu  Y. Jeon  and Y. Lin. Nonparametric density estimation in high-dimensions. Statistica Sinica 

[8] J. Lafferty  H. Liu  and L. Wasserman. Sparse nonparametric graphical models. Statistical Science 

Biostatistics  9(3):432–441  2008.

23:1131–1153  2013.

27(4):519–537  2012.

[9] L. Lin  M. Drton  A. Shojaie  et al. Estimation of high-dimensional graphical models using regularized

score matching. Electronic Journal of Statistics  10(1):806–854  2016.

[10] H. Liu  F. Han  M. Yuan  J. Lafferty  and L. Wasserman. High dimensional semiparametric gaussian

copula graphical models. Annals of Statistics  40(4):2293–2326  2012.

[11] H. Liu  J. Lafferty  and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimen-

sional undirected graphs. Journal of Machine Learning Research  10:2295–2328  2009.

[12] N. Meinshausen and P. Bühlmann. High-dimensional graphs and variable selection with the lasso. Annals

of Statistics  34(3):1436–1462  2006.

[13] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Statistical Science  27(4):538–557  2012.

[14] A. B. Owen. Monte Carlo theory  methods and examples. 2013.
[15] P. Ravikumar  M. Wainwright  and J. Lafferty. High-dimensional ising model selection using l1-

regularized logistic regression. Annals of Statistics  38(3):1287–1319  2010.

[16] P. Ravikumar  M. J. Wainwright  G. Raskutti  and B. Yu. High-dimensional covariance estimation by
minimizing ℓ1-penalized log-determinant divergence. Electronic Journal of Statistics  5:935–980  2011.
[17] A. J. Rothman  P. J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance estimation.

Electronic Journal of Statistics  2:494–515  2008.

[18] M. Schmidt  N. L. Roux  and F. R. Bach. Convergence rates of inexact proximal-gradient methods for
convex optimization. In Proceedings of the 25th Annual Conference on Neural Information Processing
Systems (NIPS’11)  pages 1458–1466  2011.

[19] T. P. Speed and H. T. Kiiveri. Gaussian markov distributions over ﬁnite graphs. Annals of Statistics 

14:138–150  1986.

[20] S. Sun  M. Kolar  and J. Xu. Learning structured densities via inﬁnite dimensional exponential families. In
Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS’15)  2015.
[21] W. Tansey  O. H. M. Padilla  A. S. Suggala  and P. Ravikumar. Vector-space markov random ﬁelds
In Proceedings of the 32nd International Conference on Machine Learning

via exponential families.
(ICML’15)  pages 684–692  2015.

Journal of Optimization  2008.

abs/1011.3027  2011.

101(1):85–101  2014.

[22] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted to SIAM

[23] R. Vershynin.

Introduction to the non-asymptotic analysis of random matrices.

CoRR  arxiv:

[24] A. Voorman  A. Shojaie  and D. Witten. Graph estimation with joint additive models. Biometrika 

[25] M. Wainwright and M. Jordan. Graphical models  exponential families  and variational inference. Foun-

dations and Trends in Machine Learning  1(1-2):1–305  2008.

[26] L. Xue and H. Zou. Regularized rank-based estimation of high-dimensional nonparanormal graphical

models. Annals of Statistics  40(5):2541–2571  2012.

[27] E. Yang  P. Ravikumar  G. I. Allen  and Z. Liu. Graphical models via univariate exponential family

distributions. Journal of Machine Learning Research  16:3813–3847  2015.

[28] Z. Yang  Y. Ning  and H. Liu. On semiparametric exponential family graphical models. arXiv preprint

arXiv:1412.8697  2014.

[29] M. Yuan. High dimensional inverse covariance matrix estimation via linear programming. Journal of

[30] C.-H. Zhang and T. Zhang. A general framework of dual certiﬁcate analysis for structured sparse recovery

Machine Learning Research  11:2261–2286  2010.

problems. CoRR  arxiv: abs/1201.3302  2012.

9

,Xiaotong Yuan
Ping Li
Tong Zhang
Guangcan Liu