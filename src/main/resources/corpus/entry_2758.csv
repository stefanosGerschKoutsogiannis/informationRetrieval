2008,Kernel Change-point Analysis,We introduce a kernel-based method for change-point analysis within a sequence of temporal observations. Change-point analysis of an (unlabelled) sample of observations consists in  first  testing whether a change in the distribution occurs within the sample  and second  if a change occurs  estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution. We propose a test statistics based upon the maximum kernel Fisher discriminant ratio as a measure of homogeneity between segments. We derive its limiting distribution under the null hypothesis (no change occurs)  and establish the consistency under the alternative hypothesis (a change occurs). This allows to build a statistical hypothesis testing procedure for testing the presence of change-point  with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting. If a change actually occurs  the test statistics also yields an estimator of the change-point location. Promising experimental results in temporal segmentation of mental tasks from BCI data and pop song indexation are presented.,Kernel Change-point Analysis

Za¨ıd Harchaoui

LTCI  TELECOM ParisTech and CNRS

46  rue Barrault  75634 Paris cedex 13  France

zaid.harchaoui@enst.fr

Francis Bach

Willow Project  INRIA-ENS

45  rue d’Ulm  75230 Paris  France
francis.bach@mines.org

´Eric Moulines

LTCI  TELECOM ParisTech and CNRS

46  rue Barrault  75634 Paris cedex 13  France

eric.moulines@enst.fr

Abstract

We introduce a kernel-based method for change-point analysis within a sequence
of temporal observations. Change-point analysis of an unlabelled sample of obser-
vations consists in  ﬁrst  testing whether a change in the distribution occurs within
the sample  and second  if a change occurs  estimating the change-point instant
after which the distribution of the observations switches from one distribution to
another different distribution. We propose a test statistic based upon the maximum
kernel Fisher discriminant ratio as a measure of homogeneity between segments.
We derive its limiting distribution under the null hypothesis (no change occurs) 
and establish the consistency under the alternative hypothesis (a change occurs).
This allows to build a statistical hypothesis testing procedure for testing the pres-
ence of a change-point  with a prescribed false-alarm probability and detection
probability tending to one in the large-sample setting. If a change actually occurs 
the test statistic also yields an estimator of the change-point location. Promising
experimental results in temporal segmentation of mental tasks from BCI data and
pop song indexation are presented.

1 Introduction

The need to partition a sequence of observations into several homogeneous segments arises in many
applications  ranging from speaker segmentation to pop song indexation. So far  such tasks were
most often dealt with using probabilistic sequence models  such as hidden Markov models [1]  or
their discriminative counterparts such as conditional random ﬁelds [2]. These probabilistic models
require a sound knowledge of the transition structure between the segments and demand careful
training beforehand to yield competitive performance; when data are acquired online  inference in
such models is also not straightforward (see  e.g.  [3  Chap. 8]). Such models essentially perform
multiple change-point estimation  while one is often also interested in meaningful quantitative mea-
sures for the detection of a change-point within a sample.

When a parametric model is available to model the distributions before and after the change  a com-
prehensive literature for change-point analysis has been developed  which provides optimal criteria
from the maximum likelihood framework  as described in [4]. Nonparametric procedures were also
proposed  as reviewed in [5]  but were limited to univariate data and simple settings. Online coun-
terparts have also been proposed and mostly built upon the cumulative sum scheme (see [6] for
extensive references). However  so far  even extensions to the case where the distribution before the
change is known  and the distribution after the change is not known  remains an open problem [7].
This brings to light the need to develop statistically grounded change-point analysis algorithms 
working on multivariate  high-dimensional  and also structured data.

1

We propose here a regularized kernel-based test statistic  which allows to simultaneously provide
quantitative answers to both questions: 1) is there a change-point within the sample? 2) if there is
one  then where is it? We prove that our test statistic for change-point analysis has a false-alarm prob-
ability tending to α and a detection probability tending to one as the number of observations tends
to inﬁnity. Moreover  the test statistic directly provides an accurate estimate of the change-point
instant. Our method readily extends to multiple change-point settings  by performing a sequence of
change-point analysis in sliding windows running along the signal. Usually  physical considerations
allow to set the window-length to a sufﬁciently small length for being guaranteed that at most one
change-point occurs within each window  and sufﬁciently large length for our decision rule to be
statistically signiﬁcant (typically n > 50).
In Section 2  we set up the framework of change-point analysis  and in Section 3  we describe how
to devise a regularized kernel-based approach to the change-point problem. Then  in Section 4
and in Section 5  we respectively derive the limiting distribution of our test statistic under the null
hypothesis H0 : ”no change occurs“  and establish the consistency in power under the alternative
HA : ”a change occurs“. These theoretical results allow to build a test statistic which has provably a
false-alarm probability tending to a prescribed level α  and a detection probability tending to one  as
the number of observations tends to inﬁnity. Finally  in Section 7  we display the performance of our
algorithm for respectively  segmentation into mental tasks from BCI data and temporal segmentation
of pop songs.

2 Change-point analysis

In this section  we outline the change-point problem  and describe formally a strategy for building
change-point analysis test statistics.

Change-point problem
change-point analysis of the sample {X1  . . .   Xn} consists in the following two steps.

Let X1  . . .   Xn be a time series of independent random variables. The

1) Decide between

H0 :
HA :

PX1 = ··· = PXk = ··· = PXn
there exists 1 < k⋆ < n such that
PX1 = ··· = PXk⋆ 6= PXk⋆+1 = ··· = PXn .
2) Estimate k⋆ from the sample {X1  . . .   Xn} if HA is true .

(1)

While sharing many similarities with usual machine learning problems  the change-point problem is
different.

Statistical hypothesis testing An important aspect of the above formulation of the change-
point problem is its natural embedding in a statistical hypothesis testing framework. Let us re-
call brieﬂy the main concepts in statistical hypothesis testing  in order to rephrase them within
the change-point problem framework (see  e.g.  [8]). The goal is to build a decision rule to
answer question 1) in the change-point problem stated above. Set a false-alarm probability α
with 0 < α < 1 (also called level or Type I error)  whose purpose is to theoretically guar-
antee that P(decide HA  when H0 is true) is close to α. Now  if there actually is a change-
point within the sample  one would like not to miss it  that is that the detection probability
π = P(decide HA  when HA is true)—also called power and equal to one minus the Type II
error—should be close to one. The purpose of Sections 4-5 is to give theoretical guarantees to those
practical requirements in the large-sample setting  that is when the number of observations n tends
to inﬁnity.

Running maximum partition strategy An efﬁcient strategy for building change-point analysis
procedures is to select the partition of the sample which yields a maximum heterogeneity between
the two segments: given a sample {X1  . . .   Xn} and a candidate change point k with 1 < k < n 
assume we may compute a measure of heterogeneity ∆n k between the segments {X1  . . .   Xk} on
the one hand  and {Xk+1  . . .   Xn} on the other hand. Then  the “running maximum partition strat-
egy” consists in using max1<k<n ∆n k as a building block for change-point analysis (cf. Figure 1).
Not only max1<k<n ∆n k may be used to test for the presence of a change-point and assess/discard

2

P(ℓ)

P(r)

















































n

1

k

k⋆

Figure 1: The running maximum strategy for change-point analysis. The test statistic for change-
point analysis runs a candidate change-point k with 1 < k < n along the sequence of observations 
hoping to catch the true change-point k⋆.

the overall homogeneity of the sample; besides  ˆk = argmax1<k<n∆n k provides a sensible estima-
tor of the true change-point instant k⋆ [5].

3 Kernel Change-point Analysis

In this section  we describe how the kernel Fisher discriminant ratio  which has proven relevant for
measuring the homogeneity of two samples in [9]  may be embedded into the running maximum par-
tition strategy to provide a powerful test statistic  coined KCpA for Kernel Change-point Analysis 
for addressing the change-point problem. This is described in the operator-theoretic framework 
developed for the statistical analysis of kernel-based learning and testing algorithms in [10  11].

Reproducing kernel Hilbert space
Let (X   d) be a separable measurable metric space. Let
X be an X -valued random variable  with probability measure P; the expectation with respect to
P is denoted by E[·] and the covariance by Cov(· ·). Consider a reproducing kernel Hilbert space
(RKHS) (H h· ·iH) of functions from X to R. To each point x ∈ X   there corresponds an element
Φ(x) ∈ H such that hΦ(x)  fiH = f (x) for all f ∈ H  and hΦ(x)  Φ(y)iH = k(x  y)  where
k : X × X → R is a positive deﬁnite kernel [12]. In the following  we exclusively work with the
Aronszajn-map  that is  we take Φ(x) = k(x ·) for all x ∈ X . It is assumed from now on that
H is a separable Hilbert space. Note that this is always the case if X is a separable metric space
and if the kernel is continuous [13]. We make the following two assumptions on the kernel (which
are satisﬁed in particular for the Gaussian kernel; see [14]): (A1) the kernel k is bounded  that is
sup(x y)∈X ×X k(x  y) < ∞  (A2) for all probability distributions P on X   the RKHS associated
with k(· ·) is dense in L2(P).
Kernel Fisher Discriminant Ratio
independent observations
X1  . . .   Xn ∈ X . For any [i  j] ⊂ {2  . . .   n − 1}  deﬁne the corresponding empirical mean el-
ements and covariance operators as follows

Consider a sequence of

ˆµi:j :=

1

j − i + 1

k(Xℓ ·)  

ˆΣi:j :=

1

j − i + 1

j

Xℓ=i

j

Xℓ=i

{k(Xℓ ·) − ˆµi:j} ⊗ {k(Xℓ ·) − ˆµi:j} .

These quantities have obvious population counterparts  the population mean element and the pop-
ulation covariance operator  deﬁned for any probability measure P as hµP  fiH := E[f (X)] for
all f ∈ H  and hf  ΣPgiH := CovP[f (X)  g(X)] for f  g ∈ H. For all k ∈ {2  . . .   n − 1} the
(maximum) kernel Fisher discriminant ratio  which we abbreviate as KFDR is deﬁned as

KFDRn k;γ(X1  . . .   Xn) :=

n

n

ˆΣ1:k +

n − k

k(n − k)

ˆΣk+1:n + γI(cid:19)−1/2
n2}  then with KFDRn1+n2 n1+1;γ(X1  . . .   Xn1   X ′

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:18) k

n

(ˆµk+1:n − ˆµ1:k)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Note that  if we merge two labelled samples {X1  . . .   Xn1} and {X ′
n2} into a single sample
as {X1  . . .   Xn1   X ′
n2 ) we re-
cover the test statistic considered in [9] for testing the homogeneity of two samples {X1  . . .   Xn1}
and {X ′
n2}.

1  . . .   X ′

1  . . .   X ′

1  . . .   X ′

1  . . .   X ′

2

H

.

3

Following [9]  we make the following assumptions on all the covariance operators Σ considered in
(Σ) < ∞  (B2) there are inﬁnitely

this paper: (B1) the eigenvalues {λp(Σ)}p≥1 satisfyP∞
many strictly positive eigenvalues {λp(Σ)}p≥1 of Σ.
Kernel change-point analysis
Now  we may apply the strategy described before (cf. Figure 1)
to obtain the main building block of our test statistic for change-point analysis. Indeed  we deﬁne
our test statistic Tn k;γ as

p=1 λ1/2

p

KFDRn k;γ − d1 n k;γ( ˆΣW
n k)

 

an<k<bn

Tn;γ(k) := max

√2 d2 n k;γ( ˆΣW
n k)
n k := k ˆΣ1:k + (n− k) ˆΣk+1:n. The quantities d1 n k;γ( ˆΣW
n k) := Tr{( ˆΣW

n k + γI)−1 ˆΣW

n k}  

where n ˆΣW
respectively as
d1 n k;γ( ˆΣW

n k) and d2 n k;γ( ˆΣW

n k)  deﬁned

n k)2}  
act as normalizing constants for Tn;γ(k) to have zero-mean and unit-variance as n tends to inﬁnity 
a standard statistical transformation known as studentization. The maximum is searched within the
interval [an  bn] with an > 1 and bn < n  which is restriction of ]1  n[  in order to prevent the
test statistic from uncontrolled behaviour in the neighborhood of the interval boundaries  which is
standard practice in this setting [15].

n k) := Tr{( ˆΣW

n k + γI)−2( ˆΣW

d2 n k;γ( ˆΣW

Remark
Note that  if the input space is Euclidean  for instance X = Rd  and if the kernel is linear
k(x  y) = xT y  then Tn;γ(k) may be interpreted as a regularized version of the classical maximum-
likelihood multivariate test statistic used to test change in mean with unequal covariances  under the
assumption of normal observations  described in [4  Chap. 3]. Yet  as the next section shall show 
our test statistic is truly nonparametric  and its large-sample properties do not require any “gaussian
in the feature space”-type of assumption. Moreover  in practice it may be computed thanks to the
kernel trick  adapted to the kernel Fisher discriminant analysis and outlined in [16  Chapter 6].

False-alarm and detection probability
In order to build a principled testing procedure  a proper
theoretical analysis from a statistical point of view is necessary. First  as the next section shows  for a
prescribed α  we may build a procedure which has  as n tends to inﬁnity  the false-alarm probability
α under the null hypothesis H0  that is when the sample is completely homogeneous and contains
no-change-point. Besides  when the sample actually contains at most one change-point  we prove
that our test statistic is able to catch it with probability one as n tends to inﬁnity.

Large-sample setting
For the sake of generality  we describe here the large-sample setting for
the change-point problem under the alternative hypothesis HA. Essentially  it corresponds to nor-
malizing the signal sampling interval to [0  1] and letting the resolution increase as we observe more
data points [4].
Assume there is 0 < k⋆ < n such that PX1 = ··· = PXk⋆ 6= PXk⋆+1 = ··· = PXn. Deﬁne
τ ⋆ := k⋆/n such that τ ⋆ ∈]0  1[  and deﬁne P(ℓ) the probability distribution prevailing within the
left segment of length τ ⋆  and P(r) the probability distribution prevailing within the right segment
of length 1 − τ ⋆. Then  we want to study what happens if we have ⌊nτ ⋆⌋ observations from P(ℓ)
(before change) and ⌊n(1 − τ ⋆)⌋ observations from P(r) (after change) where n is large and τ ⋆ is
kept ﬁxed.

4 Limiting distribution under the null hypothesis
Throughout this section  we work under the null hypothesis H0 that is PX1 = ··· = PXk = ··· =
PXn for all 2 ≤ k ≤ n. The ﬁrst result gives the limiting distribution of Tn;γ(k) as the number of
observations n tends to inﬁnity.
Before stating the theoretical results  let us describe informally the crux of our approach. We may
prove  under H0  using operator-theoretic pertubation results similar to [9]  that it is sufﬁcient to
study the large-sample behaviour of ˜Tn;γ(k) := maxan<k<bn(√2 d2;γ(Σ))−1Qn ∞;γ(k) where

Qn ∞;γ(k) :=

k(n − k)

n

(Σ + γI)−1/2 (ˆµk+1:n − ˆµ1:k)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)

4

2

H − d1;γ(Σ)  

1 < k < n  

(2)

and d1;γ(Σ) and d2;γ(Σ) are respectively the population recentering and rescaling quantities with
1:n the within-class covariance operator. Note that the only remaining stochastic
Σ = Σ1:n = ΣW
term in (2) is ˆµk+1:n − ˆµ1:k. Let us expand (2) onto the eigenbasis {λp  ep}p≥1 of the covariance
operator Σ  as follows:

(λp + γ)−1(cid:26) k(n − k)
i=1 λ−1/2

n

hµk+1:n − µ1:k  epi2 − λp(cid:27)  
(ep(Xi) − E[ep(X1)])  we may rewrite Qn ∞;γ(k) as
n S1:n p  which yields

1 < k < n .

(3)

Then  deﬁning S1:k p := n−1/2Pk
an inﬁnite-dimensional quadratic form in the tied-down partial sums S1:k p − k
− 1)  

(λp + γ)−1λp( n2

k(n − k)(cid:18)S1:k p −

S1:n p(cid:19)2

Qn ∞;γ(k) =

k
n

∞

p

1 < k < n .

(4)

Qn ∞;γ(k) =

∞

Xp=1

Xp=1

The idea is to view {Qn ∞;γ(k)}1<k<n as a stochastic process  that is a random function [k 7→
Qn ∞;γ(k  ω)] for any ω ∈ Ω  where (Ω F  P) is a probability space. Then  invoking the so-
called invariance principle in distribution [17]  we realize that the random sum S1:⌊nt⌋ p(ω)  which
for all ω linearly interpolates between the values S1:i/n p(ω) at points i/n for i = 1  . . .   n  be-
haves  asymptotically as n tends to inﬁnity  like a Brownian motion (also called Wiener process)
{Wp(t)}0<t<1. Hence  along each component ep  we may deﬁne a Brownian bridge {Bp(t)}0<t<1 
that is a tied-down brownian motion Bp(t) := Wp(t) − tWp(1) which yields continuous approx-
imation in distribution of the corresponding {S1:k p − k
n S1:n p}1<k<n. The proof (omitted due to
space limitations) consists in deriving a functional (noncentral) limit theorem for KFDRn k;γ  and
then applying a continuous mapping argument.

Proposition 1 Assume (A1) and (B1)  and that H0 holds  that is PXi = P for all 1 ≤ i ≤ n.
Assume in addition that the regularization parameter γ is held ﬁxed as n tends to inﬁnity  and that
an/n → u > 0 and bn/n → v < 1 as n tends to inﬁnity. Then 

∞

1

u<t<v

p(t)

λp(Σ)

λp(Σ) + γ B2

t(1 − t) − 1!  

Q∞;γ(t) :=

√2d2;γ(Σ)

Tn;γ(k) D−→ sup

Xp=1
where {λp(Σ)}p≥1 is the sequence of eigenvalues of the overall covariance operator Σ  while
{Bp(t)}p≥1 is a sequence of independent brownian bridges.
Deﬁne t1−α as the (1− α)-quantile of supu<t<v Q∞;γ(t). We may compute t1−α either by Monte-
Carlo simulations  as described in [18]  or by bootstrap resampling under the null hypothesis (see).
The next result proves that  using the limiting distribution under the null stated above  we may build
a test statistic with prescribed false-alarm probability α for large n.
Corollary 2 The test maxan<k<bn Tn γ(k) ≥ t1−α(Σ  γ) has false-alarm probability α  as n tends
to inﬁnity.

Besides  when the sequence of regularization parameters {γn}n≥1 decreases to zero slowly enough
(in particular slower than n−1/2)  the test statistic maxan<k<bn Tn γn(k) turns out to be asymptot-
ically kernel-independent as n tends to inﬁnity. While the proof hinges upon martingale functional
limit theorems [17]  still  we may point out that if we replace γ by γn in the limiting null distribution 
then Q∞;γ(·) is correctly normalized for all n ≥ 1 to have zero-mean and variance one.
Proposition 3 Assume (A1) and (B1-B2) and that H0 holds  that is PXi = P for all 1 ≤ i ≤ n.
Assume in addition that the regularization parameters {γn}n≥1 is such that

γn +

d1 n;γn(Σ)
d2 n;γn(Σ)

γ−1
n n−1/2 → 0  

and that an/n → u > 0 and bn/n → v < 1 as n tends to inﬁnity. Then 
.
pt(1 − t)

Tn;γn(k) D−→ sup

an<k<bn

B(t)

max

u<t<v

5

Remark
A closer look at Proposition 1 brings to light that the reweighting by t(1 − t) of the
squared brownian bridges on each component is crucial for our test statistic to be immune against
imbalance between segment lengths under the alternative HA  that is when τ ⋆ is far from 1/2.
Indeed  swapping out the reweighting by t(1− t)  to simply consider the corresponding unweighted
test statistic is hazardous  and yields a loss of power for alternatives when τ ⋆ is far from 1/2.
This section allowed us get an α-level test statistic for the change-point problem  by looking at the
large-sample behaviour of the test statistic under the null hypothesis H0. The next step is to prove
that the test statistic is consistent in power  that is the detection probability tends to one as n tends
to inﬁnity under the alternative hypothesis HA.

5 Consistency in power

This section shows that  when the alternative hypothesis HA holds  our test statistic is able to detect
presence of a change with probability one in the large-sample setting. The next proposition is proved
within the same framework as the one considered in the previous section  except that now  along each
component ep  one has to split the random sum into three parts [1  k]  [k + 1  k⋆]  [k⋆ + 1  n]  and
then the large-sample behaviour of each projected random sum is the one of a two-sided Brownian
motion with drifts.

Proposition 4 Assume (A1-A2) and (B1-B2)  and that HA holds  that is there is exists u < τ ⋆ < v
with u > 0 and v < 1 such that PX⌊nτ ⋆ ⌋ 6= PX⌊nτ ⋆ ⌋+1 for all 1 ≤ i ≤ n. Assume in addition that
the regularization parameter γ is held ﬁxed as n tends to inﬁnity  and that limn→∞ an/n > u and
limn→∞ bn/n < v. Then  for any 0 < α < 1  we have

PHA(cid:18) max

an<k<bn

Tn;γ(k) > t1−α(cid:19) → 1  

as n → ∞ .

(5)

6 Extensions and related works

Extensions
It is worthwhile to note that we may also have built similar procedures from the
maximum mean discrepancy (MMD) test statistic proposed by [19]. Note also that  instead of the
Tikhonov-type regularization of the covariance operator  other regularization schemes may also be
applied  such as the spectral truncation regularization of the covariance operator  equivalent to pre-
processing by a centered kernel principal component analysis [20  21]  as used in [22] for instance.

Related works
A related problem is the abrupt change detection problem  explored in [23] 
which is naturally also encompassed by our framework. Here  one is interested in the early de-
tection of a change from a nominal distribution to an erratic distribution. The procedure KCD of
[23] consists in running a window-limited detection algorithm  using two one-class support vector
machines trained respectively on the left and the right part of the window  and comparing the sets
of obtained weights; Their approach differs from our in two points. First  we have the limiting
null distribution of KCpA  which allows to compute decision thresholds in a principled way. Sec-
ond  our test statistic incorporates a reweighting to keep power against alternatives with unbalanced
segments.

7 Experiments

Computational considerations
In all experiments  we set γ = 10−5 and took the Gaussian ker-
nel with isotropic bandwidth set by the plug-in rule used in density estimation. Second  since from k
to k + 1  the test statistic changes from KFDRn k;γ to KFDRn k+1;γ  it corresponds to take into ac-
count the change from {(X1  Y1 = −1)  . . .   (Xk  Yk = −1)  (Xk+1  Yk+1 = +1)  . . .   (Xn  Yn =
+1)} to {(X1  Y1 = −1)  . . .   (Xk  Yk = −1)  (Xk+1  Yk+1 = −1)  (Xk+2  Yk+2 =
+1) . . .   (Xn  Yn = +1)} in the labelling in KFDR [9  16]. This motivates an efﬁcient strategy
for the computation of the test statistic. We compute the matrix inversion of the regularized kernel
gram matrix once for all  at the cost of O(n3)  and then compute all values of the test statistic for all
partitions in one matrix multiplication—in O(n2). As for computing the decision threshold t1−α 
we used bootstrap resampling calibration with 10  000 runs. Other Monte-Carlo based calibration
procedures are possible  but are left for future research.

6

Subject 1

Subject 2

Subject 3

KCpA
SVM

79%
76%

74%
69%

61%
60%

Table 1: Average classiﬁcation accuracy for each subject

Brain-computer interface data
Signals acquired during Brain-Computer Interface (BCI) trial
experiments naturally exhibit temporal structure. We considered a dataset proposed in BCI compe-
tition III1 acquired during 4 non-feedback sessions on 3 normal subjects  where each subject was
asked to perform different tasks  the time where the subject switches from one task to another being
random (see also [24]). Mental tasks segmentation is usually tackled with supervised classiﬁcation
algorithms  which require labelled data to be acquired beforehand. Besides  standard supervised
classiﬁcation algorithms are context-sensitive  and sometimes yield poor performance on BCI data.
We performed a sequence of change-point analysis on sliding windows overlapping by 20% along
the signals. We provide here two ways of measuring the performance of our method. First  in Fig-
ure 2 (left)  we give in the empirical ROC-curve of our test statistic  averaged over all the signals at
hand. This shows that our test statistic yield competitive performance for testing the presence of a
change-point  when compared with a standard parametric multivariate procedure (param) [4]. Sec-
ond  in Table 1  we give experimental results in terms of classiﬁcation accuracy  which proves that
we can reach comparable/better performance as supervised multi-class (one-versus-one) classiﬁca-
tion algorithms (SVM) with our completely unsupervised kernel change-point analysis algorithm.
If each segment is considered as a sample of a given class  then the classiﬁcation accuracy corre-
sponds here to the proportion of correctly assigned points at the end of the segmentation process.
This also clearly shows that KCpA algorithm give accurate estimates of the change-points  since the
change-point estimation error is directly measured by the classiﬁcation accuracy.

r
e
w
o
P

1

0.8

0.6

0.4

0.2

0
 
0

ROC Curve

 

KCpA
param

0.1

0.2

Level

0.3

0.4

0.5

r
e
w
o
P

1

0.8

0.6

0.4

0.2

0
 
0

ROC Curve

 

KCpA
KCD

0.1

0.2

Level

0.3

0.4

0.5

Figure 2: Comparison of ROC curves for task segmentation from BCI data (left)  and pop songs
segmentation (right).

Pop song segmentation
Indexation of music signals aims to provide a temporal segmentation
into several sections with different dynamic or tonal or timbral characteristics. We investigated
the performance of KCpA on a database of 100 full-length “pop music” signals  whose manual
segmentation is available. In Figure 2 (right)  we provide the respective ROC-curves of KCD of [23]
and KCpA. Our approach is indeed competitive in this context.

8 Conclusion

We proposed a principled approach for the change-point analysis of a time-series of independent
observations. It provides a powerful testing procedure for testing the presence of a change in distri-
bution in a sample. Moreover  we saw in experiments that it also allows to accurately estimate the
change-point when a change occurs. We are currently exploring several extensions of KCpA. Since
experimental results are promising on real data  in which the assumption of independence is rather
unrealistic  it is worthwhile to analyze the effect of dependence on the large-sample behaviour of our

1see http://ida.first.fraunhofer.de/projects/bci/competition_iii/

7

test statistic  and explain why the test statistic remains powerful even for (weakly) dependent data.
We are also investigating adaptive versions of the change-point analysis  in which the regularization
parameter γ and the reproducing kernel k are learned from the data.

Acknowledgments

This work has been supported by Agence Nationale de la Recherche under contract ANR-06-BLAN-
0078 KERNSIG.

References
[1] F. De la Torre Frade  J. Campoy  and J. F. Cohn. Temporal segmentation of facial behavior. In

ICCV  2007.

[2] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for

segmenting and labeling sequence data. In Proc. ICML  2001.

[3] O. Capp´e  E. Moulines  and T. Ryden. Inference in Hidden Markov Models. Springer  2005.
[4] J. Chen and A.K. Gupta. Parametric Statistical Change-point Analysis. Birkh¨auser  2000.
[5] M. Cs¨org¨o and L. Horv´ath. Limit Theorems in Change-Point Analysis. Wiley and sons  1998.
[6] M. Basseville and N. Nikiforov. Detection of abrupt changes. Prentice-Hall  1993.
[7] T. L. Lai. Sequential analysis: some classical problems and new challenges. Statistica Sinica 

11  2001.

[8] E. Lehmann and J. Romano. Testing Statistical Hypotheses (3rd ed.). Springer  2005.
[9] Z. Harchaoui  F. Bach  and E. Moulines. Testing for homogeneity with kernel Fisher discrimi-

nant analysis. In Adv. NIPS  2007.

[10] G. Blanchard  O. Bousquet  and L. Zwald. Statistical properties of kernel principal component

analysis. Machine Learning  66  2007.

[11] K. Fukumizu  F. Bach  and A. Gretton. Statistical convergence of kernel canonical correlation

analysis. JLMR  8  2007.

[12] C. Gu. Smoothing Spline ANOVA Models. Springer  2002.
[13] I. Steinwart  D. Hush  and C. Scovel. An explicit description of the rkhs of gaussian RBF

kernels. IEEE Trans. on Inform. Th.  2006.

[14] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  G. R. G. Lanckriet  and B. Sch¨olkopf. Injective

hilbert space embeddings of probability measures. In COLT  2008.

[15] B. James  K. L. James  and D. Siegmund. Tests for a change-point. Biometrika  74  1987.
[16] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Camb. UP  2004.
[17] P. Billingsley. Convergence of Probability Measures (2nd ed.). Wiley Interscience  1999.
[18] P. Glasserman. Monte Carlo Methods in Financial Engineering (1rst ed.). Springer  2003.
[19] A. Gretton  K. Borgwardt  M. Rasch  B. Schoelkopf  and A.J. Smola. A kernel method for the

two-sample problem. In Adv. NIPS  2006.

[20] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press  2002.
[21] G. Blanchard and L. Zwald. Finite-dimensional projection for classiﬁcation and statistical

learning. IEEE Transactions on Information Theory  54(9):4169–4182  2008.

[22] Z. Harchaoui  F. Vallet  A. Lung-Yut-Fong  and O. Capp´e. A regularized kernel-based approach

to unsupervised audio segmentation. In ICASSP  2009.

[23] F. D´esobry  M. Davy  and C. Doncarli. An online kernel change detection algorithm. IEEE

Trans. on Signal Processing  53(8):2961–2974  August 2005.

[24] Z. Harchaoui and O. Capp´e. Retrospective multiple change-point estimation with kernels. In

IEEE Workshop on Statistical Signal Processing (SSP)  2007.

8

,Young Hun Jung
Jack Goetz
Ambuj Tewari