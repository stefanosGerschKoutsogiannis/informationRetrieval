2010,Energy Disaggregation via Discriminative Sparse Coding,Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy  but current electricity meters only report whole-home data. Thus  developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper  we examine a large scale energy disaggregation task  and apply a novel extension of sparse coding to this problem. In particular  we develop a method  based upon structured prediction  for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance. We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.,Energy Disaggregation via Discriminative

Sparse Coding

J. Zico Kolter

Computer Science and

Artiﬁcial Intelligence Laboratory

Massachusetts Institute of Technology

Cambridge  MA 02139

kolter@csail.mit.edu

Siddarth Batra  Andrew Y. Ng
Computer Science Department

Stanford University
Stanford  CA 94305

{sidbatra ang}@cs.stanford.edu

Abstract

Energy disaggregation is the task of taking a whole-home energy signal and sep-
arating it into its component appliances. Studies have shown that having device-
level energy information can cause users to conserve signiﬁcant amounts of en-
ergy  but current electricity meters only report whole-home data. Thus  developing
algorithmic methods for disaggregation presents a key technical challenge in the
effort to maximize energy conservation. In this paper  we examine a large scale
energy disaggregation task  and apply a novel extension of sparse coding to this
problem. In particular  we develop a method  based upon structured prediction 
for discriminatively training sparse coding algorithms speciﬁcally to maximize
disaggregation performance. We show that this signiﬁcantly improves the perfor-
mance of sparse coding algorithms on the energy task and illustrate how these
disaggregation results can provide useful information about energy usage.

Introduction

1
Energy issues present one of the largest challenges facing our society. The world currently consumes
an average of 16 terawatts of power  86% of which comes from fossil fuels [28]; without any effort
to curb energy consumption or use different sources of energy  most climate models predict that the
earth’s temperature will increase by at least 5 degrees Fahrenheit in the next 90 years [1]  a change
that could cause ecological disasters on a global scale. While there are of course numerous facets to
the energy problem  there is a growing consensus that many energy and sustainability problems are
fundamentally informatics problems  areas where machine learning can play a signiﬁcant role.

This paper looks speciﬁcally at the task of energy disaggregation  an informatics task relating to
energy efﬁciency. Energy disaggregation  also called non-intrusive load monitoring [11]  involves
taking an aggregated energy signal  for example the total power consumption of a house as read by
an electricity meter  and separating it into the different electrical appliances being used. Numerous
studies have shown that receiving information about ones energy usage can automatically induce
energy-conserving behaviors [6  19]  and these studies also clearly indicate that receiving appliance-
speciﬁc information leads to much larger gains than whole-home data alone ([19] estimates that
appliance-level data could reduce consumption by an average of 12% in the residential sector). In
the United States  electricity constitutes 38% of all energy used  and residential and commercial
buildings together use 75% of this electricity [28]; thus  this 12% ﬁgure accounts for a sizable
amount of energy that could potentially be saved. However  the widely-available sensors that provide
electricity consumption information  namely the so-called “Smart Meters” that are already becoming
ubiquitous  collect energy information only at the whole-home level and at a very low resolution
(typically every hour or 15 minutes). Thus  energy disaggregation methods that can take this whole-
home data and use it to predict individual appliance usage present an algorithmic challenge where
advances can have a signiﬁcant impact on large-scale energy efﬁciency issues.

1

Energy disaggregation methods do have a long history in the engineering community  including
some which have applied machine learning techniques — early algorithms [11  26] typically looked
for “edges” in power signal to indicate whether a known device was turned on or off; later work
focused on computing harmonics of steady-state power or current draw to determine more complex
device signatures [16  14  25  2]; recently  researchers have analyzed the transient noise of an elec-
trical circuit that occurs when a device changes state [15  21]. However  these and all other studies
we are aware of were either conducted in artiﬁcial laboratory environments  contained a relatively
small number of devices  trained and tested on the same set of devices in a house  and/or used cus-
tom hardware for very high frequency electrical monitoring with an algorithmic focus on “event
detection” (detecting when different appliances were turned on and off). In contrast  in this paper
we focus on disaggregating electricity using low-resolution  hourly data of the type that is readily
available via smart meters (but where most single-device “events” are not apparent); we speciﬁcally
look at the generalization ability of our algorithms for devices and homes unseen at training time;
and we consider a data set that is substantially larger than those previously considered  with 590
homes  10 165 unique devices  and energy usage spanning a time period of over two years.

The algorithmic approach we present in this paper builds upon sparse coding methods and recent
work in single-channel source separation [24  23  22]. Speciﬁcally  we use a sparse coding algorithm
to learn a model of each device’s power consumption over a typical week  then combine these
learned models to predict the power consumption of different devices in previously unseen homes 
using their aggregate signal alone. While energy disaggregation can naturally be formulated as such
a single-channel source separation problem  we know of no previous application of these methods
to the energy disaggregation task. Indeed  the most common application of such algorithm is audio
signal separation  which typically has very high temporal resolution; thus  the low-resolution energy
disaggregation task we consider here poses a new set of challenges for such methods  and existing
approaches alone perform quite poorly.

As a second major contribution of the paper  we develop a novel approach for discriminatively train-
ing sparse coding dictionaries for disaggregation tasks  and show that this signiﬁcantly improves
performance on our energy domain. Speciﬁcally  we formulate the task of maximizing disaggrega-
tion performance as a structured prediction problem  which leads to a simple and effective algorithm
for discriminatively training such sparse representation for disaggregation tasks. The algorithm is
similar in spirit to a number of recent approaches to discriminative training of sparse representations
[12  17  18]. However  these past works were interested in discriminatively training sparse cod-
ing representation speciﬁcally for classiﬁcation tasks  whereas we focus here on discriminatively
training the representation for disaggregation tasks  which naturally leads to substantially different
algorithmic approaches.

2 Discriminative Disaggregation via Sparse Coding
We begin by reviewing sparse coding methods and their application to disaggregation tasks. For con-
creteness we use the terminology of our energy disaggregation domain throughout this description 
but the algorithms can apply equally to other domains. Formally  assume we are given k differ-
ent classes  which in our setting corresponds to device categories such as televisions  refrigerators 
heaters  etc. For every i = 1  . . .   k  we have a matrix Xi ∈ RT ×m where each column of Xi
contains a week of energy usage (measured every hour) for a particular house and for this particular
type of device. Thus  for example  the jth column of X1  which we denote x(j)
1   may contain weekly
energy consumption for a refrigerator (for a single week in a single house) and x(j)
could contain
2
weekly energy consumption of a heater (for this same week in the same house). We denote the
i=1 Xi so that the jth column of ¯X 
¯x(j)  contains a week of aggregated energy consumption for all devices in a given house. At training
time  we assume we have access to the individual device energy readings X1  . . .   Xk (obtained for
example from plug-level monitors in a small number of instrumented homes). At test time  however 
we assume that we have access only to the aggregate signal of a new set of data points ¯X′ (as would
be reported by smart meter)  and the goal is to separate this signal into its components  X′
k.
1  . . .   X′
The sparse coding approach to source separation (e.g.  [24  23])  which forms for the basis for our
disaggregation approach  is to train separate models for each individual class Xi  then use these
models to separate an aggregate signal. Formally  sparse coding models the ith data matrix using the
approximation Xi ≈ BiAi where the columns of Bi ∈ RT ×n contain a set of n basis functions  also
called the dictionary  and the columns of Ai ∈ Rn×m contain the activations of these basis functions

aggregate power consumption over all device types as ¯X ≡ Pk

2

[20]. Sparse coding additionally imposes the the constraint that the activations Ai be sparse  i.e. 
that they contain mostly zero entries  which allows us to learn overcomplete representations of the
data (more basis functions than the dimensionality of the data). A common approach for achieving
this sparsity is to add an ℓ1 regularization penalty to the activations.
Since energy usage is an inherently non-negative quantity  we impose the further constraint that the
activations and bases be non-negative  an extension known as non-negative sparse coding [13  7].
Speciﬁcally  in this paper we will consider the non-negative sparse coding objective

min

Ai≥0 Bi≥0

1
2

kXi − BiAik2

F + λXp q

(Ai)pq

subject to kb(j)

i k2 ≤ 1  j = 1  . . .   n

(1)

(Pp q Ypq)1/2 is the Frobenius norm  and kyk2 ≡ (Pp y2

where Xi  Ai  and Bi are deﬁned as above  λ ∈ R+ is a regularization parameter  kYkF ≡
p)1/2 is the ℓ2 norm. This optimization
problem is not jointly convex in Ai and Bi  but it is convex in each optimization variable when
holding the other ﬁxed  so a common strategy for optimizing (1) is to alternate between minimizing
the objective over Ai and Bi.
After using the above procedure to ﬁnd representations Ai and Bi for each of the classes i =
1  . . .   k  we can disaggregate a new aggregate signal ¯X ∈ RT ×m′ (without providing the algorithm
its individual components)  using the following procedure (used by  e.g.  [23]  amongst others). We
concatenate the bases to form single joint set of basis functions and solve the optimization problem

ˆA1:k = arg min

A1:k≥0(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

A1:k≥0

¯X − [B1 · · · Bk]


F ( ¯X  B1:k  A1:k)

A1
...
Ak

2

F




(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(Ai)pq

+ λXi p q

(2)

≡ arg min

where for ease of notation we use A1:k as shorthand for A1  . . .   Ak  and we abbreviate the opti-
mization objective as F ( ¯X  B1:k  A1:k). We then predict the ith component of the signal to be

ˆXi = Bi ˆAi.

(3)

The intuition behind this approach is that if Bi is trained to reconstruct the ith class with small
activations  then it should be better at reconstructing the ith portion of the aggregate signal (i.e. 
require smaller activations) than all other bases Bj for j 6= i. We can evaluate the quality of the
resulting disaggregation by what we refer to as the disaggregation error 

E(X1:k  B1:k) ≡

1
2

kXi − Bi ˆAik2

F subject to ˆA1:k = arg min
A1:k≥0

k

Xi=1

F k
Xi=1

Xi  B1:k  A1:k!  

(4)
which quantiﬁes how accurately we reconstruct each individual class when using the activations
obtained only via the aggregated signal.

2.1 Structured Prediction for Discriminative Disaggregation Sparse Coding
An issue with using sparse coding alone for disaggregation tasks is that the bases are not trained to
minimize the disaggregation error. Instead  the method relies on the hope that learning basis func-
tions for each class individually will produce bases that are distinct enough to also produce small
disaggregation error. Furthermore  it is very difﬁcult to optimize the disaggregation error directly
over B1:k  due to the non-differentiability (and discontinuity) of the argmin operator with a non-
negativity constraint. One could imagine an alternating procedure where we iteratively optimize
over B1:k  ignoring the the dependence of ˆA1:k on B1:k  then re-solve for the activations ˆA1:k;
but ignoring how ˆA1:k depends on B1:k loses much of the problem’s structure and this approach
performs very poorly in practice. Alternatively  other methods (though in a different context from
disaggregation) have been proposed that use a differentiable objective function and implicit differ-
entiation to explicitly model the derivative of the activations with respect to the basis functions [4];
however  this formulation loses some of the beneﬁts of the standard sparse coding formulation  and
computing these derivatives is a computationally expensive procedure.

3

Instead  we propose in this paper a method for optimizing disaggregation performance based upon
structured prediction methods [27]. To describe our approach  we ﬁrst deﬁne the regularized disag-
gregation error  which is simply the disaggregation error plus a regularization penalty on ˆA1:k 

Ereg(X1:k  B1:k) ≡ E(X1:k  B1:k) + λXi p q

( ˆAi)pq

(5)

where ˆA is deﬁned as in (2). This criterion provides a better optimization objective for our algorithm 
as we wish to obtain a sparse set of coefﬁcients that can achieve low disaggregation error. Clearly 
the best possible value of ˆAi for this objective function is given by

A⋆

i = arg min
Ai≥0

1
2

kXi − BiAik2

(Ai)pq 

(6)

F + λXp q

which is precisely the activations obtained after an iteration of sparse coding on the data matrix Xi.
Motivated by this fact  the ﬁrst intuition of our algorithm is that in order to minimize disaggregation
error  we can discriminatively optimize the bases B1:k that such performing the optimization (2)
produces activations that are as close to A⋆
1:k as possible. Of course  changing the bases B1:k to
optimize this criterion would also change the resulting optimal coefﬁcients A⋆
1:k. Thus  the second
intuition of our method is that the bases used in the optimization (2) need not be the same as the bases
used to reconstruct the signals. We deﬁne an augmented regularized disaggregation error objective

˜Ereg(X1:k  B1:k  ˜B1:k) ≡

k

Xi=1 1

2

kXi − Bi ˆAik2

F + λXp q

subject to ˆA1:k = arg min
A1:k≥0

( ˆAi)pq!
F k
Xi=1

(7)

Xi  ˜B1:k  A1:k!  

where the B1:k bases (referred to as the reconstruction bases) are the same as those learned from
sparse coding while the ˜B1:k bases (refereed to as the disaggregation bases) are discriminatively
optimized in order to move ˆA1:k closer to A⋆
Discriminatively training the disaggregation bases ˜B1:k is naturally framed as a structured prediction
1:k  the model parameters are ˜B1:k  and the
task: the input is ¯X  the multi-variate desired output is A⋆
discriminant function is F ( ¯X  ˜B1:k  A1:k).1 In other words  we seek bases ˜B1:k such that (ideally)

1:k  without changing these targets.

A⋆

1:k = arg min
A1:k≥0

F ( ¯X  ˜B1:k  A1:k).

(8)

While there are many potential methods for optimizing such a prediction task  we use a simple
method based on the structured perceptron algorithm [5]. Given some value of the parameters ˜B1:k 
we ﬁrst compute ˆA using (2). We then perform the perceptron update with a step size α 

˜B1:k ← ˜B1:k − α(cid:16)∇ ˜B1:k

F ( ¯X  ˜B1:k  A⋆

1:k) − ∇ ˜B1:k

or more explicitly  deﬁning ˜B =h ˜B1 · · · ˜Bki  A⋆ =hA⋆

1

F ( ¯X  ˜B1:k  ˆA1:k)(cid:17)
TiT

T · · · A⋆
1

(and similarly for ˆA) 

(9)

(10)

˜B ← ˜B − α(cid:16)( ¯X − ˜B ˆA) ˆAT − ( ¯X − ˜BA⋆)A⋆T(cid:17) .

To keep ˜B1:k in a similar form to B1:k  we keep only the positive part of ˜B1:k and we re-normalize
each column to have unit norm. One item to note is that  unlike typical structured prediction where
the discriminant is a linear function in the parameters (which guarantees convexity of the problem) 
here our discriminant is a quadratic function of the parameters  and so we no longer expect to
necessarily reach a global optimum of the prediction problem; however  since sparse coding itself
is a non-convex problem  this is not overly concerning for our setting. Our complete method for
discriminative disaggregation sparse coding  which we call DDSC  is shown in Algorithm 1.

1The structured prediction task actually involves m examples (where m is the number of columns of ¯X)  and
1:k)(j)  for the jth example ¯x(j). However  since the function F
the goal is to output the desired activations (a⋆
decomposes across the columns of X and A  the above notation is equivalent to the more explicit formulation.

4

Algorithm 1 Discriminative disaggregation sparse coding
Input: data points for each individual source Xi ∈ RT ×m  i = 1  . . .   k  regularization parameter
λ ∈ R+  gradient step size α ∈ R+.
Sparse coding pre-training:

1. Initialize Bi and Ai with positive values and scale columns of Bi such that kb(j)
2. For each i = 1  . . .   k  iterate until convergence:

i k2 = 1.

(a) Ai ← arg minA≥0 kXi − BiAk2
(b) Bi ← arg minB≥0 kb(j)k2≤1 kXi − BAik2
F

F + λPp q Apq

Discriminative disaggregation training:
1:k ← A1:k  ˜B1:k ← B1:k.

3. Set A⋆
4. Iterate until convergence:

(a) ˆA1:k ← arg minA1:k≥0 F ( ¯X  ˜B1:k  A1:k)

(b) ˜B ←h ˜B − α(cid:16)( ¯X − ˜B ˆA) ˆAT − ( ¯X − ˜BA⋆)(A⋆)T(cid:17)i+

(c) For all i  j  b(j)

i ← b(j)
Given aggregated test examples ¯X′:

i /kb(j)

i k2.

1:k ← arg minA1:k≥0 F ( ¯X′  ˜B1:k  A1:k)

5. ˆA′
6. Predict ˆX′

i = Bi ˆA′
i.

2.2 Extensions
Although  as we show shortly  the discriminative training procedure has made the largest difference
in terms of improving disaggregation performance in our domain  a number of other modiﬁcations
to the standard sparse coding formulation have also proven useful. Since these are typically trivial
extensions or well-known algorithms  we mention them only brieﬂy here.
Total Energy Priors. One deﬁciency of the sparse coding framework for energy disaggregation
is that the optimization objective does not take into consideration the size of an energy signal for
determinining which class it belongs to  just its shape. Since total energy used is obviously a dis-
criminating factor for different device types  we consider an extension that penalizes the ℓ2 deviation
between a device and its mean total energy. Formally  we augment the objective F with the penalty

k

FT EP ( ¯X  B1:k  A1:k) = F ( ¯X  B1:k  A1:k) + λT EP

kµi1T − 1T BiAik2
2

(11)

Xi=1

where 1 denotes a vector of ones of the appropriate size  and µi = 1
total energy of device class i.
Group Lasso. Since the data set we consider exhibits some amount of sparsity at the device level
(i.e.  several examples have zero energy consumed by certain device types  as there is either no such
device in the home or it was not being monitored)  we also would like to encourage a grouping effect
to the activations. That is  we would like a certain coefﬁcient being active for a particular class to
encourage other coefﬁcients to also be active in that class. To achieve this  we employ the group
Lasso algorithm [29]  which adds an ℓ2 norm penalty to the activations of each device

m 1T Xi denotes the average

FGL( ¯X  B1:k  A1:k) = F ( ¯X  B1:k  A1:k) + λGL

k

m

Xi=1

Xj=1

ka(j)

i k2.

(12)

Shift Invariant Sparse Coding. Shift invariant  or convolutional sparse coding is an extension
to the standard sparse coding framework where each basis is convolved over the input data  with
a separate activation for each shift position [3  10]. Such a scheme may intuitively seem to be
beneﬁcial for the energy disaggregation task  where a given device might exhibit the same energy
signature at different times. However  as we will show in the next section  this extension actually
perform worse in our domain; this is likely due to the fact that  since we have ample training data

5

and a relatively low-dimensional domain (each energy signal has 168 dimensions  24 hours per
day times 7 days in the week)  the standard sparse coding bases are able to cover all possible shift
positions for typical device usage. However  pure shift invariant bases cannot capture information
about when in the week or day each device is typically used  and such information has proven crucial
for disaggregation performance.

Implementation

2.3
Space constraints preclude a full discussion of the implementation details of our algorithms  but for
the most part we rely on standard methods for solving the optimization problems. In particular 
most of the time spent by the algorithm involves solving sparse optimization problems to ﬁnd the
activation coefﬁcients  namely steps 2a and 4a in Algorithm 1. We use a coordinate descent approach
here  both for the standard and group Lasso version of the optimization problems  as these have been
recently shown to be efﬁcient algorithms for ℓ1-type optimization problems [8  9]  and have the
added beneﬁt that we can warm-start the optimization with the solution from previous iterations. To
solve the optimization over Bi in step 2b  we use the multiplicative non-negative matrix factorization
update from [7].

3 Experimental Results
3.1 The Plugwise Energy Data Set and Experimental Setup
We conducted this work using a data set provided by Plugwise  a European manufacturer of plug-
level monitoring devices. The data set contains hourly energy readings from 10 165 different devices
in 590 homes  collected over more than two years. Each device is labeled with one of 52 device
types  which we further reduce to ten broad categories of electrical devices: lighting  TV  computer 
other electronics  kitchen appliances  washing machine and dryer  refrigerator and freezer  dish-
washer  heating/cooling  and a miscellaneous category. We look at time periods in blocks of one
week  and try to predict the individual device consumption over this week given only the whole-
home signal (since the data set does not currently contain true whole-home energy readings  we
approximate the home’s overall energy usage by aggregating the individual devices). Crucially  we
focus on disaggregating data from homes that are absent from the training set (we assigned 70% of
the homes to the training set  and 30% to the test set  resulting in 17 133 total training weeks and
6846 testing weeks); thus  we are attempting to generalize over the basic category of devices  not
just over different uses of the same device in a single house. We ﬁt the hyper-parameters of the
algorithms (number of bases and regularization parameters) using grid search over each parameter
independently on a cross validation set consisting of 20% of the training homes.

3.2 Qualitative Evaluation of the Disaggregation Algorithms
We ﬁrst look qualitatively at the results obtained by the method. Figure 1 shows the true energy en-
ergy consumed by two different houses in the test set for two different weeks  along with the energy
consumption predicted by our algorithms. The ﬁgure shows both the predicted energy of several
devices over the whole week  as well as a pie chart that shows the relative energy consumption of
different device types over the whole week (a more intuitive display of energy consumed over the
week). In many cases  certain devices like the refrigerator  washer/dryer  and computer are predicted
quite accurately  both in terms the total predicted percentage and in terms of the signals themselves.
There are also cases where certain devices are not predicted well  such as underestimating the heat-
ing component in the example on the left  and a predicting spike in computer usage in the example
on the right when it was in fact a dishwasher. Nonetheless  despite some poor predictions at the
hourly device level  the breakdown of electric consumption is still quite informative  determining
the approximate percentage of many devices types and demonstrating the promise of such feedback.

In addition to the disaggregation results themselves  sparse coding representations of the different
device types are interesting in their own right  as they give a good intuition about how the different
devices are typically used. Figure 2 shows a graphical representation of the learned basis functions.
In each plot  the grayscale image on the right shows an intensity map of all bases functions learned
for that device category  where each column in the image corresponds to a learned basis. The plot
on the left shows examples of seven basis functions for the different device types. Notice  for
example  that the bases learned for the washer/dryer devices are nearly all heavily peaked  while
the refrigerator bases are much lower in maximum magnitude. Additionally  in the basis images
devices like lighting demonstrate a clear “band” pattern  indicating that these devices are likely to

6

 

e
m
o
H
e
o
h
W

l

3

2

1

0

 

r
e

t

u
p
m
o
C

r
e
y
r
D

/
r
e
h
s
a
W

r
e
h
s
a
w
h
s
D

i

0.4

0.3

0.2

0.1

0

2

1.5

1

0.5

0

1

0.5

0

0.1

r
o

t

a
r
e
g
i
r
f

e
R

0.05

0

0.4

0.3

0.2

0.1

0

g
n

i
l

o
o
C
g
n

/

i
t

a
e
H

Actual Energy

Predicted Energy

1

1

1

1

1

1

2

2

2

2

2

2

3

3

3

3

3

3

4

4

4

4

4

4

5

5

5

5

5

5

6

6

6

6

6

6

True Usage

Predicted Usage

 

7

7

7

7

7

7

 

 

e
m
o
H
e
o
h
W

l

r
e

t

u
p
m
o
C

r
e
y
r
D

/
r
e
h
s
a
W

r
e
h
s
a
w
h
s
D

i

2

1.5

1

0.5

0

 

0.4

0.3

0.2

0.1

0

1.5

1

0.5

0

1

0.5

0

0.1

r
o

t

a
r
e
g
i
r
f

e
R

g
n

i
l

o
o
C
g
n

/

i
t

a
e
H

0.05

0

0.06

0.04

0.02

0

Actual Energy

Predicted Energy

1

1

1

1

1

1

2

2

2

2

2

2

3

3

3

3

3

3

4

4

4

4

4

4

5

5

5

5

5

5

6

6

6

6

6

6

True Usage

Predicted Usage

 

7

7

7

7

7

7

 

Lighting
TV
Computer
Electronics
Kitchen Appliances
Washer/Dryer
Dishwasher
Refrigerator
Heating/Cooling
Other

 

Lighting
TV
Computer
Electronics
Kitchen Appliances
Washer/Dryer
Dishwasher
Refrigerator
Heating/Cooling
Other

 

Figure 1: Example predicted energy proﬁles and total energy percentages (best viewed in color).
Blue lines show the true energy usage  and red the predicted usage  both in units of kWh.

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

g
n

i
t

h
g
L

i

t

r
o
a
r
e
g
d
i
r
f
e
R

r
e
y
r
D

/
r
e
h
s
a
W

Figure 2: Example basis functions learned from three device categories (best viewed in color). The
plot of the left shows seven example bases  while the image on the right shows all learned basis
functions (one basis per column).
be on and off during certain times of the day (each basis covers a week of energy usage  so the seven
bands represent the seven days). The plots also suggests why the standard implementation of shift
invariance is not helpful here. There is sufﬁcient training data such that  for devices like washers and
dryers  we learn a separate basis for all possible shifts. In contrast  for devices like lighting  where
the time of usage is an important factor  simple shift-invariant bases miss key information.

3.3 Quantitative Evaluation of the Disaggregation Methods
There are a number of components to the ﬁnal algorithm we have proposed  and in this section
we present quantitative results that evaluate the performance of each of these different components.
While many of the algorithmic elements improve the disaggregation performance  the results in this
section show that the discriminative training in particular is crucial for optimizing disaggregation
performance. The most natural metric for evaluating disaggregation performance is the disaggrega-
tion error in (4). However  average disaggregation error is not a particularly intuitive metric  and so
we also evaluate a total-week accuracy of the prediction system  deﬁned formally as

Accuracy ≡ Pi q minnPp(Xi)pq Pp(Bi ˆAi)pqo

¯Xp q

Pp q

7

.

(13)

Method

Training Set

Disagg. Err.

Acc.

Test Accuracy

Disagg. Err.

Acc.

Predict Mean Energy

SISC

Sparse Coding

Sparse Coding + TEP
Sparse Coding + GL

Sparse Coding + TEP + GL

DDSC

DDSC + TEP
DDSC + GL

DDSC + TEP + GL

20.98
20.84
10.54
11.27
10.55
9.24
7.20
8.99
7.59
7.92

45.78%
41.87%
56.96%
55.52%
54.98%
58.03%
64.42%
59.61%
63.09%
61.64%

21.72
24.08
18.69
16.86
17.18
14.05
15.59
15.61
14.58
13.20

47.41%
41.79%
48.00%
50.62%
46.46%
52.52%
53.70%
53.23%
52.20%
55.05%

Table 1: Disaggregation results of algorithms (TEP = Total Energy Prior  GL = Group Lasso  SISC
= Shift Invariant Sparse Coding  DDSC = Discriminative Disaggregation Sparse Coding).

 

0.58

0.56

0.54

Training Set

 

0.64

14.5

Disaggregation Error
Accuracy

Test Set

Disaggregation Error
Accuracy

9.5

9

8.5

8

7.5
 
0
0

20
20

0.62

0.6

0.58

14

13.5

80
80

0.56

100
100

13
 
0
0

20
20

40
40

60
60

DDSC Iteration

40
40

60
60

DDSC Iteration

80
80

0.52

100
100

Figure 3: Evolution of training and testing errors for iterations of the discriminative DDSC updates.

Despite the complex deﬁnition  this quantity simply captures the average amount of energy predicted
correctly over the week (i.e.  the overlap between the true and predicted energy pie charts).

Table 1 shows the disaggregation performance obtained by many different prediction methods. The
advantage of the discriminative training procedure is clear: all the methods employing discrimina-
tive training perform nearly as well or better than all the methods without discriminative training;
furthermore  the system with all the extensions  discriminative training  a total energy prior  and
the group Lasso  outperforms all competing methods on both metrics. To put these accuracies in
context  we note that separate to the results presented here we trained an SVM  using a variety
of hand-engineered features  to classify individual energy signals into their device category  and
were able to achieve at most 59% classiﬁcation accuracy. It therefore seems unlikely that we could
disaggregate a signal to above this accuracy and so  informally speaking  we expect the achievable
performance on this particular data set to range between 47% for the baseline of predicting mean en-
ergy (which in fact is a very reasonable method  as devices often follow their average usage patterns)
and 59% for the individual classiﬁcation accuracy. It is clear  then  that the discriminative training
is crucial to improving the performance of the sparse coding disaggregation procedure within this
range  and does provide a signiﬁcant improvement over the baseline. Finally  as shown in Figure 3 
both the training and testing error decrease reliably with iterations of DDSC  and we have found that
this result holds for a wide range of parameter choices and step sizes (though  as with all gradient
methods  some care be taken to choose a step size that is not prohibitively large).

4 Conclusion
Energy disaggregation is a domain where advances in machine learning can have a signiﬁcant impact
on energy use. In this paper we presented an application of sparse coding algorithms to this task 
focusing on a large data set that contains the type of low-resolution data readily available from smart
meters. We developed the discriminative disaggregation sparse coding (DDSC) algorithm  a novel
discriminative training procedure  and show that this algorithm signiﬁcantly improves the accuracy
of sparse coding for the energy disaggregation task.
Acknowledgments This work was supported by ARPA-E (Advanced Research Projects Agency–
Energy) under grant number DE-AR0000018. We are very grateful to Plugwise for providing us
with their plug-level energy data set  and in particular we thank Willem Houck for his assistance
with this data. We also thank Carrie Armel and Adrian Albert for helpful discussions.

8

References

[1] D. Archer. Global Warming: Understanding the Forecast. Blackwell Publishing  2008.
[2] M. Berges  E. Goldman  H. S. Matthews  and L Soibelman. Learning systems for electric comsumption

of buildings. In ASCI International Workshop on Computing in Civil Engineering  2009.

[3] T. Blumensath and M. Davies. On shift-invariant sparse coding. Lecture Notes in Computer Science 

3195(1):1205–1212  2004.

[4] D. Bradley and J.A. Bagnell. Differentiable sparse coding. In Advances in Neural Information Processing

Systems  2008.

[5] M. Collins. Discriminative training methods for hidden markov models: Theory and experiements with
In Proceedings of the Conference on Empirical Methods in Natural Language

perceptron algorithms.
Processing  2002.

[6] S. Darby. The effectiveness of feedback on energy consumption. Technical report  Environmental Change

Institute  University of Oxford  2006.

[7] J. Eggert and E. Korner. Sparse coding and NMF. In IEEE International Joint Conference on Neural

Networks  2004.

[8] J. Friedman  T. Hastie  H Hoeﬂing  and R. Tibshirani. Pathwise coordinate optimization. The Annals of

Applied Statistics  2(1):302–332  2007.

[9] J. Friedman  T. Hastie  and R. Tibshirani. A note on the group lasso and a sparse group lasso. Technical

report  Stanford University  2010.

[10] R. Grosse  R. Raina  H. Kwong  and A. Y. Ng. Shift-invariant sparse coding for audio classiﬁcation. In

Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence  2007.

[11] G. Hart. Nonintrusive appliance load monitoring. Proceedings of the IEEE  80(12)  1992.
[12] S. Hasler  H. Wersin  and E Korner. Combinging reconstruction and discrimination with class-speciﬁc

sparse coding. Neural Computation  19(7):1897–1918  2007.

[13] P.O. Hoyer. Non-negative sparse coding. In IEEE Workshop on Neural Networks for Signal Processing 

2002.

[14] C. Laughman  K. Lee  R. Cox  S. Shaw  S. Leeb  L. Norford  and P. Armstrong. Power signature analysis.

IEEE Power & Energy Magazine  2003.

[15] C. Laughman  S. Leeb  and Lee. Advanced non-intrusive monitoring of electric loads. IEEE Power and

Energy  2003.

[16] W. Lee  G. Fung  H. Lam  F. Chan  and M. Lucente. Exploration on load signatures.

Conference on Electrical Engineering (ICEE)  2004.

International

[17] J. Mairal  F. Bach  J. Ponce  G. Sapiro  and A. Zisserman. Supervised dictionary learning. In Advances

in Neural Information Processing Systems  2008.

[18] J. Mairal  M. Leordeanu  F. Bach  M. Hebert  and J. Ponce. Discriminative sparse image models for
In European Conference on Computer Vision 

class-speciﬁc edge detection and image interpretation.
2008.

[19] B. Neenan and J. Robinson. Residential electricity use feedback: A research synthesis and economic

framework. Technical report  Electric Power Research Institute  2009.

[20] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381:607–609  1996.

[21] S. N. Patel  T. Robertson  J. A. Kientz  M. S. Reynolds  and G. D. Abowd. At the ﬂick of a switch: De-
tecting and classifying unique electrical events on the residential power line. 9th international conference
on Ubiquitous Computing (UbiComp 2007)  2007.

[22] S. T. Roweis. One microphone source separation. In Advances in Neural Information Processing Systems 

2000.

[23] M. N. Schmidt  J. Larsen  and F. Hsiao. Wind noise reduction using non-negative sparse coding. In IEEE

Workshop on Machine Learning for Signal Processing  2007.

[24] M N. Schmidt and R. K. Olsson. Single-channel speech separation using sparse non-negative matrix

factorization. In International Conference on Spoken Language Processing  2006.

[25] S. R. Shaw  C. B. Abler  R. F. Lepard  D. Luo  S. B. Leeb  and L. K. Norford. Instrumentation for high

performance nonintrusive electrical load monitoring. ASME  120(224)  1998.

[26] F. Sultanem. Using appliance signatures for monitoring residential loads at meter panel level.

Transaction on Power Delivery  6(4)  1991.

IEEE

[27] B. Taskar  V. Chatalbashev  D. Koller  and C. Guestrin. Learning structured prediction models: A large

margin approach. In International Conference on Machine Learning  2005.

[28] Various. Annual Energy Review 2009. U.S. Energy Information Administration  2009.
[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statisical Society  Series B  68(1):49–67  2007.

9

,Yifei Ma
Roman Garnett
Jeff Schneider
Abhishek Sharma
Oncel Tuzel
Ming-Yu Liu
Pinar Yanardag
S.V.N. Vishwanathan