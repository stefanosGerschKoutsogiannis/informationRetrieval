2019,Iterative Least Trimmed Squares for Mixed Linear Regression,Given a linear regression setting  Iterative Least Trimmed Squares (ILTS) involves alternating between (a) selecting the subset of samples with lowest current loss  and (b) re-fitting the linear model only on that subset. Both steps are very fast and simple. In this paper  we analyze ILTS  in the setting of mixed linear regression with corruptions (MLR-C). We first establish deterministic conditions (on the features etc.) under which the ILTS iterate converges linearly to the closest mixture component. We also provide a global algorithm that uses ILTS as a subroutine  to fully solve mixed linear regressions with corruptions. We then evaluate it for the widely studied setting of isotropic Gaussian features  and establish that we match or better existing results in terms of sample complexity. Finally  we provide an ODE analysis for a gradient-descent variant of ILTS that has optimal time complexity. Our results provide initial theoretical evidence that iteratively fitting to the best subset of samples -- a potentially widely applicable idea -- can provably provide state of the art performance in bad training data settings.,Iterative Least Trimmed Squares for Mixed Linear

Regression

Yanyao Shen
ECE Department

University of Texas at Austin

Austin  TX 78712

shenyanyao@utexas.edu

Sujay Sanghavi
ECE Department

University of Texas at Austin

Austin  TX 78712

sanghavi@mail.utexas.edu

Abstract

Given a linear regression setting  Iterative Least Trimmed Squares (ILTS) involves
alternating between (a) selecting the subset of samples with lowest current loss 
and (b) re-ﬁtting the linear model only on that subset. Both steps are very fast and
simple. In this paper we analyze ILTS in the setting of mixed linear regression with
corruptions (MLR-C). We ﬁrst establish deterministic conditions (on the features
etc.) under which the ILTS iterate converges linearly to the closest mixture
component. We also evaluate it for the widely studied setting of isotropic Gaussian
features  and establish that we match or better existing results in terms of sample
complexity. We then provide a global algorithm that uses ILTS as a subroutine  to
fully solve mixed linear regressions with corruptions. Finally  we provide an ODE
analysis for a gradient-descent variant of ILTS that has optimal time complexity.
Our results provide initial theoretical evidence that iteratively ﬁtting to the best
subset of samples – a potentially widely applicable idea – can provably provide
state-of-the-art performance in bad training data settings.

1

Introduction

In vanilla linear regression  one (implicitly) assumes that each sample is a linear measurement of
a single unknown vector  which needs to be recovered from these measurements. Statistically  it is
typically studied in the setting where the samples come from such a ground truth unknown vector 
and we are interested in the (computational/statistical complexity of) recovery of this ground truth
vector. Mixed linear regression (MLR for brevity) is the problem where there are multiple unknown
vectors  and each sample can come from any one of them (and we do not know which one  a-priori).
Our objective is again to recover all (or some  or one) of them from the samples. In this paper  we
consider MLR with the additional presence of corruptions – i.e. adversarial additive errors in the
responses – for some unknown subset of the samples. There is now a healthy and quickly growing
body of work on algorithms  and corresponding theoretical guarantees  for MLR with and without
additive noise and corruptions; we review these in detail in the related work section.
In our paper we start from a classical (but hard to compute) approach from robust statistics: least
trimmed squares [19]. This advocates ﬁtting a model so as to minimize the loss on only a fraction ⌧
of the samples  instead of all of them – but crucially  the subset S of samples chosen and the model to
ﬁt them are to be estimated jointly. To be more speciﬁc  suppose our samples are (xi  yi)  for i 2 [n].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Then the least squares (LS) and least trimmed squares (LTS) estimates are:

b✓LS = arg min
b✓LTS = arg min

✓

(yi  hxi ✓ i)2  

✓ Xi2[n]
S : |S|=b⌧nc Xi2S

min

(yi  hxi ✓ i)2 .

Note that least trimmed squares involves a parameter: the fraction ⌧ of samples we want to ﬁt. Solving

for the least trimmed squares estimateb✓LTS needs to address the combinatorial issue of ﬁnding the

best subset to ﬁt  but the goodness of a subset is only known once it is ﬁt. LTS is shown to have
computation lower bound exponential in the dimension of x [17].
LTS  if one could solve it  would be a candidate algorithm for MLR as follows: suppose we knew a
lower bound on the number of samples corresponding to a single component (i.e. generated using one
of the unknown vectors). Then one would choose the fraction ⌧ in the LTS procedure to be smaller
than this lower bound on the fraction of samples that belong to a component. Ideally  this would
lead the LTS to choose a subset S of samples that all correspond to a single component  and the
least squares on that set S would ﬁnd the corresponding unknown vector. This is easiest to see in the
noiseless corruption-less setting where each sample is just a pure linear equation in the corresponding
unknown vector. In this case  an S containing samples only from one component  and a ✓ which
is the corresponding ground truth vector  would give 0 error and hence would be the best solutions
to LTS. Hence  to summarize  one can use LTS to solve MLR by estimating a single ground truth
vector at a time.
However  LTS is intractable  and we instead study the natural iterative variant of LTS  which alternates
between ﬁnding the set S ⇢ n of samples to be ﬁt  and the ✓ that ﬁts it. In particular  our procedure –
which we call iterative least trimmed squares (ILTS) – ﬁrst picks a fraction ⌧ and then proceeds
in iterations (denoted by t) as follows: starting from an initial ✓0 

St = arg

(yi  hxi ✓ ti)2  

✓t+1 = arg min

(yi  hxi ✓ i)2 .

min

S : |S|=b⌧nc Xi2S
✓ Xi2St

Note that now  as opposed to before  ﬁnding the subset St is trivial: just sort the samples by their
current squared errors (yi hxi ✓ ti)2  and pick the ⌧n that have smallest loss. Similarly  the ✓ update
now is a simple least squared problem on a pre-selected subset of samples. Note also that each of the
above steps decreases the function a(✓  S)  Pi2S (yi  hxi ✓ i)2. This has also been referred to

as iterative hard thresholding and studied for the different but related problem of robust regression 
again please see related work for known results. Our motivations for studying ILTS are several: (1)
it is very simple and natural  and easy to implement in much more general scenarios beyond least
squares. Linear regression represents in some sense the simplest statistical setting to understand this
approach. (2) In spite of its simplicity  we show in the following that it manages to get state-of-the-art
performance for MLR with corruptions  with weaker assumptions than several existing results.
Again as before  one can use ILTS for MLR by choosing a ⌧ that is smaller than the number of
samples in a component. However  additionally  we now also need to choose an initial ✓0 that is
closer to one component than the others. In the following  we thus give two kinds of theoretical
guarantees on its performance: a local one that shows linear convergence to the closest ground truth
vector  and a global one that adds a step for good initialization.
Main contributions and outline:
• We propose a simple and efﬁcient algorithm ILTS for solving MLR with adversarial corruptions;
we precisely describe the problem setting in Section 3. ILTS starts with an initial estimate
of a single unknown ✓  and alternates between selecting the size ⌧n subset of the samples best
explained by ✓  and updating the ✓ to best ﬁt this set. Each of these steps is very fast and easy.

• Our ﬁrst result  Theorem 4 in Section 4 establishes deterministic conditions – on the features 
the initialization  and the numbers of samples in each component – under which ILTS linearly
converges to the ground truth vector that is closest to the initialization. Theorem 7 in Section
4 specializes this to the (widely studied) case when the features are isotropic Gaussians. The

2

sample complexity is nearly optimal in both dimension d and the number of components m  while
previous state-of-the-art results are nearly optimal in d  but can be exponential in m. Our analysis
for inputs following isotropic Gaussian distribution is easy to generalize to more general class of
sub-Gaussian distributions.

• To solve the full MLR problem  we identify ﬁnding the subspace spanned by the true MLR
components as a core problem for initialization. In the case of isotropic Gaussian features  this
is known to be possible by existing results in robust PCA (when corruptions exist) or standard
spectral methods (when there are no corruptions). Given a good approximation of this subspace 
one can use the ILTS process above as a subroutine with an “outer loop" that tries out many
initializations (which can be done in parallel  and are not too many when number of components
is ﬁxed and small) and evaluates whether the ﬁnal estimate is to be accepted as an estimate for a
ground truth vector (Global-ILTS). We specify and analyze it in Section 5 for the case of random
isotropic Gaussian features and also discuss the feasibility of ﬁnding such a subspace.

2 Related Work

Mixed linear regression Learning MLR even in the two mixture setting is NP hard in general [26]. As
a result  provably efﬁcient algorithmic solutions under natural assumptions on the data  e.g.  all inputs
are i.i.d. isotropic Gaussian  are studied. Efﬁcient algorithms that provably ﬁnd both components
include the idea of using spectral initialization with local alternating minimization [26]  and classical
EM approach with ﬁner analysis [1  12  13]. In the multiple components setting  substituting spectral
initialization by tensor decomposition brings provable algorithmic solutions [5  27  29  20]. Recently 
[14] proposes an algorithm with nearly optimal complexity using quite different ideas. They relate
MLR problem with learning GMMs and use the black-box algorithm in [16].
In Table 1  we
summarize the sample and computation complexity of the three most related work. Previous literature
focus on the dependency on dimension d  for all these algorithms that achieve near optimal sample
complexity  the dependencies on m for all the algorithms are expoential (notice that the guarantees
in [27] contains a m term  which can be exponentially small in m without further assumptions  as
pointed out by [14])  and [14] requires exponential in m2 number of samples for a more general
class of Gaussian distributions. Notice that while it is reasonable to assume m being a constant 
this exponential dependency on m or m2 could dominate the sample complexity in practice. From
robustness point of view  the analysis of all these algorithms rely heavily on exact model assumptions
and are restricted to Gaussian distributions. While recent approaches on robust algorithms are able to
deal with strongly convex functions  e.g.  [9]  with corruption in both inputs and outputs  [29] showed
local strong convexity of MLR with small local region ˜O(d(md)m)  under ˜⌦(dmm) samples. To
the best of our knowledge  we are not aware of any previous work study the algorithmic behavior
under mis-speciﬁed MLR model settings. We provide ﬁne-grained analysis for a simple algorithm
that achieves nearly optimal sample and computation complexity.
Robust regression Our algorithm idea is similar to least trimmed square estimator (LTS) proposed
by [19]. The hardness of ﬁnding the exact LTS estimator is discussed in [17]  which shows an
exponential in d computation lower bound under the hardness of afﬁne degeneracy conjecture. While
our algorithm is similar to the previous hard thresholding solutions proposed in [2]  their analysis
does not handle the MLR setting  and only guarantees parameter recovery given a small constant
fraction of corruption. Algorithmic solutions based on LTS for solving more general problems have
been proposed in [25  23  21]. [10] studies the l1 regression and gives a tight analysis for recoverable
corruption ratio. Another line of research focus on robust regression where both the inputs and
outputs can be corrupted  e.g.  [6]. There are provable recovery guarantees under constant ratio of
corruption using using robust gradient methods [9  18  15]  and sum of squares method [11]. We focus
on computationally efﬁcient method with nearly optimal computation time that is easily scalable in
practice.

3 Problem Setup and Preliminaries

We consider the standard (noiseless) MLR model with corruptions  which we will abbreviate to
(MLR-C); each sample is a linear measurement of one of m unknown “ground truth" vectors – but
we do not know which one. Our task is to ﬁnd the ground truth vectors  and this is made harder by a
constant fraction of all samples having an additional error in responses. We now specify this formally.

3

setting

N (0  Id)  k

linearly independent ✓(j)s
N (0  Id)  constant Q

N (0  ⌃(j))

[27]

[29]

[14]

Ours

local
global

sample (n)

poly(m)d
poly(m)d/5
m

mmd

dpoly( m

Q )m2

Q ) + ( cm
md
-

computation
nd2 + md3

nd2 + poly(m)

nd

nd

nd2 (nd for GD-ILTS)

robust  not limited to N (0  ⌃(j))
good estimate of the subspace

local
global

subspace est. +( cm

Q )m · nd
Table 1: Compare with previous results in the setting of balanced MLR  i.e.  each component has n/m
samples. Q represents a separation property of the mixture components (see Deﬁnition 1 for details).
For conciseness  we only keep the main factors in the complexity terms. The inspiring algorithms
listed here achieve nearly optimal sample complexity (nearly linear in d) under certain settings 
which are helpful for understanding the limit of learning MLR. Note that we have ˜O(nd2 log 1
" )
computation for ILTS and ˜O(nd log2 1
" ) for GD-ILTS (in Section B  a direct gradient variant).
Sample complexity for our global step depends on the hardness of ﬁnding the subspace. Our local
requirement only needs the current estimation to be close to one of the components  which is much
easier to satisfy than the local notion in [27]. Methods in [5  20] require ˜⌦(d6) and ˜⌦(d3) sample
complexity (they can handle more general settings)  [28] uses sparse graph codes for sparse MLR.
Therefore  we do not list their results here (hard to compare with).

(MLR-C): We are given n samples of the form (xi  yi) for i = 1  . . .   n  where each yi 2 R and xi 2
Rd. Unkown to us  there are m “ground truth" vectors ✓?
(m)  each in Rd; correspondingly 
and again unknown to us  the set of samples is partitioned into disjoint sets S(1)  . . .   S(m). If the ith
sample is in set S(j) for some j 2 [m]  it satisﬁes

(1)  . . .  ✓ ?

yi = hxi ✓ ?

(j)i + ri 

for i 2 S(j)

(MLR-C).

(1)  . . .  ✓ ?

Here  ri denotes the possible additive corruption – a fraction of the r1  . . .   rn are arbitrary unknown
values  and the remaining are 0 (and again  we are not told which).
Our objective is: given only the samples (xi  yi)  ﬁnd the ground truth vectors ✓?
(m). In
particular  we do not have a-priori knowledge of any of the sets S(j)  or the values/support of the
corruptions. We now develop some notation for the sizes of the components etc.
Sizes of sets: Let R? = {i 2 [n] s.t. ri 6= 0} denote the set of corrupted samples; note that this
set can overlap with any / all of the components’ sets S(j)s. Let S?
(j) = S(j)\R? be the uncorrupted
set of samples from the S(j)  for all j 2 [m]. Let ⌧ ?
(j)|/n denote the fraction of uncorrupted
samples in each component j  and ⌧ ?
(j) denote the smallest such fraction. Let
min) be the ratio of the number of corrupted samples to the size of the smallest
? = |R?|/(n⌧ ?
component 1. Notice that ? = 0 corresponds to the MLR model without corruption. We do not make
any assumptions on which speciﬁc samples are corrupted; R? can be any subset of size ?⌧ ?
minn of
the set of n samples. Thus a ? = 1 situation can prevent the recovery of the smallest component.
(l)  8j 2 [m]  X = [x1 ···   xn]> 2
Finally  for convenience  we denote S?
Rn⇥d  and y = [y1 ··· yn]. Note that we consider the case without additive stochastic noise  which
is the same setting as in [27  29  14].

(j) = |S?
min = minj2[m] ⌧ ?

(j) := [l2[m]\{j}S?

3.1 Preliminaries

We now develop our way to making a few basic assumptions on the model setting; our main results
show that under these assumptions the simple ILTS algorithm succeeds. The ﬁrst deﬁnition quantiﬁes
the separation between the ground truth vectors.

1the component with fewest samples

4

i=1  initial ✓0  fraction of samples to be retained ⌧

Algorithm 1 ILTS (for recovering a single component)
1: Input: Samples Dn = {xi  yi}n
2: Output: Final estimationb✓
3: Parameters: Number of rounds T
4: for t = 0 to T  1 do
St index set of b⌧nc samples with smallest residuals (yi  hxi ✓ ti)2  i 2 [n]
5:
✓t+1 = arg min✓ Pi2St
6:
7: Output:b✓ = ✓T
Deﬁnition 1 (Q-separation). For the set of components {✓?
(i) the set of components is Q-separated if Q 
(ii) local separation Qj is deﬁned as Qj =

(1) ···  ✓ ?
(i)✓?
mini j2[m] i6=j k✓?
maxj2[m] k✓?
(j)k
(l)✓?
(j)k2

(yi  hxi ✓ i)2

(m)} 
(j)k2
;

minl2[m]\{j} k✓?
(j)k

k✓?

  8j 2 [m].

By deﬁnition  it is clear that Q  Qj  8j 2 [m]. In fact  Q represents the global separation property 
which is required by previous literatures for solving MLR [27  29  14]  while Qj describes the
local separation property for the jth component  and gives us a better characterization of the local
convergence property for a single component. We now turn to the features; let X denote the n ⇥ d
matrix of features  with the ith row being x>i – the features of the ith sample.
Deﬁnition 2 (( +  )-feature regularity). Deﬁne Sk to be the set of all subsets in [n] with size k 
and let XS be the sub-matrix of X with rows indexed by some S ⇢ [n]. Deﬁne

 +(k) = max
S2Sk

max(X>S XS) 

and (k) = min
S2Sk

min(X>S XS) 

(1)

where functions +(k)  (k) are feature regularity upper bound and lower bound  respectively.
max(A)(min(A)) represents the largest (smallest) eigenvalue of a symmetric matrix A.

Clearly  if + is too large or  is too small  identifying samples belonging to a certain component
or not  even given a very good estimate of the true component  can become extremely difﬁcult. For
example  if the true component coincides with the top eigenvalue direction of its feature covariance
matrix  then  even if the current estimate is close within `2  the prediction error can still be quite
large due to the X. On the other hand  if each row in X follows i.i.d. isotropic Gaussian distribution 
 +(k) and (k) are upper and lower bounded by ⇥(n) for k being a constant factor of n (when n
is large enough). This is shown in Lemma 5. Next  we deﬁne -afﬁne error  a property of the data
that is closely connected with our analysis of ILTS in Section 4.
Deﬁnition 3 (-afﬁne error /V()). For 8j 2 [m]  denote X(j) as the sub-matrix with rows from
(j) with size n⌧ ?
(j)  let ⌧(j) = c⌧ ⌧ ?
S?
for some ﬁxed constant c⌧ < 1. Deﬁne -afﬁne error V() to be the maximum value of integer V
such that the following holds for some v1  v2 2 Rd with kv1k2/kv2k2 =  1 and j 2 [m]:

(j)  X(j) as the sub-matrix with rows from S?

(j) with size ⌧ ?

(j)

[|X(j)v1|](V +d(⌧ ?

j ⌧j )ne)th largest  [|X(j)v2|](V )th smallest.

(2)

(j) and S?

(j) by ranking and ﬁnding the smallest
This is saying  when we pick samples from set S?
b⌧jnc samples based on the projected values to v1  v2  the number of samples from S?
(j) is at
most V(). For example  given current estimate ✓  the residual of a sample from component j is
(j)  ✓. As a result  this deﬁnition helps quantify the
hxi ✓ ?
number of mis-classiﬁed samples from other components  see Figure 1 for another illustration. If
each row in X follows i.i.d. isotropic Gaussian distribution  V() scales linearly with  for large
enough n. This is shown in Lemma 6.

(j)  ✓i  and v1 can be considered as ✓?

4

ILTS and Local Analysis

Algorithm 1 presents the procedure of ILTS: Starting from initial parameter ✓0  the algorithm
alternates between (a) selecting samples with smallest residuals  and (b) getting the least square

5

Figure 1: A two-dimensional illustration of -afﬁne error V() in Deﬁnition 3 for kv1k2/kv2k2 =
(for simplicity  assume ⌧ ?
j = ⌧j). V() can be interpreted as the number of mistakenly ﬁltered
samples in any directions. The plot in the middle contains blue dots from one component X(j)  and
red dots from other components X(j). The plots on the left and right illustrate how the histogram
looks like for X(j)v1 (in blue) and X(j)v2 (in red)  for two sets of v1 and v2. Areas in blue represent
the samples that may be mistakenly ﬁltered out. The maximum V that satisﬁes (2) is larger on the
right side plot since projected values for samples from S?
(j) are more concentrated. V() is an
upper bound of the maximum V on all possible directions.

solution on the selected set of samples as the new parameter. Intuitively  ILTS succeeds if (a) ✓0 is
close to the targeted component  and (b) for each round of update  the new parameter is getting closer
to the targeted component. For our analysis  we assume the chosen fraction of samples to be retained
is strictly less than the number of samples from the interested component  i.e.  ⌧ = c0⌧ ?
(j) for some
universal constant c0. We ﬁrst provide local recovery results using the structural deﬁnition we made
in Section 3  for both no corruption setting and corruption setting. Then  we present the result under
Gaussian design matrix. All proofs can be found in the Appendix.
Theorem 4 (deterministic features). Consider (MLR-C) using Algorithm 1 with ⌧<⌧
(j). Given
?
iterate ✓t at round t  which is closer to the j-th component in Euclidean distance and satisﬁes
(l)k2  then the next iterate ✓t+1 of the algorithm satisﬁes
k✓t  ✓?
Qj ·

2 minl2[m]\{j} k✓?

(j)k2  1

(j)  ✓?

2k✓t✓?

(j)k2

(j)k2 ⌘ + ?⌧ ?

minn⌘

k✓?
 (⌧n )

k✓t  ✓?

(j)k2.

(3)

k✓t+1  ✓?

(j)k2 

2 +⇣V⇣ 1

(j) and the iterate ✓t is
The above one-step update rule (3) holds as long as Algorithm 1 uses ⌧<⌧
?
closer to the j-th component. However  in order to make ✓t+1 getting closer to ✓?
(j)  the contraction
term on the RHS of (3) needs to be less than 1  which may require stronger conditions on ✓t 
depending on what xis are. The denominator term (⌧n ) is due to the selection bias on a subset
of samples  which scales with n as long as the inputs have good regularity property. The numerator
term is due to the incorrect samples selected by St  which consists of: (a) samples from other mixture
components  and (b) corrupted samples. (a) is controlled by the afﬁne error  which depends on (a1)
the local separation of components Qj  and (a2) the relative closeness of ✓t to ✓?
(j)  and scales with n.
(j). For (b)  the
The afﬁne error V gets larger if the separation is small  or ✓t is not close enough to ✓?
number of all corrupted samples is controlled by ?⌧ ?
minn  which is not large given ? being a small
constant.
Theorem 4 gives a general update rule for any given dataset according to Deﬁnitions 1-2. Next  we
present the local convergence result for the speciﬁc setting of Gaussian input vectors  by giving a tight
analysis for feature regularity in Lemma 5 and a tight bound for the afﬁne error V() in Lemma 6.
Lemma 5. Let +(k)  (k) be deﬁned as in (1)  and assume each xi ⇠N (0  Id). Then  for
k = ckn with constant ck  for n =⌦ ✓ d log 1

ck ◆  with high probability 
 +(k)  c1 · k  (k)  c2 · k 

ck

6

where c1  c2 are constants that depend on ck: c1  1 + 3eq6 log 2
constants C1  C2.
Lemma 6. Suppose we have xi ⇠N (0  Id)  ⌧ ?
⌦( d log log d/⌧ ?

ck

min)  with high probability  the design matrix satisﬁes V()  c{n _ log n}.

(j)n samples for each class S(j). Then  for n =

+ C1
ck

  c2  C2ck  for universal

Plug in Lemma 5 and Lemma 6 to Theorem 4  we have:
Theorem 7 (Gaussian features). For (MLR-C)  assume xi ⇠N (0  Id)  consider using Algo-
rithm 1 with ⌧<⌧
(j)k 
(j)) for some j 2 [m] 
cj minl2[m]\{j} k✓?
then  w.h.p.  the next iterate ✓t+1 of the algorithm satisﬁes

min ⌘. If the iterate satisﬁes k✓t  ✓?

(j)k2 (where cj is a constant depending on ⌧ and ⌧ ?

(j)  8j 2 [m]  and n =⌦ ⇣ d log log d
(j)k2  tk✓t  ✓?

(l)  ✓?

(j)k2 

(4)

⌧ ?

?

where t = c0

⌧n⇣n 1

Qj ·

k✓t+1  ✓?
n _ log no + ?⌧ ?

(j)k2

2k✓t✓?
(j)k2

k✓?

minn⌘ < 1  for some small constant ?.

(j) up to arbitrary accuracy with ˜O(d/⌧ ?

Note that in this Theorem  c0 is a constant such that t < 1  and such a c0 corresponds to an upper
bound on cj  i.e.  the local region. Theorem 7 shows that  as long as ✓t is contant time closer to
min) samples. In fact  Lemma 5 and
(j)  we can recover ✓?
✓?
Lemma 6 (and hence Theorem 7) are generalizable to more general distributions  including the setting
studied in [14]. The initial condition simply changes by a factor of   where  is the upper bound of
the covariance matrix. The formal statement is as follows:
Corollary 8 (features with non-isotropic Gaussians). Consider (MLR-C)  where each xi ⇠
N (0  ⌃(j)) for i 2 S(j)  I  ⌃(j)  I. Under the same setting as in Theorem 7  convergence
property (4) holds as long as iterate ✓t satisﬁes k✓t  ✓?
Discussion We summarize our results from the following four perspectives:
• Our results can generalize to a wide class of distributions: e.g.  Gaussians or a sub-class of
sub-Gaussians with different covariance matrix. This is because the proof technique for showing
Lemma 5 and Lemma 6 only exploits the property of (a) concentration of order statistics; (b)
anti-concentration of Gaussian-type distributions.

 minl2[m]\{j} k✓?

(j)k  cj

(l)  ✓?

(j)k2.

⌧

• Super-linear convergence speed for ? = 0: When ? = 0  t / k✓t  ✓?
• Optimal local sample dependency on m: Notice that locally  in the balanced setting  where
(j) = 1/m  the sample dependency on m is linear. This dependency is optimal since for each
⌧ ?
component  we want n/m > d to make the problem identiﬁable. 2

(j)k2 in Theorem 7.

• ILTS learns each component separately: Different from the local alternating minimization approach
by [27]  recovering one component does not require good estimates of any other components. E.g. 
if we are only interested in the j-th component  then  the sample complexity is ˜O⇣d/⌧ ?
(j)⌘.

5 Global ILTS and Its Analysis

(1) ✓ ?

(2) ···  ✓ ?

In Section 4  we show that as long as the initialization is closer to the targeted component with
constant factor  we can locally recover the component  even under a constant fraction of corruptions.
In this part  we discuss the initialization condition. Let us deﬁne the targeted subspace Um as:
(m)o   and for any subspace U  we denote U as the corresponding
Um := spann✓?
subspace matrix  with orthonormal columns. We deﬁne the concept of ✏-close subspace as follows:
Deﬁnition 9 (✏-close subspace). bU2 Rd⇥ ˜m is an ✏-close subspace to Um if ˜m = O(m)  and their
corresponding subspace matrices bU  Um satisfy:⇣Id  bUbU>⌘ · Um2  ✏.

2Notice that the larger m becomes  the smaller the local region becomes  since cj depends on m. However 

according to our bound for + and   the dependency of cj on m is still polynomial.

7

i=1

(j)k2

j=1  small error 

Remove samples in set Sj from Dn

Algorithm 2 GLOBAL-ILTS (for recovering all components )
1: Input: Samples Dn = {xi  yi}n
2: Output:b✓1 ···  b✓m
3: Parameters: Granularity ✏  estimate {⌧j}m
4: Find a ✏-close subspace Um
5: Generate an ✏-net ⇥✏ covering the centered sphere in Um with radius k maxj2[m] ✓?
6: for j = 1 to m do
7:
8:
9:
10:
11:
12:

for ˜✓ randomly drawn from ⇥✏ do
✓ ILTS(Dn  ˜✓  ⌧j)
Sj = {i | (yi  hxi ✓ i)2 < 2}
if |Sj| b ⌧jnc then
b✓j = ✓  break
13: Return:b✓1 ···  b✓m
An interpretation of an ✏-close subspace U is as follows: for any unit vector v from subspace Um 
there exists a vector v0 in subspace U with norm less than 1  such that kv  v0k2  ✏. We also deﬁne
"-recovery  to help with stating our theorem.
Deﬁnition 10 ("-recovery). b⇥= hb✓1 ···  b✓mi is a "-recovery of ⇥? = h✓?
(m)i if
minP2Pm kb⇥P  ⇥?k2 1  "  where Pm is the class of all m-dimensional permutation matrices.
The procedure for Global-ILTS is shown in Algorithm 2. The algorithm takes a subspace as its
input  which should be a good approximation of the subspace spanned by the correct ✓?
(j)s. Given
the subspace  Global-ILTS constructs an ✏-net over a sphere in subspace Um The algorithm then
iteratively removes samples once the ILTS sub-routine ﬁnds a valid component. Notice that we
require the estimates ⌧js to satisfy ⌧j <⌧ ?
Theorem 11 (Global algorithm). For (MLR-C)  assume xi ⇠N (0  Id). Following Algorithm 2  we
  and with small   " (e.g.   = cplog n" with
can ﬁnd an ✏-close subspace U with ✏ = cl minj2[m] ⌧j
(j) for all j 2 [m]  we are able to have "-recovery over all components
" small enough) and ⌧j <⌧ ?
minQ⌘O(m)
with n =⌦ ⇣ d log log d

min ⌘ samples  in O✓⇣ 1

(j). 3 We have the following global recovery result:

"◆ time.

(1) ···  ✓ ?

⌧ ?

2

⌧ ?

nd2 log 1

Several merits of Theorem 11: First  our result clearly separates the problem into (a) globally
ﬁnding a subspace; and (b) locally recovering a single component with ILTS. Second  the nd2
computation dependency is due to ﬁnding the exact least squares. Alternatively  one can take gradient
descent to ﬁnd an approximation to the true component. The convergence property of a gradient
descent variant of ILTS is shown in Section B  where we further discuss the ideal number of gradient
updates to make for each round  so that the algorithm can be more efﬁcient. Third  the exp(O(m))
dependency in runtime can be practically avoided  since our algorithm is easy to run in parallel.
Feasibility of getting U. Let L = [y1x1; y2x2;··· ; ynxn]  then the column space of L is close to
Um for ? = 0  when xis have identity covariance. For ? = 0  the standard top-m SVD on L in
O(m2n) time with ⌦⇣ d
poly log(d)⌘ samples is guaranteed to get a ✏-close estimate  following
the well-known sin-theta theorem [7  4]. For ? 6= 0 under the same setting  we can use robust
PCA methods to robustly ﬁnd the subspace. For example  the state-of-the-art result in [8] provides
a near optimal recovery guarantee  with slightly larger sample size (i.e.  ⌦(d2/✏2)). Closing this
sample complexity gap is an interesting open problem for outlier robust PCA. Notice that instead of
estimating the subspace  [14] uses the strong distributional assumption to calculate higher moments
of Gaussian  and suffers from exponential dependency in m in their sample complexity.

✏2⌧ ?

min

3To satisfy this  one can always search through the set {1  c  c2  c3 ···} (for some constant c < 1) and get

an estimate in the interval [c⌧ ?

(j) ⌧ ?

(j)).

8

6 Discussion

Iterative least trimmed squares is the simplest instance of a much more general principle: that one can
make learning robust to bad training data by iteratively updating a model using only the samples it
best ﬁts currently. In this paper we provide rigorous theoretical evidence that it obtains state-of-the-art
results for a speciﬁc simple setting: mixed linear regression with corruptions. It is very interesting to
see if this positive evidence can be established in other (and more general) settings.
While it seems similar at ﬁrst glance  we note that our algorithm is not an instance of the Expectation-
Maximization (EM) algorithm. In particular  it is tempting to associate a binary selection “hidden
variable" zi for every sample i  and then use EM to minimize an overall loss that depends on ✓ and
the z’s. However  this EM approach needs us to posit a model for the data under both the zi = 0 (i.e.
“discarded sample") and zi = 1 (i.e. “chosen sample") choices. ILTS on the other hand only needs a
model for the zi = 1 case.

Acknowledgement

We would like to acknowledge NSF grants 1302435 and 1564000 for supporting this research.

9

References
[1] Sivaraman Balakrishnan  Martin J Wainwright  Bin Yu  et al. Statistical guarantees for the em
algorithm: From population to sample-based analysis. The Annals of Statistics  45(1):77–120 
2017.

[2] Kush Bhatia  Prateek Jain  and Purushottam Kar. Robust regression via hard thresholding. In

Advances in Neural Information Processing Systems  pages 721–729  2015.

[3] Stéphane Boucheron  Maud Thomas  et al. Concentration inequalities for order statistics.

Electronic Communications in Probability  17  2012.

[4] Tony Cai  Zongming Ma  and Yihong Wu. Optimal estimation and rank detection for sparse

spiked covariance matrices. Probability theory and related ﬁelds  161(3-4):781–815  2015.

[5] Arun Tejasvi Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear

regressions. In International Conference on Machine Learning  pages 1040–1048  2013.

[6] Yudong Chen  Constantine Caramanis  and Shie Mannor. Robust sparse regression under
adversarial corruption. In International Conference on Machine Learning  pages 774–782 
2013.

[7] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii.

SIAM Journal on Numerical Analysis  7(1):1–46  1970.

[8] Ilias Diakonikolas  Gautam Kamath  Daniel M Kane  Jerry Li  Ankur Moitra  and Alistair
Stewart. Being robust (in high dimensions) can be practical.
In Proceedings of the 34th
International Conference on Machine Learning-Volume 70  pages 999–1008. JMLR. org  2017.

[9] Ilias Diakonikolas  Gautam Kamath  Daniel M Kane  Jerry Li  Jacob Steinhardt  and Alis-
tair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint
arXiv:1803.02815  2018.

[10] Sushrut Karmalkar and Eric Price. Compressed sensing with adversarial sparse noise via l1
regression. In 2nd Symposium on Simplicity in Algorithms (SOSA 2019). Schloss Dagstuhl-
Leibniz-Zentrum fuer Informatik  2018.

[11] Adam R. Klivans  Pravesh K. Kothari  and Raghu Meka. Efﬁcient algorithms for outlier-robust

regression. In Conference on Learning Theory  pages 1420–1430  2018.

[12] Jason M Klusowski  Dana Yang  and WD Brinda. Estimating the coefﬁcients of a mixture of
two linear regressions by expectation maximization. IEEE Transactions on Information Theory 
2019.

[13] Jeongyeol Kwon and Constantine Caramanis. Global convergence of em algorithm for mixtures

of two component linear regression. arXiv preprint arXiv:1810.05752  2018.

[14] Yuanzhi Li and Yingyu Liang. Learning mixtures of linear regressions with nearly optimal

complexity. In Conference on Learning Theory  pages 1125–1144  2018.

[15] Liu Liu  Yanyao Shen  Tianyang Li  and Constantine Caramanis. High dimensional robust

sparse regression. arXiv preprint arXiv:1805.11643  2018.

[16] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians.
In Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  pages
93–102. IEEE  2010.

[17] David M Mount  Nathan S Netanyahu  Christine D Piatko  Ruth Silverman  and Angela Y Wu.

On the least trimmed squares estimator. Algorithmica  69(1):148–183  2014.

[18] Adarsh Prasad  Arun Sai Suggala  Sivaraman Balakrishnan  and Pradeep Ravikumar. Robust

estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485  2018.

10

[19] Peter J Rousseeuw. Least median of squares regression. Journal of the American statistical

association  79(388):871–880  1984.

[20] Hanie Sedghi  Majid Janzamin  and Anima Anandkumar. Provable tensor methods for learning
mixtures of generalized linear models. In Artiﬁcial Intelligence and Statistics  pages 1223–1231 
2016.

[21] Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss

minimization. International Conference on Machine Learning  2019.

[22] Arun Suggala  Adarsh Prasad  and Pradeep K Ravikumar. Connecting optimization and
regularization paths. In Advances in Neural Information Processing Systems  pages 10631–
10641  2018.

[23] Daniel Vainsencher  Shie Mannor  and Huan Xu. Ignoring is a bliss: Learning with large noise
through reweighting-minimization. In Conference on Learning Theory  pages 1849–1881  2017.

[24] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[25] Eunho Yang  Aurélie C Lozano  Aleksandr Aravkin  et al. A general family of trimmed estima-
tors for robust high-dimensional data analysis. Electronic Journal of Statistics  12(2):3519–3553 
2018.

[26] Xinyang Yi  Constantine Caramanis  and Sujay Sanghavi. Alternating minimization for mixed

linear regression. In International Conference on Machine Learning  pages 613–621  2014.

[27] Xinyang Yi  Constantine Caramanis  and Sujay Sanghavi. Solving a mixture of many ran-
dom linear equations by tensor decomposition and alternating minimization. arXiv preprint
arXiv:1608.05749  2016.

[28] Dong Yin  Ramtin Pedarsani  Yudong Chen  and Kannan Ramchandran. Learning mixtures of
sparse linear regressions using sparse graph codes. IEEE Transactions on Information Theory 
2018.

[29] Kai Zhong  Prateek Jain  and Inderjit S Dhillon. Mixed linear regression with multiple compo-

nents. In Advances in neural information processing systems  pages 2190–2198  2016.

11

,Yanyao Shen
Sujay Sanghavi