2014,Pre-training of Recurrent Neural Networks via Linear Autoencoders,We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences  i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution  however  is computationally very demanding  so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network  which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music  we show that the proposed pre-training approach is highly effective  since it allows to largely improve the state of the art results on all the considered datasets.,Pre-training of Recurrent Neural Networks via

Linear Autoencoders

Luca Pasa  Alessandro Sperduti

Department of Mathematics
University of Padova  Italy

{pasa sperduti}@math.unipd.it

Abstract

We propose a pre-training technique for recurrent neural networks based on linear
autoencoder networks for sequences  i.e. linear dynamical systems modelling the
target sequences. We start by giving a closed form solution for the deﬁnition of
the optimal weights of a linear autoencoder given a training set of sequences. This
solution  however  is computationally very demanding  so we suggest a procedure
to get an approximate solution for a given number of hidden units. The weights
obtained for the linear autoencoder are then used as initial weights for the input-
to-hidden connections of a recurrent neural network  which is then trained on the
desired task. Using four well known datasets of sequences of polyphonic music 
we show that the proposed pre-training approach is highly effective  since it allows
to largely improve the state of the art results on all the considered datasets.

1

Introduction

Recurrent Neural Networks (RNN) constitute a powerful computational tool for sequences mod-
elling and prediction [1]. However  training a RNN is not an easy task  mainly because of the well
known vanishing gradient problem which makes difﬁcult to learn long-term dependencies [2]. Al-
though alternative architectures  e.g. LSTM networks [3]  and more efﬁcient training procedures 
such as Hessian Free Optimization [4]  have been proposed to circumvent this problem  reliable and
effective training of RNNs is still an open problem.
The vanishing gradient problem is also an obstacle to Deep Learning  e.g.  [5  6  7]. In that context 
there is a growing evidence that effective learning should be based on relevant and robust internal
representations developed in autonomy by the learning system. This is usually achieved in vectorial
spaces by exploiting nonlinear autoencoder networks to learn rich internal representations of input
data which are then used as input to shallow neural classiﬁers or predictors (see  for example  [8]).
The importance to start gradient-based learning from a good initial point in the parameter space has
also been pointed out in [9]. Relationship between autoencoder networks and Principal Component
Analysis (PCA) [10] is well known since late ‘80s  especially in the case of linear hidden units [11 
12]. More recently  linear autoencoder networks for structured data have been studied in [13  14  15] 
where an exact closed-form solution for the weights is given in the case of a number of hidden units
equal to the rank of the full data matrix.
In this paper  we borrow the conceptual framework presented in [13  16] to devise an effective pre-
training approach  based on linear autoencoder networks for sequences  to get a good starting point
into the weight space of a RNN  which can then be successfully trained even in presence of long-
term dependencies. Speciﬁcally  we revise the theoretical approach presented in [13] by: i) giving
a simpler and direct solution to the problem of devising an exact closed-form solution (full rank
case) for the weights of a linear autoencoder network for sequences  highlighting the relationship
between the proposed solution and PCA of the input data; ii) introducing a new formulation of

1

the autoencoder learning problem able to return an optimal solution also in the case of a number
of hidden units which is less than the rank of the full data matrix; iii) proposing a procedure for
approximate learning of the autoencoder network weights under the scenario of very large sequence
datasets. More importantly  we show how to use the linear autoencoder network solution to derive a
good initial point into a RNN weight space  and how the proposed approach is able to return quite
impressive results when applied to prediction tasks involving long sequences of polyphonic music.

2 Linear Autoencoder Networks for Sequences
In [11  12] it is shown that principal directions of a set of vectors xi ∈ Rk are related to solutions
obtained by training linear autoencoder networks

oi = WoutputWhiddenxi  i = 1  . . .   n 

(1)
where Whidden ∈ Rp×k  Woutput ∈ Rk×p  p (cid:28) k  and the network is trained so to get oi = xi  ∀i.
When considering a temporal sequence x1  x2  . . .   xt  . . . of input vectors  where t is a discrete time
index  a linear autoencoder can be deﬁned by considering the coupled linear dynamical systems

yt = Axt + Byt−1

(2)

= Cyt

(3)

(cid:21)

(cid:20) xt

yt−1

It should be noticed that eqs. (2) and (3) extend the linear transformation deﬁned in eq. (1) by
introducing a memory term involving matrix B ∈ Rp×p. In fact  yt−1 is inserted in the right part
of equation (2) to keep track of the input history through time: this is done exploiting a state space
representation. Eq. (3) represents the decoding part of the autoencoder: when a state yt is multiplied
by C  the observed input xt at time t and state at time t − 1  i.e. yt−1  are generated. Decoding
can then continue from yt−1. This formulation has been proposed  for example  in [17] where an
iterative procedure to learn weight matrices A and B  based on Oja’s rule  is presented. No proof
of convergence for the proposed procedure is however given. More recently  an exact closed-form
solution for the weights has been given in the case of a number of hidden units equal to the rank of
the full data matrix (full rank case) [13  16]. In this section  we revise this result. In addition  we
give an exact solution also for the case in which the number of hidden units is strictly less than the
rank of the full data matrix.
The basic idea of [13  16] is to look for directions of high variance into the state space of the
dynamical linear system (2). Let start by considering a single sequence x1  x2  . . .   xt  . . .   xn and
the state vectors of the corresponding induced state sequence collected as rows of a matrix Y =
[y1  y2  y3 ···   yn]T. By using the initial condition y0 = 0 (the null vector)  and the dynamical
linear system (2)  we can rewrite the Y matrix as



(cid:124)

Y =

xT
0
1
2 xT
xT
1
xT
3 xT
2
...
...
xT
n xT

0
0
xT
1
...
n−1 xT

n−2

(cid:123)(cid:122)

Ξ

0
0
0

···
···
···
...
...
··· xT

2

0
0
0

...
xT
1



(cid:125)



(cid:124)

AT
ATBT
ATB2T

ATBn−1T

...
(cid:123)(cid:122)

Ω



(cid:125)

where  given s = kn  Ξ ∈ Rn×s is a data matrix collecting all the (inverted) input subsequences
(including the whole sequence) as rows  and Ω is the parameter matrix of the dynamical system.
Now  we are interested in using a state space of dimension p (cid:28) n  i.e. yt ∈ Rp  such that as
much information as contained in Ω is preserved. We start by factorizing Ξ using SVD  obtaining
Ξ = VΛUT where V ∈ Rn×n is an unitary matrix  Λ ∈ Rn×s is a rectangular diagonal matrix
with nonnegative real numbers on the diagonal with λ1 1 ≥ λ2 2 ≥ ··· ≥ λn n (the singular values) 
and UT ∈ Rs×n is a unitary matrix.
It is important to notice that columns of UT which correspond to nonzero singular values  apart
some mathematical technicalities  basically correspond to the principal directions of data  i.e. PCA.
If the rank of Ξ is p  then only the ﬁrst p elements of the diagonal of Λ are not null  and the
above decomposition can be reduced to Ξ = V(p)Λ(p)U(p)T where V(p) ∈ Rn×p  Λ(p) ∈ Rp×p 

2

and U(p)T ∈ Rp×n. Now we can observe that U(p)T
U(p) = I (where I is the identity matrix of
dimension p)  since by deﬁnition the columns of U(p) are orthogonal  and by imposing Ω = U(p) 
we can derive “optimal” matrices A ∈ Rp×k and B ∈ Rp×p for our dynamical system  which will
have corresponding state space matrix Y(p) = ΞΩ = ΞU(p) = V(p)Λ(p)U(p)T
U(p) = V(p)Λ(p).
  each of size k × p  the problem
Thus  if we represent U(p) as composed of n submatrices U(p)
reduces to ﬁnd matrices A and B such that



Ω =

 =



U(p)
1
U(p)
2
U(p)
3 ...
U(p)
n

i

 = U(p).

AT
ATBT
ATB2T

...

ATBn−1T

(4)

The reason to impose Ω = U(p) is to get a state space where the coordinates are uncorrelated so
to diagonalise the empirical sample covariance matrix of the states. Please  note that in this way
each state (i.e.  row of the Y matrix) corresponds to a row of the data matrix Ξ  i.e. the unrolled
(sub)sequence read up to a given time t. If the rows of Ξ were vectors  this would correspond to
compute PCA  keeping only the ﬁst p principal directions.
In the following  we demonstrate that there exists a solution to the above equation. We start
by observing that Ξ owns a special structure  i.e. given Ξ = [Ξ1 Ξ2 ··· Ξn]  where Ξi ∈
Rn×k  then for i = 1  . . .   n − 1  Ξi+1 = RnΞi =
Ξi   and
RnΞn = 0  i.e. the null matrix of size n × k. Moreover  by singular value decomposition  we
have Ξi = V(p)Λ(p)U(p)
V(p) = I  and
for i = 1  . . .   n − 1  and t =
combining the above equations  we get U(p)
i Qt 
1  . . .   n − i  where Q = Λ(p)V(p)T
n Q = 0 since
)TV(p)Λ(p)−1. Thus  eq. (4) is satisﬁed by
U(p)

nV(p)Λ(p)−1. Moreover  we have that U(p)
RT
nV(p)Λ(p)−1
RT
= (RnΞn

i = 1  . . .   n. Using the fact that V(p)T

(cid:20) 01×(n−1)

I(n−1)×(n−1) 0(n−1)×1

n Λ(p)V(p)T

i+t = U(p)

n Q = U(p)

01×1

(cid:21)

for

i

T

 

T

A = U(p)
1
computing Y(p)U(p)T

and B = QT. It is interesting to note that the original data Ξ can be recovered by

= V(p)Λ(p)U(p)T

= Ξ  which can be achieved by running the system

=0

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:20) AT

=

BT

(cid:21)

yt

(cid:21)

(cid:20) xt

yt−1

starting from yn  i.e.

is the matrix C deﬁned in eq. (3).

Finally  it is important to remark that the above construction works not only for a single sequence 
but also for a set of sequences of different length. For example  let consider the two sequences
(xa

2). Then  we have

1  xb

3) and (xb

2  xa

1  xa

T 0
T xa
1
T xa
2

0
T 0
T xa
1

T

1
xa
2
xa
3

(cid:34)

 and Ξb =
(cid:20)
(cid:21)

Ξa

Ξb 02×1

T

T

xb
1
xb
2

0
xb
1

T

(cid:35)
(cid:20)

(cid:21)

.

R4

R2 02×1

which can be collected together to obtain Ξ =

  and R =

As a ﬁnal remark  it should be stressed that the above construction only works if p is equal to the
rank of Ξ. In the next section  we treat the case in which p < rank(Ξ).

2.1 Optimal solution for low dimensional autoencoders

T(cid:54)= Ξi  and
When p < rank(Ξ) the solution given above breaks down because ˜Ξi = V(p)L(p)U(p)
consequently ˜Ξi+1 (cid:54)= Rn ˜Ξi. So the question is whether the proposed solutions for A and B still
hold the best reconstruction error when p < rank(Ξ).

i

3

BT

(cid:21)

(cid:20) AT
 xa

Ξa =

In this paper  we answer in negative terms to this question by resorting to a new formulation of our
i ∈ Rk×p  i = 1  . . .   n + 1 collecting the
problem where we introduce slack-like matrices E(p)
n+1(cid:88)
reconstruction errors  which need to be minimised:



min

Q∈Rp×p E(p)

i

1

U(p)
U(p)
U(p)

1 + E(p)
2 + E(p)
3 + E(p)

2

3

...

U(p)

n + E(p)
n

i=1

(cid:107)E(p)
i (cid:107)2

F

 Q =





U(p)
U(p)

2 + E(p)
3 + E(p)

2

3

...

U(p)

n + E(p)
n
E(p)
n+1

subject to :

(5)

i and Q∗  from which we can derive AT = U(p)

Notice that the problem above is convex both in the objective function and in the constraints; thus
it only has global optimal solutions E∗
1 and
BT = Q∗. Speciﬁcally  when p = rank(Ξ)  RT
s kU(p) is in the span of U(p) and the optimal
i = 0k×p ∀i  and Q∗ = U(p)T
solution is given by E∗
s kU(p)  i.e. the solution we have already
described. If p < rank(Ξ)  the optimal solution cannot have ∀i  E∗
i = 0k×p. However  it is not
difﬁcult to devise an iterative procedure to reach the minimum. Since in the experimental section we
do not exploit the solution to this problem for reasons that we will explain later  here we just sketch
such procedure. It helps to observe that  given a ﬁxed Q  the optimal solution for E(p)

1 + E∗

is given by

RT

[ ˜E(p)

1   ˜E(p)

2   . . .   ˜E(p)

n+1] = [U(p)

1 Q − U(p)

i

4   . . .] M+
Q



1 Q2 − U(p)
2   U(p)
−Q −Q2 −Q3
0
I
0
0
0
I

0
I
0

 .

1 Q3 − U(p)
3   U(p)
···
···
···
···
...

...

where M+

Q is the pseudo inverse of MQ =

...

  ˜E(p)T
In general  ˜E(p) =
span of U(p) and a component E(p)⊥
(part of) the other component can be absorbed into Q by deﬁning ˜U(p) = U(p) + E(p)⊥

(cid:104) ˜E(p)T
˜Q = ( ˜U(p))+(cid:104) ˜U(p)T

...
 ···   ˜E(p)T
orthogonal to it. Notice that E(p)⊥
(cid:105)T

 ···   ˜U(p)T

  ˜U(p)T

  E(p)T

  ˜E(p)T

(cid:105)T

n

2

1

3

.

2

3

n

n+1

cannot be reduced  while
and taking

can be decomposed into a component in the

Given ˜Q  the new optimal values for E(p)

i

are obtained and the process iterated till convergence.

3 Pre-training of Recurrent Neural Networks

Here we deﬁne our pre-training procedure for recurrent neural networks with one hidden layer of p
units  and O output units:

ot = σ(Woutputh(xt)) ∈ RO  h(xt) = σ(Winputxt + Whiddenh(xt−1)) ∈ Rp

(6)
where Woutput ∈ RO×p  Whidden ∈ Rp×k  for a vector z ∈ Rm  σ(z) = [σ(z1)  . . .   σ(zm)]T 
and here we consider the symmetric sigmoid function σ(zi) = 1−e−zi
1+e−zi .
The idea is to exploit the hidden state representation obtained by eqs. (2) as initial hidden state repre-
sentation for the RNN described by eqs. (6). This is implemented by initialising the weight matrices
Winput and Whidden of (6) by using the matrices that jointly solve eqs. (2) and eqs. (3)  i.e. A and
B (since C is function of A and B). Speciﬁcally  we initialize Winput with A  and Whidden with
B. Moreover  the use of symmetrical sigmoidal functions  which do give a very good approximation
of the identity function around the origin  allows a good transferring of the linear dynamics inside

4

RNN. For what concerns Woutput  we initialise it by using the best possible solution  i.e. the pseu-
doinverse of H times the target matrix T  which does minimise the output squared error. Learning
is then used to introduce nonlinear components that allow to improve the performance of the model.
More formally  let consider a prediction task where for each sequence sq ≡ (xq
2  . . .   xq
)
lq
of length lq in the training set  a sequence tq of target vectors is deﬁned  i.e. a training se-
i ∈ RO. Given a train-
quence is given by (cid:104)sq  tq(cid:105) ≡ (cid:104)(xq
q=1 lq  as

ing set with N sequences  let deﬁne the target matrix T ∈ RL×O  where L = (cid:80)N
T = (cid:2)t1

(cid:3)T. The input matrix Ξ will have size L × k. Let p∗ be the de-

)(cid:105)  where tq

1  tq

1)  (xq

2  tq

2)  . . .   (xq
lq

sired number of hidden units for the recurrent neural network (RNN). Then the pre-training proce-
dure can be deﬁned as follows: i) compute the linear autoencoder for Ξ using p∗ principal direc-
tions  obtaining the optimal matrices A∗ ∈ Rp∗×k and B∗ ∈ Rp∗×p∗
; i) set Winput = A∗ and
Whidden = B∗; iii) run the RNN over the training sequences  collecting the hidden activities vec-
tors (computed using symmetrical sigmoidal functions) over time as rows of matrix H ∈ RL×p∗
;
iv) set Woutput = H+T  where H+ is the (left) pseudoinverse of H.

1  t1

2  . . .   t1
l1

  t2

1  . . .   tN
lN

1  xq

  tq
lq

3.1 Computing an approximate solution for large datasets

In real world scenarios the application of our approach may turn difﬁcult because of the size of
the data matrix. In fact  stable computation of principal directions is usually obtained by SVD de-
composition of the data matrix Ξ  that in typical application domains involves a number of rows
and columns which is easily of the order of hundreds of thousands. Unfortunately  the computa-
tional complexity of SVD decomposition is basically cubic in the smallest of the matrix dimensions.
Memory consumption is also an important issue. Algorithms for approximate computation of SVD
have been suggested (e.g.  [18])  however  since for our purposes we just need matrices V and Λ
with a predeﬁned number of columns (i.e. p)  here we present an ad-hoc algorithm for approximate
computation of these matrices. Our solution is based on the following four main ideas: i) divide Ξ
in slices of k (i.e.  size of input at time t) columns  so to exploit SVD decomposition at each slice
separately; ii) compute approximate V and Λ matrices  with p columns  incrementally via truncated
SVD of temporary matrices obtained by concatenating the current approximation of VΛ with a new
slice; iii) compute the SVD decomposition of a temporary matrix via either its kernel or covariance
matrix  depending on the smallest between the number of rows and the number of columns of the
temporary matrix; iv) exploit QR decomposition to compute SVD decomposition.
Algorithm 1 shows in pseudo-code the main steps of our procedure. It maintains a temporary matrix
T which is used to collect incrementally an approximation of the principal subspace of dimension p
of Ξ. Initially (line 4) T is set equal to the last slices of Ξ  in a number sufﬁcient to get a number
of columns larger than p (line 2). Matrices V and Λ from the p-truncated SVD decomposition of
T are computed (line 5) via the KECO procedure  described in Algorithm 2  and used to deﬁne a
new T matrix by concatenation with the last unused slice of Ξ. When all slices are processed  the
current V and Λ matrices are returned. The KECO procedure  described in Algorithm 2   reduces
the computational burden by computing the p-truncated SVD decomposition of the input matrix
M via its kernel matrix (lines 3-4) if the number of rows of M is no larger than the number of
columns  otherwise the covariance matrix is used (lines 6-8). In both cases  the p-truncated SVD
decomposition is implemented via QR decomposition by the INDIRECTSVD procedure described in
Algorithm 3. This allows to reduce computation time when large matrices must be processed [19].
Finally  matrices V and S 1
2 (both kernel and covariance matrices have squared singular values of
M) are returned.
We use the strategy to process slices of Ξ in reverse order since  moving versus columns with larger
indices  the rank as well as the norm of slices become smaller and smaller  thus giving less and less
contribution to the principal subspace of dimension p. This should reduce the approximation error
cumulated by dropping the components from p + 1 to p + k during computation [20]. As a ﬁnal
remark  we stress that since we compute an approximate solution for the principal directions of Ξ 
it makes no much sense to solve the problem given in eq. (5): learning will quickly compensate
for the approximations and/or sub-optimality of A and B obtained by matrices V and Λ returned
by Algorithm 1. Thus  these are the matrices we have used for the experiments described in next
section.

5

end for
return V  Λ

nStart = (cid:100)p/k(cid:101)
nSlice = (Ξ.columns/k) − nStart
T = Ξ[:  k ∗ nSlice : Ξ.columns]
V  Λ =KECO(T  p)
for i in REVERSED(range(nSlice)) do
T = [Ξ[:  i ∗ k:(i + 1) ∗ k]  VΛ]
V  Λ =KECO(T  p)

Algorithm 1 Approximated V and Λ with p components
1: function SVFORBIGDATA(Ξ  k  p)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end function
Algorithm 2 Kernel vs covariance computation
1: function KECO(M  p)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end function

C = MTM
V  Ssqr  UT =INDIRECTSVD(C  p)
V = MUTS

K = MMT
V  Ssqr  UT =INDIRECTSVD(K  p)

if M.rows <= Ξ.columns then

else

− 1
2
sqr

end if
return V  S

1
2
sqr

(cid:46) Number of starting slices
(cid:46) Number of remaining slices

(cid:46) Computation of V and Λ for starting slices
(cid:46) Computation of V and Λ for remaining slices

Algorithm 3 Truncated SVD by QR
1: function INDIRECTSVD(M  p)
2:
3:
4:
5:
6:
7:
8:
9: end function

Q  R =QR(M)
Vr  S  UT =SVD(R)
V = QVr
S = S[1 : p  1 : p]
V = V[1 : p  :]
UT = UT[:  1 : p]
return V  S  UT

4 Experiments

In order to evaluate our pre-training approach  we decided to use the four polyphonic music se-
quences datasets used in [21] for assessing the prediction abilities of the RNN-RBM model. The
prediction task consists in predicting the notes played at time t given the sequence of notes played
till time t − 1. The RNN-RBM model achieves state-of-the-art in such demanding prediction task.
As performance measure we adopted the accuracy measure used in [21] and described in [22]. Each
dataset is split in training set  validation set  and test set. Statistics on the datasets  including largest
sequence length  are given in columns 2-4 of Table 1. Each sequence in the dataset represents a song
having a maximum polyphony of 15 notes (average 3.9); each time step input spans the whole range
of piano from A0 to C8 and it is represented by using 88 binary values (i.e. k = 88).
Our pre-training approach (PreT-RNN) has been assessed by using a different number of hidden
units (i.e.  p is set in turn to 50  100  150  200  250) and 5000 epochs of RNN training1 using the
Theano-based stochastic gradient descent software available at [23].
Random initialisation (Rnd) has also been used for networks with the same number of hidden units.
Speciﬁcally  for networks with 50 hidden units  we have evaluated the performance of 6 different
random initialisations. Finally  in order to verify that the nonlinearity introduced by the RNN is
actually useful to solve the prediction task  we have also evaluated the performance of a network
with linear units (250 hidden units) initialised with our pre-training procedure (PreT-Lin250).
To give an idea of the time performance of pre-training with respect to the training of a RNN  in
column 5 of Table 1 we have reported the time in seconds needed to compute pre-training matrices
(Pre-) (on Intel c(cid:13) Xeon c(cid:13) CPU E5-2670 @2.60GHz with 128 GB) and to perform training of a
RNN with p = 50 for 5000 epochs (on GPU NVidia K20). Please  note that for larger values of p 
the increase in computation time of pre-training is smaller than the increment in computation time
needed for training a RNN.

1Due to early overﬁtting  for the Muse dataset we used 1000 epochs.

6

Dataset

Nottingham

Piano-midi.de

MuseData

JSB Chorales

Set

# Samples Max length

(Pre-)Training Time

Training

(39165 × 56408)

(70672 × 387640)

Test

Validation
Training

Test

Validation
Training

Test

Validation
Training

Test

Validation

(248479 × 214192)

(27674 × 22792)

195

170
173
87

25
12
524

25
135
229

77
76

641

1495
1229
4405

2305
1740
2434

2305
2523
259

320
289

seconds

(226) 5837
p = 50

5000 epochs

seconds

(2971) 4147

p = 50

5000 epochs

seconds

(7338) 4190

p = 50

5000 epochs

seconds
(79) 6411
p = 50

5000 epochs

Model
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250

ACC% [21]
62.93 (66.64)

75.40

75.23 (p = 250)

73.19

19.33 (23.34)

37.74 (p = 250)

23.25 (30.49)

57.57 (p = 200)

28.46 (29.41)

65.67 (p = 250)

28.92

16.87

34.02

3.56

33.12

38.32

Table 1: Datasets statistics including data matrix size for the training set (columns 2-4)  computa-
tional times in seconds to perform pre-training and training for 5000 epochs with p = 50 (column
5)  and accuracy results for state-of-the-art models [21] vs our pre-training approach (columns 6-7).
The acronym (w. HF) is used to identify an RNN trained by Hessian Free Optimization [4].

Training and test curves for all the models described above are reported in Figure 1. It is evident that
random initialisation does not allow the RNN to improve its performance in a reasonable amount of
epochs. Speciﬁcally  for random initialisation with p = 50 (Rnd 50)  we have reported the average
and range of variation over the 6 different trails: different initial points do not change substantially
the performance of RNN. Increasing the number of hidden units allows the RNN to slightly increase
its performance. Using pre-training  on the other hand  allows the RNN to start training from a quite
favourable point  as demonstrated by an early sharp improvement of performances. Moreover  the
more hidden units are used  the more the improvement in performance is obtained  till overﬁtting is
observed. In particular  early overﬁtting occurs for the Muse dataset. It can be noticed that the linear
model (Linear) reaches performances which are in some cases better than RNN without pre-training.
However  it is important to notice that while it achieves good results on the training set (e.g. JSB and
Piano-midi)  the corresponding performance on the test set is poor  showing a clear evidence of over-
ﬁtting. Finally  in column 7 of Table 1  we have reported the accuracy obtained after validation on
the number of hidden units and number of epochs for our approaches (PreT-RNN and PreT-Lin250)
versus the results reported in [21] for RNN (also using Hessian Free Optimization) and RNN-RBM.
In any case  the use of pre-training largely improves the performances over standard RNN (with
or without Hessian Free Optimization). Moreover  with the exception of the Nottingham dataset 
the proposed approach outperforms the state-of-the-art results achieved by RNN-RBM. Large im-
provements are observed for the Muse and JSB datasets. Performance for the Nottingham dataset
is basically equivalent to the one obtained by RNN-RBM. For this dataset  also the linear model
with pre-training achieves quite good results  which seems to suggest that the prediction task for
this dataset is much easier than for the other datasets. The linear model outperforms RNN without
pre-training on Nottingham and JSB datasets  but shows problems with the Muse dataset.

5 Conclusions

We have proposed a pre-training technique for RNN based on linear autoencoders for sequences.
For this kind of autoencoders it is possible to give a closed form solution for the deﬁnition of the
“optimal” weights  which however  entails the computation of the SVD decomposition of the full
data matrix. For large data matrices exact SVD decomposition cannot be achieved  so we proposed
a computationally efﬁcient procedure to get an approximation that turned to be effective for our
goals. Experimental results for a prediction task on datasets of sequences of polyphonic music
show the usefulness of the proposed pre-training approach  since it allows to largely improve the
state of the art results on all the considered datasets by using simple stochastic gradient descend for
learning. Even if the results are very encouraging the method needs to be assessed on data from
other application domains. Moreover  it is interesting to understand whether the analysis performed
in [24] on linear deep networks for vectors can be extended to recurrent architectures for sequences
and  in particular  to our method.

7

Figure 1: Training (left column) and test (right column) curves for the assessed approaches on the
four datasets. Curves are sampled at each epoch till epoch 100  and at steps of 100 epochs afterwards.

8

-0.3-0.2-0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0 200 400 600 800 1000AccuracyEpochMuse Dataset Test SetRnd 50 (6 trials)Linear 250Rnd 100Rnd 150Rnd 200Rnd 250PreT 50PreT 150PreT 100PreT 200PreT 250 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000AccuracyEpochNottingham Training Set 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000AccuracyEpochNottingham Test Set 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000AccuracyEpochPiano-Midi.de Training Set 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000AccuracyEpochPiano-Midi.de Test Set 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 200 400 600 800 1000AccuracyEpochMuse Dataset Training Set 0 0.1 0.2 0.3 0.4 0.5 0.6 0 200 400 600 800 1000AccuracyEpochMuse Dataset Test Set 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000AccuracyEpochJSB Chorales Training Set 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000AccuracyEpochJSB Chorales Test SetReferences
[1] S. C. Kremer. Field Guide to Dynamical Recurrent Networks. Wiley-IEEE Press  2001.
[2] Y. Bengio  P. Simard  and P. Frasconi. Learning long-term dependencies with gradient descent

is difﬁcult. IEEE Transactions on Neural Networks  5(2):157–166  1994.

[3] S. Hochreiter and J. Schmidhuber. Lstm can solve hard long time lag problems. In NIPS  pages

473–479  1996.

[4] J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimization.

In ICML  pages 1033–1040  2011.

[5] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural net-

works. Science  313(5786):504–507  July 2006.

[6] G. E. Hinton  S. Osindero  and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural

Computation  18(7):1527–1554  2006.

[7] P. di Lena  K. Nagata  and P. Baldi. Deep architectures for protein contact map prediction.

Bioinformatics  28(19):2449–2457  2012.

[8] Y. Bengio. Learning deep architectures for ai. Foundations and Trends in Machine Learning 

2(1):1–127  2009.

[9] I. Sutskever  J. Martens  G. E. Dahl  and G. E. Hinton. On the importance of initialization and

momentum in deep learning. In ICML (3)  pages 1139–1147  2013.

[10] I.T. Jolliffe. Principal Component Analysis. Springer-Verlag New York  Inc.  2002.
[11] H. Bourlard and Y. Kamp. Auto-association by multilayer perceptrons and singular value

decomposition. Biological Cybernetics  59(4-5):291–294  1988.

[12] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from

examples without local minima. Neural Networks  2(1):53–58  1989.

[13] A. Sperduti. Exact solutions for recursive principal components analysis of sequences and

trees. In ICANN (1)  pages 349–356  2006.

[14] A. Micheli and A. Sperduti. Recursive principal component analysis of graphs. In ICANN (2) 

pages 826–835  2007.

[15] A. Sperduti. Efﬁcient computation of recursive principal component analysis for structured

input. In ECML  pages 335–346  2007.

[16] A. Sperduti. Linear autoencoder networks for structured data. In NeSy’13:Ninth International

Workshop onNeural-Symbolic Learning and Reasoning  2013.

[17] T. Voegtlin. Recursive principal components analysis. Neural Netw.  18(8):1051–1063  2005.
[18] G. Martinsson et al. Randomized methods for computing the singular value decomposition
(svd) of very large matrices. In Works. on Alg. for Modern Mass. Data Sets  Palo Alto  2010.

[19] E. Rabani and S. Toledo. Out-of-core svd and qr decompositions. In PPSC  2001.
[20] Z. Zhang and H. Zha. Structure and perturbation analysis of truncated svds for column-

partitioned matrices. SIAM J. on Mat. Anal. and Appl.  22(4):1245–1262  2001.

[21] N. Boulanger-Lewandowski  Y. Bengio  and P. Vincent. Modeling temporal dependencies in
high-dimensional sequences: Application to polyphonic music generation and transcription.
In ICML  2012.

[22] M. Bay  A. F. Ehmann  and J. S. Downie. Evaluation of multiple-f0 estimation and tracking

systems. ISMIR  pages 315–320  2009.
[23] https://github.com/gwtaylor/theano-rnn.
[24] A. M. Saxe  J. L. McClelland  and S. Ganguli. Exact solutions to the nonlinear dynamics of

learning in deep linear neural networks. arXiv preprint arXiv:1312.6120  2013.

9

,Luca Pasa
Alessandro Sperduti