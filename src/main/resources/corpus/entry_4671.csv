2011,Reinforcement Learning using Kernel-Based Stochastic Factorization,Kernel-based reinforcement-learning (KBRL) is a method for learning a decision policy from a set of sample transitions which stands out for its strong theoretical guarantees. However  the size of the approximator grows with the number of transitions  which makes the approach impractical for large problems.  In this paper we introduce a novel algorithm to improve the scalability of KBRL. We resort to a special decomposition of a transition matrix  called stochastic factorization  to fix the size of the approximator while at the same time incorporating all the information contained in the data. The resulting algorithm  kernel-based stochastic factorization (KBSF)  is much faster but still converges to a unique solution. We derive a theoretical upper bound for the distance between the value functions computed by KBRL and KBSF. The effectiveness of our method is illustrated with computational experiments on four reinforcement-learning problems  including a difficult task in which the goal is to learn a neurostimulation policy to suppress the occurrence of seizures in epileptic rat brains. We empirically demonstrate that the proposed approach is able to compress the information contained in KBRL's model. Also  on the tasks studied  KBSF outperforms two of the most prominent reinforcement-learning algorithms  namely least-squares policy iteration and fitted Q-iteration.,Reinforcement Learning using Kernel-Based

Stochastic Factorization

Andr´e M. S. Barreto

Doina Precup

Joelle Pineau

School of Computer Science

School of Computer Science

School of Computer Science

McGill University
Montreal  Canada

McGill University
Montreal  Canada

McGill University
Montreal  Canada

amsb@cs.mcgill.ca

dprecup@cs.mcgill.ca

jpineau@cs.mcgill.ca

Abstract

Kernel-based reinforcement-learning (KBRL) is a method for learning a decision
policy from a set of sample transitions which stands out for its strong theoretical
guarantees. However  the size of the approximator grows with the number of tran-
sitions  which makes the approach impractical for large problems. In this paper
we introduce a novel algorithm to improve the scalability of KBRL. We resort
to a special decomposition of a transition matrix  called stochastic factorization 
to ﬁx the size of the approximator while at the same time incorporating all the
information contained in the data. The resulting algorithm  kernel-based stochas-
tic factorization (KBSF)  is much faster but still converges to a unique solution.
We derive a theoretical upper bound for the distance between the value functions
computed by KBRL and KBSF. The effectiveness of our method is illustrated with
computational experiments on four reinforcement-learning problems  including a
difﬁcult task in which the goal is to learn a neurostimulation policy to suppress
the occurrence of seizures in epileptic rat brains. We empirically demonstrate that
the proposed approach is able to compress the information contained in KBRL’s
model. Also  on the tasks studied  KBSF outperforms two of the most promi-
nent reinforcement-learning algorithms  namely least-squares policy iteration and
ﬁtted Q-iteration.

1

Introduction

Recent years have witnessed the emergence of several reinforcement-learning techniques that make
it possible to learn a decision policy from a batch of sample transitions. Among them  Ormoneit
and Sen’s kernel-based reinforcement learning (KBRL) stands out for two reasons [1]. First  unlike
other approximation schemes  KBRL always converges to a unique solution. Second  KBRL is
consistent in the statistical sense  meaning that adding more data always improves the quality of the
resulting policy and eventually leads to optimal performance.

Despite its nice theoretical properties  KBRL has not been widely adopted by the reinforcement
learning community. One possible explanation for this is its high computational complexity. As
discussed by Ormoneit and Glynn [2]  KBRL can be seen as the derivation of a ﬁnite Markov
decision process whose number of states coincides with the number of sample transitions collected
to perform the approximation. This gives rise to a dilemma: on the one hand one wants as much
data as possible to describe the dynamics of the decision problem  but on the other hand the number
of transitions should be small enough to allow for the numerical solution of the resulting model.

In this paper we describe a practical way of weighting the relative importance of these two con-
ﬂicting objectives. We rely on a special decomposition of a transition matrix  called stochastic
factorization  to rewrite it as the product of two stochastic matrices of smaller dimension. As we

1

will see  the stochastic factorization possesses a very useful property: if we swap its factors  we
obtain another transition matrix which retains some fundamental characteristics of the original one.
We exploit this property to ﬁx the size of KBRL’s model. The resulting algorithm  kernel-based
stochastic factorization (KBSF)  is much faster than KBRL but still converges to a unique solution.
We derive a theoretical bound on the distance between the value functions computed by KBRL and
KBSF. We also present experiments on four reinforcement-learning domains  including the double
pole-balancing task  a difﬁcult control problem representative of a wide class of unstable dynamical
systems  and a model of epileptic rat brains in which the goal is to learn a neurostimulation policy
to suppress the occurrence of seizures. We empirically show that the proposed approach is able to
compress the information contained in KBRL’s model  outperforming both the least-squares policy
iteration algorithm and ﬁtted Q-iteration on the tasks studied [3  4].

2 Background

The KBRL algorithm solves a continuous state-space Markov Decision Process (MDP) using a ﬁnite
model approximation. A ﬁnite MDP is deﬁned by a tuple M ≡ (S  A  Pa  ra  γ) [5]. The ﬁnite sets
S and A are the state and action spaces. The matrix Pa ∈ R|S|×|S| gives the transition probabilities
associated with action a ∈ A and the vector ra ∈ R|S| stores the corresponding expected rewards. The
discount factor γ ∈ [0  1) is used to give smaller weights to rewards received further in the future.

In the case of a ﬁnite MDP  we can use dynamic programming to ﬁnd an optimal decision-policy
π ∗ ∈ A|S| in polynomial time [5]. As well known  this is done using the concept of a value function.
Throughout the paper  we use v ∈ R|S| to denote the state-value function and Q ∈ R|S|×|A| to refer to
the action-value function. Let the operator Γ : R|S|×|A| 7→ R|S| be given by ΓQ = v  with vi = max j qi j 
and deﬁne ∆ : R|S| 7→ R|S|×|A| as ∆v = Q  where the ath column of Q is given by qa = ra + γPav.
A fundamental result in dynamic programming states that  starting from v(0) = 0  the expression
v(t) = Γ∆v(t−1) gives the optimal t-step value function  and as t → ∞ the vector v(t) approaches v∗ 
from which any optimal decision policy π ∗ can be derived [5].

Consider now an MDP with continuous state space S ⊂ Rd and let Sa = {(sa
be a set of sample transitions associated with action a ∈ A  where sa
k   ˆsa
constructed by KBRL has the following transition and reward functions:

k   ˆsa
k ∈ S and ra

k   ra

k)|k = 1  2  ...  na}
k ∈ R. The model

ˆPa(s j|si) =(cid:26) κ a(si  sa
k) is a weighting kernel centered at sa

k)  if s j = ˆsa
k  

0  otherwise

and

ˆRa(si  s j) =(cid:26) ra

k   if s j = ˆsa
k  
0  otherwise 

k and deﬁned in such a way that ∑na

where κ a(·  sa
k) = 1
for all si ∈ S (for example  κ a can be a normalized Gaussian function; see [1] and [2] for a formal
deﬁnition and other examples of valid kernels). Since only transitions ending in the states ˆsa
k have a
non-zero probability of occurrence  one can solve a ﬁnite MDP ˆM whose space is composed solely
of these n = ∑a na states [2  6]. After the optimal value function of ˆM has been found  the value of
any state si ∈ S can be computed as Q(si  a) = ∑na
k)(cid:3) . Ormoneit and Sen [1]
proved that  if na → ∞ for all a ∈ A and the widths of the kernels κ a shrink at an “admissible” rate 
the probability of choosing a suboptimal action based on Q(si  a) converges to zero.

k=1 κ a(si  sa

k=1 κ a(si  sa

k + γ ˆV ∗(ˆsa

k)(cid:2)ra

As discussed in the introduction  the problem with the practical application of KBRL is that  as n
increases  so does the cost of solving the MDP derived by this algorithm. To alleviate this problem 
Jong and Stone [6] propose growing incrementally the set of sample transitions  using a prioritized
sweeping approach to guide the exploration of the state space. In this paper we present a new method
for addressing this problem  using stochastic factorization.

3 Stochastic factorization

A stochastic matrix has only non-negative elements and each of its rows sums to 1. That said  we
can introduce the concept that will serve as a cornerstone for the rest of the paper:

Deﬁnition 1 Given a stochastic matrix P ∈ Rn×p  the relation P = DK is called a stochastic factor-
ization of P if D ∈ Rn×m and K ∈ Rm×p are also stochastic matrices. The integer m > 0 is the order
of the factorization.

2

This mathematical concept has been explored before. For example  Cohen and Rothblum [7] brieﬂy
discuss it as a special case of non-negative matrix factorization  while Cutler and Breiman [8] focus
on slightly modiﬁed versions of the stochastic factorization for statistical data analysis. However  in
this paper we will focus on a useful property of this type of factorization that seems to have passed
unnoticed thus far. We call it the “stochastic-factorization trick”:

Given a stochastic factorization of a square matrix  P = DK  swapping the factors of the fac-
torization yields another transition matrix ¯P = KD  potentially much smaller than the original 
which retains the basic topology and properties of P.

The stochasticity of ¯P follows immediately from the same property of D and K. What is perhaps
more surprising is the fact that this matrix shares some fundamental characteristics with the orig-
inal matrix P. Speciﬁcally  it is possible to show that: (i) for each recurrent class in P there is a
corresponding class in ¯P with the same period and  given some simple assumptions about the fac-
torization  (ii) P is irreducible if and only if ¯P is irreducible and (iii) P is regular if and only if ¯P is
regular (see [9] for details).
Given the strong connection between P ∈ Rn×n and ¯P ∈ Rm×m  the idea of replacing the former by the
latter comes almost inevitably. The motivation for this would be  of course  to save computational
resources when m < n.
In this paper we are interested in exploiting the stochastic-factorization
trick to reduce the computational cost of dynamic programming. The idea is straightforward: given
stochastic factorizations of the transition matrices Pa  we can apply our trick to obtain a reduced
MDP that will be solved in place of the original one.
In the most general scenario  we would
have one independent factorization Pa = DaKa for each action a ∈ A. However  in the current
work we will focus on a particular case which will prove to be convenient both mathematically and
computationally. When all factorizations share the same matrix D  it is easy to derive theoretical
guarantees regarding the quality of the solution of the reduced MDP:

Proposition 1 Let M ≡ (S  A  Pa  ra  γ) be a ﬁnite MDP with |S| = n and 0 ≤ γ < 1. Let DKa = Pa
be |A| stochastic factorizations of order m and let ¯ra be vectors in Rm such that D¯ra = ra for all
a ∈ A. Deﬁne the MDP ¯M ≡ ( ¯S  A  ¯Pa  ¯ra  γ)  with | ¯S| = m and ¯Pa = KaD. Then 

kv∗ − ˜vk∞ ≤

¯C

(1 − γ)2

max

i

(1 − max

j

di j) 

(1)

where ˜v = ΓD ¯Q∗  ¯C = maxa k ¯ra

k − mina k ¯ra

k   and k·k∞ is the maximum norm.

max j di j) ¯δ (t)

  where ¯δ (t)

i

¯M 

¯Q

Proof. Since ra = D¯ra and D ¯Pa = DKaD = PaD for all a ∈ A  the stochastic matrix D satisﬁes Sorg
and Singh’s deﬁnition of a soft homomorphism between M and ¯M (see equations (25)–(28) in [10]).

t-step action-value function of

i = max j:di j >0 k ¯q
(t)

(t)
jk − min j:di j >0 k ¯q
= ∆¯v(t−1).

Γ(Q∗ − D ¯Q∗)(cid:13)(cid:13)∞ ≤ (1 − γ)−1 supi t (1 −

(t)
jk and ¯q

Applying Theorem 1 by the same authors  we know that (cid:13)(cid:13)
Γ(Q∗ − D ¯Q∗)(cid:13)(cid:13)∞ and  for all t > 0  ¯δ (t)

(t)
jk are elements of the optimal
In order to obtain our bound  we note that
k ). 2
Proposition 1 elucidates the basic mechanism through which one can exploit the stochastic-
factorization trick to reduce the number of states in an MDP. However  in order to apply this idea
in practice  one must actually compute the factorizations. This computation can be expensive  ex-
ceeding the computational effort necessary to calculate v∗ [11  9]. In the next section we discuss a
strategy to reduce the computational cost of the proposed approach.

ΓQ∗ − ΓD ¯Q∗(cid:13)(cid:13)∞ ≤(cid:13)(cid:13)
(cid:13)(cid:13)

i ≤ (1 − γ)−1(maxa k ¯ra

k − mina k ¯ra

4 Kernel-based stochastic factorization

In Section 2 we presented KBRL  an approximation scheme for reinforcement learning whose main
drawback is its high computational complexity.
In Section 3 we discussed how the stochastic-
factorization trick can in principle be useful to reduce an MDP  as long as one circumvents the
computational burden imposed by the calculation of the matrices involved in the process. We now
show how to leverage these two components to produce an algorithm called kernel-based stochastic
factorization (KBSF) that overcomes these computational limitations.

3

As outlined in Section 2  KBRL deﬁnes the probability of a transition from state ˆsb
k via
i   sa
kernel-averaging  formally denoted κ a(ˆsb
k)  where a  b ∈ A. So for each action a ∈ A  the state
j ∈ R1×n whose non-zero entries correspond to the function
i has an associated stochastic vector ˆpa
ˆsb
κ a(ˆsb
k   k = 1  2  . . .   na. Recall that we are dealing with a continuous state space 
so it is possible to compute an analogous vector for any si ∈ S. Therefore  we can link each state of
the original MDP with |A| n-dimensional stochastic vectors. The core strategy of KBSF is to ﬁnd
i ∈ R1×n whose convex combination can
a set of m representative states associated with vectors ka
approximate the rows of the corresponding ˆPa.

i   ·) evaluated at sa

i to state ˆsa

KBRL’s matrices ˆPa have a very speciﬁc structure  since only transitions ending in states ˆsa
i asso-
ciated with action a have a non-zero probability of occurrence. Suppose now we want to apply the
stochastic-factorization trick to KBRL’s MDP. Assuming that the matrices Ka have the same struc-
ture as ˆPa  when computing ¯Pa = KaD we only have to look at the submatrices of Ka and D corre-
sponding to the na non-zero columns of Ka. We call these matrices ˙Ka ∈ Rm×na and ˙Da ∈ Rna×m.
Let { ¯s1  ¯s2  ...  ¯sm} be a set of representative states in S. KBSF computes matrices ˙Da and ˙Ka with
elements ˙da
j )  where ¯κ is also a kernel. Obviously  once we have ˙Da
and ˙Ka it is trivial to compute D and Ka. Depending on how the states ¯si and the kernels ¯κ are
deﬁned  we have DKa ≈ ˆPa for all a ∈ A. The important point here is that the matrices Pa = DKa
are never actually computed  but instead we solve an MDP with m states whose dynamics are given
by ¯Pa = KaD = ˙Ka ˙Da. Algorithm 1 gives a step-by-step description of KBSF.

i j = κ a( ¯si  sa

i   ¯s j) and ˙ka

i j = ¯κ(ˆsa

Algorithm 1 KBSF

Input: Sa for all a ∈ A  m
Select a set of representative states { ¯s1  ¯s2  ...  ¯sm}
for each a ∈ A do

Compute matrix ˙Da: ˙da
Compute matrix ˙Ka: ˙ka
Compute vector ¯ra: ¯ra

i j = ¯κ(ˆsa
i   ¯s j)
i j = κ a( ¯si  sa
j )
i = ∑ j

˙ka
i j

ra
j

end for
Solve ¯M ≡ ( ¯S  A  ¯Pa  ¯ra  γ)  with ¯Pa= ˙Ka ˙Da

Return ˜v = ΓD ¯Q∗  where D⊺ =h(cid:0) ˙Da1(cid:1)⊺(cid:0) ˙Da2(cid:1)⊺

...(cid:0) ˙Da|A|(cid:1)⊺i

Observe that we did not describe how to deﬁne the representative states ¯si. Ideally  these states
i forming a convex hull which contains the rows of ˆPa. In practice  we
would be linked to vectors ka
can often resort to simple methods to pick states ¯si in strategic regions of S. In Section 5 we give
an example of how to do so. Also  the reader might have noticed that the stochastic factorizations
computed by KBSF are in fact approximations of the matrices ˆPa. The following proposition extends
the result of the previous section to the approximate case:

Proposition 2 Let ˆM ≡ (S  A  ˆPa  ˆra  γ) be the ﬁnite MDP derived by KBRL and let D  Ka  and ¯ra be
the matrices and vector computed by KBSF. Then 

kˆv∗ − ˜vk∞ ≤

1

1 − γ

max

a

kˆra − D¯rak∞ +

where ˆC = maxa i ˆra

i − mina i ˆra
i .

1

(1 − γ)2 (cid:18) ¯Cmax

i

(1 − max

j

di j) +

ˆCγ
2

max

a (cid:13)(cid:13)

ˆPa − DKa(cid:13)(cid:13)∞(cid:19)  

(2)

Proof. Let M ≡ (S  A  DKa  D¯ra  γ). It is obvious that

(3)
In order to provide a bound for kˆv∗ − v∗k∞  we apply Whitt’s Theorem 3.1 and Corollary (b) of his
Theorem 6.1 [12]  with all mappings between ˆM and M taken to be identities  to obtain

kˆv∗ − ˜vk∞ ≤ kˆv∗ − v∗k∞ + kv∗ − ˜vk∞.

Resorting to Proposition 1  we can substitute (1) and (4) in (3) to obtain (2). 2

ˆPa − DKa(cid:13)(cid:13)∞(cid:19) .

(4)

kˆv∗ − v∗k∞ ≤

1

1 − γ (cid:18)max

a

kˆra − D¯rak∞ +

ˆCγ

2(1 − γ)

max

a (cid:13)(cid:13)

4

Notice that when D is deterministic—that is  when all its non-zero elements are 1—expression (2)
reduces to Whitt’s classical result regarding state aggregation in dynamic programming [12]. On
the other hand  when the stochastic factorizations are exact  we recover (1)  which is a computable
version of Sorg and Singh’s bound for soft homomorphisms [10]. Finally  if we have exact deter-
ministic factorizations  the right-hand side of (2) reduces to zero. This also makes sense  since in
this case the stochastic-factorization trick gives rise to an exact homomorphism [13].

As shown in Algorithm 1  KBSF is very simple to understand and to implement. It is also fast 
requiring only O(nm2|A|) operations to build a reduced version of an MDP. Finally  and perhaps
most importantly  KBSF always converges to a unique solution whose distance to the optimal one is
bounded. In the next section we show how all these qualities turn into practical beneﬁts.

5 Experiments

We now present a series of computational experiments designed to illustrate the behavior of KBSF
in a variety of challenging domains. We start with a simple problem showing that KBSF is indeed
capable of compressing the information contained in KBRL’s model. We then move to more difﬁcult
tasks  and compare KBSF with other state-of-the-art reinforcement-learning algorithms.

All problems considered in this section have a continuous state space and a ﬁnite number of actions
and were modeled as discounted tasks with γ = 0.99. The algorithms’s results correspond to the
performance of the greedy decision policy derived from the ﬁnal value function computed. In all
cases  the decision policies were evaluated on a set of test states from which the tasks cannot be
easily solved. This makes the tasks considerably harder  since the algorithms must provide a good
approximation of the value function over a larger region of the state space.

k   ra

k   ˆsa

The experiments were carried out in the same way for all tasks: ﬁrst  we collected a set of n sample
transitions (sa
k were grouped
by the k-means algorithm into m clusters and a Gaussian kernel ¯κ was positioned at the center of
each resulting cluster [14]. These kernels deﬁned the models used by KBSF to approximate the
value function. This process was repeated 50 times for each task.

k) using a uniformly-random exploration policy. Then the states ˆsa

We adopted the same width for all kernels. The algorithms were executed on each task with the fol-
lowing values for this parameter: {1  0.1  0.01}. The results reported represent the best performance
of the algorithms over the 50 runs; that is  for each n and each m we picked the width that gener-
ated the maximum average return. Throughout this section we use the following convention to refer
to speciﬁc instances of each method: the ﬁrst number enclosed in parentheses after an algorithm’s
name is n  the number of sample transitions used in the approximation  and the second one is m  the
size of the model used to approximate the value function. Note that for KBRL n and m coincide.

Figure 1 shows the results obtained by KBRL and KBSF on the puddle-world task [15]. In Fig-
ure 1a and 1b we observe the effect of ﬁxing the number of transitions n and varying the number
of representative states m. As expected  the results of KBSF improve as m → n. More surprising
is the fact that KBSF has essentially the same performance as KBRL using models one order of
magnitude smaller. This indicates that KBSF is summarizing well the information contained in the
data. Depending on the values of n and m  this compression may represent a signiﬁcant reduction of
computational resources. For example  by replacing KBRL(8000) with KBSF(8000  90)  we obtain
a decrease of more than 99% on the number of operations performed to ﬁnd a policy  as shown in
Figure 1b (the cost of constructing KBSF’s MDP is included in all reported run times).

In Figures 1c and 1d we ﬁx m and vary n. Observe in Figure 1c how KBRL and KBSF have similar
performances  and both improve as n → ∞. However  since KBSF is using a model of ﬁxed size  its
computational cost depends only linearly on n  whereas KBRL’s cost grows with n3. This explains
the huge difference in the algorithms’s run times shown in Figure 1d.

Next we evaluate how KBSF compares to other reinforcement-learning approaches. We ﬁrst contrast
our method with Lagoudakis and Parr’s least-squares policy iteration algorithm (LSPI) [3]. LSPI is
a natural candidate here because it also builds an approximator of ﬁxed size out of a batch of sample
transitions. In all experiments LSPI used the same data and approximation architectures as KBSF
(to be fair  we ﬁxed the width of KBSF’s kernel κ a at 1 in the comparisons).

Figure 2 shows the results of LSPI and KBSF on the single and double pole-balancing tasks [16].
We call attention to the fact that the version of the problems used here is signiﬁcantly harder than

5

n
r
u
t
e
R

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

KBRL(8000)
KBSF(8000 m)

100 120 140

20

40

60

80

m

(a) Performance as a function of m

0
.
3

5

.

2

0

.

2

5

.

1

0

.

1

n
r
u

t

e
R

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G KBRL(n)

KBSF(n 100)

3
0
+
e
1

2
0
+
e
1

1
0
+
e
1

0
0
+
e
1

1
0
−
e
1

2
0
+
e
5

1
0
+
e
5

0
0
+
e
5

1
0
−
e
5

)
g
o
l
(
 
s
d
n
o
c
e
S

)
g
o
l
(
 
s
d
n
o
c
e
S

KBRL(8000)
KBSF(8000 m)

20

40

60

80

m

100 120 140

(b) Run time as a function of m

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G
G
G

G KBRL(n)

KBSF(n 100)

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

n

n

(c) Performance as a function of n

(d) Run time as a function of n

Figure 1: Results on the puddle-world task averaged over 50 runs. The algorithms were evaluated
on two sets of states distributed over the region of the state space surrounding the “puddles”. The
ﬁrst set was a 3 × 3 grid over [0.1  0.3] × [0.3  0.5] and the second one was composed of four states:
{0.1  0.3} × {0.9  1.0}. The shadowed regions represent 99% conﬁdence intervals.

the more commonly-used variants in which the decision policies are evaluated on a single state close
to the origin. This is probably the reason why LSPI achieves a success rate of no more than 60% on
the single pole-balancing task  as shown in Figure 2a. In contrast  KBSF’s decision policies are able
to balance the pole in 90% of the attempts  on average  using as few as m = 30 representative states.

The results of KBSF on the double pole-balancing task are still more impressive. As Wieland [17]
rightly points out  this version of the problem is considerably more difﬁcult than its single pole
variant  and previous attempts to apply reinforcement-learning techniques to this domain resulted
in disappointing performance [18]. As shown in Figure 2c  KBSF(106  200) is able to achieve a
success rate of more than 80%. To put this number in perspective  recall that some of the test states
are quite challenging  with the two poles inclined and falling in opposite directions.

The good performance of KBSF comes at a relatively low computational cost. A conservative esti-
mate reveals that  were KBRL(106) run on the same computer used for these experiments  we would
have to wait for more than 6 months to see the results. KBSF(106  200) delivers a decision policy in
less than 7 minutes. KBSF’s computational cost also compares well with that of LSPI  as shown in
Figures 2b and 2d. LSPI’s policy-evaluation step involves the update and solution of a linear system
of equations  which take O(nm2) and O(m3|A|3)  respectively. In addition  the policy-update stage
requires the deﬁnition of π(ˆsa
k) for all n states in the set of sample transitions. In contrast  KBSF
only performs O(m3) operations to evaluate a decision policy and O(m2|A|) operations to update it.

We conclude our empirical evaluation of KBSF by using it to learn a neurostimulation policy for the
treatment of epilepsy. In order to do so  we use a generative model developed by Bush et al. [19]
based on real data collected from epileptic rat hippocampus slices. This model was shown to re-

6

i

s
e
d
o
s
p
e
 
l
u
f
s
s
e
c
c
u
S

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

)
g
o
l
(
 
s
d
n
o
c
e
S

0
0
5

0
5

5

1

LSPI(5x104 m)
KBSF(5x104 m)

20

40

60

80

m

100 120 140

20

40

60

LSPI(5x104 m)
KBSF(5x104 m)

100 120 140

80

m

(a) Performance on single pole-balancing

(b) Run time on single pole-balancing

s
e
d
o
s
p
e

i

 
l

u

f
s
s
e
c
c
u
S

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

LSPI(106 m)
KBSF(106 m)

)
g
o
l
(
 
s
d
n
o
c
e
S

0
0
0
0
1

0
0
0
1

0
0
2

0
5

LSPI(106 m)
KBSF(106 m)

50

100

150

200

50

100

150

200

m

m

(c) Performance on double pole-balancing

(d) Run time on double pole-balancing

Figure 2: Results on the pole-balancing tasks averaged over 50 runs. The values correspond to the
fraction of episodes initiated from the test states in which the pole(s) could be balanced for 3000
steps (one minute of simulated time). The test sets were regular grids deﬁned over the hypercube
centered at the origin and covering 50% of the state-space axes in each dimension (we used a resolu-
tion of 3 and 2 cells per dimension for the single and double versions of the problem  respectively).
Shadowed regions represent 99% conﬁdence intervals.

produce the seizure pattern of the original dynamical system and was later validated through the
deployment of a learned treatment policy on a real brain slice [20]. The associated decision problem
has a ﬁve-dimensional continuous state space and extremely non-linear dynamics. At each state the
agent must choose whether or not to apply an electrical pulse. The goal is to suppress seizures while
minimizing the total amount of stimulation needed to do so.

We use as a baseline for our comparisons the ﬁxed-frequency stimulation policies usually adopted
in standard in vitro clinical studies [20]. In particular  we considered policies that apply electrical
pulses at frequencies of 0 Hz  0.5 Hz  1 Hz  and 1.5 Hz. For this task we ran LSPI and KBSF
with sparse kernels  that is  we only computed the value of the Gaussian function at the 6-nearest
neighbors of a given state (thus deﬁning a simplex with the same dimension as the state space). This
modiﬁcation made it possible to use m = 50  000 representative states with KBSF. Since for LSPI
the reduction on the computational cost was not very signiﬁcant  we ﬁxed m = 50 to keep its run
time within reasonable bounds.

We compare the decision policies returned by KBSF and LSPI with those computed by ﬁtted Q-
iteration using Ernst et al.’s extra-trees algorithm [4]. This approach has shown excellent perfor-
mance on several reinforcement-learning tasks [4]. We used the extra-trees algorithm to build an
ensemble of 30 trees. The algorithm was run for 50 iterations  with the structure of the trees ﬁxed
after the 10th one. The number of cut-directions evaluated at each node was ﬁxed at dim(S) = 5  and
the minimum number of elements required to split a node  denoted here by ηmin  was selected from
the set {20  30  ...  50  100  150  ...  200}. In general  we observed that the performance of the tree-

7

based method improved with smaller values for ηmin  with an expected increase in the computational
cost. Thus  in order to give an overall characterization of the performance of ﬁtted Q-iteration  we
report the results obtained with the extreme values of ηmin. The respective instances of the tree-based
approach are referred to as T20 and T200.

Figure 3 shows the results on the epilepsy-suppression task. In order to obtain different compro-
mises of the problem’s two conﬂicting objectives  we varied the relative magnitude of the penalties
associated with the occurrence of seizures and with the application of an electrical pulse [19  20].
In particular  we ﬁxed the latter at −1 and varied the former with values in {−10  −20  −40}. This
appears in the plots as subscripts next to the algorithms’s names. As shown in Figure 3a  LSPI’s poli-
cies seem to prioritize reduction of stimulation at the expense of higher seizure occurrence  which
is clearly sub-optimal from a clinical point of view. T200 also performs poorly  with solutions rep-
resenting no advance over the ﬁxed-frequency stimulation strategies. In contrast  T20 and KBSF
are both able to generate decision policies superior to the 1 Hz policy  which is the most efﬁcient
stimulation regime known to date in the clinical literature [21]. However  as shown in Figure 3b 
KBSF is able to do it at least 100 times faster than the tree-based method.

s
e
r
u
z
e
s
 
f

i

o

 

n
o

i
t
c
a
r
F

0
2

.

0

5
1

.

0

0
1

.

0

LSPI−10

0Hz

0.5Hz

LSPI−20

LSPI−40

T200−10

T200−20

T200−40

KBSF−10

T20−10

T20−20

1Hz

KBSF−20

1.5Hz

T20−40

KBSF−40

KBSF−40

LSPI−40

T200−40

T20−40

KBSF−20

LSPI−20

T200−20

T20−20

KBSF−10

LSPI−10

T200−10

T20−10

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35

50

200

1000

5000

Fraction of stimulation

Seconds (log)

(a) Performance. The length of the rectangles’s edges repre-
sent 99% conﬁdence intervals.

(b) Run times (conﬁdence intervals
do not show up in logarithmic scale)

Figure 3: Results on the epilepsy-suppression problem averaged over 50 runs. The algorithms used
n = 500  000 sample transitions to build the approximations. The decision policies were evaluated
on episodes of 105 transitions starting from a ﬁxed set of 10 test states drawn uniformly at random.

6 Conclusions
We presented KBSF  a reinforcement-learning algorithm that emerges from the application of the
stochastic-factorization trick to KBRL. As discussed  our algorithm is simple  fast  has good theo-
retical guarantees  and always converges to a unique solution. Our empirical results show that KBSF
is able to learn very good decision policies with relatively low computational cost. It also has pre-
dictable behavior  generally improving its performance as the number of sample transitions or the
size of its approximation model increases. In the future  we intend to investigate more principled
strategies to select the representative states  based on the large body of literature available on kernel
methods. We also plan to extend KBSF to the on-line scenario  where the intermediate decision
policies generated during the learning process guide the collection of new sample transitions.

Acknowledgments

The authors would like to thank Keith Bush for making the epilepsy simulator available and also Yuri
Grinberg for helpful discussions regarding this work. Funding for this research was provided by the
National Institutes of Health (grant R21 DA019800) and the NSERC Discovery Grant program.

8

References

[1] D. Ormoneit and S. Sen. Kernel-based reinforcement learning. Machine Learning  49 (2–3):

161–178  2002.

[2] D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems.

IEEE Transactions on Automatic Control  47(10):1624–1636  2002.

[3] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning

Research  4:1107–1149  2003.

[4] D. Ernst  P. Geurts  and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal

of Machine Learning Research  6:503–556  2005.

[5] M. L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.  1994.

[6] N. Jong and P. Stone. Kernel-based models for reinforcement learning in continuous state
spaces. In Proceedings of the International Conference on Machine Learning—Workshop on
Kernel Machines and Reinforcement Learning  2006.

[7] J. E. Cohen and U. G. Rothblum. Nonnegative ranks  decompositions and factorizations of

nonnegative matrices. Linear Algebra and its Applications  190:149–168  1991.

[8] A. Cutler and L. Breiman. Archetypal analysis. Technometrics  36(4):338–347  1994.

[9] A. M. S. Barreto and M. D. Fragoso. Computing the stationary distribution of a ﬁnite Markov
chain through stochastic factorization. SIAM Journal on Matrix Analysis and Applications. In
press.

[10] J. Sorg and S. Singh. Transfer via soft homomorphisms. In Autonomous Agents & Multiagent

Systems / Agent Theories  Architectures  and Languages  pages 741–748  2009.

[11] S. A. Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal on

Optimization  20:1364–1377  2009.

[12] W. Whitt. Approximations of dynamic programs  I. Mathematics of Operations Research  3

(3):231–243  1978.

[13] B. Ravindran. An Algebraic Approach to Abstraction in Reinforcement Learning. PhD thesis 

University of Massachusetts  Amherst  MA  2004.

[14] L. Kaufman and P. J. Rousseeuw. Finding Groups in Data: an Introduction to Cluster Analysis.

John Wiley and Sons  1990.

[15] R. S. Sutton. Generalization in reinforcement learning: Successful examples using sparse
coarse coding. In Advances in Neural Information Processing Systems  volume 8  pages 1038–
1044  1996.

[16] F. J. Gomez. Robust Non-linear Control Through Neuroevolution. PhD thesis  The University

of Texas at Austin  2003.

[17] A. P. Wieland. Evolving neural network controllers for unstable systems. In Proceedings of

the International Joint Conference on Neural Networks  volume 2  pages 667–673  1991.

[18] F. Gomez  J. Schmidhuber  and R. Miikkulainen. Efﬁcient non-linear control through neu-
roevolution. In Proceedings of the 17th European Conference on Machine Learning  pages
654–662  2006.

[19] K. Bush  J. Pineau  and M. Avoli. Manifold embeddings for model-based reinforcement learn-
In Proceedings of the ICML / UAI / COLT Workshop on

ing of neurostimulation policies.
Abstraction in Reinforcement Learning  2009.

[20] K. Bush and J. Pineau. Manifold embeddings for model-based reinforcement learning under
partial observability. In Advances in Neural Information Processing Systems  volume 22  pages
189–197  2009.

[21] K. Jerger and S. J. Schiff. Periodic pacing an in vitro epileptic focus. Journal of Neurophysi-

ology  (2):876–879  1995.

9

,Zi Yin
Yuanyuan Shen