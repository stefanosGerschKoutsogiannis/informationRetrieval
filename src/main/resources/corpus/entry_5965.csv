2019,Worst-Case Regret Bounds for Exploration via Randomized Value Functions,This paper studies a recent proposal to use randomized value functions to drive exploration in reinforcement learning. These  randomized value functions are generated by injecting random noise into the training data  making the approach compatible with many popular methods for estimating parameterized value functions. By providing a worst-case regret bound for tabular finite-horizon Markov decision processes  we show that planning with respect to these randomized value functions can induce provably efficient exploration.,Worst-Case Regret Bounds for Exploration via

Randomized Value Functions

Daniel Russo

Columbia University

djr2174@gsb.columbia.edu

Abstract

This paper studies a recent proposal to use randomized value functions to drive
exploration in reinforcement learning. These randomized value functions are
generated by injecting random noise into the training data  making the approach
compatible with many popular methods for estimating parameterized value func-
tions. By providing a worst-case regret bound for tabular ﬁnite-horizon Markov
decision processes  we show that planning with respect to these randomized value
functions can induce provably efﬁcient exploration.

1

Introduction

Exploration is one of the central challenges in reinforcement learning (RL). A large theoretical
literature treats exploration in simple ﬁnite state and action MDPs  showing that it is possible
to efﬁciently learn a near optimal policy through interaction alone [5  8  10  11  13–16  25  26].
Overwhelmingly  this literature focuses on optimistic algorithms  with most algorithms explicitly
maintaining uncertainty sets that are likely to contain the true MDP.
It has been difﬁcult to adapt these exploration algorithms to the more complex problems investigated
in the applied RL literature. Most applied papers seem to generate exploration through –greedy
or Boltzmann exploration. Those simple methods are compatible with practical value function
learning algorithms  which use parametric approximations to value functions to generalize across
high dimensional state spaces. Unfortunately  such exploration algorithms can fail catastrophically in
simple ﬁnite state MDPs [See e.g. 24]. This paper is inspired by the search for principled exploration
algorithms that both (1) are compatible with practical function learning algorithms and (2) provide
robust performance  at least when specialized to simple benchmarks like tabular MDPs.
Our focus will be on methods that generate exploration by planning with respect to randomized value
function estimates. This idea was ﬁrst proposed in a conference paper by [22] and is investigated more
thoroughly in the journal paper [24]. It is inspired by work on posterior sampling for reinforcement
learning (a.k.a Thompson sampling) [20  27]  which could be interpreted as sampling a value function
from a posterior distribution and following the optimal policy under that value function for some
extended period of time before resampling. A number of papers have subsequently investigated
approaches that generate randomized value functions in complex reinforcement learning problems
[6  9  12  21  23  28  29]. Our theory will focus on a speciﬁc approach of [22  24]  dubbed randomized
least squares value iteration (RLSVI)  as specialized to tabular MDPs. The name is a play on the
classic least-squares policy iteration algorithm (LSPI) of [17]. RLSVI generates a randomized value
function (essentially) by judiciously injecting Gaussian noise into the training data and then applying
applying LSPI to this noisy dataset. One could naturally follow the same template while using other
value learning algorithms in place of LSPI.
This is a strikingly simple algorithm  but providing rigorous theoretical guarantees has proved
challenging. One challenge is that  despite the appealing conceptual connections  there are signiﬁcant
subtleties to any precise link between RLSVI and posterior sampling. The issue is that posterior

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

sampling based approaches are derived from a true Bayesian perspective in which one maintains
beliefs over the underlying MDP. The approaches of [6  9  12  23  24  28  29] model only the value
function  so Bayes rule is not even well deﬁned.1 The work of [22  24] uses stochastic dominance
arguments to relate the value function sampling distribution of RLSVI to a correct posterior in a
Bayesian model where the true MDP is randomly drawn. This gives substantial insight  but the
resulting analysis is not entirely satisfying as a robustness guarantee. It bounds regret on average
over MDPs with transitions kernels drawn from a particular Dirichilet prior  but one may worry that
hard reinforcement learning instances are extremely unlikely under this particular prior.
This paper develops a very different proof strategy and provides a worst-case regret bound for RLSVI
applied to tabular ﬁnite-horizon MDPs. The crucial proof steps are to show that each randomized
value function sampled by RLSVI has a signiﬁcant probability of being optimistic (see Lemma 4)
and then to show that from this property one can reduce regret analysis to concentration arguments
pioneered by [13] (see Lemmas 6  7). This approach is inspired by frequentist analysis of Thompson
sampling for linear bandits [2] and especially the lucid description of [1]. However  applying these
ideas in reinforcement learning appears to require novel analysis. The only prior extension of these
proof techniques to tabular reinforcement learning was carried out by [3]. Reﬂecting the difﬁculty of
such analyses  that paper does not provide regret bounds for a pure Thompson sampling algorithm;
instead their algorithm samples many times from the posterior to form an optimistic model  as in
the BOSS algorithm [4]. Also  unfortunately there is a signiﬁcant error that paper’s analysis and the
correction has not yet been posted online  making a careful comparison difﬁcult at this time.
The established regret bounds are not state of the art for tabular ﬁnite-horizon MDPs. Two steps in
the proofs introduce extra factors of
S in the bounds  where S denotes the number of states. I hope
some smart reader can improve this by intelligently adapting the techniques of [5  11]. However  the
primary goal of the paper is not to give the tightest possible regret bound  but to broaden the set of
exploration approaches known to satisfy polynomial worst-case regret bounds. To this author  it is
both fascinating and beautiful that carefully adding noise to the training data generates sophisticated
exploration and proving this formally is worthwhile.

√

2 Problem formulation

We consider the problem of learning to optimize performance through repeated interactions with an
unknown ﬁnite horizon MDP M = (H S A  P R  s1). The agent interacts with the environment
across K episodes. Each episode proceeds over H periods  where for period h ∈ {1  . . .   H} of
h ∈ A = {1  . . .   A}  observes the
episode k the agent is in state sk
reward rk
h) :
h = 1  . . . H  i = 1  . . .   k − 1} denote the history of interactions prior to episode k. The Markov
transition kernel P encodes the transition probabilities  with

h ∈ [0  1] and  for h < H  also observes next state sk

h ∈ S = {1  . . .   S}  takes action ak

h+1 ∈ S. Let Hk−1 = {(si

h  ai

h  ri

The reward distribution is encoded in R  with

Ph sk

h ak
h

Rh sk

h ak
h

(s) = P(sk

(dr) = P(cid:0)rk

h+1 = s | ak

h  sk

h  . . .   ak

1  sk

1 Hk−1).

h = dr | ak

h  sk

h  . . .   ak

1  sk

1 Hk−1

(cid:1) .

h s a |
We usually instead refer to expected rewards encoded in a vector R that satisﬁes Rh s a = E[rk
h = a]. We then refer to an MDP (H S A  P  R  s1)  described in terms of its expected
h = s  ak
sk
rewards rather than its reward distribution  as this is sufﬁcient to determine the expected value accrued
by any policy. The variable s1 denotes a deterministic initial state and we assume sk
1 = s1 for every
episode k. At the expense of complicating some formulas  the entire paper could also be written
assuming initial states are drawn from some distribution over S  which is more standard in the
literature.
A deterministic Markov policy π = (π1  . . .   πH ) is a sequence of functions  where each πh : S → A
prescribes an action to play in each state. We let Π denote the space of all such policies. We use
h ∈ RS to denote the value function associated with policy π in the sub-episode consisting of
V π
1The precise issue is that  even given a prior over value functions  there is no likelihood function. Given and
MDP  there is a well speciﬁed likelihood of transitioning from state s to another s(cid:48)  but a value function does not
specify a probabilistic data-generating model.

2

H+1 = 0 ∈ RS. Then the value
s ∈ S  h = 1  . . .   H.

periods {h  . . .   H}. To simplify many expressions  we set V π
functions for h ≤ H are the unique solution to the the Bellman equations

(cid:88)

Ps h π(s)(s(cid:48))V π

h+1(s(cid:48))

s(cid:48)∈S

h (s) = maxπ∈Π V π

V π
h (s) = Rh s π(s) +
The optimal value function is V ∗
An episodic reinforcement learning algorithm Alg is a possibly randomized procedure that associates
each history with a policy to employ throughout the next episode. Formally  a randomized algorithm
can depend on random seeds {ξk}k∈N drawn independently of the past from some prespeciﬁed
distribution. Such an episodic reinforcement learning algorithm selects a policy πk = Alg(Hk−1  ξk)
to be employed throughout episode k.
The cumulative expected regret incurred by Alg over K episodes of interaction with the MDP M is

h (s).

Regret(M  K  Alg) = EAlg

V ∗
1 (sk

1) − V πk

1 (sk
1)

(cid:34) K(cid:88)

k=1

(cid:35)

where the expectation is taken over the random seeds used by a randomized algorithm and the
randomness in the observed rewards and state transitions that inﬂuence the algorithm’s chosen
policies. This expression captures the algorithm’s cumulative expected shortfall in performance
relative to an omniscient benchmark  which knows and always employs the true optimal policy.
Of course  regret as formulated above depends on the MDP M to which the algorithm is applied. Our
goal is not to minimize regret under a particular MDP but to provide a guarantee that holds uniformly
across a class of MDPs. This can be expressed more formally by considering a class M containing
all MDPs with S states  A actions  H periods  and rewards distributions bounded in [0  1]. Our goal
is to bound the worst-case regret supM∈M Regret(M  K  Alg) incurred by an algorithm throughout
K episodes of interaction with an unknown MDP in this class. We aim for a bound on worst-case
regret that scales sublinearly in K and has some reasonable polynomial dependence in the size of
state space  action space  and horizon. We won’t explicitly maximize over M in the analysis. Instead 
we ﬁx an arbitrary MDP M and seek to bound regret in a way that does not depend on the particular
transition probabilities or reward distributions under M.
It is worth remarking that  as formulated  our algorithm knows S  A  and H but does not have
knowledge of the number of episodes K. Indeed  we study a so-called anytime algorithm that has
good performance for all sufﬁciently long sequences of interaction.

Notation for empirical estimates. We deﬁne nk(h  s  a) = (cid:80)k−1

h) = (s  a)} to be
the number of times action a has been sampled in state s  period h. For every tuple (h  s  a) with
nk(h  s  a) > 0  we deﬁne the empirical mean reward and empirical transition probabilities up to
period h by

1{(s(cid:96)

h  a(cid:96)

(cid:96)=1

k−1(cid:88)
k−1(cid:88)

(cid:96)=1

ˆRk

h s a =

1

nk(h  s  a)

h s a(s(cid:48)) =
ˆP k

1

nk(h  s  a)

(cid:96)=1

1{(s(cid:96)

h  a(cid:96)

h) = (s  a)}r(cid:96)

h

1{(s(cid:96)

h  a(cid:96)

h  s(cid:96)

h+1) = (s  a  s(cid:48))} ∀s(cid:48) ∈ S.

(1)

(2)

If (h  s  a) was never sampled before episode k  we deﬁne ˆRk

h s a = 0 and ˆP k

h s a = 0 ∈ RS.

3 Randomized Least Squares Value Iteration

This section describes an algorithm called Randomized Least Squares Value Iteration (RLSVI).
We describe RLSVI as specialized to a simple tabular problem in a way that is most convenient
for the subsequent theoretical analysis. A mathematically equivalent deﬁnition – which deﬁnes
RSLVI as estimating a value function on randomized training data – extends more gracefully . This
interpretation is given at the end of the section and more carefully in [24].
At the start of episode k  the agent has observed a history of interactions Hk−1. Based on this  it is
natural to consider an estimated MDP ˆM k = (H S A  ˆP k  ˆRk  s1) with empirical estimates of mean

3

(cid:113) βk

rewards and transition probabilities. These are precisely deﬁned in Equation (2) and the surrounding
text. We could use backward recursion to solve for the optimal policy and value functions under the
empirical MDP  but applying this policy would not generate exploration.
RLSVI builds on this idea  but to induce exploration it judiciously adds Gaussian noise before solving
for an optimal policy. We can deﬁne RLSVI concisely as follows. In episode k it samples a random

vector with independent components wk ∈ RHSA  where wk(h  s  a) ∼ N(cid:0)0  σ2

k(h  s  a)(cid:1). We

deﬁne σk(h  s  a) =
nk(h s a)+1  where βk is a tuning parameter and the denominator shrinks
like the standard deviation of the average of nk(h  s  a) i.i.d samples. Given wk  we construct a
= (H S A  ˆP k  ˆRk + wk  s1) by adding the
randomized perturbation of the empirical MDP M
Gaussian noise to estimated rewards. RLSVI solves for the optimal policy πk under this MDP and
applies it throughout the episode. This policy is  of course  greedy with respect to the (randomized)
k. The random noise wk in RLSVI should be large enough to dominate the
value functions under M
error introduced by performing a noisy Bellman update using ˆP k and ˆRk. We set βk = ˜O(H 3) in the
analysis  where functions of H offer a coarse bound on quantities like the variance of an empirically
estimated Bellman update. For β = {βk}k∈N  we denote this algorithm by RLSVIβ.

k

RLSVI as regression on perturbed data. To extend beyond simple tabular problems  it is fruit-
ful to view RLSVI–like in Algorithm 1–as an algorithm that performs recursive least squares
estimation on the state-action value function. Randomization is injected into these value func-
tion estimates by perturbing observed rewards and by regularizing to a randomized prior sam-
ple. This prior sample is essential  as otherwise there would no be randomness in the esti-
mated value function in initial periods. This procedure is the LSPI algorithm of [17] applied
with noisy data and a tabular representation. The paper [24] includes many experiments with
non-tabular representations.
It should be stressed that although data-perturbations are some-
times used to regularize machine learning algorithms  here it is used only to drive exploration.
Algorithm 1: RLSVI for Tabular  Finite Horizon  MDPs
input

:H  S  A  tuning parameters {βk}k∈N

(1) for episodes k = 1  2  . . . do

L(Q | Qnext D) =(cid:80)

/* Define squared temporal difference error

*/

(s a r s(cid:48))∈D (Q(s  a) − r − maxa(cid:48)∈A Qnext(s(cid:48)  a(cid:48)))2 ;
h+1) : (cid:96) < k}
h  s(cid:96)
H  ∅) : (cid:96) < k};

h < H ;

/* Past data */

*/

/* Draw prior sample */

*/

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19) end

h  a(cid:96)
H   a(cid:96)

h  r(cid:96)
H   r(cid:96)

Dh = {(s(cid:96)
DH = {(s(cid:96)
/* Randomly perturb data
for time periods h = 1  . . .   H do
Sample array ˜Qh ∼ N (0  βkI) ;
˜Dh ← {};
for (s  a  r  s(cid:48)) ∈ Dh do

sample w ∼ N (0  βk);
˜Dh ← ˜Dh ∪ {(s  a  r + w  s(cid:48))};

end

end
/* Estimate Q on noisy data
Deﬁne terminal value Qk
for time periods h = H  . . .   1 do

H+1(s  a) ← 0 ∀s  a ;

ˆQh ← argminQ∈RSA L(Q | Qh+1  ˜Dh) + (cid:107)Q − ˜Qh(cid:107)2
2 ;

end
Apply greedy policy with respect to ( ˆQ1  . . . ˆQH ) throughout episode;
Observe data sk

1  rk

1   . . . sk

H   ak

H   rk

1  ak

H ;

To understand this presentation of RLSVI  it is helpful to understand an equivalence between
posterior sampling in a Bayesian linear model and ﬁtting a regularized least squares estimate to

4

(cid:16) 1

randomly perturbed data. We refer to [24] for a full discussion of this equivalence and review
the scalar case here. Consider Bayes updating of a scalar parameter θ ∼ N (0  β) based on noisy
observations Y = (y1  . . .   yn) where yi | θ ∼ N (0  β). The posterior distribution has the closed
form θ | Y ∼ N
. We could generate a sample from this distribution by ﬁtting a
least squares estimate to noise perturbed data. Sample W = (w1  . . .   wn) where each wi ∼ N (0  β)
is drawn independently and sample ˜θ ∼ N (0  β). Set ˜yi = yi + wi. Then

(cid:80)n

1 yi  

(cid:17)

n+1

n+1

β

(θ − ˜yi)2 + (θ − ˜θ)2 =

1

n + 1

˜yi + ˜θ

(3)

(cid:33)

(cid:32) n(cid:88)

i=1

n(cid:88)

i=1

ˆθ = argmin

(cid:16) 1

θ∈R

(cid:80)n

(cid:17)

(cid:32)

(cid:88)

s(cid:48)∈S

(cid:33)

.

satisﬁes ˆθ | Y ∼ N
. For more complex models  where exact posterior sampling
is impossible  we may still hope estimation on randomly perturbed data generates samples that reﬂect
uncertainty in a sensible way. As far as RLSVI is concerned  roughly the same calculation shows that
in Algorithm 1 ˆQh(s  a) is equal to an empirical Bellman update plus Gaussian noise:

1 yi  

n+1

n+1

β

ˆQh(s  a) | ˆQh+1 ∼ N

ˆRh s a +

ˆPh s a(s(cid:48)) max
a(cid:48)∈A

ˆQh+1(s(cid:48)  a(cid:48))  

βk

nk(h  s  a) + 1

It is worth noting that Algorithm 1 can be naturally applied to settings with function approximation.
In line 15  instead of minimizing over all possible state-action value functions Q ∈ RS×A  we
minimize over the parameter θ deﬁning some approximate value function Qθ. Instead of regularizing
toward a random prior sample ˜Qh in line 15  the methods in [24] regularize toward a random prior
parameter ˜θh. See [23] for a study of these randomized prior samples in deep reinforcement learning.

4 Main result

Theorem 1 establishes that RLSVI satisﬁes a worst-case polynomial regret bound for tabular ﬁnite-
horizon MDPs. It is worth contrasting RLSVI to –greedy exploration and Boltzmann exploration 
which are both widely used randomization approaches to exploration. Those simple methods explore
by directly injecting randomness to the action chosen at each timestep. Unfortunately  they can
fail catastrophically even on simple examples with a ﬁnite state space – requiring a time to learn
that scales exponentially in the size of the state space. Instead  RLSVI generates randomization by
training value functions with randomly perturbed rewards. Theorem 1 conﬁrms that this approach
generates a sophisticated form of exploration fundamentally different from –greedy exploration and
Boltzmann exploration. The notation ˜O ignores poly-logarithmic factors in H  S  A and K.
Theorem 1. Let M denote the set of MDPs with horizon H  S states  A actions  and rewards bounded
in [0 1]. Then for a tuning parameter sequence β = {βk}k∈N with βk = 1

2 SH 3 log(2HSAk) 

(cid:16)

(cid:17)

√

Regret(M  K  RLSVIβ) ≤ ˜O

sup
M∈M

H 3S3/2

AK

.

√

This bound is not state of the art and that is not the main goal of this paper. I conjecture that the extra
factor of S can be removed from this bound through a careful analysis  making the dependence on
S  A  and K  optimal. This conjecture is supported by numerical experiments and (informally) by
a Bayesian regret analysis [24]. One extra
S appears to come from a step at the very end of the
proof in Lemma 7  where we bound a certain L1 norm as in the analysis style of [13]. For optimistic
algorithms  some recent work has avoided directly bounding that L1-norm  yielding a tighter regret
guarantee [5  11]. Another factor of
S stems from the choice of βk  which is used in the proof of
Lemma 5. This seems similar to an extra
d factor that appears in worst-case regret upper bounds
for Thompson sampling in d-dimensional linear bandit problems [1].
Remark 1. Some translation is required to relate the dependence on H with other literature. Many
results are given in terms of the number of periods T = KH  which masks a factor of H. Also unlike
e.g. [5]  this paper treats time-inhomogenous transition kernels. In some sense agents must learn
about H extra state/action pairs. Roughly speaking then  our result exactly corresponds to what one
would get by applying the UCRL2 analysis [13] to a time-inhomogenous ﬁnite-horizon problem.

√

√

5

5 Proof of Theorem 1

The proof follows from several lemmas. Some are (possibly complex) technical adaptations of ideas
present in many regret analyses. Lemmas 4 and 6 are the main discoveries that prompted this paper.
Throughout we use the following notation: for any MDP ˜M = (H S A  ˜P   ˜R  s1)  let V ( ˜M   π) ∈ R
denote the value function corresponding to policy π from the initial state s1. In this notation  for the
true MDP M we have V (M  π) = V π

1 (s1).

A concentration inequality. Through a careful application of Hoeffding’s inequality  one can give
a high probability bound on the error in applying a Bellman update to the (non-random) optimal value
function V ∗
h+1. Through this  and a union bound  Lemma bounds 2 bounds the expected number of
times the empirically estimated MDP falls outside the conﬁdence set

(cid:26)

Mk =

(H S A  P (cid:48)  R(cid:48)  s1) :

∀(h  s  a)|(R(cid:48)

h s a − Ps a h   V ∗

h+1(cid:105)|

≤(cid:113)

(cid:27)

ek(h  s  a)

h s a − Rh s a) + (cid:104)P (cid:48)
(cid:115)
P(cid:16) ˆM k /∈ Mk(cid:17) ≤ π2

log (2HSAk)
nk(s  h  a) + 1

.

6 .

where we deﬁne

(cid:113)

ek(h  s  a) = H

This set is a only a tool in the analysis and cannot be used by the agent since V ∗

Lemma 2 (Validity of conﬁdence sets). (cid:80)∞

k=1

h+1 is unknown.

From value function error to on policy Bellman error. For some ﬁxed policy π  the next simple
lemma expresses the gap between the value functions under two MDPs in terms of the differences
between their Bellman operators. Results like this are critical to many analyses in the RL literature.
Notice the asymmetric role of ˜M and M. The value functions correspond to one MDP while the state
trajectory is sampled in the other. We’ll apply the lemma twice: once where ˜M is the true MDP and
M is estimated one used by RLSVI and once where the role is reversed.
Lemma 3. Consider any policy π and two MDPs ˜M = (H S A  ˜P   ˜R  s1) and M =
(H S A  P   R  s1). Let ˜V π
h denote the respective value functions of π under ˜M and
M. Then
1 (s1)− ˜V π

+ (cid:104)P h sh π(sh) − ˜Ph sh π(sh)   ˜V π

Rh sh π(sh) − ˜Rh sh π(sh)

(cid:34) H(cid:88)

1 (s1) = E

h and V

h+1(cid:105)

(cid:16)

(cid:17)

V

π

π

(cid:35)

π M

 

h=1

H+1 ≡ 0 ∈ RS and the expectation is over the sampled state trajectory s1  . . . sH drawn

where ˜V π
from following π in the MDP M.

Proof.

π

1 (s1) − ˜V π

1 (s1)

V
=R1 s1 π(s1) + (cid:104)P 1 s1 π(s1)   V
=R1 s1 π(s1) − ˜R1 s1 π(s1) + (cid:104)P 1 s1 π(s1) − ˜P1 s1 π(s1)   ˜V π
=R1 s1 π(s1) − ˜R1 s1 π(s1) + (cid:104)P 1 s1 π(s1) − ˜P1 s1 π(s1)   ˜V π

2(cid:105) − ˜R1 s1 π(s1) − (cid:104) ˜P1 s1 π(s1)   ˜V π
2 (cid:105)

π

(cid:104)

2 (cid:105) + (cid:104)P 1 s1 π(s1)   V
2 (cid:105) + E

2 − ˜V π
2 (cid:105)
2 (s2) − ˜V π
2 (s2)

V

π M

π

π

(cid:105)

.

Expanding this recursion gives the result.

Sufﬁcient optimism through randomization. There is always the risk that  based on noisy obser-
vations  an RL algorithm incorrectly forms a low estimate of the value function at some state. This
may lead the algorithm to avoid that state  therefore failing to gather the data needed to correct its
faulty estimate. To avoid such scenarios  nearly all provably efﬁcient RL exploration algorithms build
purposefully optimistic estimates. RLSVI does not do this and instead generates a randomized value

6

function. The following lemma is key to our analysis. It shows that  except in the rare event when it
has grossly mis-estimated the underlying MDP  RLSVI has at least a constant chance of sampling an
optimistic value function. Similar results can be proved for Thompson sampling with linear models
[1]. Recall M is the unknown true MDP with optimal policy π∗ and M
k is RLSVI’s noise perturbed
MDP under which πk is an optimal policy.
Lemma 4. Let π∗ be an optimal policy for the true MDP M.

If ˆM k ∈ Mk 

then

(cid:17) ≥ Φ(−1).

P(cid:16)

k

V (M

  πk) ≥ V (M  π∗) | Hk−1

This result is more easily established through the following lemma  which avoids the need to carefully
condition on the history Hk−1 at each step. We conclude with the proof of Lemma 4 after.
Lemma 5. Fix any policy π = (π1  . . .   πH ) and vector e ∈ RHSA with e(h  s  a) ≥ 0. Consider
the MDP M = (H S A  P  R  s1) and alternative ¯R and ¯P obeying the inequality

−(cid:112)e(h  s  a) ≤ ¯Rh s a − Rh s a + (cid:104) ¯Ph s a − Ph s a  Vh+1(cid:105) ≤(cid:112)e(h  s  a)

for every s ∈ S  a ∈ A and h ∈ {1  . . .   H}. Take W ∈ RHSA to be a random vector with
independent components where w(h  s  a) ∼ N (0  HSe(h  s  a)). Let ¯V π
1 W denote the (random)
value function of the policy π under the MDP ¯M = (H S A  ¯P   ¯R + W ). Then

P(cid:0) ¯V π

1 (s1)(cid:1) ≥ Φ(−1).

1 W (s1) ≥ V π

1 w(s1) − V π

Proof. To start  we consider an arbitrary deterministic vector w ∈ RHSA (thought of as a possible
realization of W ) and evaluate the gap in value functions ¯V π
1 (s1). We can re-write this
quantity by applying Lemma 3. Let s = (s1  . . .   sH ) denote a random sequence of states drawn by
simulating the policy π in the MDP ¯M from the deterministic initial state s1. Set ah = π(sh) for
h = 1  . . .   H. Then
(cid:20) H(cid:88)
1 w(s1) − V π
¯V π
w(h  sh  πh(sh)) + ¯Rh sh πh(sh) − Rh sh πh(sh) + (cid:104) ¯Ph sh πh(sh) − Ph sh πh(sh)   V π
(cid:34)
(cid:16)
H(cid:88)

w(h  sh  πh(sh)) −(cid:112)e(h  sh  πh(sh))

≥ HE

(cid:17)(cid:35)

(cid:21)
h w(cid:105)

= E

1 (s1)

h=1

1
H

h=1

where the expectation is taken over the sequence of sates s = (s1  . . .   sH ). Deﬁne d(h  s) =
1
H

P(sh = s) for every h ≤ H and s ∈ S. Then the above equation can be written as

1
H

(cid:0) ¯V π
≥ (cid:88)
 (cid:88)

s∈S h≤H

≥

1 (s1)(cid:1)
(cid:17)
(cid:16)
w(h  s  πh(s)) −(cid:112)e(h  s  πh(s))
1 w(s1) − V π
(cid:115) (cid:88)

d(h  s)

√

 −

d(h  s)w(h  s  πh(s))

HS

s∈S h≤H

d(h  s)2e(h  s  πh(s)) := X(w)

s∈S h≤H

(cid:88)

 .

where the second inequality applies Cauchy-Shwartz. Now  since

d(h  s)W (h  s  πh(s)) ∼ N (0  d(h  s)2HSe(h  s  πh(s))) 

we have

X(W ) ∼ N

−

(cid:115)

(cid:88)

s∈S h≤H

HS

d(h  s)2e(h  s  πh(s))  HS

d(h  s)2e(h  s  πh(s))

By standardization  P(X(W ) ≥ 0) = Φ(−1). Therefore  P( ¯V π

s∈S h≤H
1 W (s1)−V π

1 (s1) ≥ 0) ≥ Φ(−1).

7

Proof of Lemma 4. Consider some history Hk−1 with ˆM k ∈ Mk. Recall πk is the policy chosen by
RLSVI  which is optimal under the MDP M
k(h  s  a) =
HSek(h  s  a)  applying Lemma 5 conditioned on Hk−1 shows that with probability at least Φ(−1) 
  πk) ≥ V (M  π∗)  since by
V (M
deﬁnition πk is optimal under M

  π∗) ≥ V (M  π∗). When this occurs  we always have V (M

= (H S A  ˆP k  ˆRk + wk  s1). Since σ2

k.

k

k

k

Reduction to bounding online prediction error. The next lemma shows that the cumulative
expected regret of RLSVI is bounded in terms of the total prediction error in estimating the value
function of πk. The critical feature of the result is it only depends on the algorithm being able
to estimate the performance of the policies it actually employs and therefore gathers data about.
From here  the regret analysis will follow only concentration arguments. For the purposes of
analysis  we let ˜M k denote an imagined second sample drawn from the same distribution as the
k under RLSVI. More formally  let ˜M k = (H S A  ˆP k  ˆRk + ˜wk  s1) where
perturbed MDP M
˜wk(h  s  a) | Hk−1 ∼ N (0  σ2
k(h  s  a)) is independent Gaussian noise. Conditioned on the history 
k  but it is statistically independent of the policy πk
˜M k has the same marginal distribution as M
selected by RLSVI.
Lemma 6. For an absolute constant c = Φ(−1)−1 < 6.31  we have

Regret(M  K  RLSVIβ) ≤(c + 1)E

|V (M

k

  πk) − V (M  πk)|

+ cE

|V ( ˜M k  πk) − V (M  πk)|

+ H

P( ˆM k /∈ Mk)

.

(cid:34) K(cid:88)
(cid:34) K(cid:88)

k=1

k=1

(cid:35)

(cid:35)

K(cid:88)
(cid:124)

k=1

(cid:123)(cid:122)

≤π2/6

(cid:125)

h s a − Rh s a ∈ R and k

Online prediction error bounds. We complete the proof with concentration arguments. Set
h s a − Ph s a ∈ RS to be the error in
R(h  s  a) = ˆRk
k
estimating mean the mean reward and transition vector corresponding to (h  s  a). The next re-
sult follows by bounding each term in Lemma 6. This is done by using lemma 3 to expand the
terms V (M   πk) − V (M  πk) and V (M   πk) − V ( ˜M   πk). We focus our analysis on bounding
. The other term can be bounded in an identical manner2  so

P (h  s  a) = ˆP k

k

k=1 |V (M

we omit this analysis.
Lemma 7. Let c = Φ(−1)−1 < 6.31. Then for any K ∈ N 

  πk) − V (M  πk)|(cid:105)
(cid:35)

|V (M

k

  πk) − V (M  πk)|

≤

E(cid:104)(cid:80)K
(cid:34) K(cid:88)

E

k=1

(cid:118)(cid:117)(cid:117)(cid:116)E
K(cid:88)
H−1(cid:88)
(cid:34) K(cid:88)
H(cid:88)

h=1

k=1

+E

(cid:118)(cid:117)(cid:117)(cid:116)E
K(cid:88)
H−1(cid:88)
(cid:34) K(cid:88)
H(cid:88)

h=1

k=1

+ E

h  ak

h)(cid:13)(cid:13)2
(cid:35)
h)|
h  ak

1

(cid:13)(cid:13)k

P (h  sk

|k
R(h  sk

(cid:13)(cid:13)V k

h+1

(cid:13)(cid:13)2

∞

|wk(h  sk

h)|
h  ak

(cid:35)

.

k=1

h=1

k=1

h=1

h+1

√

√

(cid:13)(cid:13)V k

(cid:13)(cid:13)∞ ≤ ˜O(H 5/2

The remaining lemmas complete the proof. At each stage  RLSVI adds Gaussian noise with stan-
dard deviation no larger than ˜O(H 3/2
S). Ignoring extremely low probability events  we expect 
∞ ≤ ˜O(H 6S). The proof of this Lemma
(cid:16)

makes this precise by applying appropriate maximal inequalities.
Lemma 8.

S) and hence(cid:80)H−1
(cid:118)(cid:117)(cid:117)(cid:116)E
H−1(cid:88)
(cid:13)(cid:13)V k

(cid:13)(cid:13)V k
(cid:13)(cid:13)2

K(cid:88)

(cid:13)(cid:13)2

(cid:17)

H 3

SK

√

h+1

h=1

h+1

∞ = ˜O

k=1

h=1

2In particular  an analogue of Lemma7 holds where we replace M

h+1 with the value function
˜V k
h+1 corresponding to policy πk in the MDP ˜M k  and the Gaussian noise wk with the ﬁctitious noise terms ˜wk.

k with ˜M k  V k

8

The next few lemmas are essentially a consequence of analysis in [13]  and many sub-
The main idea is to apply
sequent papers. We give proof sketches in the appendix.
1  |k
h)|
h  ak
The pigeonhole principle gives

known concentration inequalities to bound (cid:13)(cid:13)k
(cid:113)
(cid:80)K

h)| or |wk(h  sk
(cid:113)

in terms of either 1/nk(h  sk

(cid:80)H−1

h  ak
h  ak

h) or 1/

R(h  sk

h  ak

h=1 (1/

nk(h  sk

h  ak

h)) =

h=1 1/nk(h  sk

h  ak

h  ak

P (h  sk
nk(h  sk

h)(cid:13)(cid:13)2
h) = O(log(SAKH) and (cid:80)K
(cid:34) K(cid:88)
H−1(cid:88)
(cid:34) K(cid:88)
H(cid:88)
(cid:34) K(cid:88)
H(cid:88)

h)(cid:13)(cid:13)2
(cid:35)
h)|
h  ak
(cid:35)
h)|
h  ak

|k
R(h  sk

|wk(h  sk

(cid:13)(cid:13)k

P (h  sk

h  ak

= ˜O

= ˜O

(cid:16)

(cid:35)

h=1

h=1

k=1

k=1

k=1

h).

(cid:80)H−1
= ˜O(cid:0)S2AH(cid:1)
(cid:16)√
(cid:17)

SAKH

H 3/2S

(cid:17)

√

AKH

E

E

1

k=1

√
O(
Lemma 9.

SAKH) .

Lemma 10.

Lemma 11.

E

k=1

h=1

6 Extensions and open directions

This paper gives the ﬁrst worst-case regret bounds for algorithms that use randomized value functions
to drive exploration. That the bounds are polynomial in all parameters indicates that adding noise
during value function training generates a sophisticated form of deep exploration that randomizing
actions does not [24]. I hope this paper serves as a useful foundation for future analysis  as many
questions remain open. One glaring open problem is to study these approaches in problems that
require generalization across large state space. Another is to study ensemble approaches [19  21  24]
that avoid re-estimating the value function in each episode.
There are also clear open questions in the tabular setting. The ﬁrst  which I am pursuing  is to tighten
the dependence on S in the bounds. Another is to tighten the dependence on H. I suspect attaining the
optimal dependence on H would require adjusting the variances of the noise perturbations in a more
adaptive manner. Another question is to extend these proof techniques to handle time-homogeneous
MDPs  where there are additional statistical dependencies that would break the current proof. Finally 
I believe the proof techniques in this paper could yield high probability bounds on regret. To see this 
set ∆k = V (M  π∗) − V (M  πk) to be the regret incurred in period k. Lemma 4 together with the
proof of Lemma 7 essentially bounds conditional expected regret E[∆k | Hk−1] with high probability.
Since each ∆k is bounded  one should be able to apply concentration inequalities to bound the sum

of martingale differences(cid:80)K

k=1 (∆k − E[∆k | Hk−1]) with high probability.

Acknowledgments. Much of my understanding of randomized value functions comes from a
collaboration with Ian Osband  Ben Van Roy  and Zheng Wen. Mark Sellke and Chao Qin each
noticed the same error in the proof of Lemma 6 in the initial draft of this paper. The lemma has now
been revised. I am extremely grateful for their careful reading of the paper.

References
[1] Marc Abeille  Alessandro Lazaric  et al. Linear thompson sampling revisited. Electronic

Journal of Statistics  11(2):5165–5197  2017.

[2] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear

payoffs. In International Conference on Machine Learning  pages 127–135  2013.

[3] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-
case regret bounds. In Advances in Neural Information Processing Systems  pages 1184–1194 
2017.

9

[4] John Asmuth  Lihong Li  Michael L Littman  Ali Nouri  and David Wingate. A bayesian
sampling approach to exploration in reinforcement learning. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artiﬁcial Intelligence  pages 19–26. AUAI Press  2009.

[5] Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax regret bounds for
In Proceedings of the 34th International Conference on Machine

reinforcement learning.
Learning-Volume 70  pages 263–272. JMLR. org  2017.

[6] Kamyar Azizzadenesheli  Emma Brunskill  and Animashree Anandkumar. Efﬁcient exploration
through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop
(ITA)  pages 1–9. IEEE  2018.

[7] Stéphane Boucheron  Gábor Lugosi  and Pascal Massart. Concentration inequalities: A

nonasymptotic theory of independence. Oxford university press  2013.

[8] Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research  3(Oct):213–231 
2002.

[9] Yuri Burda  Harrison Edwards  Amos Storkey  and Oleg Klimov. Exploration by random

network distillation. In International Conference on Learning Representations  2019.

[10] Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforce-
ment learning. In Advances in Neural Information Processing Systems  pages 2818–2826 
2015.

[11] Christoph Dann  Tor Lattimore  and Emma Brunskill. Unifying pac and regret: Uniform pac
bounds for episodic reinforcement learning. In Advances in Neural Information Processing
Systems  pages 5713–5723  2017.

[12] Meire Fortunato  Mohammad Gheshlaghi Azar  Bilal Piot  Jacob Menick  Ian Osband  Alex
Graves  Vlad Mnih  Remi Munos  Demis Hassabis  Olivier Pietquin  et al. Noisy networks for
exploration. In International Conference on Learning Representations  2018.

[13] T. Jaksch  R. Ortner  and P. Auer. Near-optimal regret bounds for reinforcement learning.

Journal of Machine Learning Research  11:1563–1600  2010.

[14] Chi Jin  Zeyuan Allen-Zhu  Sebastien Bubeck  and Michael I Jordan. Is q-learning provably

efﬁcient? In Advances in Neural Information Processing Systems  pages 4863–4873  2018.

[15] Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD

thesis  University of London London  England  2003.

[16] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine learning  49(2-3):209–232  2002.

[17] Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine

learning research  4(Dec):1107–1149  2003.

[18] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. preprint  2018.

[19] Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. In Advances in neural information

processing systems  pages 3258–3266  2017.

[20] Ian Osband  Daniel Russo  and Benjamin Van Roy. (more) efﬁcient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems  pages 3003–3011 
2013.

[21] Ian Osband  Charles Blundell  Alexander Pritzel  and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems  pages 4026–4034 
2016.

[22] Ian Osband  Benjamin Van Roy  and Zheng Wen. Generalization and exploration via randomized
value functions. In International Conference on Machine Learning  pages 2377–2386  2016.

10

[23] Ian Osband  John Aslanides  and Albin Cassirer. Randomized prior functions for deep rein-
forcement learning. In Advances in Neural Information Processing Systems  pages 8617–8629 
2018.

[24] Ian Osband  Benjamin Van Roy  Daniel J. Russo  and Zheng Wen. Deep exploration via

randomized value functions. Journal of Machine Learning Research  20(124):1–62  2019.

[25] Alexander L Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael L Littman. Pac
model-free reinforcement learning. In Proceedings of the 23rd international conference on
Machine learning  pages 881–888. ACM  2006.

[26] Alexander L Strehl  Lihong Li  and Michael L Littman. Reinforcement learning in ﬁnite mdps:

Pac analysis. Journal of Machine Learning Research  10(Nov):2413–2444  2009.

[27] Malcolm Strens. A bayesian framework for reinforcement learning. In ICML  volume 2000 

pages 943–950  2000.

[28] Ahmed Touati  Harsh Satija  Joshua Romoff  Joelle Pineau  and Pascal Vincent. Randomized
value functions via multiplicative normalizing ﬂows. In Proceedings of the 35th Conference on
Uncertainty in Artiﬁcial Intelligence.

[29] Nikolaos Tziortziotis  Christos Dimitrakakis  and Michalis Vazirgiannis. Randomised bayesian

least-squares policy iteration. arXiv preprint arXiv:1904.03535  2019.

[30] Tsachy Weissman  Erik Ordentlich  Gadiel Seroussi  Sergio Verdu  and Marcelo J Weinberger.
Inequalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs  Tech. Rep 
2003.

11

,Daniel Russo