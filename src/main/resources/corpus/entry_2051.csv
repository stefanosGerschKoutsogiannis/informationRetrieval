2019,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes,The goal of this paper is to design image classification systems that  after an initial multi-task training phase  can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose  and establish connections to the meta- and few-shot learning literature. The resulting approach  called CNAPs  comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPs achieves state-of-the-art results on the challenging Meta-Dataset benchmark indicating high-quality transfer-learning. We show that the approach is robust  avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPs is computationally efficient at test-time as it does not involve gradient based adaptation. Finally  we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.,Fast and Flexible Multi-Task Classiï¬cation Using

Conditional Neural Adaptive Processes

James Requeimaâˆ—

University of Cambridge

Invenia Labs

jrr41@cam.ac.uk

Jonathan Gordonâˆ—

University of Cambridge

jg801@cam.ac.uk

John Bronskillâˆ—

University of Cambridge

jfb54@cam.ac.uk

Sebastian Nowozin

Google Research Berlin
nowozin@google.com

Richard E. Turner

University of Cambridge

Microsoft Research
ret26@cam.ac.uk

Abstract

The goal of this paper is to design image classiï¬cation systems that  after an initial
multi-task training phase  can automatically adapt to new tasks encountered at test
time. We introduce a conditional neural process based approach to the multi-task
classiï¬cation setting for this purpose  and establish connections to the meta-learning
and few-shot learning literature. The resulting approach  called CNAPS  comprises
a classiï¬er whose parameters are modulated by an adaptation network that takes the
current taskâ€™s dataset as input. We demonstrate that CNAPS achieves state-of-the-
art results on the challenging META-DATASET benchmark indicating high-quality
transfer-learning. We show that the approach is robust  avoiding both over-ï¬tting
in low-shot regimes and under-ï¬tting in high-shot regimes. Timing experiments
reveal that CNAPS is computationally efï¬cient at test-time as it does not involve
gradient based adaptation. Finally  we show that trained models are immediately
deployable to continual learning and active learning where they can outperform
existing approaches that do not leverage transfer learning.

1

Introduction

We consider the development of general purpose image classiï¬cation systems that can handle tasks
from a broad range of data distributions  in both the low and high data regimes  without the need for
costly retraining when new tasks are encountered. We argue that such systems require mechanisms
that adapt to each task  and that these mechanisms should themselves be learned from a diversity of
datasets and tasks at training time. This general approach relates to methods for meta-learning [1  2]
and few-shot learning [3]. However  existing work in this area typically considers homogeneous task
distributions at train and test-time that therefore require only minimal adaptation. To handle the more
challenging case of different task distributions we design a fully adaptive system  requiring speciï¬c
design choices in the model and training procedure.
Current approaches to meta-learning and few-shot learning for classiï¬cation are characterized by two
fundamental trade-offs. (i) The number of parameters that are adapted to each task. One approach
adapts only the top  or head  of the classiï¬er leaving the feature extractor ï¬xed [4  5]. While useful in
simple settings  this approach is prone to under-ï¬tting when the task distribution is heterogeneous
[6]. Alternatively  we can adapt all parameters in the feature extractor [7  8] thereby increasing

âˆ—Authors contributed equally

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a)

(b)

Figure 1: (a) Probabilistic graphical model detailing the CNP [13] framework. (b) Computational diagram
depicting the CNAPS model class. Red boxes imply parameters in the model architecture supplied by adaptation
networks. Blue shaded boxes depict the feature extractor and the gold box depicts the linear classiï¬er.

ï¬tting capacity  but incurring a computation cost and opening the door to over-ï¬tting in the low-shot
regime. What is needed is a middle ground which strikes a balance between model capacity and
reliability of the adaptation. (ii) The adaptation mechanism. Many approaches use gradient-based
adaptation [7  9]. While this approach can incorporate training data in a very ï¬‚exible way  it is
computationally inefï¬cient at test-time  may require expertise to tune the optimization procedure 
and is again prone to over-ï¬tting. Conversely  function approximators can be used to directly map
training data to the desired parameters (we refer to this as amortization) [5  10]. This yields ï¬xed-cost
adaptation mechanisms  and enables greater sharing across training tasks. However  it may under-ï¬t
if the function approximation is not sufï¬ciently ï¬‚exible. On the other hand  high-capacity function
approximators require a large number of training tasks to be learned.
We introduce a modelling class that is well-positioned with respect to these two trade-offs for the
multi-task classiï¬cation setting called Conditional Neural Adaptive Processes (CNAPS).2 CNAPS
directly model the desired predictive distribution [11  12]  thereby introducing a conditional neural
processes (CNPs) [13] approach to the multi-task classiï¬cation setting. CNAPS handles varying
way classiï¬cation tasks and introduces a parametrization and training procedure enabling the model
to learn to adapt the feature representation for classiï¬cation of diverse tasks at test time. CNAPS
utilize i) a classiï¬cation model with shared global parameters and a small number of task-speciï¬c
parameters. We demonstrate that by identifying a small set of key parameters  the model can balance
the trade-off between ï¬‚exibility and robustness. ii) A rich adaptation neural network with a novel
auto-regressive parameterization that avoids under-ï¬tting while proving easy to train in practice with
existing datasets [6]. In Section 5 we evaluate CNAPS. Recently  Triantaï¬llou et al. [6] proposed
META-DATASET  a few-shot classiï¬cation benchmark that addresses the issue of homogeneous train
and test-time tasks and more closely resembles real-world few-shot multi-task learning. Many of the
approaches that achieved excellent performance on simple benchmarks struggle with this collection
of diverse tasks. In contrast  we show that CNAPS achieve state-of-the-art performance on the
META-DATASET benchmark  often by comfortable margins and at a fraction of the time required by
competing methods. Finally  we showcase the versatility of the model class by demonstrating that
CNAPS can be applied â€œout of the boxâ€ to continual learning and active learning.

2 Model Design

We consider a setup where a large number of training tasks are available  each composed of a set of
inputs x and labels y. The data for task Ï„ includes a context set DÏ„ = {(xÏ„
n=1  with inputs
and outputs observed  and a target set {(xÏ„âˆ—m   yÏ„âˆ—m )}MÏ„
m=1 for which we wish to make predictions (yÏ„âˆ—
are only observed during training). CNPs [13] construct predictive distributions given xâˆ— as:

n)}NÏ„

n  yÏ„

p (yâˆ—|xâˆ—  Î¸  DÏ„ ) = p (yâˆ—|xâˆ—  Î¸  ÏˆÏ„ = ÏˆÏ† (DÏ„ )) .

(1)
Here Î¸ are global classiï¬er parameters shared across tasks. ÏˆÏ„ are local task-speciï¬c parameters 
produced by a function ÏˆÏ†(Â·) that acts on DÏ„ . ÏˆÏ†(Â·) has another set of global parameters Ï† called
adaptation network parameters. Î¸ and Ï† are the learnable parameters in the model (see Figure 1a).

2Source code available at https://github.com/cambridge-mlg/cnaps.

2

xÏ„âˆ—myÏ„âˆ—mÏˆÏ†(DÏ„)DÏ„Î¸m=1 ...Ï„=1 ...SoftmaxOutputğ‘ğ’šâˆ—ğ’™âˆ— ğœ½ ğğœ)ğğ‘“ğœğğ‘¤ğœğ‘¤(â‹…;ğğ‘“)ğ‘“ğœƒ(ğ’™âˆ—;ğğ‘“ğœ)ğ’™âˆ—ğ‘“ğœƒ(â‹…;ğğ‘“)ğğ‘“(â‹…;ğœ™ğ‘“)ğğ‘¤(â‹…;ğœ™ğ‘¤){ğ’™ğœ}{ğ’šğœ}ğ‘“ğœƒ(â‹…;ğğ‘“)ğœ½{ğ‘“ğœƒ(ğ’™ğœ;ğğ‘“ğœ)}ğğ‘“ğœğœ½Adaptation NetworksClassification Model(a) A FiLM layer.

(b) A ResNet basic block with FiLM layers.

Figure 2: (Left) A FiLM layer operating on convolutional feature maps indexed by channel ch. (Right) How a
FiLM layer is used within a basic Residual network block [14].

CNAPS is a model class that specializes the CNP framework for the multi-task classiï¬cation setting.
The model-class is characterized by a number of design choices  made speciï¬cally for the multi-task
image classiï¬cation setting. CNAPS employ global parameters Î¸ that are trained ofï¬‚ine to capture
high-level features  facilitating transfer and multi-task learning. Whereas CNPs deï¬ne ÏˆÏ„ to be a
ï¬xed dimensional vector used as an input to the model  CNAPS instead let ÏˆÏ„ be speciï¬c parameters
of the model itself. This increases the ï¬‚exibility of the classiï¬er  enabling it to model a broader range
of input / output distributions. We discuss our choices (and associated trade-offs) for these parameters
below. Finally  CNAPS employ a novel auto-regressive parameterization of ÏˆÏ†(Â·) that signiï¬cantly
improves performance. An overview of CNAPS and its key components is illustrated in Figure 1b.

2.1 Speciï¬cation of the classiï¬er: global Î¸ and task-speciï¬c parameters ÏˆÏ„

We begin by specifying the classiï¬erâ€™s global parameters Î¸ followed by how these are adapted by the
local parameters ÏˆÏ„ .
Global Classiï¬er Parameters. The global classiï¬er parameters will parameterize a feature extractor
fÎ¸(x) whose output is fed into a linear classiï¬er  described below. A natural choice for fÎ¸(Â·) in the
image setting is a convolutional neural network  e.g.  a ResNet [14]. In what follows  we assume that
the global parameters Î¸ are ï¬xed and known. In Section 3 we discuss the training of Î¸.
Task-Speciï¬c Classiï¬er Parameters: Linear Classiï¬cation Weights. The ï¬nal classiï¬cation layer
must be task-speciï¬c as each task involves distinguishing a potentially unique set of classes. We
use a task speciï¬c afï¬ne transformation of the feature extractor output  followed by a softmax. The
w âˆˆ RdfÃ—CÏ„ (suppressing the biases to simplify notation)  where
task-speciï¬c weights are denoted ÏˆÏ„
df is the dimension of the feature extractor output fÎ¸(x) and C Ï„ is the number of classes in task Ï„.
Task-Speciï¬c Classiï¬er Parameters: Feature Extractor Parameters. A sufï¬ciently ï¬‚exible
model must have capacity to adapt its feature representation fÎ¸(Â·) as well as the classiï¬cation
layer (e.g. compare the optimal features required for ImageNet versus Omiglot). We therefore
f   and denote fÎ¸(Â·) the unadapted feature
introduce a set of local feature extractor parameters ÏˆÏ„
extractor  and fÎ¸(Â·; ÏˆÏ„
It is critical in few-shot multi-task learning to adapt the feature extractor in a parameter-efï¬cient
manner. Unconstrained adaptation of all the feature extractor parameters (e.g. by ï¬ne-tuning [9])
gives ï¬‚exibility  but it is also slow and prone to over-ï¬tting [6]. Instead  we employ linear modulation
of the convolutional feature maps as proposed by Perez et al. [15]  which adapts the feature extractor
through a relatively small number of task speciï¬c parameters.
A Feature-wise Linear Modulation (FiLM) layer [15] scales and shifts the ith unadapted feature map
fi in the feature extractor FiLM(fi; Î³Ï„
i and
i . Figure 2a illustrates a FiLM layer operating on a convolutional layer  and Figure 2b illustrates
Î²Ï„
how a FiLM layer can be added to a standard Residual network block [14]. A key advantage of
FiLM layers is that they enable expressive feature adaptation while adding only a small number of
parameters [15]. For example  in our implementation we use a ResNet18 with FiLM layers after
i }) constitute
every convolutional layer. The set of task speciï¬c FiLM parameters (ÏˆÏ„
fewer than 0.7% of the parameters in the model. Despite this  as we show in Section 5  they allow the
model to adapt to a broad class of datasets.

f ) the feature extractor adapted to task Ï„.

i using two task speciï¬c parameters  Î³Ï„

f = {Î³Ï„

i   Î²Ï„

i   Î²Ï„

i ) = Î³Ï„

i fi + Î²Ï„

3

FiLMğ’‡ğ‘–ğ›¾ğ‘– 1ğ›½ğ‘– 1+ğ›¾ğ‘– ğ‘â„ğ›½ğ‘– ğ‘â„â€¦3x3BNFiLM ğ’‡ğ‘1ReLU3x3BNFiLM ğ’‡ğ‘2+ReLUblock ğ‘ğ‘1ğ‘1ğ‘2ğ‘2Figure 3: Implementation of functional representation of the class-speciï¬c parameters Ïˆw. In this parameteriza-
tion  Ïˆc

w are the linear classiï¬cation parameters for class c  and Ï†w are the learnable parameters.

f   ÏˆÏ„

2.2 Computing the local parameters via adaptation networks
The previous sections have speciï¬ed the form of the classiï¬er p (yâˆ—|xâˆ—  Î¸  ÏˆÏ„ ) in terms of the global
w}. The local parameters could now be learned
and task speciï¬c parameters  Î¸ and ÏˆÏ„ = {ÏˆÏ„
separately for every task Ï„ via optimization. While in practice this is feasible for small numbers
of tasks (see e.g.  [16  17])  this approach is computationally demanding  requires expert oversight
(e.g. for tuning early stopping)  and can over-ï¬t in the low-data regime.
Instead  CNAPS uses a function  such as a neural network  that takes the context set DÏ„ as an input
and returns the task-speciï¬c parameters  ÏˆÏ„ = ÏˆÏ† (DÏ„ ). The adaptation network has parameters
Ï† that will be trained on multiple tasks to learn how to produce local parameters that result in
good generalisation  a form of meta-learning. Sacriï¬cing some of the ï¬‚exibility of the optimisation
approach  this method is comparatively cheap computationally (only involving a forward pass through
the adaptation network)  automatic (with no need for expert oversight)  and employs explicit parameter
sharing (via Ï†) across the training tasks.
Adaptation Network: Linear Classiï¬er Weights. CNAPS represents the linear classiï¬er weights
w = Ïˆw(DÏ„ ; Ï†w  Ïˆf   Î¸)  denoted Ïˆw(DÏ„ ) for brevity.
ÏˆÏ„
There are three challenges with this approach: ï¬rst  the dimensionality of the weights depends on the
task (ÏˆÏ„
w is a matrix with a column for each class  see Figure 3) and thus the network must output
parameters of different dimensionalities; second  the number of datapoints in DÏ„ will also depend
on the task and so the network must be able to take inputs of variable cardinality; third  we would
like the model to support continual learning. To handle the ï¬rst two challenges we follow Gordon
et al. [5]. First  each column of the weight matrix is generated independently from the context points
from that class ÏˆÏ„
C)]  an approach which scales to arbitrary numbers
of classes. Second  we employ a permutation invariant architecture [18  19] for Ïˆw(Â·) to handle the
variable input cardinality (see Appendix E for details). Third  as permutation invariant architectures
can be incrementally updated [20]  continual learning is supported (as discussed in Section 5).
Intuitively  the classiï¬er weights should be determined by the representation of the data points
emerging from the adapted feature extractor. We therefore input the adapted feature representation
of the data points into the network  rather than the raw data points (hence the dependency of Ïˆw on
Ïˆf and Î¸). To summarize  Ïˆw(Â·) is a function on sets that accepts as input a set of adapted feature
representations from DÏ„

w as a parameterized function of the form ÏˆÏ„

c   and outputs the cth column of the linear classiï¬cation matrix  i.e. 
c ; Ï†w  Ïˆf   Î¸) = Ïˆw ({fÎ¸ (xm; Ïˆf )|xm âˆˆ DÏ„   ym = c}; Ï†w) .

w = [Ïˆw (DÏ„

. . .   Ïˆw (DÏ„

Ïˆw (DÏ„

1 )  

(2)

Here Ï†w are learnable parameters of Ïˆw(Â·). See Figure 3 for an illustration.
Adaptation Network: Feature Extractor Parameters. CNAPS represents the task-speciï¬c feature
extractor parameters ÏˆÏ„
f   comprising the parameters of the FiLM layers Î³Ï„ and Î²Ï„ in our imple-
mentation  as a parameterized function of the context-set DÏ„ . Thus  Ïˆf (Â·; Ï†f   Î¸) is a collection of
functions (one for each FiLM layer) with parameters Ï†f   many of which are shared across functions.
We denote the function generating the parameters for the ith FiLM layer Ïˆi
Our experiments (Section 5) show that this mapping requires careful parameterization. We propose a
novel parameterization that improves performance in complex settings with diverse datasets. Our
implementation contains two components: a task-speciï¬c representation that provides context about
the task to all layers of the feature extractor (denoted zÏ„
G)  and an auto-regressive component that
provides information to deeper layers in the feature extractor concerning how shallower layers have
adapted to the task (denoted zi
G is computed
for every task Ï„ by passing the inputs xÏ„

n through a global set encoder g with parameters in Ï†f .

f (Â·) network is zi = (zÏ„

AR). The input to the Ïˆi

f (Â·) for brevity.

AR). zÏ„

G  zi

4

Splitğ‘“ğœƒ(ğ’™;ğğ‘“ğœ)by class label ğ‘¦{ğ‘“ğœƒ(ğ’™;ğğ‘“ğœ)}{ğ’š}ğğ‘¤(Â·;ğœ™ğ‘¤ ğğ‘“ğœ ğœ½){ğ‘“ğœƒ(ğ‘¥ğ‘˜ğ‘;ğğ‘“ğœ)}ğ‘˜=1ğ‘˜ğ‘(i.e. ğ‘˜ğ‘train examples from each class ğ‘)ğ‘“ğœƒ(ğ’™âˆ—;ğğ‘“ğœ)Linear Classifier ğ‘¤Softmaxğ‘ğ’šâˆ—ğ’™âˆ— ğœ½ ğğœ)ğ‘¤1ğ‘¤ğ‘ğ‘¤ğ¶â€¦ğ‘ğ‘ğ‘1ğ‘ğ¶â€¦â€¦â€¦Shared network for each class ğ‘in CMean Poolingğœ™ğ‘ğœ™ğ‘¤ğ‘§ğ¶ğœ“ğ‘¤ğ·ğ‘–ğœâ‰”ğ‘¤ğ‘–;ğ‘ğ‘–Figure 4: Implementation of the feature-extractor: an independently learned set encoder g provides a ï¬xed
context that is concatenated to the (processed) activations of x from the previous ResNet block. The inputs
f (Â·)  which outputs the FiLM parameters for layer i. Green arrows correspond
zi = (zÏ„
AR is computed by
to propagation of auto-regressive representations. Note that the auto-regressive component zi
processing the adapted activations {f i

f )} of the previous convolutional block.

Î¸(x; ÏˆÏ„

G  zi

AR) are then fed to Ïˆi

Figure 5: Adaptation network Ï†f . RÎ³ibj ch and RÎ²ibj ch denote a vector of regularization weights that are
learned with an l2 penalty.

To adapt the lth layer in the feature extractor  it is useful for the system to have access to the
representation of task-relevant inputs from layer l âˆ’ 1. While zG could in principle encode how layer
l âˆ’ 1 has adapted  we opt to provide this information directly to the adaptation network adapting layer
l by passing the adapted activations from layer lâˆ’ 1. The auto-regressive component zi
AR is computed
by processing the adapted activations of the previous convolutional block with a layer-speciï¬c set
encoder (except for the ï¬rst residual block  whose auto-regressive component is given by the un-
adapted initial pre-processing stage in the ResNet). Both the global and all layer-speciï¬c set-encoders
are implemented as permutation invariant functions [18  19] (see Appendix E for details). The full
f (Â·) networks is illustrated in
parameterization is illustrated in Figure 4  and the architecture of Ïˆi
Figure 5.

3 Model Training

The previous section has speciï¬ed the model (see Figure 1b for a schematic). We now describe how
to train the global classiï¬er parameters Î¸ and the adaptation network parameters Ï† = {Ï†f   Ï†w}.
Training the global classiï¬er parameters Î¸. A natural approach to training the model (originally
employed by CNPs [13]) would be to maximize the likelihood of the training data jointly over Î¸ and
Ï†. However  experiments (detailed in Appendix D.3) showed that it is crucially important to adopt a
two stage process instead. In the ï¬rst stage  Î¸ are trained on a large dataset (e.g.  the training set of
ImageNet [21  6]) in a full-way classiï¬cation procedure  mirroring standard pre-training. Second  Î¸
are ï¬xed and Ï† are trained using episodic training over all meta-training datasets in the multi-task
setting. We hypothesize that two-stage training is important for two reasons: (i) during the second
stage  Ï†f are trained to adapt fÎ¸(Â·) to tasks Ï„ by outputting ÏˆÏ„
f . As Î¸ has far more capacity than ÏˆÏ„
f  
if they are trained in the context of all tasks  there is no need for ÏˆÏ„
f to adapt the feature extractor 
resulting in little-to-no training signal for Ï†f and poor generalisation. (ii) Allowing Î¸ to adapt during

5

ğ‘“ğœƒğ’™âˆ—Preğ’™ğ’™ğ‘“ğœƒ(ğ’™âˆ—;ğğ‘“ğœ)ğ‘“ğœƒ(ğ’™;ğğ‘“ğœ)Postğğ’‡(ğ·ğœ)ğ‘”Set Encoderğ’›ğºğœblock2ğ›¾1 ğ›½1block 1layer 1ğœ“ğ’‡1ğ’›AR1ğœ“ğ’‡1SetEncoderblock2ğ›¾2 ğ›½2block 1layer 2ğœ“ğ’‡2ğ’›AR2ğœ“ğ’‡2SetEncoderblock2ğ›¾3 ğ›½3block 1layer 3ğœ“ğ’‡3ğ’›AR3ğœ“ğ’‡3SetEncoderblock2ğ›¾4 ğ›½4block 1layer 4ğœ“ğ’‡4ğ’›AR4ğœ“ğ’‡4SetEncoderğ‘“ğœ½1(ğ’™)ğ‘“ğœ½2(ğ’™;ğğ‘“ğœ)ğ‘“ğœ½3(ğ’™;ğğ‘“ğœ)ğ‘“ğœ½4(ğ’™;ğğ‘“ğœ)ğ’ğºConcatenateğœ™ğ’‡ğ‘–ğ’›ğ´ğ‘…ğ‘–(ğ‘™2penalty)ğœ™ğ‘“ğœ·ğ‘1ğœ·ğ‘–ğ‘1ğœ™ğ‘“ğœ¸ğ‘1(ğ‘™2penalty)ğœ¸ğ‘–ğ‘11(ğ‘™2penalty)ğœ™ğ‘“ğœ·ğ‘2ğœ·ğ‘–ğ‘2ğœ™ğ‘“ğœ¸ğ‘2(ğ‘™2penalty)ğœ¸ğ‘–ğ‘21Figure 6: Model design space. The y-axis repre-
sents the number of task-speciï¬c parameters |ÏˆÏ„|.
Increasing |ÏˆÏ„| increases model ï¬‚exibility  but also
the propensity to over-ï¬t. The x-axis represents the
complexity of the mechanism used to adapt the task-
speciï¬c parameters to training data Ïˆ(DÏ„ ). On the
right are amortized approaches (i.e. using ï¬xed func-
tions). On the left is gradient-based adaptation. Mixed
approaches lie between. Computational efï¬ciency in-
creases to the right. Flexibility increases to the left 
but with it over-ï¬tting and need for hand tuning.

the second phase violates the principle of â€œtrain as you testâ€  i.e.  when test tasks are encountered 
Î¸ will be ï¬xed  so it is important to simulate this scenario during training. Finally  ï¬xing Î¸ during
meta-training is desireable as it results in a dramatic decrease in training time.

Training the adaptation network parameters Ï†. Following the work of Garnelo et al. [13]  we
train Ï† with maximum likelihood. An unbiased stochastic estimator of the log-likelihood is:

(cid:88)

m Ï„

Ë†L (Ï†) =

1

M T

log p (yâˆ—Ï„

m |xâˆ—Ï„

m   ÏˆÏ† (DÏ„ )   Î¸)  

(3)

m   xâˆ—Ï„

m   DÏ„} âˆ¼ Ë†P   with Ë†P representing the data distribution (e.g.  sampling tasks and
where {yâˆ—Ï„
splitting them into disjoint context (DÏ„ ) and target data {(xâˆ—Ï„
m=1). Maximum likelihood
training therefore naturally uses episodic context / target splits often used in meta-learning. In our
experiments we use the protocol deï¬ned by Triantaï¬llou et al. [6] and META-DATASET for this
sampling procedure. Algorithm A.1 details computation of the stochastic estimator for a single task.

m )}Mt

m   yâˆ—Ï„

4 Related Work

Our work frames multi-task classiï¬cation as directly modelling the predictive distribution
p(yâˆ—|xâˆ—  Ïˆ(DÏ„ )). The perspective allows previous work [7  5  15  22  16  17  23  4  6  24  9  25  26]
to be organised in terms of i) the choice of the parameterization of the classiï¬er (and in particular the
nature of the local parameters)  and ii) the function used to compute the local parameters from the
training data. This space is illustrated in Figure 6  and further elaborated upon in Appendix B.
One of the inspirations for our work is conditional neural processes (CNPs) [13]. CNPs directly model
the predictive distribution p(yâˆ—|xâˆ—  Ïˆ(DÏ„ )) and train the parameters using maximum likelihood.
Whereas previous work on CNPs has focused on homogeneous regression and classiï¬cation datasets
and fairly simple models  here we study multiple heterogeneous classiï¬cation datasets and use a
more complex model to handle this scenario. In particular  whereas the original CNP approach to
classiï¬cation required pre-specifying the number of classes in advance  CNAPS handles varying way
classiï¬cation tasks  which is required for e.g. the meta-dataset benchmark. Further  CNAPS employs
a parameter-sharing hierarchy that parameterizes the feature extractor. This contrasts to the original
CNP approach that shared all parameters across tasks  and use latent inputs to the decoder to adapt to
new tasks. Finally  CNAPS employs a meta-training procedure geared towards learning to adapt
to diverse tasks. Similarly  our work can be viewed as a deterministic limit of ML-PIP [5] which
employs a distributional treatment of the local-parameters Ïˆ.
A model with design choices closely related to CNAPS is TADAM [27]. TADAM employs a
similar set of local parameters  allowing for adaptation of both the feature extractor and classiï¬cation
layer. However  it uses a far simpler adaptation network (lacking auto-regressive structure) and
an expensive and ad-hoc training procedure. Moreover  TADAM was applied to simple few-shot
learning benchmarks (e.g. CIFAR100 and mini-ImageNet) and sees little gain from feature extractor
adaptation. In contrast  we see a large beneï¬t from adapting the feature extractor. This may in part
reï¬‚ect the differences in the two models  but we observe that feature extractor adaptation has the
largest impact when used to adapt to different datasets and that two stage training is required to see
this. Further differences are our usage of the CNP framework and the ï¬‚exible deployment of CNAPS
to continual learning and active learning (see Section 5).

6

Adaptation Mechanism ğ(ğ·ğœ)Faster at Test-Time# Task-specific Parameters ğğœAllClassifierandFeatureAdaptersClassifierOnlyMulti-step GradientFew-step GradientSemi-AmortizedAmortizedFinetune [9]ResidualAdapters [16  17]LEO [23] Proto-MAML [6]CNAPS TADAM [27]VERSA [5] Proto Nets [4] Matching Nets [24]MAML [7]Meta-LSTM [22]Model FlexibilityCAVIA [25]Disc. k-shot [26]5 Experiments and Results

The experiments target three key questions: (i) Can CNAPS improve performance in multi-task
few-shot learning? (ii) Does the use of an adaptation network beneï¬t computational-efï¬ciency and
data-efï¬ciency? (iii) Can CNAPS be deployed directly to complex learning scenarios like continual
learning and active learning? The experiments use the following modelling choices (see Appendix E
for full details). While CNAPS can utilize any feature extractor  a ResNet18 [14] is used throughout to
enable fair comparison with Triantaï¬llou et al. [6]. To ensure that each task is handled independently 
batch normalization statistics [28] are learned (and ï¬xed) during the pre-training phase for Î¸. Actual
batch statistics of the test data are never used during meta-training or testing.

Few Shot Classiï¬cation. The ï¬rst experiment tackles a demanding few-shot classiï¬cation chal-
lenge called META-DATASET [6]. META-DATASET is composed of ten (eight train  two test) image
classiï¬cation datasets. The challenge constructs few-shot learning tasks by drawing from the follow-
ing distribution. First  one of the datasets is sampled uniformly; second  the â€œwayâ€ and â€œshotâ€ are
sampled randomly according to a ï¬xed procedure; third  the classes and context / target instances are
sampled. Where a hierarchical structure exists in the data (ILSVRC or OMNIGLOT)  task-sampling
respects the hierarchy. In the meta-test phase  the identity of the original dataset is not revealed
and the tasks must be treated independently (i.e. no information can be transferred between them).
Notably  the meta-training set comprises a disjoint and dissimilar set of classes from those used for
meta-test. Full details are available in Appendix C.1 and [6].
Triantaï¬llou et al. [6] consider two stage training: an initial stage that trains a feature extractor in a
standard classiï¬cation setting  and a meta-training stage of all parameters in an episodic regime. For
the meta-training stage  they consider two settings: meta-training only on the META-DATASET version
of ILSVRC  and on all meta-training data. We focus on the latter as CNAPS rely on training data
from a variety of training tasks to learn to adapt  but provide results for the former in Appendix D.1.
We pre-train Î¸ on the meta-training set of the META-DATASET version of ILSVRC  and meta-train
Ï† in an episodic fashion using all meta-training data. We compare CNAPS to models considered
by Triantaï¬llou et al. [6]  including their proposed method (Proto-MAML) in Table 1. We meta-test
CNAPS on three additional held-out datasets: MNIST [29]  CIFAR10 [30]  and CIFAR100 [30]. As
an ablation study  we compare a version of CNAPS that does not make use of the auto-regressive
component zAR  and a version that uses no feature extractor adaptation. In our analysis of Table 1  we
distinguish between two types of generalization: (i) unseen tasks (classes) in meta-training datasets 
and (ii) unseen datasets.

Unseen tasks: CNAPS achieve signiï¬cant improvements over existing methods on seven of the
eight datasets. The exception is the TEXTURES dataset  which has only seven test classes and
accuracy is highly sensitive to the train / validation / test class split. The ablation study demonstrates
that removing zAR from the feature extractor adaptation degrades accuracy in most cases  and that
removing all feature extractor adaptation results in drastic reductions in accuracy.

Unseen datasets: CNAPS-models outperform all competitive models with the exception of FINE-
TUNE on the TRAFFIC SIGNS dataset. Removing zAR from the feature extractor decreases accuracy
and removing the feature extractor adaptation entirely signiï¬cantly impairs performance. The degra-
dation is particularly pronounced when the held out dataset differs substantially from the dataset used
to pretrain Î¸  e.g. for MNIST.
Note that the superior results when using the auto-regressive component can not be attributed to
increased network capacity alone. In Appendix D.4 we demonstrate that CNAPS yields superior
classiï¬cation accuracy when compared to parallel residual adapters [17] even though CNAPS requires
signiï¬cantly less network capacity in order to adapt the feature extractor to a given task.

Additional results: Results when meta-training only on the META-DATASET version of ILSVRC
are given in Table D.3. In Appendix D.2  we visualize the task encodings and parameters  demon-
strating that the model is able to learn meaningful task and dataset level representations and parame-
terizations. The results support the hypothesis that learning to adapt key parts of the network is more
robust and achieves signiï¬cantly better performance than existing approaches.

7

Finetune
43.1 Â± 1.1
71.1 Â± 1.4
72.0 Â± 1.1
59.8 Â± 1.2
69.1 Â± 0.9
47.0 Â± 1.2
38.2 Â± 1.0
85.3 Â± 0.7
66.7 Â± 1.2
35.2 Â± 1.1

MatchingNet
36.1 Â± 1.0
78.3 Â± 1.0
69.2 Â± 1.0
56.4 Â± 1.0
61.8 Â± 0.7
60.8 Â± 1.0
33.7 Â± 1.0
81.9 Â± 0.7
55.6 Â± 1.1
28.8 Â± 1.0

ProtoNet
44.5 Â± 1.1
79.6 Â± 1.1
71.1 Â± 0.9
67.0 Â± 1.0
65.2 Â± 0.8
64.9 Â± 0.9
40.3 Â± 1.1
86.9 Â± 0.7
46.5 Â± 1.0
39.9 Â± 1.1

fo-MAML
32.4 Â± 1.0
71.9 Â± 1.2
52.8 Â± 0.9
47.2 Â± 1.1
56.7 Â± 0.7
50.5 Â± 1.2
21.0 Â± 1.0
70.9 Â± 1.0
34.2 Â± 1.3
24.1 Â± 1.1

Proto-MAML
47.9 Â± 1.1
82.9 Â± 0.9
74.2 Â± 0.8
70.0 Â± 1.0
67.9 Â± 0.8
66.6 Â± 0.9
42.0 Â± 1.1
88.5 Â± 0.7
52.3 Â± 1.1
41.3 Â± 1.0

Dataset

ILSVRC [21]
Omniglot [31]
Aircraft [32]
Birds [33]
Textures [34]
Quick Draw [35]
Fungi [36]
VGG Flower [37]
Trafï¬c Signs [38]
MSCOCO [39]
MNIST [29]
CIFAR10 [30]
CIFAR100 [30]

CNAPS
(no Ïˆf )
43.8 Â± 1.0
60.1 Â± 1.3
53.0 Â± 0.9
55.7 Â± 1.0
60.5 Â± 0.8
58.1 Â± 1.0
28.6 Â± 0.9
75.3 Â± 0.7
55.0 Â± 0.9
41.2 Â± 1.0
76.0 Â± 0.8
61.5 Â± 0.7
44.8 Â± 1.0

CNAPS
(no zAR)
51.3 Â± 1.0
88.0 Â± 0.7
76.8 Â± 0.8
71.4 Â± 0.9
62.5 Â± 0.7
71.9 Â± 0.8
46.0 Â± 1.1
89.2 Â± 0.5
60.1 Â± 0.9
42.0 Â± 1.0
88.6 Â± 0.5
60.0 Â± 0.8
48.1 Â± 1.0

CNAPS
52.3 Â± 1.0
88.4 Â± 0.7
80.5 Â± 0.6
72.2 Â± 0.9
58.3 Â± 0.7
72.5 Â± 0.8
47.4 Â± 1.0
86.0 Â± 0.5
60.2 Â± 0.9
42.6 Â± 1.1
92.7 Â± 0.4
61.5 Â± 0.7
50.1 Â± 1.0

Table 1: Few-shot classiï¬cation results on META-DATASET [6] using models trained on all training datasets. All
ï¬gures are percentages and the Â± sign indicates the 95% conï¬dence interval over tasks. Bold text indicates the
scores within the conï¬dence interval of the highest score. Tasks from datasets below the dashed line were not
used for training. Competing methodsâ€™ results from [6].

Figure 7: Comparing CNAPS to gradient based feature extractor adaptation: accuracy on 5-way classiï¬cation
tasks from withheld datasets as a function of processing time. Dot size reï¬‚ects shot number (1 to 25 shots).

FiLM Parameter Learning Performance: Speed-Accuracy Trade-off. CNAPS generate FiLM
layer parameters for each task Ï„ at test time using the adaptation network Ïˆf (DÏ„ ). It is also possible
to learn the FiLM parameters via gradient descent (see [16  17]). Here we compare CNAPS to this
approach. Figure 7 shows plots of 5-way classiï¬cation accuracy versus time for four held out data
sets as the number of shots was varied. For gradient descent  we used a ï¬xed learning rate of 0.001
and took 25 steps for each point. The overall time required to produce the plot was 1274 and 7214
seconds for CNAPS and gradient approaches  respectively  on a NVIDIA Tesla P100-PCIE-16GB
GPU. CNAPS is at least 5 times faster at test time than gradient-based optimization requiring only a
single forward pass through the network while gradient based approaches require multiple forward
and backward passes. Further  the accuracy achieved with adaptation networks is signiï¬cantly higher
for fewer shots as it protects against over-ï¬tting. For large numbers of shots  gradient descent catches
up  albeit slowly.

Complex Learning Scenarios: Continual Learning.
In continual learning [40] new tasks appear
over time and existing tasks may change. The goal is to adapt accordingly  but without retaining old
data which is challenging for artiï¬cial systems. To demonstrate the the versatility CNAPS we show
that  although it has not been explicitly trained for continual learning  we are able to apply the same
model trained for the few-shot classiï¬cation experiments (without the auto-regressive component) to
standard continual learning benchmarks on held out datasets: Split MNIST [41] and Split CIFAR100
[42]. We modify the model to compute running averages for the representations of both ÏˆÏ„
w and ÏˆÏ„
f
(see Appendix F for further details)  in this way it performs incremental updates using the new data
and the old model  and does not need to access old data. Figure 8 (left) shows the accumulated multi-
and single-head [42] test accuracy averaged over 30 runs (further results and more detailed ï¬gures are
in Appendix G). Figure 8 (right) shows average results at the ï¬nal task comparing to SI [41]  EWC
[43]  VCL [44]  and Riemannian Walk [42].
Figure 8 demonstrates that CNAPS naturally resists catastrophic forgetting [43] and compares
favourably to competing methods  despite the fact that it was not exposed to these datasets during
training  observes orders of magnitude fewer examples  and was not trained explicitly to perform
continual learning. CNAPS performs similarly to  or better than  the state-of-the-art Riemannian
Walk method which departs from the pure continual learning setting by maintaining a small number
of training samples across tasks. Conversely  CNAPS has the advantage of being exposed to a larger

8

20100300Time (seconds)3040506070Accuracy (%)COCO20100300Time (seconds)405060708090Traffic Signs20100300Time (seconds)405060708090100MNIST20100300Time (seconds)304050607080CIFAR 10CNAPsGradient DescentMethod
SI [41]
EWC [43]
VCL [44]

RWalk [42]
CNAPS

MNIST

CIFAR100

-

Single
22.8
23.1

Single Multi
73.2
57.6
55.8
72.8

Multi
99.3
99.3
98.5
Â± 0.4
99.3
34.0
37.2
98.9
Â± 0.2 Â± 0.9 Â± 0.5 Â± 0.6

-

74.2
76.0

-

82.5
80.9

Figure 8: Continual learning classiï¬cation results on Split MNIST and Split CIFAR100 using a model trained
on all training datasets. (Left) The plots show accumulated accuracy averaged over 30 runs for both single-
and multi-head scenarios. (Right) Average accuracy at ï¬nal task computed over 30 experiments (all ï¬gures are
percentages). Errors are one standard deviation. Additional results from [42  45].

Figure 9: Accuracy vs active learning iterations for held-out classes / languages. (Top) CNAPS and (bottom)
prototypical networks. Error shading is one standard error. CNAPS achieves better accuracy than prototypical
networks and improvements over random acquisition  whereas prototypical networks do not.

range of datasets and can therefore leverage task transfer. We emphasize that this is not meant to be
an â€œapples-to-applesâ€ comparison  but rather  the goal is to demonstrate the out-of-the-box versatility
and strong performance of CNAPS in new domains and learning scenarios.
Complex Learning Scenarios: Active Learning. Active learning [46  47] requires accurate data-
efï¬cient learning that returns well-calibrated uncertainty estimates. Figure 9 compares the perfor-
mance of CNAPS and prototypical networks using two standard active learning acquisition functions
(variation ratios and predictive entropy [46]) against random acquisition on the FLOWERS dataset and
three representative held-out languages from OMNIGLOT (performance on all languages is presented
in Appendix H). Figure 9 and Appendix H show that CNAPS achieves higher accuracy on average
than prototypical networks. Moreover  CNAPS achieves signiï¬cant improvements over random
acquisition  whereas prototypical networks do not. These tests indicates that CNAPS is more accurate
and suggest that CNAPS has better calibrated uncertainty estimates than prototypical networks.

6 Conclusions

This paper has introduced CNAPS  an automatic  fast and ï¬‚exible modelling approach for multi-
task classiï¬cation. We have demonstrated that CNAPS achieve state-of-the-art performance on the
META-DATASET challenge  and can be deployed â€œout-of-the-boxâ€ to diverse learning scenarios such
as continual and active learning where they are competitive with the state-of-the-art. Future avenues
of research are to consider the exploration of the design space by introducing gradients and function
approximation to the adaptation mechanisms  as well as generalizing the approach to distributional
extensions of CNAPS [48  49].

9

1234592.595.097.5100.0Accuracy (%)MNIST Multi-headRWalk 10k-shotCNAPs 1-shotCNAPs 10-shotCNAPs 100-shot1234560708090100MNIST Single-head12345678910Tasks4050607080Accuracy (%)CIFAR100 Multi-head12345678910Tasks20406080CIFAR100 Single-head657075C-NAPsVGG Flower758085Avesta606570Kannada6570MalayalamVar RatPred EntRand0102030Acquisitions60657075Proto-Net0102030Acquisitions7580850102030Acquisitions6065700102030Acquisitions6570Acknowledgments

The authors would like to thank Ambrish Rawat for helpful discussions and David Duvenaud  Wessel
Bruinsma  Will Tebbutt AdriÃ  Garriga Alonso  Eric Nalisnick  and Lyndon White for the insightful
comments and feedback. Richard E. Turner is supported by Google  Amazon  Improbable and
EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] JÃ¼rgen Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis  Technische

UniversitÃ¤t MÃ¼nchen  1987.

[2] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media 

2012.

[3] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept

learning through probabilistic program induction. Science  350(6266):1332â€“1338  2015.

[4] Jake Snell  Kevin Swersky  and Richard Zemel. Prototypical networks for few-shot learning. In

Advances in Neural Information Processing Systems  pages 4080â€“4090  2017.

[5] Jonathan Gordon  John Bronskill  Matthias Bauer  Sebastian Nowozin  and Richard Turner.
Meta-learning probabilistic inference for prediction. In International Conference on Learning
Representations  2019. URL https://openreview.net/forum?id=HkxStoC5F7.

[6] Eleni Triantaï¬llou  Tyler Zhu  Vincent Dumoulin  Pascal Lamblin  Kelvin Xu  Ross Goroshin 
Carles Gelada  Kevin Swersky  Pierre-Antoine Manzagol  and Hugo Larochelle. Meta-dataset:
A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096 
2019.

[7] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adapta-
tion of deep networks. In International Conference on Machine Learning  pages 1126â€“1135 
2017.

[8] Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint

arXiv:1803.02999  2018.

[9] Jason Yosinski  Jeff Clune  Yoshua Bengio  and Hod Lipson. How transferable are features in
deep neural networks? In Advances in neural information processing systems  pages 3320â€“3328 
2014.

[10] Siyuan Qiao  Chenxi Liu  Wei Shen  and Alan Yuille. Few-shot image recognition by predicting

parameters from activations. arXiv preprint arXiv:1706.03466  2017.

[11] Seymour Geisser. On the prediction of observables: a selective update. Technical report 

University of Minnesota  1983.

[12] Seymour Geisser. Predictive inference. Routledge  2017.

[13] Marta Garnelo  Dan Rosenbaum  Chris J Maddison  Tiago Ramalho  David Saxton  Murray
Shanahan  Yee Whye Teh  Danilo J Rezende  and SM Eslami. Conditional neural processes.
arXiv preprint arXiv:1807.01613  2018.

[14] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770â€“778  2016.

[15] Ethan Perez  Florian Strub  Harm De Vries  Vincent Dumoulin  and Aaron Courville. FiLM:
Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on
Artiï¬cial Intelligence  2018.

[16] Sylvestre-Alvise Rebufï¬  Hakan Bilen  and Andrea Vedaldi. Learning multiple visual domains
with residual adapters. In Advances in Neural Information Processing Systems  pages 506â€“516 
2017.

10

[17] Sylvestre-Alvise Rebufï¬  Hakan Bilen  and Andrea Vedaldi. Efï¬cient parametrization of multi-
domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 8119â€“8127  2018.

[18] Manzil Zaheer  Satwik Kottur  Siamak Ravanbakhsh  Barnabas Poczos  Ruslan R Salakhutdinov 
and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems 
pages 3394â€“3404  2017.

[19] Charles R Qi  Hao Su  Kaichun Mo  and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3d classiï¬cation and segmentation. Proc. Computer Vision and Pattern Recognition
(CVPR)  IEEE  1(2):4  2017.

[20] Manasi Vartak  Arvind Thiagarajan  Conrado Miranda  Jeshua Bratman  and Hugo
Larochelle. A meta-learning perspective on cold-start recommendations for items.
In
I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and
R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages
6904â€“6914. Curran Associates  Inc.  2017. URL http://papers.nips.cc/paper/
7266-a-meta-learning-perspective-on-cold-start-recommendations-for-items.
pdf.

[21] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al. Imagenet large scale visual
recognition challenge. International journal of computer vision  115(3):211â€“252  2015.

[22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceed-

ings of the International Conference on Learning Representations (ICLR)  2017.

[23] Andrei A Rusu  Dushyant Rao  Jakub Sygnowski  Oriol Vinyals  Razvan Pascanu  Simon
Osindero  and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
arXiv:1807.05960  2018.

[24] Oriol Vinyals  Charles Blundell  Tim Lillicrap  Daan Wierstra  et al. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems  pages 3630â€“3638  2016.

[25] Luisa M Zintgraf  Kyriacos Shiarlis  Vitaly Kurin  Katja Hofmann  and Shimon Whiteson.

CAML: Fast context adaptation via meta-learning. arXiv preprint arXiv:1810.03642  2018.

[26] Matthias Bauer  Mateo Rojas-Carulla  Jakub BartÅ‚omiej Â´Swi Ë›atkowski  Bernhard SchÃ¶lkopf  and
Richard E Turner. Discriminative k-shot learning using probabilistic models. arXiv preprint
arXiv:1706.00326  2017.

[27] Boris N Oreshkin  Alexandre Lacoste  and Pau Rodriguez. TADAM: Task dependent adaptive

metric for improved few-shot learning. arXiv preprint arXiv:1805.10123  2018.

[28] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448â€“456  2015.

[29] Yann LeCun  Corinna Cortes  and CJ Burges. MNIST handwritten digit database. AT&T Labs

[Online]. Available: http://yann. lecun. com/exdb/mnist  2:18  2010.

[30] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

[31] Brenden Lake  Ruslan Salakhutdinov  Jason Gross  and Joshua Tenenbaum. One shot learning
of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science
Society  volume 33  2011.

[32] Subhransu Maji  Esa Rahtu  Juho Kannala  Matthew Blaschko  and Andrea Vedaldi. Fine-

grained visual classiï¬cation of aircraft. arXiv preprint arXiv:1306.5151  2013.

[33] Catherine Wah  Steve Branson  Peter Welinder  Pietro Perona  and Serge Belongie. The

caltech-ucsd birds-200-2011 dataset. 2011.

11

[34] Mircea Cimpoi  Subhransu Maji  Iasonas Kokkinos  Sammy Mohamed  and Andrea Vedaldi.
Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 3606â€“3613  2014.

[35] David Ha and Douglas Eck. A neural representation of sketch drawings. arXiv preprint

arXiv:1704.03477  2017.

[36] Brigit Schroeder and Yin Cui. Fgvcx fungi classiï¬cation challenge at fgvc5. https://www.

kaggle.com/c/fungi-challenge-fgvc-2018  2018.

[37] Maria-Elena Nilsback and Andrew Zisserman. Automated ï¬‚ower classiï¬cation over a large
number of classes. In 2008 Sixth Indian Conference on Computer Vision  Graphics & Image
Processing  pages 722â€“729. IEEE  2008.

[38] Sebastian Houben  Johannes Stallkamp  Jan Salmen  Marc Schlipsing  and Christian Igel.
Detection of trafï¬c signs in real-world images: The german trafï¬c sign detection benchmark. In
The 2013 international joint conference on neural networks (IJCNN)  pages 1â€“8. IEEE  2013.

[39] Tsung-Yi Lin  Michael Maire  Serge Belongie  James Hays  Pietro Perona  Deva Ramanan  Piotr
DollÃ¡r  and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision  pages 740â€“755. Springer  2014.

[40] Mark B Ring. Child: A ï¬rst step towards continual learning. Machine Learning  28(1):77â€“104 

1997.

[41] Friedemann Zenke  Ben Poole  and Surya Ganguli. Continual learning through synaptic
intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume
70  pages 3987â€“3995. JMLR. org  2017.

[42] Arslan Chaudhry  Puneet K Dokania  Thalaiyasingam Ajanthan  and Philip HS Torr. Riemannian
walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of
the European Conference on Computer Vision (ECCV)  pages 532â€“547  2018.

[43] James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  Joel Veness  Guillaume Desjardins 
Andrei A Rusu  Kieran Milan  John Quan  Tiago Ramalho  Agnieszka Grabska-Barwinska  et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of
sciences  114(13):3521â€“3526  2017.

[44] Cuong V Nguyen  Yingzhen Li  Thang D Bui  and Richard E Turner. Variational continual

learning. arXiv preprint arXiv:1710.10628  2017.

[45] Siddharth Swaroop  Cuong V Nguyen  Thang D Bui  and Richard E Turner. Improving and

understanding variational continual learning. arXiv preprint arXiv:1905.02099  2019.

[46] David A Cohn  Zoubin Ghahramani  and Michael I Jordan. Active learning with statistical

models. Journal of artiï¬cial intelligence research  4:129â€“145  1996.

[47] Burr Settles. Active learning. Synthesis Lectures on Artiï¬cial Intelligence and Machine

Learning  6(1):1â€“114  2012.

[48] Marta Garnelo  Jonathan Schwarz  Dan Rosenbaum  Fabio Viola  Danilo J Rezende  SM Eslami 

and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622  2018.

[49] Hyunjik Kim  Andriy Mnih  Jonathan Schwarz  Marta Garnelo  Ali Eslami  Dan Rosenbaum 
Oriol Vinyals  and Yee Whye Teh. Attentive neural processes. In International Conference on
Learning Representations  2019. URL https://openreview.net/forum?id=SkE6PjC9KX.

12

,James Requeima
Jonathan Gordon
John Bronskill
Sebastian Nowozin
Richard Turner