2019,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes,The goal of this paper is to design image classification systems that  after an initial multi-task training phase  can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose  and establish connections to the meta- and few-shot learning literature. The resulting approach  called CNAPs  comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPs achieves state-of-the-art results on the challenging Meta-Dataset benchmark indicating high-quality transfer-learning. We show that the approach is robust  avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPs is computationally efficient at test-time as it does not involve gradient based adaptation. Finally  we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.,Fast and Flexible Multi-Task Classiﬁcation Using

Conditional Neural Adaptive Processes

James Requeima∗

University of Cambridge

Invenia Labs

jrr41@cam.ac.uk

Jonathan Gordon∗

University of Cambridge

jg801@cam.ac.uk

John Bronskill∗

University of Cambridge

jfb54@cam.ac.uk

Sebastian Nowozin

Google Research Berlin
nowozin@google.com

Richard E. Turner

University of Cambridge

Microsoft Research
ret26@cam.ac.uk

Abstract

The goal of this paper is to design image classiﬁcation systems that  after an initial
multi-task training phase  can automatically adapt to new tasks encountered at test
time. We introduce a conditional neural process based approach to the multi-task
classiﬁcation setting for this purpose  and establish connections to the meta-learning
and few-shot learning literature. The resulting approach  called CNAPS  comprises
a classiﬁer whose parameters are modulated by an adaptation network that takes the
current task’s dataset as input. We demonstrate that CNAPS achieves state-of-the-
art results on the challenging META-DATASET benchmark indicating high-quality
transfer-learning. We show that the approach is robust  avoiding both over-ﬁtting
in low-shot regimes and under-ﬁtting in high-shot regimes. Timing experiments
reveal that CNAPS is computationally efﬁcient at test-time as it does not involve
gradient based adaptation. Finally  we show that trained models are immediately
deployable to continual learning and active learning where they can outperform
existing approaches that do not leverage transfer learning.

1

Introduction

We consider the development of general purpose image classiﬁcation systems that can handle tasks
from a broad range of data distributions  in both the low and high data regimes  without the need for
costly retraining when new tasks are encountered. We argue that such systems require mechanisms
that adapt to each task  and that these mechanisms should themselves be learned from a diversity of
datasets and tasks at training time. This general approach relates to methods for meta-learning [1  2]
and few-shot learning [3]. However  existing work in this area typically considers homogeneous task
distributions at train and test-time that therefore require only minimal adaptation. To handle the more
challenging case of different task distributions we design a fully adaptive system  requiring speciﬁc
design choices in the model and training procedure.
Current approaches to meta-learning and few-shot learning for classiﬁcation are characterized by two
fundamental trade-offs. (i) The number of parameters that are adapted to each task. One approach
adapts only the top  or head  of the classiﬁer leaving the feature extractor ﬁxed [4  5]. While useful in
simple settings  this approach is prone to under-ﬁtting when the task distribution is heterogeneous
[6]. Alternatively  we can adapt all parameters in the feature extractor [7  8] thereby increasing

∗Authors contributed equally

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a)

(b)

Figure 1: (a) Probabilistic graphical model detailing the CNP [13] framework. (b) Computational diagram
depicting the CNAPS model class. Red boxes imply parameters in the model architecture supplied by adaptation
networks. Blue shaded boxes depict the feature extractor and the gold box depicts the linear classiﬁer.

ﬁtting capacity  but incurring a computation cost and opening the door to over-ﬁtting in the low-shot
regime. What is needed is a middle ground which strikes a balance between model capacity and
reliability of the adaptation. (ii) The adaptation mechanism. Many approaches use gradient-based
adaptation [7  9]. While this approach can incorporate training data in a very ﬂexible way  it is
computationally inefﬁcient at test-time  may require expertise to tune the optimization procedure 
and is again prone to over-ﬁtting. Conversely  function approximators can be used to directly map
training data to the desired parameters (we refer to this as amortization) [5  10]. This yields ﬁxed-cost
adaptation mechanisms  and enables greater sharing across training tasks. However  it may under-ﬁt
if the function approximation is not sufﬁciently ﬂexible. On the other hand  high-capacity function
approximators require a large number of training tasks to be learned.
We introduce a modelling class that is well-positioned with respect to these two trade-offs for the
multi-task classiﬁcation setting called Conditional Neural Adaptive Processes (CNAPS).2 CNAPS
directly model the desired predictive distribution [11  12]  thereby introducing a conditional neural
processes (CNPs) [13] approach to the multi-task classiﬁcation setting. CNAPS handles varying
way classiﬁcation tasks and introduces a parametrization and training procedure enabling the model
to learn to adapt the feature representation for classiﬁcation of diverse tasks at test time. CNAPS
utilize i) a classiﬁcation model with shared global parameters and a small number of task-speciﬁc
parameters. We demonstrate that by identifying a small set of key parameters  the model can balance
the trade-off between ﬂexibility and robustness. ii) A rich adaptation neural network with a novel
auto-regressive parameterization that avoids under-ﬁtting while proving easy to train in practice with
existing datasets [6]. In Section 5 we evaluate CNAPS. Recently  Triantaﬁllou et al. [6] proposed
META-DATASET  a few-shot classiﬁcation benchmark that addresses the issue of homogeneous train
and test-time tasks and more closely resembles real-world few-shot multi-task learning. Many of the
approaches that achieved excellent performance on simple benchmarks struggle with this collection
of diverse tasks. In contrast  we show that CNAPS achieve state-of-the-art performance on the
META-DATASET benchmark  often by comfortable margins and at a fraction of the time required by
competing methods. Finally  we showcase the versatility of the model class by demonstrating that
CNAPS can be applied “out of the box” to continual learning and active learning.

2 Model Design

We consider a setup where a large number of training tasks are available  each composed of a set of
inputs x and labels y. The data for task τ includes a context set Dτ = {(xτ
n=1  with inputs
and outputs observed  and a target set {(xτ∗m   yτ∗m )}Mτ
m=1 for which we wish to make predictions (yτ∗
are only observed during training). CNPs [13] construct predictive distributions given x∗ as:

n)}Nτ

n  yτ

p (y∗|x∗  θ  Dτ ) = p (y∗|x∗  θ  ψτ = ψφ (Dτ )) .

(1)
Here θ are global classiﬁer parameters shared across tasks. ψτ are local task-speciﬁc parameters 
produced by a function ψφ(·) that acts on Dτ . ψφ(·) has another set of global parameters φ called
adaptation network parameters. θ and φ are the learnable parameters in the model (see Figure 1a).

2Source code available at https://github.com/cambridge-mlg/cnaps.

2

xτ∗myτ∗mψφ(Dτ)Dτθm=1 ...τ=1 ...SoftmaxOutput𝑝𝒚∗𝒙∗ 𝜽 𝝍𝜏)𝝍𝑓𝜏𝝍𝑤𝜏𝑤(⋅;𝝍𝑓)𝑓𝜃(𝒙∗;𝝍𝑓𝜏)𝒙∗𝑓𝜃(⋅;𝝍𝑓)𝝍𝑓(⋅;𝜙𝑓)𝝍𝑤(⋅;𝜙𝑤){𝒙𝜏}{𝒚𝜏}𝑓𝜃(⋅;𝝍𝑓)𝜽{𝑓𝜃(𝒙𝜏;𝝍𝑓𝜏)}𝝍𝑓𝜏𝜽Adaptation NetworksClassification Model(a) A FiLM layer.

(b) A ResNet basic block with FiLM layers.

Figure 2: (Left) A FiLM layer operating on convolutional feature maps indexed by channel ch. (Right) How a
FiLM layer is used within a basic Residual network block [14].

CNAPS is a model class that specializes the CNP framework for the multi-task classiﬁcation setting.
The model-class is characterized by a number of design choices  made speciﬁcally for the multi-task
image classiﬁcation setting. CNAPS employ global parameters θ that are trained ofﬂine to capture
high-level features  facilitating transfer and multi-task learning. Whereas CNPs deﬁne ψτ to be a
ﬁxed dimensional vector used as an input to the model  CNAPS instead let ψτ be speciﬁc parameters
of the model itself. This increases the ﬂexibility of the classiﬁer  enabling it to model a broader range
of input / output distributions. We discuss our choices (and associated trade-offs) for these parameters
below. Finally  CNAPS employ a novel auto-regressive parameterization of ψφ(·) that signiﬁcantly
improves performance. An overview of CNAPS and its key components is illustrated in Figure 1b.

2.1 Speciﬁcation of the classiﬁer: global θ and task-speciﬁc parameters ψτ

We begin by specifying the classiﬁer’s global parameters θ followed by how these are adapted by the
local parameters ψτ .
Global Classiﬁer Parameters. The global classiﬁer parameters will parameterize a feature extractor
fθ(x) whose output is fed into a linear classiﬁer  described below. A natural choice for fθ(·) in the
image setting is a convolutional neural network  e.g.  a ResNet [14]. In what follows  we assume that
the global parameters θ are ﬁxed and known. In Section 3 we discuss the training of θ.
Task-Speciﬁc Classiﬁer Parameters: Linear Classiﬁcation Weights. The ﬁnal classiﬁcation layer
must be task-speciﬁc as each task involves distinguishing a potentially unique set of classes. We
use a task speciﬁc afﬁne transformation of the feature extractor output  followed by a softmax. The
w ∈ Rdf×Cτ (suppressing the biases to simplify notation)  where
task-speciﬁc weights are denoted ψτ
df is the dimension of the feature extractor output fθ(x) and C τ is the number of classes in task τ.
Task-Speciﬁc Classiﬁer Parameters: Feature Extractor Parameters. A sufﬁciently ﬂexible
model must have capacity to adapt its feature representation fθ(·) as well as the classiﬁcation
layer (e.g. compare the optimal features required for ImageNet versus Omiglot). We therefore
f   and denote fθ(·) the unadapted feature
introduce a set of local feature extractor parameters ψτ
extractor  and fθ(·; ψτ
It is critical in few-shot multi-task learning to adapt the feature extractor in a parameter-efﬁcient
manner. Unconstrained adaptation of all the feature extractor parameters (e.g. by ﬁne-tuning [9])
gives ﬂexibility  but it is also slow and prone to over-ﬁtting [6]. Instead  we employ linear modulation
of the convolutional feature maps as proposed by Perez et al. [15]  which adapts the feature extractor
through a relatively small number of task speciﬁc parameters.
A Feature-wise Linear Modulation (FiLM) layer [15] scales and shifts the ith unadapted feature map
fi in the feature extractor FiLM(fi; γτ
i and
i . Figure 2a illustrates a FiLM layer operating on a convolutional layer  and Figure 2b illustrates
βτ
how a FiLM layer can be added to a standard Residual network block [14]. A key advantage of
FiLM layers is that they enable expressive feature adaptation while adding only a small number of
parameters [15]. For example  in our implementation we use a ResNet18 with FiLM layers after
i }) constitute
every convolutional layer. The set of task speciﬁc FiLM parameters (ψτ
fewer than 0.7% of the parameters in the model. Despite this  as we show in Section 5  they allow the
model to adapt to a broad class of datasets.

f ) the feature extractor adapted to task τ.

i using two task speciﬁc parameters  γτ

f = {γτ

i   βτ

i   βτ

i ) = γτ

i fi + βτ

3

FiLM𝒇𝑖𝛾𝑖 1𝛽𝑖 1+𝛾𝑖 𝑐ℎ𝛽𝑖 𝑐ℎ…3x3BNFiLM 𝒇𝑏1ReLU3x3BNFiLM 𝒇𝑏2+ReLUblock 𝑏𝑏1𝑏1𝑏2𝑏2Figure 3: Implementation of functional representation of the class-speciﬁc parameters ψw. In this parameteriza-
tion  ψc

w are the linear classiﬁcation parameters for class c  and φw are the learnable parameters.

f   ψτ

2.2 Computing the local parameters via adaptation networks
The previous sections have speciﬁed the form of the classiﬁer p (y∗|x∗  θ  ψτ ) in terms of the global
w}. The local parameters could now be learned
and task speciﬁc parameters  θ and ψτ = {ψτ
separately for every task τ via optimization. While in practice this is feasible for small numbers
of tasks (see e.g.  [16  17])  this approach is computationally demanding  requires expert oversight
(e.g. for tuning early stopping)  and can over-ﬁt in the low-data regime.
Instead  CNAPS uses a function  such as a neural network  that takes the context set Dτ as an input
and returns the task-speciﬁc parameters  ψτ = ψφ (Dτ ). The adaptation network has parameters
φ that will be trained on multiple tasks to learn how to produce local parameters that result in
good generalisation  a form of meta-learning. Sacriﬁcing some of the ﬂexibility of the optimisation
approach  this method is comparatively cheap computationally (only involving a forward pass through
the adaptation network)  automatic (with no need for expert oversight)  and employs explicit parameter
sharing (via φ) across the training tasks.
Adaptation Network: Linear Classiﬁer Weights. CNAPS represents the linear classiﬁer weights
w = ψw(Dτ ; φw  ψf   θ)  denoted ψw(Dτ ) for brevity.
ψτ
There are three challenges with this approach: ﬁrst  the dimensionality of the weights depends on the
task (ψτ
w is a matrix with a column for each class  see Figure 3) and thus the network must output
parameters of different dimensionalities; second  the number of datapoints in Dτ will also depend
on the task and so the network must be able to take inputs of variable cardinality; third  we would
like the model to support continual learning. To handle the ﬁrst two challenges we follow Gordon
et al. [5]. First  each column of the weight matrix is generated independently from the context points
from that class ψτ
C)]  an approach which scales to arbitrary numbers
of classes. Second  we employ a permutation invariant architecture [18  19] for ψw(·) to handle the
variable input cardinality (see Appendix E for details). Third  as permutation invariant architectures
can be incrementally updated [20]  continual learning is supported (as discussed in Section 5).
Intuitively  the classiﬁer weights should be determined by the representation of the data points
emerging from the adapted feature extractor. We therefore input the adapted feature representation
of the data points into the network  rather than the raw data points (hence the dependency of ψw on
ψf and θ). To summarize  ψw(·) is a function on sets that accepts as input a set of adapted feature
representations from Dτ

w as a parameterized function of the form ψτ

c   and outputs the cth column of the linear classiﬁcation matrix  i.e. 
c ; φw  ψf   θ) = ψw ({fθ (xm; ψf )|xm ∈ Dτ   ym = c}; φw) .

w = [ψw (Dτ

. . .   ψw (Dτ

ψw (Dτ

1 )  

(2)

Here φw are learnable parameters of ψw(·). See Figure 3 for an illustration.
Adaptation Network: Feature Extractor Parameters. CNAPS represents the task-speciﬁc feature
extractor parameters ψτ
f   comprising the parameters of the FiLM layers γτ and βτ in our imple-
mentation  as a parameterized function of the context-set Dτ . Thus  ψf (·; φf   θ) is a collection of
functions (one for each FiLM layer) with parameters φf   many of which are shared across functions.
We denote the function generating the parameters for the ith FiLM layer ψi
Our experiments (Section 5) show that this mapping requires careful parameterization. We propose a
novel parameterization that improves performance in complex settings with diverse datasets. Our
implementation contains two components: a task-speciﬁc representation that provides context about
the task to all layers of the feature extractor (denoted zτ
G)  and an auto-regressive component that
provides information to deeper layers in the feature extractor concerning how shallower layers have
adapted to the task (denoted zi
G is computed
for every task τ by passing the inputs xτ

n through a global set encoder g with parameters in φf .

f (·) network is zi = (zτ

AR). The input to the ψi

f (·) for brevity.

AR). zτ

G  zi

4

Split𝑓𝜃(𝒙;𝝍𝑓𝜏)by class label 𝑦{𝑓𝜃(𝒙;𝝍𝑓𝜏)}{𝒚}𝝍𝑤(·;𝜙𝑤 𝝍𝑓𝜏 𝜽){𝑓𝜃(𝑥𝑘𝑐;𝝍𝑓𝜏)}𝑘=1𝑘𝑐(i.e. 𝑘𝑐train examples from each class 𝑐)𝑓𝜃(𝒙∗;𝝍𝑓𝜏)Linear Classifier 𝑤Softmax𝑝𝒚∗𝒙∗ 𝜽 𝝍𝜏)𝑤1𝑤𝑐𝑤𝐶…𝑏𝑐𝑏1𝑏𝐶………Shared network for each class 𝑐in CMean Pooling𝜙𝑏𝜙𝑤𝑧𝐶𝜓𝑤𝐷𝑖𝜏≔𝑤𝑖;𝑏𝑖Figure 4: Implementation of the feature-extractor: an independently learned set encoder g provides a ﬁxed
context that is concatenated to the (processed) activations of x from the previous ResNet block. The inputs
f (·)  which outputs the FiLM parameters for layer i. Green arrows correspond
zi = (zτ
AR is computed by
to propagation of auto-regressive representations. Note that the auto-regressive component zi
processing the adapted activations {f i

f )} of the previous convolutional block.

θ(x; ψτ

G  zi

AR) are then fed to ψi

Figure 5: Adaptation network φf . Rγibj ch and Rβibj ch denote a vector of regularization weights that are
learned with an l2 penalty.

To adapt the lth layer in the feature extractor  it is useful for the system to have access to the
representation of task-relevant inputs from layer l − 1. While zG could in principle encode how layer
l − 1 has adapted  we opt to provide this information directly to the adaptation network adapting layer
l by passing the adapted activations from layer l− 1. The auto-regressive component zi
AR is computed
by processing the adapted activations of the previous convolutional block with a layer-speciﬁc set
encoder (except for the ﬁrst residual block  whose auto-regressive component is given by the un-
adapted initial pre-processing stage in the ResNet). Both the global and all layer-speciﬁc set-encoders
are implemented as permutation invariant functions [18  19] (see Appendix E for details). The full
f (·) networks is illustrated in
parameterization is illustrated in Figure 4  and the architecture of ψi
Figure 5.

3 Model Training

The previous section has speciﬁed the model (see Figure 1b for a schematic). We now describe how
to train the global classiﬁer parameters θ and the adaptation network parameters φ = {φf   φw}.
Training the global classiﬁer parameters θ. A natural approach to training the model (originally
employed by CNPs [13]) would be to maximize the likelihood of the training data jointly over θ and
φ. However  experiments (detailed in Appendix D.3) showed that it is crucially important to adopt a
two stage process instead. In the ﬁrst stage  θ are trained on a large dataset (e.g.  the training set of
ImageNet [21  6]) in a full-way classiﬁcation procedure  mirroring standard pre-training. Second  θ
are ﬁxed and φ are trained using episodic training over all meta-training datasets in the multi-task
setting. We hypothesize that two-stage training is important for two reasons: (i) during the second
stage  φf are trained to adapt fθ(·) to tasks τ by outputting ψτ
f . As θ has far more capacity than ψτ
f  
if they are trained in the context of all tasks  there is no need for ψτ
f to adapt the feature extractor 
resulting in little-to-no training signal for φf and poor generalisation. (ii) Allowing θ to adapt during

5

𝑓𝜃𝒙∗Pre𝒙𝒙𝑓𝜃(𝒙∗;𝝍𝑓𝜏)𝑓𝜃(𝒙;𝝍𝑓𝜏)Post𝝍𝒇(𝐷𝜏)𝑔Set Encoder𝒛𝐺𝜏block2𝛾1 𝛽1block 1layer 1𝜓𝒇1𝒛AR1𝜓𝒇1SetEncoderblock2𝛾2 𝛽2block 1layer 2𝜓𝒇2𝒛AR2𝜓𝒇2SetEncoderblock2𝛾3 𝛽3block 1layer 3𝜓𝒇3𝒛AR3𝜓𝒇3SetEncoderblock2𝛾4 𝛽4block 1layer 4𝜓𝒇4𝒛AR4𝜓𝒇4SetEncoder𝑓𝜽1(𝒙)𝑓𝜽2(𝒙;𝝍𝑓𝜏)𝑓𝜽3(𝒙;𝝍𝑓𝜏)𝑓𝜽4(𝒙;𝝍𝑓𝜏)𝒁𝐺Concatenate𝜙𝒇𝑖𝒛𝐴𝑅𝑖(𝑙2penalty)𝜙𝑓𝜷𝑏1𝜷𝑖𝑏1𝜙𝑓𝜸𝑏1(𝑙2penalty)𝜸𝑖𝑏11(𝑙2penalty)𝜙𝑓𝜷𝑏2𝜷𝑖𝑏2𝜙𝑓𝜸𝑏2(𝑙2penalty)𝜸𝑖𝑏21Figure 6: Model design space. The y-axis repre-
sents the number of task-speciﬁc parameters |ψτ|.
Increasing |ψτ| increases model ﬂexibility  but also
the propensity to over-ﬁt. The x-axis represents the
complexity of the mechanism used to adapt the task-
speciﬁc parameters to training data ψ(Dτ ). On the
right are amortized approaches (i.e. using ﬁxed func-
tions). On the left is gradient-based adaptation. Mixed
approaches lie between. Computational efﬁciency in-
creases to the right. Flexibility increases to the left 
but with it over-ﬁtting and need for hand tuning.

the second phase violates the principle of “train as you test”  i.e.  when test tasks are encountered 
θ will be ﬁxed  so it is important to simulate this scenario during training. Finally  ﬁxing θ during
meta-training is desireable as it results in a dramatic decrease in training time.

Training the adaptation network parameters φ. Following the work of Garnelo et al. [13]  we
train φ with maximum likelihood. An unbiased stochastic estimator of the log-likelihood is:

(cid:88)

m τ

ˆL (φ) =

1

M T

log p (y∗τ

m |x∗τ

m   ψφ (Dτ )   θ)  

(3)

m   x∗τ

m   Dτ} ∼ ˆP   with ˆP representing the data distribution (e.g.  sampling tasks and
where {y∗τ
splitting them into disjoint context (Dτ ) and target data {(x∗τ
m=1). Maximum likelihood
training therefore naturally uses episodic context / target splits often used in meta-learning. In our
experiments we use the protocol deﬁned by Triantaﬁllou et al. [6] and META-DATASET for this
sampling procedure. Algorithm A.1 details computation of the stochastic estimator for a single task.

m )}Mt

m   y∗τ

4 Related Work

Our work frames multi-task classiﬁcation as directly modelling the predictive distribution
p(y∗|x∗  ψ(Dτ )). The perspective allows previous work [7  5  15  22  16  17  23  4  6  24  9  25  26]
to be organised in terms of i) the choice of the parameterization of the classiﬁer (and in particular the
nature of the local parameters)  and ii) the function used to compute the local parameters from the
training data. This space is illustrated in Figure 6  and further elaborated upon in Appendix B.
One of the inspirations for our work is conditional neural processes (CNPs) [13]. CNPs directly model
the predictive distribution p(y∗|x∗  ψ(Dτ )) and train the parameters using maximum likelihood.
Whereas previous work on CNPs has focused on homogeneous regression and classiﬁcation datasets
and fairly simple models  here we study multiple heterogeneous classiﬁcation datasets and use a
more complex model to handle this scenario. In particular  whereas the original CNP approach to
classiﬁcation required pre-specifying the number of classes in advance  CNAPS handles varying way
classiﬁcation tasks  which is required for e.g. the meta-dataset benchmark. Further  CNAPS employs
a parameter-sharing hierarchy that parameterizes the feature extractor. This contrasts to the original
CNP approach that shared all parameters across tasks  and use latent inputs to the decoder to adapt to
new tasks. Finally  CNAPS employs a meta-training procedure geared towards learning to adapt
to diverse tasks. Similarly  our work can be viewed as a deterministic limit of ML-PIP [5] which
employs a distributional treatment of the local-parameters ψ.
A model with design choices closely related to CNAPS is TADAM [27]. TADAM employs a
similar set of local parameters  allowing for adaptation of both the feature extractor and classiﬁcation
layer. However  it uses a far simpler adaptation network (lacking auto-regressive structure) and
an expensive and ad-hoc training procedure. Moreover  TADAM was applied to simple few-shot
learning benchmarks (e.g. CIFAR100 and mini-ImageNet) and sees little gain from feature extractor
adaptation. In contrast  we see a large beneﬁt from adapting the feature extractor. This may in part
reﬂect the differences in the two models  but we observe that feature extractor adaptation has the
largest impact when used to adapt to different datasets and that two stage training is required to see
this. Further differences are our usage of the CNP framework and the ﬂexible deployment of CNAPS
to continual learning and active learning (see Section 5).

6

Adaptation Mechanism 𝝍(𝐷𝜏)Faster at Test-Time# Task-specific Parameters 𝝍𝜏AllClassifierandFeatureAdaptersClassifierOnlyMulti-step GradientFew-step GradientSemi-AmortizedAmortizedFinetune [9]ResidualAdapters [16  17]LEO [23] Proto-MAML [6]CNAPS TADAM [27]VERSA [5] Proto Nets [4] Matching Nets [24]MAML [7]Meta-LSTM [22]Model FlexibilityCAVIA [25]Disc. k-shot [26]5 Experiments and Results

The experiments target three key questions: (i) Can CNAPS improve performance in multi-task
few-shot learning? (ii) Does the use of an adaptation network beneﬁt computational-efﬁciency and
data-efﬁciency? (iii) Can CNAPS be deployed directly to complex learning scenarios like continual
learning and active learning? The experiments use the following modelling choices (see Appendix E
for full details). While CNAPS can utilize any feature extractor  a ResNet18 [14] is used throughout to
enable fair comparison with Triantaﬁllou et al. [6]. To ensure that each task is handled independently 
batch normalization statistics [28] are learned (and ﬁxed) during the pre-training phase for θ. Actual
batch statistics of the test data are never used during meta-training or testing.

Few Shot Classiﬁcation. The ﬁrst experiment tackles a demanding few-shot classiﬁcation chal-
lenge called META-DATASET [6]. META-DATASET is composed of ten (eight train  two test) image
classiﬁcation datasets. The challenge constructs few-shot learning tasks by drawing from the follow-
ing distribution. First  one of the datasets is sampled uniformly; second  the “way” and “shot” are
sampled randomly according to a ﬁxed procedure; third  the classes and context / target instances are
sampled. Where a hierarchical structure exists in the data (ILSVRC or OMNIGLOT)  task-sampling
respects the hierarchy. In the meta-test phase  the identity of the original dataset is not revealed
and the tasks must be treated independently (i.e. no information can be transferred between them).
Notably  the meta-training set comprises a disjoint and dissimilar set of classes from those used for
meta-test. Full details are available in Appendix C.1 and [6].
Triantaﬁllou et al. [6] consider two stage training: an initial stage that trains a feature extractor in a
standard classiﬁcation setting  and a meta-training stage of all parameters in an episodic regime. For
the meta-training stage  they consider two settings: meta-training only on the META-DATASET version
of ILSVRC  and on all meta-training data. We focus on the latter as CNAPS rely on training data
from a variety of training tasks to learn to adapt  but provide results for the former in Appendix D.1.
We pre-train θ on the meta-training set of the META-DATASET version of ILSVRC  and meta-train
φ in an episodic fashion using all meta-training data. We compare CNAPS to models considered
by Triantaﬁllou et al. [6]  including their proposed method (Proto-MAML) in Table 1. We meta-test
CNAPS on three additional held-out datasets: MNIST [29]  CIFAR10 [30]  and CIFAR100 [30]. As
an ablation study  we compare a version of CNAPS that does not make use of the auto-regressive
component zAR  and a version that uses no feature extractor adaptation. In our analysis of Table 1  we
distinguish between two types of generalization: (i) unseen tasks (classes) in meta-training datasets 
and (ii) unseen datasets.

Unseen tasks: CNAPS achieve signiﬁcant improvements over existing methods on seven of the
eight datasets. The exception is the TEXTURES dataset  which has only seven test classes and
accuracy is highly sensitive to the train / validation / test class split. The ablation study demonstrates
that removing zAR from the feature extractor adaptation degrades accuracy in most cases  and that
removing all feature extractor adaptation results in drastic reductions in accuracy.

Unseen datasets: CNAPS-models outperform all competitive models with the exception of FINE-
TUNE on the TRAFFIC SIGNS dataset. Removing zAR from the feature extractor decreases accuracy
and removing the feature extractor adaptation entirely signiﬁcantly impairs performance. The degra-
dation is particularly pronounced when the held out dataset differs substantially from the dataset used
to pretrain θ  e.g. for MNIST.
Note that the superior results when using the auto-regressive component can not be attributed to
increased network capacity alone. In Appendix D.4 we demonstrate that CNAPS yields superior
classiﬁcation accuracy when compared to parallel residual adapters [17] even though CNAPS requires
signiﬁcantly less network capacity in order to adapt the feature extractor to a given task.

Additional results: Results when meta-training only on the META-DATASET version of ILSVRC
are given in Table D.3. In Appendix D.2  we visualize the task encodings and parameters  demon-
strating that the model is able to learn meaningful task and dataset level representations and parame-
terizations. The results support the hypothesis that learning to adapt key parts of the network is more
robust and achieves signiﬁcantly better performance than existing approaches.

7

Finetune
43.1 ± 1.1
71.1 ± 1.4
72.0 ± 1.1
59.8 ± 1.2
69.1 ± 0.9
47.0 ± 1.2
38.2 ± 1.0
85.3 ± 0.7
66.7 ± 1.2
35.2 ± 1.1

MatchingNet
36.1 ± 1.0
78.3 ± 1.0
69.2 ± 1.0
56.4 ± 1.0
61.8 ± 0.7
60.8 ± 1.0
33.7 ± 1.0
81.9 ± 0.7
55.6 ± 1.1
28.8 ± 1.0

ProtoNet
44.5 ± 1.1
79.6 ± 1.1
71.1 ± 0.9
67.0 ± 1.0
65.2 ± 0.8
64.9 ± 0.9
40.3 ± 1.1
86.9 ± 0.7
46.5 ± 1.0
39.9 ± 1.1

fo-MAML
32.4 ± 1.0
71.9 ± 1.2
52.8 ± 0.9
47.2 ± 1.1
56.7 ± 0.7
50.5 ± 1.2
21.0 ± 1.0
70.9 ± 1.0
34.2 ± 1.3
24.1 ± 1.1

Proto-MAML
47.9 ± 1.1
82.9 ± 0.9
74.2 ± 0.8
70.0 ± 1.0
67.9 ± 0.8
66.6 ± 0.9
42.0 ± 1.1
88.5 ± 0.7
52.3 ± 1.1
41.3 ± 1.0

Dataset

ILSVRC [21]
Omniglot [31]
Aircraft [32]
Birds [33]
Textures [34]
Quick Draw [35]
Fungi [36]
VGG Flower [37]
Trafﬁc Signs [38]
MSCOCO [39]
MNIST [29]
CIFAR10 [30]
CIFAR100 [30]

CNAPS
(no ψf )
43.8 ± 1.0
60.1 ± 1.3
53.0 ± 0.9
55.7 ± 1.0
60.5 ± 0.8
58.1 ± 1.0
28.6 ± 0.9
75.3 ± 0.7
55.0 ± 0.9
41.2 ± 1.0
76.0 ± 0.8
61.5 ± 0.7
44.8 ± 1.0

CNAPS
(no zAR)
51.3 ± 1.0
88.0 ± 0.7
76.8 ± 0.8
71.4 ± 0.9
62.5 ± 0.7
71.9 ± 0.8
46.0 ± 1.1
89.2 ± 0.5
60.1 ± 0.9
42.0 ± 1.0
88.6 ± 0.5
60.0 ± 0.8
48.1 ± 1.0

CNAPS
52.3 ± 1.0
88.4 ± 0.7
80.5 ± 0.6
72.2 ± 0.9
58.3 ± 0.7
72.5 ± 0.8
47.4 ± 1.0
86.0 ± 0.5
60.2 ± 0.9
42.6 ± 1.1
92.7 ± 0.4
61.5 ± 0.7
50.1 ± 1.0

Table 1: Few-shot classiﬁcation results on META-DATASET [6] using models trained on all training datasets. All
ﬁgures are percentages and the ± sign indicates the 95% conﬁdence interval over tasks. Bold text indicates the
scores within the conﬁdence interval of the highest score. Tasks from datasets below the dashed line were not
used for training. Competing methods’ results from [6].

Figure 7: Comparing CNAPS to gradient based feature extractor adaptation: accuracy on 5-way classiﬁcation
tasks from withheld datasets as a function of processing time. Dot size reﬂects shot number (1 to 25 shots).

FiLM Parameter Learning Performance: Speed-Accuracy Trade-off. CNAPS generate FiLM
layer parameters for each task τ at test time using the adaptation network ψf (Dτ ). It is also possible
to learn the FiLM parameters via gradient descent (see [16  17]). Here we compare CNAPS to this
approach. Figure 7 shows plots of 5-way classiﬁcation accuracy versus time for four held out data
sets as the number of shots was varied. For gradient descent  we used a ﬁxed learning rate of 0.001
and took 25 steps for each point. The overall time required to produce the plot was 1274 and 7214
seconds for CNAPS and gradient approaches  respectively  on a NVIDIA Tesla P100-PCIE-16GB
GPU. CNAPS is at least 5 times faster at test time than gradient-based optimization requiring only a
single forward pass through the network while gradient based approaches require multiple forward
and backward passes. Further  the accuracy achieved with adaptation networks is signiﬁcantly higher
for fewer shots as it protects against over-ﬁtting. For large numbers of shots  gradient descent catches
up  albeit slowly.

Complex Learning Scenarios: Continual Learning.
In continual learning [40] new tasks appear
over time and existing tasks may change. The goal is to adapt accordingly  but without retaining old
data which is challenging for artiﬁcial systems. To demonstrate the the versatility CNAPS we show
that  although it has not been explicitly trained for continual learning  we are able to apply the same
model trained for the few-shot classiﬁcation experiments (without the auto-regressive component) to
standard continual learning benchmarks on held out datasets: Split MNIST [41] and Split CIFAR100
[42]. We modify the model to compute running averages for the representations of both ψτ
w and ψτ
f
(see Appendix F for further details)  in this way it performs incremental updates using the new data
and the old model  and does not need to access old data. Figure 8 (left) shows the accumulated multi-
and single-head [42] test accuracy averaged over 30 runs (further results and more detailed ﬁgures are
in Appendix G). Figure 8 (right) shows average results at the ﬁnal task comparing to SI [41]  EWC
[43]  VCL [44]  and Riemannian Walk [42].
Figure 8 demonstrates that CNAPS naturally resists catastrophic forgetting [43] and compares
favourably to competing methods  despite the fact that it was not exposed to these datasets during
training  observes orders of magnitude fewer examples  and was not trained explicitly to perform
continual learning. CNAPS performs similarly to  or better than  the state-of-the-art Riemannian
Walk method which departs from the pure continual learning setting by maintaining a small number
of training samples across tasks. Conversely  CNAPS has the advantage of being exposed to a larger

8

20100300Time (seconds)3040506070Accuracy (%)COCO20100300Time (seconds)405060708090Traffic Signs20100300Time (seconds)405060708090100MNIST20100300Time (seconds)304050607080CIFAR 10CNAPsGradient DescentMethod
SI [41]
EWC [43]
VCL [44]

RWalk [42]
CNAPS

MNIST

CIFAR100

-

Single
22.8
23.1

Single Multi
73.2
57.6
55.8
72.8

Multi
99.3
99.3
98.5
± 0.4
99.3
34.0
37.2
98.9
± 0.2 ± 0.9 ± 0.5 ± 0.6

-

74.2
76.0

-

82.5
80.9

Figure 8: Continual learning classiﬁcation results on Split MNIST and Split CIFAR100 using a model trained
on all training datasets. (Left) The plots show accumulated accuracy averaged over 30 runs for both single-
and multi-head scenarios. (Right) Average accuracy at ﬁnal task computed over 30 experiments (all ﬁgures are
percentages). Errors are one standard deviation. Additional results from [42  45].

Figure 9: Accuracy vs active learning iterations for held-out classes / languages. (Top) CNAPS and (bottom)
prototypical networks. Error shading is one standard error. CNAPS achieves better accuracy than prototypical
networks and improvements over random acquisition  whereas prototypical networks do not.

range of datasets and can therefore leverage task transfer. We emphasize that this is not meant to be
an “apples-to-apples” comparison  but rather  the goal is to demonstrate the out-of-the-box versatility
and strong performance of CNAPS in new domains and learning scenarios.
Complex Learning Scenarios: Active Learning. Active learning [46  47] requires accurate data-
efﬁcient learning that returns well-calibrated uncertainty estimates. Figure 9 compares the perfor-
mance of CNAPS and prototypical networks using two standard active learning acquisition functions
(variation ratios and predictive entropy [46]) against random acquisition on the FLOWERS dataset and
three representative held-out languages from OMNIGLOT (performance on all languages is presented
in Appendix H). Figure 9 and Appendix H show that CNAPS achieves higher accuracy on average
than prototypical networks. Moreover  CNAPS achieves signiﬁcant improvements over random
acquisition  whereas prototypical networks do not. These tests indicates that CNAPS is more accurate
and suggest that CNAPS has better calibrated uncertainty estimates than prototypical networks.

6 Conclusions

This paper has introduced CNAPS  an automatic  fast and ﬂexible modelling approach for multi-
task classiﬁcation. We have demonstrated that CNAPS achieve state-of-the-art performance on the
META-DATASET challenge  and can be deployed “out-of-the-box” to diverse learning scenarios such
as continual and active learning where they are competitive with the state-of-the-art. Future avenues
of research are to consider the exploration of the design space by introducing gradients and function
approximation to the adaptation mechanisms  as well as generalizing the approach to distributional
extensions of CNAPS [48  49].

9

1234592.595.097.5100.0Accuracy (%)MNIST Multi-headRWalk 10k-shotCNAPs 1-shotCNAPs 10-shotCNAPs 100-shot1234560708090100MNIST Single-head12345678910Tasks4050607080Accuracy (%)CIFAR100 Multi-head12345678910Tasks20406080CIFAR100 Single-head657075C-NAPsVGG Flower758085Avesta606570Kannada6570MalayalamVar RatPred EntRand0102030Acquisitions60657075Proto-Net0102030Acquisitions7580850102030Acquisitions6065700102030Acquisitions6570Acknowledgments

The authors would like to thank Ambrish Rawat for helpful discussions and David Duvenaud  Wessel
Bruinsma  Will Tebbutt Adrià Garriga Alonso  Eric Nalisnick  and Lyndon White for the insightful
comments and feedback. Richard E. Turner is supported by Google  Amazon  Improbable and
EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] Jürgen Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis  Technische

Universität München  1987.

[2] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media 

2012.

[3] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept

learning through probabilistic program induction. Science  350(6266):1332–1338  2015.

[4] Jake Snell  Kevin Swersky  and Richard Zemel. Prototypical networks for few-shot learning. In

Advances in Neural Information Processing Systems  pages 4080–4090  2017.

[5] Jonathan Gordon  John Bronskill  Matthias Bauer  Sebastian Nowozin  and Richard Turner.
Meta-learning probabilistic inference for prediction. In International Conference on Learning
Representations  2019. URL https://openreview.net/forum?id=HkxStoC5F7.

[6] Eleni Triantaﬁllou  Tyler Zhu  Vincent Dumoulin  Pascal Lamblin  Kelvin Xu  Ross Goroshin 
Carles Gelada  Kevin Swersky  Pierre-Antoine Manzagol  and Hugo Larochelle. Meta-dataset:
A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096 
2019.

[7] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adapta-
tion of deep networks. In International Conference on Machine Learning  pages 1126–1135 
2017.

[8] Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint

arXiv:1803.02999  2018.

[9] Jason Yosinski  Jeff Clune  Yoshua Bengio  and Hod Lipson. How transferable are features in
deep neural networks? In Advances in neural information processing systems  pages 3320–3328 
2014.

[10] Siyuan Qiao  Chenxi Liu  Wei Shen  and Alan Yuille. Few-shot image recognition by predicting

parameters from activations. arXiv preprint arXiv:1706.03466  2017.

[11] Seymour Geisser. On the prediction of observables: a selective update. Technical report 

University of Minnesota  1983.

[12] Seymour Geisser. Predictive inference. Routledge  2017.

[13] Marta Garnelo  Dan Rosenbaum  Chris J Maddison  Tiago Ramalho  David Saxton  Murray
Shanahan  Yee Whye Teh  Danilo J Rezende  and SM Eslami. Conditional neural processes.
arXiv preprint arXiv:1807.01613  2018.

[14] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[15] Ethan Perez  Florian Strub  Harm De Vries  Vincent Dumoulin  and Aaron Courville. FiLM:
Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence  2018.

[16] Sylvestre-Alvise Rebufﬁ  Hakan Bilen  and Andrea Vedaldi. Learning multiple visual domains
with residual adapters. In Advances in Neural Information Processing Systems  pages 506–516 
2017.

10

[17] Sylvestre-Alvise Rebufﬁ  Hakan Bilen  and Andrea Vedaldi. Efﬁcient parametrization of multi-
domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 8119–8127  2018.

[18] Manzil Zaheer  Satwik Kottur  Siamak Ravanbakhsh  Barnabas Poczos  Ruslan R Salakhutdinov 
and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems 
pages 3394–3404  2017.

[19] Charles R Qi  Hao Su  Kaichun Mo  and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3d classiﬁcation and segmentation. Proc. Computer Vision and Pattern Recognition
(CVPR)  IEEE  1(2):4  2017.

[20] Manasi Vartak  Arvind Thiagarajan  Conrado Miranda  Jeshua Bratman  and Hugo
Larochelle. A meta-learning perspective on cold-start recommendations for items.
In
I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and
R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages
6904–6914. Curran Associates  Inc.  2017. URL http://papers.nips.cc/paper/
7266-a-meta-learning-perspective-on-cold-start-recommendations-for-items.
pdf.

[21] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al. Imagenet large scale visual
recognition challenge. International journal of computer vision  115(3):211–252  2015.

[22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceed-

ings of the International Conference on Learning Representations (ICLR)  2017.

[23] Andrei A Rusu  Dushyant Rao  Jakub Sygnowski  Oriol Vinyals  Razvan Pascanu  Simon
Osindero  and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
arXiv:1807.05960  2018.

[24] Oriol Vinyals  Charles Blundell  Tim Lillicrap  Daan Wierstra  et al. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems  pages 3630–3638  2016.

[25] Luisa M Zintgraf  Kyriacos Shiarlis  Vitaly Kurin  Katja Hofmann  and Shimon Whiteson.

CAML: Fast context adaptation via meta-learning. arXiv preprint arXiv:1810.03642  2018.

[26] Matthias Bauer  Mateo Rojas-Carulla  Jakub Bartłomiej ´Swi ˛atkowski  Bernhard Schölkopf  and
Richard E Turner. Discriminative k-shot learning using probabilistic models. arXiv preprint
arXiv:1706.00326  2017.

[27] Boris N Oreshkin  Alexandre Lacoste  and Pau Rodriguez. TADAM: Task dependent adaptive

metric for improved few-shot learning. arXiv preprint arXiv:1805.10123  2018.

[28] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

[29] Yann LeCun  Corinna Cortes  and CJ Burges. MNIST handwritten digit database. AT&T Labs

[Online]. Available: http://yann. lecun. com/exdb/mnist  2:18  2010.

[30] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

[31] Brenden Lake  Ruslan Salakhutdinov  Jason Gross  and Joshua Tenenbaum. One shot learning
of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science
Society  volume 33  2011.

[32] Subhransu Maji  Esa Rahtu  Juho Kannala  Matthew Blaschko  and Andrea Vedaldi. Fine-

grained visual classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151  2013.

[33] Catherine Wah  Steve Branson  Peter Welinder  Pietro Perona  and Serge Belongie. The

caltech-ucsd birds-200-2011 dataset. 2011.

11

[34] Mircea Cimpoi  Subhransu Maji  Iasonas Kokkinos  Sammy Mohamed  and Andrea Vedaldi.
Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 3606–3613  2014.

[35] David Ha and Douglas Eck. A neural representation of sketch drawings. arXiv preprint

arXiv:1704.03477  2017.

[36] Brigit Schroeder and Yin Cui. Fgvcx fungi classiﬁcation challenge at fgvc5. https://www.

kaggle.com/c/fungi-challenge-fgvc-2018  2018.

[37] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large
number of classes. In 2008 Sixth Indian Conference on Computer Vision  Graphics & Image
Processing  pages 722–729. IEEE  2008.

[38] Sebastian Houben  Johannes Stallkamp  Jan Salmen  Marc Schlipsing  and Christian Igel.
Detection of trafﬁc signs in real-world images: The german trafﬁc sign detection benchmark. In
The 2013 international joint conference on neural networks (IJCNN)  pages 1–8. IEEE  2013.

[39] Tsung-Yi Lin  Michael Maire  Serge Belongie  James Hays  Pietro Perona  Deva Ramanan  Piotr
Dollár  and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision  pages 740–755. Springer  2014.

[40] Mark B Ring. Child: A ﬁrst step towards continual learning. Machine Learning  28(1):77–104 

1997.

[41] Friedemann Zenke  Ben Poole  and Surya Ganguli. Continual learning through synaptic
intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume
70  pages 3987–3995. JMLR. org  2017.

[42] Arslan Chaudhry  Puneet K Dokania  Thalaiyasingam Ajanthan  and Philip HS Torr. Riemannian
walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of
the European Conference on Computer Vision (ECCV)  pages 532–547  2018.

[43] James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  Joel Veness  Guillaume Desjardins 
Andrei A Rusu  Kieran Milan  John Quan  Tiago Ramalho  Agnieszka Grabska-Barwinska  et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of
sciences  114(13):3521–3526  2017.

[44] Cuong V Nguyen  Yingzhen Li  Thang D Bui  and Richard E Turner. Variational continual

learning. arXiv preprint arXiv:1710.10628  2017.

[45] Siddharth Swaroop  Cuong V Nguyen  Thang D Bui  and Richard E Turner. Improving and

understanding variational continual learning. arXiv preprint arXiv:1905.02099  2019.

[46] David A Cohn  Zoubin Ghahramani  and Michael I Jordan. Active learning with statistical

models. Journal of artiﬁcial intelligence research  4:129–145  1996.

[47] Burr Settles. Active learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine

Learning  6(1):1–114  2012.

[48] Marta Garnelo  Jonathan Schwarz  Dan Rosenbaum  Fabio Viola  Danilo J Rezende  SM Eslami 

and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622  2018.

[49] Hyunjik Kim  Andriy Mnih  Jonathan Schwarz  Marta Garnelo  Ali Eslami  Dan Rosenbaum 
Oriol Vinyals  and Yee Whye Teh. Attentive neural processes. In International Conference on
Learning Representations  2019. URL https://openreview.net/forum?id=SkE6PjC9KX.

12

,James Requeima
Jonathan Gordon
John Bronskill
Sebastian Nowozin
Richard Turner