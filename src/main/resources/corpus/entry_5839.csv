2013,Variance Reduction for Stochastic Gradient Optimization,Stochastic gradient optimization is a class of widely used algorithms for training machine learning models. To optimize an objective  it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset. However  when the variance of the noisy gradient is large  the algorithm might spend much time bouncing around  leading to slower convergence and worse performance. In this paper  we develop a general approach of using control variate for variance reduction in stochastic gradient. Data statistics such as low-order moments (pre-computed or estimated online) is used to form the control variate. We demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization. One is convex---the MAP estimation for logistic regression  and the other is non-convex---stochastic variational inference for latent Dirichlet allocation. On both problems  our approach shows faster convergence and better performance than the classical approach.,Variance Reduction for

Stochastic Gradient Optimization

Chong Wang Xi Chen∗ Alex Smola Eric P. Xing

Carnegie Mellon University  University of California  Berkeley∗

{chongw xichen epxing}@cs.cmu.edu

alex@smola.org

Abstract

Stochastic gradient optimization is a class of widely used algorithms for training
machine learning models. To optimize an objective  it uses the noisy gradient
computed from the random data samples instead of the true gradient computed
from the entire dataset. However  when the variance of the noisy gradient is
large  the algorithm might spend much time bouncing around  leading to slower
convergence and worse performance. In this paper  we develop a general approach
of using control variate for variance reduction in stochastic gradient. Data statistics
such as low-order moments (pre-computed or estimated online) is used to form
the control variate. We demonstrate how to construct the control variate for two
practical problems using stochastic gradient optimization. One is convex—the
MAP estimation for logistic regression  and the other is non-convex—stochastic
variational inference for latent Dirichlet allocation. On both problems  our approach
shows faster convergence and better performance than the classical approach.

1

Introduction

Stochastic gradient (SG) optimization [1  2] is widely used for training machine learning models with
very large-scale datasets. It uses the noisy gradient (a.k.a. stochastic gradient) estimated from random
data samples rather than that from the entire data. Thus  stochastic gradient algorithms can run many
more iterations in a limited time budget. However  if the noisy gradient has a large variance  the
stochastic gradient algorithm might spend much time bouncing around  leading to slower convergence
and worse performance. Taking a mini-batch with a larger size for computing the noisy gradient could
help to reduce its variance; but if the mini-batch size is too large  it can undermine the advantage in
efﬁciency of stochastic gradient optimization.
In this paper  we propose a general remedy to the “noisy gradient” problem ubiquitous to all stochastic
gradient optimization algorithms for different models. Our approach builds on a variance reduction
technique  which makes use of control variates [3] to augment the noisy gradient and thereby reduce
its variance. The augmented “stochastic gradient” can be shown to remain an unbiased estimate of
the true gradient  a necessary condition that ensures the convergence. For such control variates to be
effective and sound  they must satisfy the following key requirements: 1) they have a high correlation
with the noisy gradient  and 2) their expectation (with respect to random data samples) is inexpensive
to compute. We show that such control variates can be constructed via low-order approximations
to the noisy gradient so that their expectation only depends on low-order moments of the data. The
intuition is that these low-order moments roughly characterize the empirical data distribution  and
can be used to form the control variate to correct the noisy gradient to a better direction. In other
words  the variance of the augmented “stochastic gradient” becomes smaller as it is derived with
more information about the data.
The rest of the paper is organized as follows. In §2  we describe the general formulation and the
theoretical property of variance reduction via control variates in stochastic gradient optimization.

1

In §3  we present two examples to show how one can construct control variates for practical algorithms.
(More examples are provided in the supplementary material.) These include a convex problem—the
MAP estimation for logistic regression  and a non-convex problem—stochastic variational inference
for latent Dirichlet allocation [22]. Finally  we demonstrate the empirical performance of our
algorithms under these two examples in §4. We conclude with a discussion on some future work.

2 Variance reduction for general stochastic gradient optimization

We begin with a description of the general formulation of variance reduction via control variate for
stochastic gradient optimization. Consider a general optimization problem over a ﬁnite set of training
data D = {xd}D
d=1 with each xd ∈ Rp. Here D is the number of the training data. We want to
maximize the following function with respect to a p-dimensional vector w 

L(w) := R(w) + (1/D)(cid:80)D

d=1 f (w; xd) 

maximize

w

where R(w) is a regularization function.1 Gradient-based algorithms can be used to maximize L(w)
at the expense of computing the gradient over the entire training set. Instead  stochastic gradient
(SG) methods use the noisy gradient estimated from random data samples. Suppose data index d is
selected uniformly from {1 ···   D} at step t 

g(w; xd) = ∇wR(w) + ∇wf (w; xd) 

(1)
(2)
where g(w; xd) is the noisy gradient that only depends on xd and ρt is a proper step size. To make
notation simple  we use gd(w) (cid:44) g(w; xd).
Following the standard stochastic optimization literature [1  4]  we require the expectation of the
noisy gradient gd equals to the true gradient 

wt+1 = wt + ρtg(w; xd) 

Ed[gd(w)] = ∇wL(w) 

(3)
to ensure the convergence of the stochastic gradient algorithm. When the variance of gd(w) is large 
the algorithm could suffer from slow convergence.
The basic idea of using control variates for variance reduction is to construct a new random vector
that has the same expectation as the target expectation but with smaller variance. In previous work [5] 
control variates were used to improve the estimate of the intractable integral in variational Bayesian
inference which was then used to compute the gradient of the variational lower bound. In our context 
we employ a random vector hd(w) of length p to reduce the variance of the sampled gradient 

noisy gradient gd(w) in Eq. 1  and thus can be used to replace gd(w) in the SG update in Eq. 2. To

(4)
where A is a p × p matrix and h(w) (cid:44) Ed[hd(w)]. (We will show how to choose hd(w) later  but it

usually depends on the form of gd(w).) The random vector(cid:101)gd(w) has the same expectation as the
reduce the variance of the noisy gradient  the trace of the covariance matrix of(cid:101)gd(w) 
must be necessarily small; therefore we set A to be the minimizer of Tr (Vard[(cid:101)gd(w)]). That is 

Vard[(cid:101)gd(w)] (cid:44) Covd[(cid:101)gd(w) (cid:101)gd(w)] = Vard[gd(w)]

− (Covd[hd(w)  gd(w)] + Covd[gd(w)  hd(w)])A + AT Vard[hd(w)]A 

(5)

A∗ = argminATr (Vard[(cid:101)gd(w)])

−1 (Covd[gd(w)  hd(w)] + Covd[hd(w)  gd(w)]) /2.

= (Vard[hd(w)])
The optimal A∗ is a function of w.

Why is(cid:101)gd(w) a better choice? Now we show that(cid:101)gd(w) is a better “stochastic gradient” under the

(cid:96)2-norm. In the ﬁrst-order stochastic oracle model  we normally assume that there exists a constant σ
such that for any estimate w in its domain [6  7]:

(6)

(cid:101)gd(w) = gd(w) − AT (hd(w) − h(w)) 

(cid:104)(cid:107)gd(w) − Ed[gd(w)](cid:107)2

(cid:105)

2

Ed

= Tr(Vard[gd(w)]) ≤ σ2.

1We follow the convention of maximizing a function f: when we mention a convex problem  we actually

mean the objective function −f is convex.

2

√
Under this assumption  the dominating term in the optimal convergence rate is O(σ/
t) for convex
problems and O(σ2/(µt)) for strongly convex problems  where µ is the strong convexity parameter
(see the deﬁnition of strong convexity on Page 459 in [8]).
Now suppose that we can ﬁnd a random vector hd(w) and compute A∗ according to Eq. 6. By
plugging A∗ back into Eq. 5 
Ed

where Vard[(cid:101)gd(w)] = Vard[gd(w)] − Covd[gd(w)  hd(w)](Vard[hd(w)])−1Covd[hd(w)  gd(w)].

(cid:104)(cid:107)(cid:101)gd(w) − Ed[(cid:101)gd(w)](cid:107)2

−1 Covd(hd  gd) is a semi-positive deﬁnite matrix.
For any estimate w  Covd(gd  hd) (Covd(hd  hd))
Therefore  its trace  which equals to the sum of the eigenvalues  is positive (or zero when hd and gd
are uncorrelated) and hence 

= Tr(Vard[˜gd(w)]) 

(cid:105)

2

(cid:104)(cid:107)˜gd(w) − Ed[˜gd(w)](cid:107)2

(cid:105) ≤ Ed

(cid:104)(cid:107)gd(w) − Ed[gd(w)](cid:107)2

(cid:105)

Ed

2

In other words  it is possible to ﬁnd a constant τ ≤ σ such that Ed
√
for all w. Therefore  when applying stochastic gradient methods  we could improve the optimal con-
vergence rate from O(σ/
t) for convex problems; and from O(σ2/(µt)) to O(τ 2/(µt))
for strongly convex problems.
Estimating optimal A∗. When estimating A∗ according to Eq. 6  one needs to compute the inverse
of Vard[hd(w)]  which could be computationally expensive. In practice  we could constrain A to be
a diagonal matrix. According to Eq. 5  when A = Diag(a11  . . .   app)  its optimal value is:

t) to O(τ /

√

2

.

(cid:104)(cid:107)˜gd(w) − Ed[˜gd(w)](cid:107)2

2

(cid:105) ≤ τ 2

a∗
ii = [Covd(gd(w) hd(w))]ii

[Vard(hd(w))]ii

.

(7)

This formulation avoids the computation of the matrix inverse  and leads to signiﬁcant reduction
of computational cost since only the diagonal elements of Covd(gd(w)  hd(w)) and Vard(hd(w)) 
instead of the full matrices  need to be evaluated. It can be shown that  this simpler surrogate to the
A∗ due to Eq. 6 still leads to a better convergence rate. Speciﬁcally:
Ed

(cid:3) = Tr(Vard(˜gd(w))) = Tr (Vard(gd(w))) −(cid:80)p

([Covd(gd(w) hd(w))]ii)2

 

[Vard(hd(w))]ii

(cid:2)(cid:107)˜gd(w) − Ed[˜gd(w)](cid:107)2
=(cid:80)p

i=1(1 − ρ2

2

(cid:2)(cid:107)gd(w) − Ed[gd(w)](cid:107)2

i=1

(cid:3)  

2

ii)Var(gd(w))ii ≤ Tr (Vard(gd(w))) = Ed

(8)

where ρii is the Pearson’s correlation coefﬁcient between [gd(w)]i and [hd(w)]i.
Indeed  an even simpler surrogate to the A∗  by reducing A to a single real number a  can also
improve convergence rate of SG. In this case  according to Eq. 5  the optimal a∗ is simply:

a∗ = Tr (Covd(gd(w)  hd(w)))/Tr (Vard(hd(w))).

(9)
To estimate the optimal A∗ or its surrogates  we need to evaluate Covd(gd(w)  hd(w)) and
Vard(hd(w)) (or their diagonal elements)  which can be approximated by the sample covariance and
variance from mini-batch samples while running the stochastic gradient algorithm. If we can not
always obtain mini-batch samples  we may use strategies like moving average across iterations  as
those used in [9  10].
From Eq. 8  we observe that when the Pearson’s correlation coefﬁcient between gd(w) and hd(w)
is higher  the control variate hd(w) will lead to a more signiﬁcant level of variance reduction and
hence faster convergence. In the maximal correlation case  one could set hd(w) = gd(w) to obtain
zero variance. But obviously  we cannot compute Ed[hd(w)] efﬁciently in this case. In practice  one
should construct hd(w) such that it is highly correlated with gd(w). In next section  we will show
how to construct control variates for both convex and non-convex problems.

3 Practicing variance reduction on convex and non-convex problems

In this section  we apply the variance reduction technique presented above to two exemplary but
practical problems: MAP estimation for logistic regression—a convex problem; and stochastic varia-
tional inference for latent Dirichlet allocation [11  22]—a non-convex problem. In the supplement 

3

Figure 1: The illustration of how data statistics help reduce variance for the noisy gradient in stochastic
optimization. The solid (red) line is the ﬁnal gradient direction the algorithm will follow. (a) The exact gradient
direction computed using the entire dataset. (b) The noisy gradient direction computed from the sampled subset 
which can have high variance. (c) The improved noisy gradient direction with data statistics  such as low-order
moments of the entire data. These low-order moments roughly characterize the data distribution  and are used to
form the control variate to aid the noisy gradient.

we show that the same principle can be applied to more problems  such as hierarchical Dirichlet
process [12  13] and nonnegative matrix factorization [14].
As we discussed in §2  the higher the correlation between gd(w) and hd(w)  the lower the variance
is. Therefore  to apply the variance reduction technique in practice  the key is to construct a random
vector hd(w) such that it has high correlations with gd(w)  but its expectation h(w) = Ed[hd(w)] is
inexpensive to compute. The principle behind our choice of h(w) is that we construct h(w) based on
some data statistics  such as low-order moments. These low-order moments roughly characterize
the data distribution which does not depend on parameter w. Thus they can be pre-computed when
processing the data or estimated online while running the stochastic gradient algorithm. Figure 1
illustrates this idea. We will use this principle throughout the paper to construct control variates for
variance reduction under different scenarios.

3.1 SG with variance reduction for logistic regression

Logistic regression is widely used for classiﬁcation [15]. Given a set of training examples (xd  yd) 
d = 1  ...  D  where yd = 1 or yd = −1 indicates class labels  the probability of yd is

p(yd | xd  w) = σ(ydw(cid:62)xd) 

where σ(z) = 1/(1 + exp(−z)) is the logistic function. The averaged log likelihood of the training
data is

(cid:8)ydw(cid:62)xd − log(cid:0)1 + exp(ydw(cid:62)xd)(cid:1)(cid:9) .

(cid:80)D

(cid:96)(w) = 1
D

d=1

(10)

(11)

An SG algorithm employs the following noisy gradient:

gd(w) = ydxdσ(−ydw(cid:62)xd).

Now we show how to construct our control variate for logistic regression. We begin with the ﬁrst-order
Taylor expansion around ˆz for the sigmoid function 

σ(z) ≈ σ(ˆz) (1 + σ(−ˆz)(z − ˆz)) .

We then apply this approximation to σ(−ydw(cid:62)xd) in Eq. 11 to obtain our control variate.2 For
logistic regression  we consider two classes separately  since data samples within each class are more
likely to be similar. We consider positive data samples ﬁrst. Let z = −w(cid:62)xd  and we deﬁne our
control variate hd(w) for yd = 1 as

d (w) (cid:44) xdσ(ˆz) (1 + σ(−ˆz)(z − ˆz)) = xdσ(ˆz)(cid:0)1 + σ(−ˆz)(−w(cid:62)xd − ˆz)(cid:1) .
Var(1)[xd] + ¯x(1)(¯x(1))(cid:62)(cid:17)

Its expectation given yd = 1 can be computed in closed-form as
¯x(1) (1 − σ(−ˆz)ˆz) − σ(−ˆz)

Ed[h(1)

d (w)| yd = 1] = σ(ˆz)

(cid:17)

 

w

(cid:16)

(cid:16)

h(1)

2Taylor expansion is not the only way to obtain control variates. Lower bounds or upper bounds of the

objective function [16] can also provide alternatives. But we will not explore those solutions in this paper.

4

(a) entire data(b) sampled subset(c) sampled subset with data statisticsexact gradient directionexact gradient directionbut unreachablenoisy gradient directionexact gradient directionbut unreachablenoisy gradient directionimproved noisy gradient directionwhere ¯x(1) and Var(1)[xd] are the mean and variance of the input features for the positive examples.
In our experiments  we choose ˆz = −w(cid:62) ¯x(1)  which is the center of the positive examples. We can
similarly derive the control variate h(−1)
(w) for negative examples and we omit the details. Given
the random sample regardless its label  the expectation of the control variate is computed as
(w)| yd = −1] 

d (w)| yd = 1] + (D(−1)/D)Ed[h(−1)

Ed[hd(w)] = (D(1)/D)Ed[h(1)

d

d

where D(1) and D(−1) are the number of positive and negative examples and D(1)/D is the probability
of choosing a positive example from the training set. With Taylor approximation  we would expect
our control variate is highly correlated with the noisy gradient. See our experiments in §4 for details.

3.2 SVI with variance reduction for latent Dirichlet allocation

The stochastic variational inference (SVI) algorithm used for latent Dirichlet allocation (LDA) [22] is
also a form of stochastic gradient optimization  therefore it can also beneﬁt from variance reduction.
The basic idea is to stochastically optimize the variational objective for LDA  using stochastic mean
ﬁeld updates augmented by control variates derived from low-order moments on the data.
Latent Dirichlet allocation (LDA). LDA is the simplest topic model for discrete data such as text
collections [17  18]. Assume there are K topics. The generative process of LDA is as follows.

1. Draw topics βk ∼ DirV (η) for k ∈ {1  . . .   K}.
2. For each document d ∈ {1  . . .   D}:

(a) Draw topic proportions θd ∼ DirK(α).
(b) For each word wdn ∈ {1  . . .   N}:
i. Draw topic assignment zdn ∼ Mult(θd).
ii. Draw word wdn ∼ Mult(βzdn ).

d=1 p(θd | α)(cid:81)N

k=1 p(βk | η)(cid:81)D

p(β  θ  z | w) ∝(cid:81)K

Given the observed words w (cid:44) w1:D  we want to estimate the posterior distribution of the latent
variables  including topics β (cid:44) β1:K  topic proportions θ (cid:44) θ1:D and topic assignments z (cid:44) z1:D 
(12)
However  this posterior is intractable. We must resort to approximation methods. Mean-ﬁeld
variational inference is a popular approach for the approximation [19].
Mean-ﬁeld variational inference for LDA. Mean-ﬁeld variational inference posits a family of dis-
tributions (called variational distributions) indexed by free variational parameters and then optimizes
these parameters to minimize the KL divergence between the variational distribution and the true
posterior. For LDA  the variational distribution is

n=1 p(zdn | θd)p(wdn | βzdn ).

q(β  θ  z) =(cid:81)K

k=1 q(βk | λk)(cid:81)D

d=1 q(θd | γd)(cid:81)N

(13)
where the variational parameters are λk (Dirichlet)  θd (Dirichlet)  and φdn (multinomial). We seek
the variational distribution (Eq. 13) that minimizes the KL divergence to the true posterior (Eq. 12).
This is equivalent to maximizing the lower bound of the log marginal likelihood of the data 

n=1 q(zdn | φdn) 

log p(w) ≥ Eq [log p(β  θ  z  w)] − Eq [log q(β  θ  z)] (cid:44) L(q) 

(14)
where Eq [·] denotes the expectation with respect to the variational distribution q(β  θ  z). Setting
the gradient of the lower bound L(q) with respect to the variational parameters to zero gives the
following coordinate ascent algorithm [17]. For each document d ∈ {1  . . .   D}  we run local
variational inference using the following updates until convergence 

dv ∝ exp{Ψ(γdk) + Ψ(λk v) − Ψ ((cid:80)
γd = α +(cid:80)V

φk

(15)
(16)
where Ψ(·) is the digamma function and ndv is the number of term v in document d. Note that here
we use φdv instead of φdn in Eq. 13 since the same term v have the same φdn. After ﬁnding the
variational parameters for each document  we update the variational Dirichlet for each topic 

v=1 ndvφdv.

v λkv)}

for v ∈ {1  . . .   V }

λkv = η +(cid:80)D

d=1 ndvφk

dv.

5

(17)

The whole coordinate ascent variational algorithm iterates over Eq. 15  16 and 17 until convergence.
However  this also reveals the drawback of this algorithm—updating the topic parameter λ in Eq. 17
depends on the variational parameters φ from every document. This is especially inefﬁcient for large-
scale datasets. Stochastic variational inference solves this problem using stochastic optimization.
Stochastic variational inference (SVI). Instead of using the coordinate ascent algorithm  SVI
optimizes the variational lower bound L(q) using stochastic optimization [22]. It draws random
samples from the corpus and use these samples to form the noisy estimate of the natural gradient [20].
Then the algorithm follows that noisy natural gradient with a decreasing step size until convergence.
The noisy gradient only depends on the sampled data and it is inexpensive to compute. This leads to
a much faster algorithm than the traditional coordinate ascent variational inference algorithm.
Let d be a random document index  d ∼ Unif(1  ...  D) and Ld(q) be the sampled lower bound. The
sampled lower bound Ld(q) has the same form as the L(q) in Eq. 14 except that the sampled lower
bound uses a virtual corpus that only contains document d replicated D times. According to [22]  for
LDA the noisy natural gradient with respect to the topic variational parameters is

gd(λkv) (cid:44) −λkv + η + Dndvφk
dv 

(18)
dv are obtained from the local variational inference by iterating over Eq. 15 and 16 until
where the φk
convergence.3 With a step size ρt  SVI uses the following update λkv ← λkv + ρtgd(λkv). However 
the sampled natural gradient gd(λkv) in Eq. 18 might have a large variance when the number of
documents is large. This could lead to slow convergence or a poor local mode.
Control variate. Now we show how to construct control variates for the noisy gradient to reduce
its variance. According to Eq. 18  the noisy gradient gd(λkv) is a function of topic assignment
parameters φdv  which in turn depends on wd  the words in document d  through the iterative updates
in Eq. 15 and 16. This is different from the case in Eq. 11. In logistic regression  the gradient is an
analytical function of the training data (Eq. 11)  while in LDA  the natural gradient directly depends
on the optimal local variational parameters (Eq. 18)  which then depends on the training data through
the local variational inference (Eq. 15). However  by carefully exploring the structure of the iterations 
we can create effective control variates.
The key idea is to run Eq. 15 and 16 only up to a ﬁxed number of iterations  together with some
additional approximations to maintain analytical tractability. Starting the iteration with γdk having
the same value  we have φk(0)
does not depend
on document d. Intuitively  φk(0)
is the probability of term v belonging to topic k out of K topics.
Next we use γdk − α to approximate exp(Ψ(γdk)) in Eq. 15.5 Plugging this approximation into
Eq. 15 and 16 leads to the update 

v ∝ exp{Ψ(λkv) − Ψ ((cid:80)

v λkv)}.4 Note that φk(0)

v

v

u=1 fduφk(0)

u

)φk(0)

v

u=1 fduφk(0)

u

φk(0)

v

(cid:17)

≈ ((cid:80)V
(cid:16)(cid:80)V
(cid:80)K

k=1

u=1

u=1 fduφk(0)

)φk(0)

u
¯fuφk(0)

u

φk(0)

v

(cid:17)

v

 

(19)

((cid:80)V
(cid:16)(cid:80)V
(cid:80)K
(cid:16)(cid:80)V

k=1

φk(1)
dv =

with ¯fu (cid:44) (1/D)(cid:80)
v (cid:44)(cid:80)K

(cid:17)

where fdv = ndv/nd is the empirical frequency of term v in document d. In addition  we replace fdu
d fdu  the averaged frequency of term u in the corpus  making the denominator
of Eq. 19  m(1)
  independent of documents. This approximation
does not change the relative importance for the topics from term v. We deﬁne our control variate as

¯fuφk(0)

φk(0)
v

u=1

k=1

u

(cid:16)

hd(λkv) (cid:44) Dndvφk(1)
dv  
u=1 nvfuφk(0)

D/m(1)
v

(cid:17)(cid:110)(cid:16)(cid:80)V

(cid:17)

(1/D)(cid:80)

d ndufdv = (1/D)(cid:80)

whose expectation is Ed[hd(λkv)] =

  where nvfu (cid:44)
d ndundv/nd. This depends on up to the second-order moments
of data  which is usually sparse. We can continue to compute φk(2)
dv   which
dv
turns out using the third-order (or higher) moments. We omit the details here. Similar ideas can be
used in deriving control variates for hierarchical Dirichlet process [12  13] and nonnegative matrix
factorization [14]. We outline these in the supplementary material.

(or higher) given φk(1)

φk(0)
v

u

(cid:111)

3Running to convergence is essential to ensure the natural gradient is valid in Eq. 18 [22].
4In our experiments  we set φk(0)
is less than 0.02. This leaves φ(0) very sparse  since a term
5The scale of the approximation does not matter—C(γdk − α)  where C is a constant  has the same effect as

usually belongs to a small set of topics. For example  in Nature data  only 6% entries are non-zero.
γdk − α. Other approximations to exp(Ψ(γdk)) can also be used as long as it is linear in term of γdk.

v = 0 if φk(0)

v

6

Figure 2: Comparison of our approach with standard SG algorithms using different constant learning rates. The
ﬁgure was created using geom smooth function in ggplot2 using local polynomial regression ﬁtting (loess). A
wider stripe indicates the result ﬂuctuates more. This ﬁgure is best viewed in color. (Decayed learning rates we
tested did not perform as well as constant ones and are not shown.) Legend “Variance Reduction-1” indicates the
algorithm with variance reduction using learning rate ρt = 1.0. (a) Optimum minus the objective on the training
data. The lower the better. (b) Test accuracy on testing data. The higher the better. From these results  we see
that variance reduction with ρt = 1.0 performs the best  while the standard SG algorithm with ρt = 1.0 learns
faster but bounces more (a wider stripe) and performs worse at the end. With ρt = 0.05  variance reduction
performs about the same as the standard algorithm and both converge slowly. These indicate that with the
variance reduction  a larger learning rate is possible to allow faster convergence without sacriﬁcing performance.

Figure 3: Pearson’s correlation coefﬁcient for ρt = 1.0 as we run the our algorithm. It is usually high  indicating
the control variate is highly correlated with the noisy gradient  leading to a large variance reduction. Other
settings are similar.

4 Experiments

In this section  we conducted experiments on the MAP estimation for logistic regression and stochastic
variational inference for LDA.6 In our experiments  we chose to estimate the optimal a∗ as a scalar
shown in Eq. 9 for simplicity.

4.1 Logistic regression

We evaluate our algorithm on stochastic gradient (SG) for logistic regression. For the standard SG
algorithm  we also evaluated the version with averaged output (ASG)  although we did not ﬁnd it
outperforms the standard SG algorithm much. Our regularization added to Eq. 10 for the MAP
estimation is − 1
2D w(cid:62)w. Our dataset contains covtype (D = 581  012  p = 54)  obtained from the
LIBSVM data website.7 We separate 5K examples as the test set. We test two types of learning rates 
constant and decayed. For constant rates  we explore ρt ∈ {0.01  0.05  0.1  0.2  0.5  1}. For decayed
rates  we explore ρt ∈ {t−1/2  t−0.75  t−1}. We use a mini-batch size of 100.
Results. We found that the decayed learning rates we tested did not work well compared with the
constant ones on this data. So we focus on the results using the constant rates. We plot three cases
in Figure 2 for ρt ∈ {0.05  0.2  1} to show the trend by comparing the objective function on the
training data and the test accuracy on the testing data. (The best result for variance reduction is
obtained when ρt = 1.0 and for standard SGD is when ρt = 0.2.) These contain the best results of

6Code will be available on authors’ websites.
7http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets

7

0.010.101100data points (x100K)Optimum minus ObjectivemethodVariance Reduction-1Standard-1Variance Reduction-0.2Standard-0.2Variance Reduction-0.05Standard-0.05(a) Optimum minus Objective on training data (b) Test Accuracy on testing data 0.650.700.751100data points (x100K)Test AccuracymethodVariance Reduction-1Standard-1Variance Reduction-0.2Standard-0.2Variance Reduction-0.05Standard-0.050.9600.9650.9700.9750306090120data points (x100K)Pearson's correlation coefficientFigure 4: Held-out log likelihood on three large corpora. (Higher numbers are better.) Legend “Standard-100”
indicates the stochastic algorithm in [10] with the batch size as 100. Our method consistently performs better
than the standard stochastic variational inference. A large batch size tends to perform better.

each. With variance reduction  a large learning rate is possible to allow faster convergence without
sacriﬁcing performance. Figure 3 shows the mean of Pearson’s correlation coefﬁcient between the
control variate and noisy gradient8  which is quite high—the control variate is highly correlated with
the noisy gradient  leading to a large variance reduction.

4.2 Stochastic variational inference for LDA

We evaluate our algorithm on stochastic variational inference for LDA. [10] has shown that the
adaptive learning rate algorithm for SVI performed better than the manually tuned ones. So we use
their algorithm to estimate adaptive learning rate. For LDA  we set the number of topics K = 100 
hyperparameters α = 0.1 and η = 0.01. We tested mini-batch sizes as 100 and 500.
Data sets. We analyzed three large corpora: Nature  New York Times  and Wikipedia. The Nature
corpus contains 340K documents and a vocabulary of 4 500 terms; the New York Times corpus
contains 1.8M documents and a vocabulary vocabulary of 8 000 terms; the Wikipedia corpus contains
3.6M documents and a vocabulary of 7 700 terms.
Evaluation metric and results. To evaluate our models  we held out 10K documents from each
corpus and calculated its predictive likelihood. We follow the metric used in recent topic modeling
literature [21  22]. For a document wd in Dtest  we split it in into halves  wd = (wd1  wd2)  and
computed the predictive log likelihood of the words in wd2 conditioned on wd1 and Dtrain. The
per-word predictive log likelihood is deﬁned as

likelihoodpw (cid:44)(cid:80)

d∈Dtest

log p(wd2|wd1 Dtrain)/(cid:80)

d∈Dtest

|wd2|.

Here | · | is the number of words. A better predictive distribution given the ﬁrst half gives higher
likelihood to the second half. We used the same strategy as in [22] to approximate its computation.
Figure 4 shows the results. On all three corpora  our algorithm gives better predictive distributions.

5 Discussions and future work

In this paper  we show that variance reduction with control variates can be used to improve stochastic
gradient optimization. We further demonstrate its usage on convex and non-convex problems 
showing improved performance on both. In future work  we would like to explore how to use
second-order methods (such as Newton’s method) or better line search algorithms to further improve
the performance of stochastic optimization. This is because  for example  with variance reduction 
second-order methods are able to capture the local curvature much better.

Acknowledgement. We thank anonymous reviewers for their helpful comments. We also thank
Dani Yogatama for helping with some experiments on LDA. Chong Wang and Eric P. Xing are
supported by NSF DBI-0546594 and NIH 1R01GM093156.

8Since the control variate and noisy gradient are vectors  we use the mean of the Pearson’s coefﬁcients

computed for each dimension between these two vectors.

8

NatureNew York TimesWikipedia-7.75-7.50-7.25-7.00-8.50-8.25-8.00-7.75-7.50-8.1-7.9-7.7-7.5-7.3-7.1051015200510152005101520time (in hours)Heldout log likelihoodmethodStandard-100Standard-500Var-Reduction-100Var-Reduction-500References
[1] Spall  J. Introduction to stochastic search and optimization: Estimation  simulation  and control. John

Wiley and Sons  2003.

[2] Bottou  L. Stochastic learning. In O. Bousquet  U. von Luxburg  eds.  Advanced Lectures on Machine
Learning  Lecture Notes in Artiﬁcial Intelligence  LNAI 3176  pages 146–168. Springer Verlag  Berlin 
2004.

[3] Ross  S. M. Simulation. Elsevier  fourth edn.  2006.
[4] Nemirovski  A.  A. Juditsky  G. Lan  et al. Robust stochastic approximation approach to stochastic

programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[5] Paisley  J.  D. Blei  M. Jordan. Variational Bayesian inference with stochastic search. In International

Conference on Machine Learning. 2012.

[6] Lan  G. An optimal method for stochastic composite optimization. Mathematical Programming  133:365–

397  2012.

[7] Chen  X.  Q. Lin  J. Pena. Optimal regularized dual averaging methods for stochastic optimization. In

Advances in Neural Information Processing Systems (NIPS). 2012.

[8] Boyd  S.  L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[9] Schaul  T.  S. Zhang  Y. LeCun. No More Pesky Learning Rates. ArXiv e-prints  2012.
[10] Ranganath  R.  C. Wang  D. M. Blei  et al. An adaptive learning rate for stochastic variational inference. In

International Conference on Machine Learning. 2013.

[11] Hoffman  M.  D. Blei  F. Bach. Online inference for latent Drichlet allocation. In Neural Information

Processing Systems. 2010.

[12] Teh  Y.  M. Jordan  M. Beal  et al. Hierarchical Dirichlet processes. Journal of the American Statistical

Association  101(476):1566–1581  2007.

[13] Wang  C.  J. Paisley  D. Blei. Online variational inference for the hierarchical Dirichlet process. In

International Conference on Artiﬁcial Intelligence and Statistics. 2011.

[14] Seung  D.  L. Lee. Algorithms for non-negative matrix factorization. In Neural Information Processing

Systems. 2001.

[15] Bishop  C. Pattern Recognition and Machine Learning. Springer New York.  2006.
[16] Jaakkola  T.  M. Jordan. A variational approach to Bayesian logistic regression models and their extensions.

In International Workshop on Artiﬁcial Intelligence and Statistics. 1996.

[17] Blei  D.  A. Ng  M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research  3:993–1022 

2003.

[18] Blei  D.  J. Lafferty. Topic models. In A. Srivastava  M. Sahami  eds.  Text Mining: Theory and Applications.

Taylor and Francis  2009.

[19] Jordan  M.  Z. Ghahramani  T. Jaakkola  et al. Introduction to variational methods for graphical models.

Machine Learning  37:183–233  1999.

[20] Amari  S. Natural gradient works efﬁciently in learning. Neural computation  10(2):251–276  1998.
[21] Asuncion  A.  M. Welling  P. Smyth  et al. On smoothing and inference for topic models. In Uncertainty in

Artiﬁcial Intelligence. 2009.

[22] Hoffman  M.  D. Blei  C. Wang  et al. Stochastic Variational Inference. Journal of Machine Learning

Research  2013.

9

,Chong Wang
Xi Chen
Alexander Smola
Eric Xing
Boris Hanin
David Rolnick