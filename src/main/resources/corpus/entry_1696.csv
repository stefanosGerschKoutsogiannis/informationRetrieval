2019,Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control,Multi-agent reinforcement learning (MARL) has recently received considerable attention due to its applicability to a wide range of real-world applications. However  achieving efficient communication among agents has always been an overarching problem in MARL. In this work  we propose Variance Based Control (VBC)  a simple yet efficient technique to improve communication efficiency in MARL. By limiting the variance of the exchanged messages between agents during the training phase  the noisy component in the messages can be eliminated effectively  while the useful part can be preserved and utilized by the agents for better performance. 
Our evaluation using multiple MARL benchmarks indicates that our method achieves $2-10\times$ lower in communication overhead than state-of-the-art MARL algorithms  while allowing agents to achieve better overall performance.,Efﬁcient Communication in Multi-Agent

Reinforcement Learning via Variance Based Control

Sai Qian Zhang
Harvard University

Qi Zhang
Amazon Inc.

Jieyu Lin

University of Toronto

Abstract

Multi-agent reinforcement learning (MARL) has recently received considerable at-
tention due to its applicability to a wide range of real-world applications. However 
achieving efﬁcient communication among agents has always been an overarching
problem in MARL. In this work  we propose Variance Based Control (VBC)  a
simple yet efﬁcient technique to improve communication efﬁciency in MARL. By
limiting the variance of the exchanged messages between agents during the training
phase  the noisy component in the messages can be eliminated effectively  while the
useful part can be preserved and utilized by the agents for better performance. Our
evaluation using multiple MARL benchmarks indicates that our method achieves
2− 10× lower in communication overhead than state-of-the-art MARL algorithms 
while allowing agents to achieve better overall performance.

1

Introduction

Many real-world applications (e.g.  autonomous driving [16]  game playing [12] and robotics con-
trol [9]) today require reinforcement learning tasks to be carried out in multi-agent settings. In MARL 
multiple agents interact with each other in a shared environment. Each agent only has access to partial
observations of the environment  and needs to make local decisions based on partial observations as
well as both direct and indirect interactions with the other agents. This complex interaction model has
introduced numerous challenges for MARL. In particular  during the training phase  each agent may
dynamically change its strategy  causing dynamics in the surrounding environment and instability in
the training process. Worse still  each agent can easily overﬁt its strategy to the behaviours of other
agents [11]  which may seriously deteriorate the overall performance.
In the research literature  there have been three lines of research that try to mitigate the instability
and inefﬁciency caused by decentralized execution. The most common approach is independent
Q-learning (IQL) [20]  which breaks down a multi-agent learning problem into multiple independent
single-agent learning problems  thus allowing each agent to learn and act independently. Unfortu-
nately  this approach does not account for instability caused by environment dynamics  and therefore
often suffer from the problem of poor convergence. The second approach adopts the centralized train-
ing and decentralized execution [18] paradigm  where a joint action value function is learned during
the training phase to better coordinate the agents’ behaviours. During execution  each agent acts
independently without direct communication. The third approach introduces communication among
agents during execution [17  3]. This approach allows each agent to dynamically adjusts its strategy
based on its local observation along with the information received from the other agents. Nonetheless 
it introduces additional communication overhead in terms of latency and bandwidth during execution 
and its effectiveness is heavily dependent on the usefulness of the received information.
In this work  we leverage the advantages of both the second and third approaches. Speciﬁcally 
we consider a fully cooperative scenario where multiple agents collaborate to achieve a common
objective. The agents are trained in a centralized fashion within the multi-agent Q-learning framework 
and are allowed to communicate with each other during execution. However  unlike previous work 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

we make a few key observations. First  for many applications  it is often superﬂuous for an agent to
wait for feedback from all surrounding agents before making an action decision. For instance  when
the front camera on an autonomous vehicle detects an obstacle within the dangerous distance limit 
it triggers the ‘brake‘ signal 2 without waiting for the feedback from the other parts of the vehicle.
Second  the feedback received from the other agents may not always provide useful information. For
example  the navigation system of the autonomous vehicle should pay more attention to the messages
sent by the perception system (e.g.  camera  radar)  and less attention to the entertainment system
inside the vehicle before taking its action. The full (i.e.  all-to-all) communication pattern among
the agents can lead to a signiﬁcant communication overhead in terms of both bandwidth and latency 
which limits its practicality and effectiveness in real applications with strict latency requirements and
bandwidth constraints (e.g.  real-time trafﬁc signal control  autonomous driving  etc). In addition  as
pointed out by Jiang et al. [7]  an excessive amount of communication may introduce useless and
even harmful information which can even impair the convergence of the learning process.
Motivated by these observations  we design a novel deep MARL architecture that can signiﬁcantly
improve inter-agent communication efﬁciency. Speciﬁcally  we introduce Variance Based Control
(VBC)  a simple yet efﬁcient approach to reduce the among of information transferred between agents.
By inserting an extra loss term on the variance of the exchanged information  the meaningful part of
the messages can be effectively extracted and utilized to beneﬁt the training of each individual agent.
Furthermore  unlike previous work  we do not require an extra decision module to dynamically adjust
the communication pattern. This allows us to reduce the model complexity signiﬁcantly. Instead  each
agent ﬁrst makes a preliminary decision based on its local information  and initiates communication
only when its conﬁdence level on this preliminary decision is low. Similarly  upon receiving the
communication request  the agent replies to the request only when its message is informative. By only
exchanging useful information among the agents  VBC not only improves agent performance  but
also substantially reduces communication overhead during execution. Lastly  it can be theoretically
shown that the resulting training algorithm provides guaranteed stability.
For evaluation  we test VBC on several MARL benchmarks  including StarCraft Multi-Agent Chal-
lenge [15]  Cooperative Navigation (CN) [10] and Predator-prey (PP) [8]. For StarCraft Multi-Agent
Challenge  VBC achieves 20% higher winning rate and 2 − 10× lower communication overhead on
average compared with the other benchmark algorithms. For both CN and PP scenarios  VBC outper-
forms the existing algorithms and incurs much lower overhead than existing communication-enabled
approaches. A video demo is available at [2] for a better illustration of the VBC performance. The
code is available at https://github.com/saizhang0218/VBC.

2 Related Work

The simplest training method for MARL is to make each agent learn independently using Independent
Q-Learning (IQL) [20]. Although IQL is successful in solving simple tasks such as Pong [19]  it
ignores the environment dynamics arose from the interactions among the agents. As a result  it suffers
from the problem of poor convergence  making it difﬁcult to handle advanced tasks.
Given the recent success on deep Q-learning [12]  some recent studies explore the scheme of
centralized training and decentralized execution. Sunehag et al. [18] propose Value Decomposition
Network (VDN)  a method that acquires the joint action value function by summing up all the action
value functions of each agent. All the agents are trained as a whole by updating the joint action value
functions iteratively. QMIX [14] sheds some light on VDN  and utilizes a neural network to represent
the joint action value function as a function of the individual action value functions and the global
state information. The authors of [10] extend the actor-critic methods to the multi-agent scenario. By
performing centralized training and decentralized execution over the agents  the agents can better
adapt to the changes in the environment and collaborate with each other. Foerster et al. [5] propose
counterfactual multi-agent policy gradient (COMA)  which employs a centralized critic function
to estimate the action value function of the joint  and decentralized actor functions to make each
agent execute independently. All the aforementioned methods assume no communication between
the agents during the execution. As a result  many subsequent approaches  including ours  can be
applied to improve the performance of these methods.
Learning the communication pattern for MARL is ﬁrst proposed by Sukhbaatar et. al. [17]. The
authors introduce CommNet  a framework that adopts continuous communication for fully cooperative

2

tasks. During the execution  each agent takes their internal states as well as the means of the internal
states of the rest agents as the input to make decision on its action. The BiCNet [13] uses a
bidirectional coordinated network to connect the agents. However  both schemes require all-to-all
communication among the agents  which can cause a signiﬁcant communication overhead and latency.
Several other proposals [3  7  8] use a selection module to dynamically adjust the communication
pattern among the agents. In Differentiable Inter-Agent Learning (DIAL) [3]  the messages produced
by an agent are selectively sent to the neighboring agents through the discretize/regularise unit
(DRU). By jointly training DRU with the agent network  the communication overhead can be
efﬁciently reduced. Jiang et. al. [7] propose an attentional communication model that learns when
the communication is required and how to aggregate the shared information. However  an agent
can only talk to the agents within its observable range at each timestep. This limits the speed of
information propagation  and restricts the possible communication patterns when the local observable
ﬁeld is small. Kim et. al. [8] propose a communication scheduling scheme for wireless environment 
but only a fraction of the agents can broadcast their messages at each time. In comparison  our
approach does not impose hard constraints on the communication pattern  which is beneﬁcial to the
learning process. Also our method does not adopt additional decision module for the communication
scheduling  which greatly reduces the model complexity.

3 Background

(cid:11). The action value function Qθ(s  a) can be trained recursively by

maximize the total expected discounted reward R =(cid:80)T
transition tuples(cid:10)st  at  st+1  rt

Deep Q-networks: We consider a standard reinforcement learning problem based on Markov
Decision Process (MDP). At each timestamp t  the agent observes the state st  and chooses an action
at. It then receives a reward rt for its action at and proceeds to the next state st+1. The goal is to
t=1 γtrt  where γ ∈ [0  1] is the discount
factor. A Deep Q-Network (DQN) use a deep neural network to represent the action value function
Qθ(s  a) = E[Rt|st = s  at = a]  where θ represents the parameters of the neural network  and Rt is
the total rewards received at and after t. During the training phase  a replay buffer is used to store the
minimizing the loss L = Est at rt st+1[yt − Qθ(st  at)]2  where yt = rt + γmaxat+1Qθ(cid:48)(st  at+1)
and θ(cid:48) represents the parameters of the target network. An action is usually selected with -greedy
policy. Namely  selecting the action with maximum action value with probability 1 −   and choosing
a random action with probability .
Multi-agent deep reinforcement learning: We consider an environment with N agents work
cooperatively to fulﬁll a given task. At timestep t  each agent i (1 ≤ i ≤ N) receives a local
observation ot
i. They then receive a joint reward rt and proceed to the next
state. We use a vector at = {at
i} to represent the joint actions taken by all the agents. The agents aim
to maximize the joint reward by choosing the best joint actions at at each timestep t.
Deep recurrent Q-networks: Traditional DQNs generate action solely based on a limited number
of local observations without considering the prior knowledge. Hausknecht et al. [6] introduce Deep
Recurrent Q-Networks (DRQN)  which models the action value function with a recurrent neural
network (RNN). The DRQN leverages its recurrent structure to integrate the previous observations
and knowledge for better decision-making. At each timestep t  the DRQN Qθ(ot
i) takes the
local observation ot
Learning the joint Q-function: Recent research effort has been made on the learning of joint
action value function for multi-agent Q-learning. Two representative works are VDN [18] and
QMIX [14]. In VDN  the joint action value function Qtot(ot  ht−1  at) is assumed to be the sum
i)  where
ot = {ot
i} are the collection of the observations  hidden states and actions
of all the agents at timestep t respectively. QMIX employs a neural network to represent the joint
value function Qtot(ot  ht−1  at) as a nonlinear function of Qi(ot

of all the individual action value functions  i.e. Qtot(ot  ht−1  at) = (cid:80)

from the previous steps as input to yield action values.

i}  ht = {ht

i} and at = {at

i and hidden state ht−1

i and executes an action at

i Qi(ot

i  ht−1

i

i  ht−1

i

  at

i

  at

i  ht−1

i

  at

i) and global state st.

4 Variance Based Control

In this section  we present the detailed design of VBC in the context of multi-agent Q-learning. The
main idea of VBC is to improve agent performance and communication efﬁciency by limiting the

3

Figure 1: (a) Agent network structure of agent 1  which consists of local agent generator  combiner
and several message encoder. (b) The mixing network takes the output Qi(ot  ht−1  at
i) from each
network of agent i  and perform centralized training. ct−i means all the ct

j(cid:54)=i.

variance of the transferred messages. During execution  each agent communicates with other agents
only when its local decision is ambiguous. The degree of ambiguity is measured by the difference
between the top two largest action values. Upon receiving the communication request from other
agents  the agent replies only if its feedback is informative  namely the variance of the feedback is
high.

4.1 Agent Network Design

  at

i

i

i and the hidden state ht−1

i is then sent to the FC layer  which outputs the local action values Qi(ot
i ∈ A  where A is the set of possible actions. The message encoder  f ij

The agent network consists of the following three networks: local action generator  message encoder
and combiner. Figure 1(a) describes the network architecture for agent 1. The local action generator
consists of a Gated Recurrent Unit (GRU) and a fully connected layer (FC). For agent i  the GRU takes
as the inputs  and generates the intermediate results
the local observation ot
i  ht−1
i. ct
i) for each
ct
action at
enc(.)  is a multi-layer
perceptron (MLP) which contains two FC layers and a leaky ReLU layer. The agent network involves
j from another agent j (j (cid:54)= i)  and outputs
multiple independent message encoders  each accepts ct
j). The outputs from local action generator and message encoder are then sent to the combiner 
enc(ct
f ij
which produces the global action value function Qi(ot  ht−1  at
i) of agent i by taking into account the
global observation ot and global history ht−1. To simplify the design and reduce model complexity 
we do not introduce extra parameters for the combiner. Instead  we make the dimension of the f ij
enc(ct
j)
  .)  and hence the combiner can simply perform
the same as the local action values Qi(ot
elementwise summation over its inputs  namely Qi(ot  ht−1  .) = Qi(ot
j).
enc(ct
The combiner chooses the action with the -greedy policy π(.). Let θi
enc denote the set
of parameters of the local action generators and the message encoders  respectively. To prevent the
lazy agent problem [18] and decrease the model complexity  we make θi
local the same for all i  and
enc the same for all i and j(j (cid:54)= i). Accordingly  we can drop the corner scripts and use
make θij
θ = {θlocal  θenc} and fenc(.) to denote the agent network parameters and the message encoder.

i  ht−1
local and θij

  .) +(cid:80)

i  ht−1

i

j(cid:54)=i f ij

i

4.2 Loss Function Deﬁnition

During the training phase  the message encoder and local action generator jointly learn to generate
the best estimation on the action values. More speciﬁcally  we employ a mixing network (shown
in Figure 1(b)) to aggregate the global action value functions Qi(ot  ht−1  at
i) from each agents i 
and yields the joint action value function  Qtot(ot  ht−1  at). To limit the variance of the messages
from the other agents  we introduce an extra loss term on the variance of the outputs of the message
encoders fenc(ct

j). The loss function during the training phase is deﬁned as:

L(θlocal  θenc) =

tot − Qtot(ob

t   hb

t−1  ab

t ; θ))2 + λ

V ar(fenc(ct b

(1)

N(cid:88)

i ))(cid:3)

B(cid:88)

T(cid:88)

(cid:2)(yb

b=1

t=1

i=1

4

CombinerCentralized training......Agent 1SumLeakyReLU...εMsgEncMsgEncMessageEncoderFC2FC1Local Action GeneratorGRUFCAgent 1Agent NMixing Network......πAgent Network(a)(b)Figure 2: In (a)  since the difference between the largest and second largest action values is greater
than δ1  no communication is required. In (b)  agent 1 broadcasts a request to agent 2 and 3  only
agent 2 replies the request since the variance of fenc(ct

2) is greater than δ2.

Algorithm 1: Communication protocol at agent i

1 Input: Conﬁdence threshold of local actions δ1  threshold on variance of message encoder output δ2. Total
2 for t ∈ T do

number of agents N.

i  ht−1

i

  .). Denote m1  m2 the top two largest values of Qi(ot

i  ht−1

i

  .).

// Decision on the action of itself:
Compute local action values Qi(ot
if m1 − m2 ≥ δ1 then

Let Qi(ot  ht−1  .) = Qi(ot

i  ht−1

i

  .).

3

4
5
6

7
8

9

10
11
12
13

else

Broadcast a request to the other agents  and receive the fenc(ct
Let Qi(ot  ht−1  .) = Qi(ot

fenc(ct

i  ht−1

j).

i

  .) +(cid:80)Nreply

j=1

j) from Nreply(Nreply ≤ N ) agents.

// Generating reply messages for the other agents:
Calculate variance of fenc(ct
if V ar(fenc(ct

i)) ≥ δ2 and Receive a request from agent j then

i)  if V ar(fenc(ct

i)) ≥ δ2  store fenc(ct

Reply the request from agent j with fenc(ct

i).

i) in the buffer.

tot = rb

t + γmaxat+1Qtot(ob

− is the parameter of the target network
where yb
which is copied from the θ periodically  V ar(.) is the variance function and λ is the weight of the
loss on it. b is the batch index. The replay buffer is refreshed periodically by running each agent
network and selecting the action which maximizes Qi(ot  ht−1  .).

t  at+1; θ

t+1  hb

)  θ

−

4.3 Communication Protocol Design

i

1

i  ht−1

1  ht−1

  .) and fenc(ct

During the execution phase  at every timestep t  the agent i ﬁrst computes the local action value
function Qi(ot
i). It then measures the conﬁdence level on the local decision by
computing the difference between the largest and the second largest element within the action values.
An example is given in Figure 2(a). Assume agent 1 has three actions to select  and the output of the
local action generator of agent 1 is Q1(ot
  .) = (0.1  1.6  3.8)  and the difference between the
largest and the second largest action values is 3.8 − 1.6 = 2.2  which is greater than the threshold
δ1 = 1.0. Given the fact that the variance of message encoder outputs fenc(ct
j) from the agent 2
and 3 is relatively small due to the additional penalty term on variance in equation 1  it is highly
possible that the global action value function Q1(ot  ht−1  .) also has the largest value in its third
element. Therefore agent 1 does not have to talk to other agents to acquire fenc(ct
j). Otherwise  agent
1 broadcasts a request to ask for help if its conﬁdence level on the local decision is low. Because the
request does not contain any actual data  it consumes very low bandwidth. Upon receiving the request 
only the agents whose message has a large variance reply (Figure 2(b))  because their messages may
change the current action decision of agent 1. This protocol not only reduces the communication
overhead considerably  but also eliminates noisy  less informative messages that may impair the
overall performance. The detailed protocol and operations performed at an agent i is summarized in
Algorithm 1.

5

(0.1 1.6. 3.8)action values 3.8-1.6 = 2.2 > ઠ1= 1.0  no communication between the agents(a)Agent 1Agent 2Agent 3Agent 1Agent 2Agent 3(0.1 2.2. 2.3)action values (0.14 2.6. 2.2)2.3-2.2 = 0.1 < ઠ1 = 1.0  agent 1 broadcasts a request to all the agentsrequestrequestVar = 0.044 >ઠ2 = 0.02Var = 0.007 <ઠ2 = 0.02(b)5 Convergence Analysis

In this section  we analyze convergence of the learning process with the loss function deﬁned in
equation (1) under the tabular setting. For the sake of simplicity  we ignore the dependency of the
action value function on the previous knowledge ht. To minimize equation (1)  given the initial state
Q0  at iteration k  the q values in the table is updated according to the following rule:

Qk+1

tot (ot  at) = Qk

tot(ot  at) + ηk

rt + γmaxaQk

tot(ot+1  a)− Qk

tot(ot  at)− λ

(cid:20)

(cid:21)

N(cid:88)

i=1

∂V ar(fenc(ct
i))
tot(ot  at)
∂Qk

(2)

Theorem 1. Assume 0 ≤ ηk ≤ 1  (cid:80)

where ηk  Qk
Let Q∗
convergence of the learning process. A detailed proof is given in the supplementary materials.

tot(.) are the learning rate and the joint action value function at iteration k respectively.
tot(.) denote the optimal joint action value function. We have the following result on the
k < ∞. Also assume the number of
tot(ot  at) −
|| ≤ G ∀i  k  t  ot  at.

possible actions and states are ﬁnite. By performing equation 2 iteratively  we have ||Qk
tot(ot  at)|| ≤ λN G ∀ot  at  as k → ∞  where G satisﬁes || ∂V ar(fenc(ct
Q∗
tot(ot at)

k ηk = ∞  (cid:80)

k η2

∂Qk

i))

6 Experiment

We evaluated the performance of VBC on the StarCraft Multi-Agent Challenge (SMAC) [15].
StarCraft II [1] is a real-time strategy (RTS) game that has recently been utilized as a benchmark by
the reinforcement learning community [14  5  13  4]. In this work  we focus on the decentralized
micromanagement problem in StarCraft II  which involves two armies  one controlled by the user
(i.e. a group of agents)  and the other controlled by the build-in StarCraft II AI. The goal of the user
is to control its allied units to destroy all enemy units  while minimizing received damage on each
unit. We consider six different battle settings. Three of them are symmetrical battles  where both the
user and the enemy groups consist of 2 Stalkers and 3 Zealots (2s3z)  3 Stalkers and 5 Zealots (2s5z) 
and 1 Medivac  2 Marauders and 7 Marines (MMM) respectively. The other three are unsymmetrical
battles  where the user and enemy groups have different army unit compositions  including: 3 Stalkers
for user versus 4 Zealots for enemy (3s_vs_4z)  6 Hydralisks for user versus 8 Zealots for enemy
(6s_vs_8z)  and 6 Zealot for user versus 24 Zerglings for enemy (6z_vs_24zerg). The unsymmetrical
battles are considered to be harder than the symmetrical battles because of the difference in army size.
At each timestep  each agent controls a single unit to perform an action  including move[direction] 
attack[enemy_id]  stop and no-op. Each agent has a limited sight range and shooting range  where
shooting range is less than the sight range. The attack operation is available only when the enemies
are within the shooting range. The joint reward received by the allied units equals to the total damage
inﬂicted on enemy units. Additionally  the agents are rewarded 100 extra points after killing each
enemy unit  and 200 extra points for killing the entire army. The user wins the battle only when
the allied units kill all the enemies within the time limit. Otherwise the built-in AI wins. The input
observation of each agent is a vector that consists of the following information of each allied unit
and enemy unit in its sight range: relative x  y coordinates  relative distance and agent type. For
the detailed game settings  hyperparameters  and additional experiment evaluation over other test
environments  please refer to supplementary materials.

6.1 Results

We compare VBC and several benchmark algorithms  including VDN [18]  QMIX [14] and Sched-
Net [8] for controlling allied units. We consider two types of VBCs by adopting the mixing networks
of VDN and QMIX  denoted as VBC+VDN and VBC+QMIX. The mixing network of VDN simply
computes the elementwise summation across all the inputs  and the mixing network of QMIX deploys
a neural network whose weight is derived from the global state st. The detailed architecture of this
mixing network can be found in [14]. Additionally  we create an algorithm FC (full communication)
by removing the penalty in Equation (1)  and dropping the limit on variance during the execution
phase (i.e.  δ1 = ∞ and δ2 = −∞). The agents are trained with the same network architecture shown
in Figure (1)  and the mixing network of VDN is used. For SchedNet  at every timestep only K out
of N agents can broadcast their messages by using Top(k) scheduling policy [8]. We usually set K
close to 0.5N  that is  each time roughly half of the allied units can broadcast their messages. The

6

(a) MMM

(b) 2s3z

(c) 3s5z

(d) 3s_vs_4z

(e) 6h_vs_8z

(f) 6z_vs_24zerg

Figure 3: Winning rates for the six tasks  the shaded regions represent the 95% conﬁdence intervals.

VBC are trained for different number of episodes based on the difﬁculties of the battles  which we
describe in detail next.
To measure the convergence speed of each algorithm  we stop the training process and save the
current model every 200 training episodes. We then run 20 test episodes and measure the winning
rates for these 20 episodes. For VBC+VDN and VBC+QMIX  the winning rates are measured by
running the communication protocol described in Algorithm 1. For easy tasks  namely MMM and
2s_vs_3z  we train the algorithms with 2 million and 4 million episodes respectively. For all the other
tasks  we train the algorithms with 10 million episodes. Each algorithm is trained 15 times. Figure 3
shows the average winning rate and 95% conﬁdence interval of each algorithm for all the six tasks.
For hyperparameters used by VBC (i.e.  λ used in equation (1)  δ1andδ2 in Algorithm 1)  we ﬁrst
search for a coarse parameter range based on random trial  experience and message statistics. We
then perform a random search within a smaller hyperparameter space. Best selections are shown in
the legend of each ﬁgure.
We observe that the algorithms that involve communication (i.e.  SchedNet  FC  VBC) outperform the
algorithms without communication (i.e.  VDN  QMIX) in all the six tasks. This is a clear indication
that communication beneﬁts the performance. Moreover  both VBC+VDN and VBC+QMIX achieve
better winning rates than SchedNet  because SchedNet only allows a ﬁxed number of agents to talk
at every timestep  which prohibits some key information to exchange in a timely fashion. Finally 
VBC achieves similar performance as FC and even outplays FC for some tasks (e.g.  2s3z 6h_vs_8z 
6z_vs_24zerg). This is because a fair amount of communication between the agents are noisy and
redundant. By eliminating these undesired messages  VBC is able to achieve both communication
efﬁciency and performance gain.

6.2 Communication Overhead

other words  the communication overhead β =(cid:80)T

We now evaluate the communication overhead of VBC. To quantify the amount of communication
involved  we run Algorithm 1 and count the total number of pairs of agents gt that conduct communi-
cation for each timestep t  then divided by the total number of pairs of agents in the user group  R. In
t=1 gt/RT . An an example  for the task 3s_vs_4z 
since the user controls 3 Stalkers  and the total number of agent pairs is R = 3 × 2 = 6. Within these
6 pairs of agents  suppose that 2 pairs involve communication  then gt = 2. Table 1 shows the β of
VBC+VDN  VBC+QMIX and SchedNet across all the test episodes at the end of the training phase
of each battle. For SchedNet  β simply equals the ratio between the number of allied agents that are
allowed to talk and the total number of allied agents. As shown in Table 1  in contrast to ScheNet 

7

0.000.250.500.751.001.251.501.752.00Training episodes1e70.00.20.40.60.81.0Winning rateWinning rate for MMMVDNQMIXVBC+VDN (=5.0 1=0.04 2=0.02)VBC+QMIX (=5.0 1=0.04 2=0.02)FCSchedNet (5 agents)0.00.51.01.52.02.53.03.54.0Training episodes1e70.00.20.40.60.81.0Winning rateWinning rate for 2s3zVDNQMIXVBC+VDN (=4.0 1=0.03 2=0.015)VBC+QMIX (=4.0 1=0.03 2=0.015)FCSchedNet (3 agents)0.00.20.40.60.81.0Training episodes1e80.00.20.40.60.8Winning rateWinning rate for 3s5zVDNQMIXVBC+VDN (=5.0 1=0.1 2=0.15)VBC+QMIX (=5.0 1=0.1 2=0.15)FCSchedNet (5 agents)0.00.20.40.60.81.0Training episodes1e80.00.20.40.60.8Winning rateWinning rate for 3s_vs_4zVDNQMIXVBC+VDN (=2.0 1=0.06 2=0.002)VBC+QMIX (=2.0 1=0.07 2=0.004)FCSchedNet (1 agents)0.00.20.40.60.81.0Training episodes1e80.00.10.20.30.40.5Winning rateWinning rate for 6h_vs_8zVDNQMIXVBC+QMIX (=1.7 1=0.11 2=0.03)VBC+VDN (=1.7 1=0.23 2=0.1)FCSchedNet (4 agents)0.00.20.40.60.81.0Training episodes1e80.00.10.20.30.40.5Winning rateWinning rate for 6z_vs_24zergVDNQMIXVBC+VDN (=5.0 1=0.04 2=0.023)VBC+QMIX (=5.0 1=0.04 2=0.019)FCSchedNet (3 agents)(a) Communication overhead

(b) VBC (6h_vs_8z)

(c) QMIX and VDN (6h_vs_8z)

(d) VBC (3s_vs_4z)

(e) VBC (6z_vs_24zergs  stage 1) (f) VBC (6z_vs_24zergs  stage 2)

Figure 4: Strategies and communication pattern for different scenarios

Table 1: Communication overhead

VBC+VDN VBC+QMIX SchedNet

β

MMM
2s3z
3s5z

3s_vs_4z
6h_vs_8z

6z_vs_24zerg

5.25%
4.33%
27.70%
5.07%
35.93%
12.13%

5.36%
4.68%
28.13%
5.19%
36.16%
13.35%

50%
60%
62.5%
33.3%
66.7%
50%

VBC+VDN and VBC+QMIX produce 10× lower communication overhead for MMM and 2s3z  and
2 − 6× less trafﬁc for the rest of tasks.

6.3 Learned Strategy

In this section  we examine the behaviors of the agents in order to better understand the strategies
adopted by the different algorithms. We have made a video demo available at [2] for better illustration.
For unsymmetrical battles  the number of allied units is less than the enemy units  and therefore the
agents are prone to be attacked by the enemies. This is exactly what happened for the QMIX and
VDN agents on 6h_vs_8z  as shown in (Figure 4(c)). Figure 4(b) shows the strategy of VBC  all the
Hydralisks are placed in a row at the bottom margin of the map. Due to the limited size of the map 
the Zealots can not go beyond the margin to surround the Hydralisks. The Hydralisks then focus their
ﬁre to kill each Zealot. Figure 4(a) shows the change on β for a sample test episode. We observe
that most of the communication appears in the beginning of the episode. This is due to the fact that
Hydralisks need to talk in order to arrange in a row formation. After the arrangement is formed  no
communication is needed until the arrangement is broken due to the deaths of some Hydralisks  as
indicated by the short spikes near the end of the episode. Finally  SchedNet and FC utilize a similar
strategy as VBC. Nonetheless  due to the restriction on communication pattern  the row formed by
the allied agents are usually not well formed  and can be easily broken by the enemies.
For 3s_vs_4z scenario  the Stalkers have a larger attack range than Zealots. All the algorithms adopt
a kiting strategy where the Stalkers form a group and attack the Zealots while kiting them. For
VBC and FC  at each timestep only the agents that are far from the enemies attack  and the rest of
the agents (usually the healthier ones) are used as a shield to protect the ﬁring agents (Figure 4(d)).
Communication only occurs when the group are broken and need to realign. In contrast  VDN and
QMIX do not have this attacking pattern  and all the Stalkers always ﬁre simultaneously  therefore
the Stalkers closest to the Zealots are get killed ﬁrst. SchedNet and FC also adopt a similar policy as
VBC  but the attacking pattern of the Stalkers is less regular  i.e.  the Stalkers close to the Zealots
also ﬁre occasionally.

8

0.00.20.40.60.81.0Test episodes (shown in percentage)0.00.20.40.60.81.0Communication overhead Agents talk with each other to discuss the arrangementCommunication is needed for keeping the arrangement between the agentsOnce the arrangementis settled  no communication is need(b) Results on Cooperative Navigation (#agents = 6)
#collisions
0.169
0.176
0.161
1.872

Methods Avg. dist
2.687
2.798
2.990
3.886

VBC+VDN
SchedNet

FC
VDN

Figure 5: (a) Results on PP with 3 predators and 3 prey. For SchedNet  we allow 1 predator/prey to
broadcast messages. (b) Results of CN. For SchedNet  we allow 3 agents to broadcast messages.

6z_vs_24zerg is the toughest scenario in our experiment. For QMIX and VDN  the 6 Zealots are
surrounded and killed by 24 Zerglings shortly after the episode starts. In contrast  VBC ﬁrst separates
the agents into two groups with two Zealots and four Zealots respectively (Figure 4(e)). The two
Zealots attract most of the Zerglings to a place far away from the rest four Zealots  and are killed
shortly. Due to the limit sight range of the Zerglings  they can not ﬁnd the rest four Zealots. On the
other side  the four Zealots kill the small part of Zerglings easily and search for the rest Zerglings.
The four Zealots take advantage of the short sight of the Zerglings. Each time the four Zealots adjust
their positions in a way such that they can only be seen by a small number of the Zerglings  the
baited Zerglings are then killed easily (Figure 4(f)). For VBC  the communication only occurs in
the beginning of the episode when the Zealots are separated into two groups  and near the end of the
episode when four Zealots adjust their positions. Both FC and SchedNet learn the strategy of splitting
the Zealots into two groups  but they fail to ﬁne-tune their positions to kill the remaining Zerglings.
For symmetrical battles  the tasks are less challenging  and we see less disparities on performances
of the algorithms. For 2s3z and 3s5z  the VDN agents attack the enemies blindly without any
cooperation. The QMIX agents learn to focus ﬁring and protect the Stalkers. The agents of VBC  FC
and SchedNet adopt a more aggressive policy  where the allied Zealots try to surround and kill the
enemy Zealots ﬁrst  and then attack the enemy Stalkers by collaborating with the allied Stalkers. This
is extremely effective because Zealots counter Stalkers  so it is important to kill the enemy Zealots
before they damage allied Stalkers. For VBC  the communication occurs mostly when the allied
Zealots try to surround the enemy Zealots. For MMM  almost all the methods learn the optimal
policy  namely killing the Medivac ﬁrst  then attack the rest of the enemy units cooperatively.

6.4 Evaluation on Cooperative Navigation and Predator-prey

To demonstrate the applicability of VBC in more general settings  we have tested VBC for two more
scenarios: (1) Cooperative Navigation (CN) which is a cooperative scenario  and (2) Predator-prey
(PP) which is a competitive scenario. The game settings are the same as what are used in [10]
and [8]  respectively. We train each method until convergence and test the result models for 2000
episodes. For PP  we make the agents of VBC compete against the agents of other methods  and
report the normalized score of Predator (Figure 5(a)). For CN we report the average distance between
agents and their destinations  and average number of collisions (Figure 5(b)). We notice that methods
which allow communication (i.e.  SchedNet  FC  VBC) outperform the others for both tasks  and
VBC achieves the best performance. Moreover  in both scenarios  VBC incurs 10× and 3× lower
communication overhead than FC and SchedNet respectively. In CN  most of the communication of
VBC occurs when the agents are close to each other to prevent collisions. In PP  the communication of
VBC occurs mainly to rearrange agent positions for better coordination. These observations conﬁrm
that VBC’s can be applied to a variety of MARL scenarios with great effectiveness.

7 Conclusion

In this work  we propose VBC  a simple and effective approach to achieve efﬁcient communication
among agents in MARL. By constraining the variance of the exchanged messages during the training
phase  VBC improves communication efﬁciency while enabling better cooperation among the agents.
The test results of multiple MARL benchmarks indicate that VBC outperforms the other state-of-the-
art methods signiﬁcantly in terms of both performance and communication overhead.

9

VBC as predatorVBC as prey0.00.20.40.60.8Normalized score of Predator(a) Performance on PP (3 predators  3 preys)VBC and SchedNetVBC and FCVBC and VDNReferences
[1] Starcraft ofﬁcial game site. https://starcraft2.com/.
[2] Vbc video demo. https://bit.ly/2VFkvCZ.
[3] J. Foerster  I. A. Assael  N. de Freitas  and S. Whiteson. Learning to communicate with deep
multi-agent reinforcement learning. In Advances in Neural Information Processing Systems 
pages 2137–2145  2016.

[4] J. N. Foerster  C. A. S. de Witt  G. Farquhar  P. H. Torr  W. Boehmer  and S. Whiteson. Multi-

agent common knowledge reinforcement learning. arXiv preprint arXiv:1810.11702  2018.

[5] J. N. Foerster  G. Farquhar  T. Afouras  N. Nardelli  and S. Whiteson. Counterfactual multi-agent

policy gradients. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[6] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. In 2015

AAAI Fall Symposium Series  2015.

[7] J. Jiang and Z. Lu. Learning attentional communication for multi-agent cooperation.

Advances in Neural Information Processing Systems  pages 7254–7264  2018.

In

[8] D. Kim  S. Moon  D. Hostallero  W. J. Kang  T. Lee  K. Son  and Y. Yi. Learning to schedule
communication in multi-agent reinforcement learning. arXiv preprint arXiv:1902.01554  2019.
[9] J. Kober  J. A. Bagnell  and J. Peters. "reinforcement learning in robotics: A survey.". The

International Journal of Robotics Research  2013.

[10] R. Lowe  Y. Wu  A. Tamar  J. Harb  O. P. Abbeel  and I. Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. In Advances in Neural Information Processing
Systems  pages 6379–6390  2017.

[11] L. Marc  V. Zambaldi  A. Gruslys  A. Lazaridou  K. Tuyls  J. Pérolat  D. Silver  and T. Graepel.
"a uniﬁed game-theoretic approach to multiagent reinforcement learning.". In Advances in
Neural Information Processing Systems  2017.

[12] V. Mnih  K. Kavukcuoglu  D. Silver  A. Graves  I. Antonoglou  D. Wierstra  and M. Riedmiller.

"playing atari with deep reinforcement learning.". arXiv preprint arXiv:1312.5602  2013.

[13] P. Peng  Y. Wen  Y. Yang  Q. Yuan  Z. Tang  H. Long  and J. Wang. Multiagent bidirectionally-
coordinated nets: Emergence of human-level coordination in learning to play starcraft combat
games. arXiv preprint arXiv:1703.10069  2017.

[14] T. Rashid  M. Samvelyan  C. S. de Witt  G. Farquhar  J. Foerster  and S. Whiteson. Qmix:
Monotonic value function factorisation for deep multi-agent reinforcement learning. arXiv
preprint arXiv:1803.11485  2018.

[15] M. Samvelyan  T. Rashid  C. S. de Witt  G. Farquhar  N. Nardelli  T. G. J. Rudner  C.-M. Hung 
P. H. S. Torr  J. Foerster  and S. Whiteson. The StarCraft Multi-Agent Challenge. CoRR 
abs/1902.04043  2019.

[16] S.-S. Shai  S. Shammah  and A. Shashua. "safe  multi-agent  reinforcement learning for

autonomous driving.". arXiv preprint arXiv:1610.03295  2016.

[17] S. Sukhbaatar  R. Fergus  et al. Learning multiagent communication with backpropagation. In

Advances in Neural Information Processing Systems  pages 2244–2252  2016.

[18] P. Sunehag  G. Lever  A. Gruslys  W. M. Czarnecki  V. Zambaldi  M. Jaderberg  M. Lanctot 
N. Sonnerat  J. Z. Leibo  K. Tuyls  et al. Value-decomposition networks for cooperative
multi-agent learning. arXiv preprint arXiv:1706.05296  2017.

[19] A. Tampuu  T. Matiisen  D. Kodelja  I. Kuzovkin  K. Korjus  J. Aru  J. Aru  and R. Vi-
cente. Multiagent cooperation and competition with deep reinforcement learning. PloS one 
12(4):e0172395  2017.

[20] M. Tan. "multi-agent reinforcement learning: Independent vs. cooperative agents.". In Proceed-

ings of the tenth international conference on machine learning. IEEE  1993.

10

,Tim van Erven
Wouter Koolen
Sai Qian Zhang
Qi Zhang
Jieyu Lin