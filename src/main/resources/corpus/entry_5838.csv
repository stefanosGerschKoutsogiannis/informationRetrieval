2019,Variance Reduction for Matrix Games,We present a randomized primal-dual algorithm that solves the problem min_x max_y y^T A x to additive error epsilon in time nnz(A) + sqrt{nnz(A) n} / epsilon  for matrix A with larger dimension n and nnz(A) nonzero entries. This improves the best known exact gradient methods by a factor of sqrt{nnz(A) / n} and is faster than fully stochastic gradient methods in the accurate and/or sparse regime epsilon < sqrt{n / nnz(A)$. Our results hold for x y in the simplex (matrix games  linear programming) and for x in an \ell_2 ball and y in the simplex (perceptron / SVM  minimum enclosing ball). Our algorithm combines the Nemirovski's "conceptual prox-method" and a novel reduced-variance gradient estimator based on "sampling from the difference" between the current iterate and a reference point.,Variance Reduction for Matrix Games

Yair Carmon  Yujia Jin  Aaron Sidford and Kevin Tian

Stanford University

{yairc yujiajin sidford kjtian}@stanford.edu

Abstract

We present a randomized primal-dual algorithm that solves the problem

trix A with larger dimension n and nnz(A) nonzero entries. This improves the best

minx maxy y>Ax to additive error ✏ in time nnz(A) +pnnz(A)n/✏  for ma-
known exact gradient methods by a factor ofpnnz(A)/n and is faster than fully
stochastic gradient methods in the accurate and/or sparse regime ✏ pn/nnz(A).

Our results hold for x  y in the simplex (matrix games  linear programming) and
for x in an `2 ball and y in the simplex (perceptron / SVM  minimum enclosing
ball). Our algorithm combines the Nemirovski’s “conceptual prox-method” and a
novel reduced-variance gradient estimator based on “sampling from the difference”
between the current iterate and a reference point.

1

Introduction

Minimax problems—or games—of the form minx maxy f (x  y) are ubiquitous in economics  statis-
tics  optimization and machine learning. In recent years  minimax formulations for neural network
training rose to prominence [15  23]  leading to intense interest in algorithms for solving large scale
minimax games [10  14  20  9  18  24]. However  the algorithmic toolbox for minimax optimization
is not as complete as the one for minimization. Variance reduction  a technique for improving
stochastic gradient estimators by introducing control variates  stands as a case in point. A multitude
of variance reduction schemes exist for ﬁnite-sum minimization [cf. 19  34  1  4  12]  and their impact
on complexity is well-understood [43]. In contrast  only a few works apply variance reduction to
ﬁnite-sum minimax problems [3  39  5  26]  and the potential gains from variance reduction are not
well-understood.
We take a step towards closing this gap by designing variance-reduced minimax game solvers that
offer strict runtime improvements over non-stochastic gradient methods  similar to that of optimal
variance reduction methods for ﬁnite-sum minimization. To achieve this  we focus on the fundamental
class of bilinear minimax games 

min
x2X

max
y2Y

y>Ax  where A 2 Rm⇥n.

In particular  we study the complexity of ﬁnding an ✏-approximate saddle point (Nash equilibrium) 
namely x  y with

max
y02Y

(y0)>Ax  min
x02X

y>Ax0  ✏.

In the setting where X and Y are both probability simplices  the problem corresponds to ﬁnding an
approximate (mixed) equilbrium in a matrix game  a central object in game theory and economics.
Matrix games are also fundamental to algorithm design due in part to their equivalence to linear
programming [8]. Alternatively  when X is an `2 ball and Y is a simplex  solving the corresponding
problem ﬁnds a maximum-margin linear classiﬁer (hard-margin SVM)  a fundamental task in machine
learning and statistics [25]. We refer to the former as an `1-`1 game and the latter as an `2-`1 game;
our primary focus is to give improved algorithms for these domains.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Our Approach
Our
point
minx2X maxy2Y f (x  y)  where f : X⇥Y !
method solves a sequence of subproblems parameterized by ↵> 0  each of the form

solving
R is convex in x and concave in y. The

is Nemirovski’s

prox-method”

“conceptual

starting

[28]

for

ﬁnd x  y s.t. 8x0  y0 hrxf (x  y)  x  x0i  hryf (x  y)  y  y0i  ↵Vx0(x0) + ↵Vy0(y0)
(1)
for some (x0  y0) 2X⇥Y   where Va(b) is a norm-suitable Bregman divergence from a to b: squared
Euclidean distance for `2 and KL divergence for `1. Combining each subproblem solution with an
extragradient step  the prox-method solves the original problem to ✏ accuracy by solving eO(↵/✏)

subproblems.1 (Solving (1) with ↵ = 0 is equivalent to to solving minx2X maxy2Y f (x  y).)
Our ﬁrst contribution is showing that if a stochastic unbiased gradient estimator ˜g satisﬁes the
“variance” bound

Ek˜g(x  y)  rf (x0  y0)k2

⇤  L2 kx  x0k2 + L2 ky  y0k2

(2)
for some L > 0  then O(L2/↵2) regularized stochastic mirror descent steps using ˜g solve (1) in a
suitable probabilistic sense. We call unbiased gradient estimators that satisfy (2) “centered.”
Our second contribution is the construction of “centered” gradient estimators for `1-`1 and `2-`1
bilinear games  where f (x  y) = y>Ax. Our `1 estimator has the following form. Suppose we wish
to estimate gx = A>y (the gradient of f w.r.t. x)  and we already have gx
0 = A>y0. Let p 2 m be
some distribution over {1  . . .   m}  draw i ⇠ p and set

˜gx = gx

0 + Ai:

[y]i  [y0]i

 

pi

where Ai: is the ith column of A>. This form is familiar from variance reduction techniques [19 
44  1]  that typically use a ﬁxed distribution p. In our setting  however  a ﬁxed p will not produce
sufﬁciently low variance. Departing from prior variance-reduction work and building on [16  6]  we
choose p based on y according to

pi(y) = [y]i  [y0]i

ky  y0k1

 

yielding exactly the variance bound we require. We call this technique “sampling from the difference.”
For our `2 gradient estimator  we sample from the squared difference  drawing X -block coordinate
j ⇠ q  where qj(x) = ([x]j  [x0]j)2/kx  x0k2
2. To strengthen our results for `2-`1 games  we
consider a reﬁned version of the “centered” criterion (2) which allows regret analysis using local
norms [37  6]. To further facilitate this analysis we follow [6] and introduce gradient clipping.
We extend our proofs to show that stochastic regularized mirror descent can solve (1) despite the
(distance-bounded) bias caused by gradient clipping.
Our gradient estimators attain the bound (2) with L equal to the Lipschitz constant of rf. Speciﬁcally 
(3)

in the `1-`1 setup
in the `2-`1 setup.

L =⇢maxij |Aij|

maxi kAi:k2

1.2 Method complexity compared with prior art

As per the discussion above  to achieve accuracy ✏ our algorithm solves eO(↵/✏) subproblems. Each

subproblem takes O(nnz(A)) time for computing two exact gradients (one for variance reduction and
one for an extragradient step)  plus an additional (m + n)L2/↵2 time for the inner mirror descent
iterations  with L as in (3). The total runtime is therefore

eO✓✓nnz(A) +

(m + n)L2

↵2

◆ ↵
✏◆ .

1 More precisely  the required number of subproblem solutions is at most ⇥ · ↵
✏   where ⇥ is a “domain
size” parameter that depends on X   Y  and the Bregman divergence V (see Section 2). In the `1 and `2 settings
considered in this paper  we have the bound ⇥  log(nm) and we use the eO notation to suppress terms
logarithmic in n and m. However  in other settings—e.g.  `1-`1 games [cf. 38  40]—making the parameter ⇥
scale logarithmically with the problem dimension is far more difﬁcult.

2

(4)

By setting ↵ optimally to be max{✏  Lp(m + n)/nnz(A)}  we obtain the runtime

eO(nnz(A) +pnnz(A) · (m + n) · L · ✏1).

Comparison with mirror-prox and dual extrapolation. Nemirovski [28] instantiates his concep-
tual prox-method by solving the relaxed proximal problem (1) with ↵ = L in time O(nnz(A))  where
L is the Lipschitz constant of rf  as given in (3). The total complexity of the resulting method is
therefore
(5)
The closely related dual extrapolation method of Nesterov [31] attains the same rate of convergence.
We refer to the running time (5) as linear since it scales linearly with the problem description size
nnz(A). Our running time guarantee (4) is never worse than (5) by more than a constant factor  and
improves on (5) when nnz(A) = !(n + m)  i.e. whenever A is not extremely sparse. In that regime 
our method uses ↵ ⌧ L  hence solving a harder version of (1) than possible for mirror-prox.
Comparison with sublinear-time methods Using a randomized algorithm  Grigoriadis and
Khachiyan [16] solve `1-`1 bilinear games in time

eO(nnz(A) · L · ✏1).

eO((m + n) · L2 · ✏2) 

(6)
and Clarkson et al. [6] extend this result to `2-`1 bilinear games  with the values of L as in (3). Since
these runtimes scale with n + m  nnz(A)  we refer to them as sublinear. Our guarantee improves
on the guarantee (6) when (m + n) · L2 · ✏2  nnz(A)  i.e. whenever (6) is not truly sublinear.
Our method carefully balances linear-time extragradient steps with cheap sublinear-time stochastic
gradient steps. Consequently  our runtime guarantee (4) inherits strengths from both the linear and
sublinear runtimes. First  our runtime scales linearly with L/✏ rather than quadratically  as does the
linear runtime (5). Second  while our runtime is not strictly sublinear  its component proportional to

Overall  our method offers the best runtime guarantee in the literature in the regime

L/✏ ispnnz(A)(n + m)  which is sublinear in nnz(A).
pnnz(A)(n + m)
min{n  m}! ⌧

✏

L ⌧r n + m

nnz(A)

 

where the lower bound on ✏ is due to the best known theoretical runtimes of interior point methods:

eO(max{n  m}! log(L/✏)) [7] and eO(nnz(A) + min{n  m}2)pmin{n  m} log(L/✏)) [21]  where

! is the (current) matrix multiplication exponent.
In the square dense case (i.e. nnz(A) ⇡ n2 = m2)  we improve on the accelerated runtime (5) by a
factor of pn  the same improvement that optimal variance-reduced ﬁnite-sum minimization methods
achieve over the fast gradient method [44  1].

1.3 Related work
Matrix games  the canonical form of discrete zero-sum games  have long been studied in economics
[32]. The classical mirror descent (i.e. no-regret) method yields an algorithm with running time

Our work builds on the extragradient scheme of Nemirovski [28] as well as the gradient estimation
and clipping technique of Clarkson et al. [6].
Balamurugan and Bach [3] apply standard variance reduction [19] to bilinear `2-`2 games by sampling
elements proportional to squared matrix entries. Using proximal-point acceleration they obtain a
✏ )  a rate we recover using our algorithm

eO(nnz(A)L2✏2) [30]. Subsequent work [16  28  31  6] improve this runtime as described above.
runtime of eO(nnz(A)+kAkFpnnz(A) max{m  n}✏1 log 1
(Appendix E). However  in this setting the mirror-prox method has runtime eO(kAkop nnz(A)✏1) 
which may be better than the result of [3] by a factor ofpmn/nnz(A) due to the discrepancy in

the norm of A. Naive application of [3] to `1 domains results in even greater potential losses. Shi
et al. [39] extend the method of [3] to smooth functions using general Bregman divergences  but their
extension is unaccelerated and appears limited to a ✏2 rate.
Chavdarova et al. [5] propose a variance-reduced extragradient method with applications to generative
adversarial training. In contrast to our algorithm  which performs extragadient steps in the outer loop 

3

the method of [5] performs stochastic extragradient steps in the inner loop  using ﬁnite-sum variance
reduction as in [19]. Chavdarova et al. [5] analyze their method in the convex-concave setting 
showing improved stability over direct application of the extragradient method to noisy gradients.
However  their complexity guarantees are worse than those of linear-time methods. Following up
on [5]  Mishchenko et al. [26] propose to reduce the variance of the stochastic extragradient method
by using the same stochastic sample for both the gradient and extragradient steps. In the Euclidean
strongly convex case  they show a convergence guarantee with a relaxed variance assumption  and in
the noiseless full-rank bilinear case they recover the guarantees of [27]. In the general convex case 
however  they only show an ✏2 rate of convergence.

1.4 Paper outline and additional contributions

We deﬁne our notation in Section 2. In Section 3.1  we review Nemirovski’s conceptual prox-method
and introduce the notion of a relaxed proximal oracle; we implement such oracle using variance-
reduced gradient estimators in Section 3.2. In Section 4  we construct these gradient estimators for
the `1-`1 and `2-`1 domain settings  and complete the analyses of the corresponding algorithms; in
Appendix E we provide analogous treatment for the `2-`2 setting  recovering the results of [3].
In Appendix F we provide three additional contributions: variance-reduction-based computation of
proximal points for arbitrary convex-concave functions (Appendix F.1); extension of our results to
“composite” saddle point problems of the form minx2X maxy2Y {f (x  y) + (x)  (y)}  where f
admits a centered gradient estimator and   are “simple” convex functions (Appendix F.2); and a
number of alternative centered gradient estimators for the `2-`1 and `2-`2 settings (Appendix F.3).

2 Notation

Problem setup. A setup is the triplet (Z k·k   r) where: (i) Z is a compact and convex subset of
Rn ⇥ Rm  (ii) k·k is a norm on Z and (iii) r is 1-strongly-convex w.r.t. Z and k·k  i.e. such that
2 kz0  zk2 for all z  z0 2Z .2 We call r the distance generating
r(z0)  r(z) + hrr(z)  z  z0i + 1
function and denote the Bregman divergence associated with it by

Vz(z0) := r(z0)  r(z)  hrr(z)  z0  zi 

1
2 kz0  zk2 .

We also denote ⇥ := maxz0 r(z0)  minz r(z) and assume it is ﬁnite.
Norms and dual norms. We write S⇤ for the set of linear functions on S. For ⇣ 2Z ⇤ we deﬁne the
:= maxkzk1 h⇣  zi. For p  1 we write the `p norm kzkp = (Pi zp
dual norm of k·k as k⇣k⇤
i )1/p
with kzk1 = maxi |zi|. The dual norm of `p is `q with q1 = 1  p1.
Domain components. We assume Z is of the form X⇥Y for convex and compact sets X⇢ Rn
and Y⇢ Rm. Particular sets of interest are the simplex d = {v 2 Rd |k vk1 = 1  v  0} and the
Euclidean ball Bd = {v 2 Rd |k vk2  1}. For any vector in z 2 Rn ⇥ Rm 

we write zx and zy for the ﬁrst n and last m coordinates of z  respectively.

When totally clear from context  we sometimes refer to the X and Y components of z directly as x
and y. We write the ith coordinate of vector v as [v]i.

Matrices. We consider a matrix A 2 Rm⇥n and write nnz(A) for the number of its nonzero
entries. For i 2 [n] and j 2 [m] we write Ai:  A:j and Aij for the corresponding row  column
and entry  respectively.3 We consider the matrix norms kAkmax := maxij |Aij|  kAkp!q :=
maxkxkp1 kAxkq and kAkF := (Pi j A2

2 For non-differentiable r  let hrr(z)  wi := sup2@r(z) h  wi  where @r(z) is the subdifferential of r at z.
3 For k 2 N  we let [k] := {1  . . .   k}.

ij)1/2.

4

3 Primal-dual variance reduction framework

In this section  we establish a framework for solving the saddle point problem

where f is convex in x and concave y  and admits a (variance-reduced) stochastic estimator for the
continuous and monotone4 gradient mapping

min
x2X

max
y2Y

f (x  y) 

g(z) = g(x  y) := (rxf (x  y) ryf (x  y)) .

Our goal is to ﬁnd an ✏-approximate saddle point (Nash equilibrium)  i.e. z 2Z := X⇥Y such that
(7)

Gap(z) := max
y02Y

f (zx  y0)  min
x02X

f (x0  zy)  ✏.
KPK

We achieve this by generating a sequence z1  z2  . . .   zk such that 1
every u 2Z and using the fact that
Gap 1
KXk=1

zk!  max

KXk=1

1
K

u2Z

K

hg(zk)  zk  ui

k=1 hg(zk)  zk  ui  ✏ for

(8)

due to convexity-concavity of f (see proof in Appendix A.1).
In Section 3.1 we deﬁne the notion of a (randomized) relaxed proximal oracle  and describe how
Nemirovski’s mirror-prox method leverages it to solve the problem (3). In Section 3.2 we deﬁne a
class of centered gradient estimators  whose variance is proportional to the squared distance from a
reference point. Given such a centered gradient estimator  we show that a regularized stochastic mirror
descent scheme constitutes a relaxed proximal oracle. For a technical reason  we limit our oracle
guarantee in Section 3.2 to the bilinear case f (x  y) = y>Ax  which sufﬁces for the applications in
Section 4. We lift this limitation in Appendix F.1  where we show a different oracle implementation
that is valid for general convex-concave f  with only a logarithmic increase in complexity.

3.1 The mirror-prox method with a randomized oracle
Recall that we assume the space Z = X⇥Y is equipped with a norm k·k and distance generating
function r : Z! R that is 1-strongly-convex w.r.t. k·k and has range ⇥. We write the induced
Bregman divergence as Vz(z0) = r(z0)r(z)hrr(z)  z0  zi. We use the following fact throughout
the paper: by deﬁnition  the Bregman divergence satisﬁes  for any z  z0  u 2Z  
(9)
For any ↵> 0 we deﬁne the ↵-proximal mapping Prox↵
z (g) to be the solution of the variational
inequality corresponding to the strongly monotone operator g + ↵rVz  i.e. the unique z↵ 2Z such
that hg(z↵) + ↵rVz(z↵)  z↵  ui  0 for all u 2Z [cf. 11]. Equivalently (by (9)) 
Prox↵
z (g) := the unique z↵ 2Z s.t. hg(z↵)  z↵  ui  ↵Vz(u)  ↵Vz↵(u)  ↵Vz(z↵) 8u 2Z .
(10)
z (g) is also the unique solution of the saddle point problem

 hrVz(z0)  z0  ui = Vz(u)  Vz0(u)  Vz(z0).

When Vz(z0) = V x

y (y0)  Prox↵
x (x0) + V y
min
x02X

max

y02Yf (x0  y0) + ↵V x

x (x0)  ↵V y

y (y0) .

Consider iterations of the form zk = Prox↵
tion (10) over k  using the bound (8) and the nonnegativity of Bregman divergences gives

zk1(g)  with z0 = arg minz r(z). Averaging the deﬁni-

Gap 1

K

KXk=1

zk!  max

u2Z

1
K

KXk=1

hg(zk)  zk  ui  max
u2Z

↵ (Vz0(u)  VzK (u))

K

↵⇥
K

.



Thus  we can ﬁnd an ✏-suboptimal point in K = ↵⇥/✏ exact proximal steps. However  computing
Prox↵
z (g) exactly may be as difﬁcult as solving the original problem. Nemirovski [28] proposes
a relaxation of the exact proximal mapping  which we slightly extend to include the possibility of
randomization  and formalize in the following.

4 A mapping q : Z!Z ⇤ is monotone if and only if hq(z0)  q(z)  z0  zi  0 for all z  z0 2Z ; g is

monotone due to convexity-concavity of f.

5

Deﬁnition 1 ((↵  ")-relaxed proximal oracle). Let g be a monotone operator and ↵  " > 0. An
(↵  ")-relaxed proximal oracle for g is a (possibly randomized) mapping O : Z!Z such that
z0 = O(z) satisﬁes

u2Zhg(z0)  z0  ui  ↵Vz(u)   ".
Emax

Note that O(z) = Prox↵
z (g) is an (↵  0)-relaxed proximal oracle. Algorithm 1 describes the
“conceptual prox-method” of Nemirovski [28]  which recovers the error guarantee of exact proximal
iterations. The kth iteration consists of (i) a relaxed proximal oracle call producing zk1/2 =
O(zk1)  and (ii) a linearized proximal (mirror) step where we replace z 7! g(z) with the constant
function z 7! g(zk1/2)  producing zk = Prox↵
zk1(g(zk1/2)). We now state the convergence
guarantee for the mirror-prox method  ﬁrst shown in [28] (see Appendix B.1 for a simple proof).

Algorithm 1: OuterLoop(O) (Nemirovski [28])
Input: (↵  ")-relaxed proximal oracle O(z) for gradient mapping g  distance-generating r
Parameters :Number of iterations K
Output: Point ¯zK with E Gap(¯z)  ↵⇥
1 z0 arg minz2Z r(z)
2 for k = 1  . . .   K do
zk1/2 O (zk1)
3
zk1(g(zk1/2)) = arg minz2Z⌦gzk1/2   z↵ + ↵Vzk1(z) 
zk Prox↵
4
KPK
5 return ¯zK = 1

. We implement O(zk1) by calling InnerLoop(zk1  ˜gzk1 ↵ )

k=1 zk1/2

K + "

Proposition 1 (Mirror prox convergence via oracles). Let O be an (↵ ")-relaxed proximal oracle
with respect to gradient mapping g and distance-generating function r with range at most ⇥. Let
z1/2  z3/2  . . .   zK1/2 be the iterates of Algorithm 1 and let ¯zK be its output. Then

E Gap(¯zK)  E max
u2Z

1
K

KXk=1⌦g(zk1/2)  zk1/2  u↵ 

↵⇥
K

+ ".

Implementation of an (↵  0)-relaxed proximal oracle

3.2
We now explain how to use stochastic variance-reduced gradient estimators to design an efﬁcient
(↵  0)-relaxed proximal oracle. We begin by introducing the bias and variance properties of the
estimators we require.
Deﬁnition 2. Let z0 2Z and L > 0. A stochastic gradient estimator ˜gz0 : Z!Z ⇤ is called
(z0  L)-centered for g if for all z 2Z

1. E [˜gz0(z)] = g(z) 
2. Ek˜gz0(z)  g(z0)k2

⇤  L2 kz  z0k2.

Lemma 1. A (z0  L)-centered estimator for g satisﬁes Ek˜gz0(z)  g(z)k2
Proof. Writing ˜ = ˜gz0(z)  g(z0)  we have E˜ = g(z)  g(z0) by the ﬁrst centered estimator
property. Therefore 

⇤  (2L)2 kz  z0k2.

Ek˜gz0(z)  g(z)k2

⇤ = Ek˜  E˜k2
⇤

 2Ek˜k2

⇤ + 2kE˜k2
⇤

(i)

(ii)

 4Ek˜k2
⇤

(iii)

 (2L)2 kz  z0k2  

where the bounds follow from (i) the triangle inequality  (ii) Jensen’s inequality and (iii) the second
centered estimator property.
Remark 1. A gradient mapping that admits a (z  L)-centered gradient estimator for every z 2Z is
2L-Lipschitz  since by Jensen’s inequality and Lemma 1 we have for all w 2Z

kg(w)  g(z)k⇤ = kE˜gz(w)  g(z)k⇤  (Ek˜gz(w)  g(z)k2

⇤)1/2  2Lkw  zk .

6

Remark 2. Deﬁnition 2 bounds the gradient variance using the distance to the reference point. Similar
bounds are used in variance reduction for bilinear saddle-point problems with Euclidean norm [3] 
as well as for ﬁnding stationary points in smooth nonconvex ﬁnite-sum problems [2  33  12  45].
However  known variance reduction methods for smooth convex ﬁnite-sum minimization require
stronger bounds [cf. 1  Section 2.1].

With the variance bounds deﬁned  we describe Algorithm 2 which (for the bilinear case) implements a
relaxed proximal oracle. The algorithm is stochastic mirror descent with an additional regularization
term around the initial point w0. Note that we do not perform extragradient steps in this stochastic
method. When combined with a centered gradient estimator  the iterates of Algorithm 2 provide the
following guarantee  which is one of our key technical contributions.

Algorithm 2: InnerLoop(w0  ˜gw0 ↵ )
Input: Initial w0 2Z   gradient estimator ˜gw0  oracle quality ↵> 0
Parameters :Step size ⌘  number of iterations T
Output: Point ¯wT satisfying Deﬁnition 1 (for appropriate ˜gw0  ⌘  T )
⌘ Vwt1(w)o
wt arg minw2Znh˜gw0(wt1)  wi + ↵

1 for t = 1  . . .   T do
2

2 Vw0(w) + 1

3 return ¯wT = 1

t=1 wt

T PT
10L2 and T  4

Proposition 2. Let ↵  L > 0  let w0 2Z and let ˜gw0 be (w0  L)-centered for monotone g. Then  for
⌘ = ↵

↵2   the iterates of Algorithm 2 satisfy

⌘↵ = 40L2

E max

u2Z24 1
T Xt2[T ]

hg(wt)  wt  ui  ↵Vw0(u)35  0.

(11)

Before discussing the proof of Proposition 2  we state how it implies the relaxed proximal oracle
property for the bilinear case.
Corollary 1. Let A 2 Rm⇥n and let g(z) = (A>zy Azx). Then  in the setting of Proposition 2 
O(w0) = InnerLoop(w0  ˜gw0 ↵ ) is an (↵  0)-relaxed proximal oracle.
Proof. Note that hg(z)  wi = hg(w)  zi for any z  w 2Z and consequently hg(z)  zi = 0.
Therefore  the iterates w1  . . .   wT of Algorithm 2 and its output ¯wT = 1
t=1 wt satisfy for every
u 2Z  

T PT

hg(wt)  wt  ui =

hg(u)  wti = hg(u)  ¯wTi = hg( ¯wT )  ¯wT  ui .

1

T Xt2[T ]

1

T Xt2[T ]

Substituting into the bound (11) yields the (↵  0)-relaxed proximal oracle property in Deﬁnition 1.

More generally  the proof of Corollary 1 shows that Algorithm 2 implements a relaxed proximal oracle
whenever z 7! hg(z)  z  ui is convex for every u. In Appendix F.1 we implement an (↵  ")-relaxed
proximal oracle without such an assumption.
The proof of Proposition 2 is a somewhat lengthy application of existing techniques for stochastic
mirror descent analysis in conjunction with Deﬁnition 2. We give it in full in Appendix B.2 and sketch
it brieﬂy here. We view Algorithm 2 as mirror descent with stochastic gradients ˜t = ˜gw0(wt)g(w0)
2 Vw0(z). For any u 2Z   the standard mirror descent analysis
and composite term hg(w0)  zi + ↵
(see Lemma 4 in Appendix A.2) bounds the regretPt2[T ]⌦˜gw0(wt) + ↵
2 rVw0(wt)  wt  u↵ in
terms of the distance to initialization Vw0(u) and the stochastic gradient norms k˜tk2
for t 2 [T ].
⇤
Bounding these norms via Deﬁnition 2 and rearranging the hrVw0(wt)  wt  ui terms  we show that
Eh 1
T Pt2[T ] hg(wt)  wt  ui  ↵Vw0(u)i  0 for all u 2Z . To reach our desired result we must

swap the order of the expectation and “for all.” We do so using the “ghost iterate” technique due
to Nemirovski et al. [29].

7

4 Application to bilinear saddle point problems

We now construct centered gradient estimators (as per Deﬁnition 2) for the linear gradient mapping

g(z) = (A>zy Azx) corresponding to the bilinear saddle point problem min
x2X

max
y2Y

y>Ax.

Sections 4.1 and 4.2 consider the `1-`1 and `2-`1 settings  respectively; in Appendix E we show
how our approach naturally extends to the `2-`2 setting as well. Throughout  we let w0 denote the
“center” (i.e. reference point) of our stochastic gradient estimator and consider a general query point
w 2Z = X⇥Y . We also recall the notation [v]i for the ith entry of vector v.
4.1
Setup. Denoting the d-dimensional simplex by d  we let X = n  Y = m and Z = X⇥Y .
We take k·k to be the `1 norm with conjugate norm k·k⇤ = k·k1
. We take the distance generating
function r to be the negative entropy  i.e. r(z) =Pi[z]i log[z]i. We note that both k·k1 and r are
separable and in particular separate over the X and Y blocks of Z. Finally we set

`1-`1 games

kAkmax := max

i j

|Aij|

and note that this is the Lipschitz constant of the gradient mapping g under the chosen norm.

Gradient estimator. Given w0 = (wx
0)  we describe the
reduced-variance gradient estimator ˜gw0(w). First  we deﬁne the probabilities p(w) 2 m and
q(w) 2 n according to 

0) and g(w0) = (A>wy

0 Awx

0  wy

and qj(w) := |[wx]j  [wx
0]j|
kwx  wx
0k1
To compute ˜gw0 we sample i ⇠ p(w) and j ⇠ q(w) independently  and set

pi(w) := |[wy]i  [wy
0]i|
kwy  wy
0k1

.

(12)

(13)

◆  

˜gw0(w) :=✓A>wy

0 + Ai:

[wy]i  [wy
0]i
pi(w)

 Awx

0  A:j

[wx]j  [wx
0]j
qj(w)

where Ai: and A:j are the ith row and jth column of A  respectively. Since the sampling distributions
p(w)  q(w) are proportional to the absolute value of the difference between blocks of w and w0  we
call strategy (12) “sampling from the difference.” Substituting (12) into (13) gives the explicit form
˜gw0(w) = g(w0) + (Ai:kwy  wy
0]j)) . (14)
A straightforward calculation shows that this construction satisﬁes Deﬁnition 2.
Lemma 2. In the `1-`1 setup  the estimator (14) is (w0  L)-centered with L = kAkmax.
Proof. The ﬁrst property (E˜gw0(w) = g(w)) follows immediately by inspection of (13). The second
property follows from (14) by noting that

0]i) A:jkwx  wx

0k1sign([wy  wy

0k1sign([wx  wx

k˜gw0(w)  g(w0)k1 = maxkAi:k1 kwy  wy
for all i  j  and therefore Ek˜gw0(w)  g(w0)k2
The proof of Lemma 2 reveals that the proposed estimator satisﬁes a stronger version of Deﬁnition 2:
the last property and also Lemma 1 hold with probability 1 rather than in expectation.

0k1   kA:jk1 kwx  wx
1  kAk2
1.
max kw  w0k2

0k1  kAkmax kw  w0k1

Runtime bound. Combining the centered gradient estimator (13)  the relaxed oracle implemen-
tation (Algorithm 2) and the extragradient outer loop (Algorithm 1)  we obtain our main result for
`1-`1 games: an accelerated stochastic variance reduction algorithm. We write the resulting complete
method explicitly as Algorithm 3 in Appendix C.1. The algorithm enjoys the following runtime
guarantee (see proof in Appendix C.2).

8

Theorem 1. Let A 2 Rm⇥n  ✏> 0  and ↵  ✏/ log(nm). Algorithm 3 outputs a point z = (zx  zy)
and runs in time

such that E⇥maxy2m y>Azx  minx2n(zy)>Ax⇤ = Ehmaxi [Azx]i  minj [A>zy]ji  ✏ 

Setting ↵ optimally  the running time is

max

(m + n)kAk2

! .
! ↵ log(mn)
O nnz(A) +
O nnz(A) +pnnz(A)(m + n)kAkmax log(mn)
! .

↵2

✏

✏

(15)

(16)

denote

2 + kzyk2

⇤ = kgxk2

`2-`1 games

4.2
Setup. We set X = Bn to be the n-dimensional Euclidean ball of radius 1  while Y = m remains
the simplex. For z = (zx  zy) 2Z = X⇥Y we deﬁne a norm by

kzk2 = kzxk2

kAk2!1 = max

2 and ry(y) =
2 + log m  log(2m). Finally  we

2 + kgyk2
1 .
2 kxk2
For distance generating function we take r(z) = rx(zx) + ry(zy) with rx(x) = 1

1 with dual norm kgk2
Pi yi log yi; r is 1-strongly convex w.r.t. to k·k and has range 1
i2[m]kAi:k2  
and note that this is the Lipschitz constant of g under k·k.
Gradient estimator. To account for the fact that X is now the `2 unit ball  we modify the sampling
distribution q in (12) to qj(w) = ([wx]j[wx
0]j )2
  and keep p the same. As we explain in detail
0k2
kwxwx
in Appendix D.1.1  substituting these probabilities into the expression (13) yields a centered gradient
estimator with a constant (Pj2[n] kA:jk2
by a factor of up to pn.
)1/2 that is larger than kAk2!1
Using local norms analysis allows us to tighten these bounds whenever the stochastic steps have
bounded inﬁnity norm. Following Clarkson et al. [6]  we enforce such bound on the step norms via
gradient clipping. The ﬁnal gradient estimator is
kwy  wy
0k1
sign([wy  wy

0  T⌧ A:j kwx  wx
0]j!!  
0k2
[wx]j  [wx

 Awx

1

2

2

0 + Ai:

˜gw0(w) := A>wy
where [T⌧ (v)]i =8<:

0]i)
[v]i < ⌧
[v]i >⌧ 

⌧
[v]i ⌧  [v]i  ⌧
⌧

The clipping operation T⌧ introduces bias to the gradient estimator  which we account for by carefully
choosing a value of ⌧ for which the bias is on the same order as the variance  and yet the resulting
steps are appropriately bounded; see Appendix D.1.2. In Appendix F.3.1 we describe an alternative
gradient estimator for which the distribution q does not depend on the current iterate w.

Runtime bound. Algorithm 4 in Appendix D.5 combines our clipped gradient estimator with our
general variance reduction framework. The analysis in Appendix D gives the following guarantee.
Theorem 2. Let A 2 Rm⇥n  ✏> 0  and any ↵  ✏/ log(2m). Algorithm 4 outputs a point z =
(zx  zy) such that E⇥maxy2m y>Azx  minx2Bn(zy)>Ax⇤ = E⇥maxi [Azx]i + kA>zyk2⇤  ✏ 
and runs in time

Setting ↵ optimally  the running time is

2!1

(m + n)kAk2

! .
! ↵ log(2m)
O nnz(A) +
! .
O nnz(A) +pnnz(A)(m + n)kAk2!1 log(2m)

↵2

✏

✏

(17)

(18)

9

Acknowledgments
YC and YJ were supported by Stanford Graduate Fellowships. AS was supported by the NSF
CAREER Award CCF-1844855. KT was supported by the NSF Graduate Fellowship DGE1656518.

References

[1] Z. Allen-Zhu. Katyusha:

In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
1200–1205  2017.

the ﬁrst direct acceleration of stochastic gradient methods.

[2] Z. Allen-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In Proceed-

ings of the 33rd International Conference on Machine Learning  pages 699–707  2016.

[3] P. Balamurugan and F. R. Bach. Stochastic variance reduction methods for saddle-point

problems. In Advances in Neural Information Processing Systems  2016.

[4] L. Bottou  F. E. Curtis  and J. Nocedal. Optimization methods for large-scale machine learning.

SIAM Review  60(2):223–311  2018.

[5] T. Chavdarova  G. Gidel  F. Fleuret  and S. Lacoste-Julien. Reducing noise in GAN training
with variance reduced extragradient. In Advances in Neural Information Processing Systems 
2019.

[6] K. L. Clarkson  E. Hazan  and D. P. Woodruff. Sublinear optimization for machine learning. In

51th Annual IEEE Symposium on Foundations of Computer Science  pages 449–457  2010.

[7] M. B. Cohen  Y. T. Lee  and Z. Song. Solving linear programs in the current matrix multiplication

time. arXiv preprint arXiv:1810.07896  2018.

[8] G. B. Dantzig. Linear Programming and Extensions. Princeton University Press  Princeton  NJ 

1953.

[9] C. Daskalakis  A. Ilyas  V. Syrgkanis  and H. Zeng. Training GANs with optimism.

International Conference on Learning Representations  2019.

In

[10] Y. Drori  S. Sabach  and M. Teboulle. A simple algorithm for a class of nonsmooth convex-

concave saddle-point problems. Operations Research Letters  43(2):209–214  2015.

[11] J. Eckstein. Nonlinear proximal point algorithms using Bregman functions  with applications to

convex programming. Mathematics of Operations Research  18(1):202–226  1993.

[12] C. Fang  C. J. Li  Z. Lin  and T. Zhang. SPIDER: near-optimal non-convex optimization via
stochastic path-integrated differential estimator. In Advances in Neural Information Processing
Systems  2018.

[13] R. Frostig  R. Ge  S. Kakade  and A. Sidford. Un-regularizing: approximate proximal point
and faster stochastic algorithms for empirical risk minimization. In Proceedings of the 32nd
International Conference on Machine Learning  pages 2540–2548  2015.

[14] G. Gidel  T. Jebara  and S. Lacoste-Julien. Frank-Wolfe algorithms for saddle point problems.
In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics 
2017.

[15] I. J. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. C. Courville 
and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing
Systems  2014.

[16] M. D. Grigoriadis and L. G. Khachiyan. A sublinear-time randomized approximation algorithm

for matrix games. Operation Research Letters  18(2):53–58  1995.

[17] J. Hiriart-Urruty and C. Lemaréchal. Convex Analysis and Minimization Algorithms I. Springer 

New York  1993.

10

[18] C. Jin  P. Netrapalli  and M. I. Jordan. Minmax optimization: Stable limit points of gradient

descent ascent are locally optimal. arXiv preprint arXiv:1902.00618  2019.

[19] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  2013.

[20] O. Kolossoski and R. D. Monteiro. An accelerated non-Euclidean hybrid proximal extragradient-
type algorithm for convex-concave saddle-point problems. Optimization Methods and Software 
32(6):1244–1272  2017.

[21] Y. T. Lee and A. Sidford. Efﬁcient inverse maintenance and faster algorithms for linear
programming. In IEEE 56th Annual Symposium on Foundations of Computer Science  pages
230–249  2015.

[22] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization.

Advances in Neural Information Processing Systems  2015.

In

[23] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations  2018.
[24] P. Mertikopoulos  H. Zenati  B. Lecouat  C.-S. Foo  V. Chandrasekhar  and G. Piliouras. Mirror
descent in saddle-point problems: Going the extra (gradient) mile. In International Conference
on Learning Representations  2019.

[25] M. Minsky and S. Papert. Perceptrons—an introduction to computational geometry. MIT Press 

1987.

[26] K. Mishchenko  D. Kovalev  E. Shulgin  P. Richtárik  and Y. Malitsky. Revisiting stochastic

extragradient. arXiv preprint arXiv:1905.11373  2019.

[27] A. Mokhtari  A. Ozdaglar  and S. Pattathil. A uniﬁed analysis of extra-gradient and opti-
mistic gradient methods for saddle point problems: Proximal point approach. arXiv preprint
arXiv:1901.08511  2019.

[28] A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAM Journal on Optimization  15(1):229–251  2004.

[29] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on optimization  19(4):1574–1609  2009.

[30] A. Nemirovsky and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. J.

Wiley & Sons  New York  NY  1983.

[31] Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and

related problems. Mathematical Programing  109(2-3):319–344  2007.

[32] J. V. Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen  100:295–320 

1928.

[33] S. J. Reddi  A. Hefny  S. Sra  B. Poczos  and A. Smola. Stochastic variance reduction for
nonconvex optimization. In Proceedings of the 33rd International Conference on Machine
Learning  pages 314–323  2016.

[34] M. W. Schmidt  N. L. Roux  and F. R. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programing  162(1-2):83–112  2017.

[35] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. Journal of Machine Learning Research  14(1):567–599  2013.

[36] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for

regularized loss minimization. Mathematical Programing  155(1-2):105–145  2016.

[37] S. Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and

Trends in Machine Learning  4(2):107–194  2012.

11

[38] J. Sherman. Area-convexity  `1 regularization  and undirected multicommodity ﬂow.

In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
452–460. ACM  2017.

[39] Z. Shi  X. Zhang  and Y. Yu. Bregman divergence for stochastic variance reduction: Saddle-point

and adversarial prediction. In Advances in Neural Information Processing Systems  2017.

[40] A. Sidford and K. Tian. Coordinate methods for accelerating `1 regression and faster approx-
imate maximum ﬂow. In 2018 IEEE 59th Annual Symposium on Foundations of Computer
Science  pages 922–933. IEEE  2018.

[41] T. Strohmer and R. Vershynin. A randomized Kaczmarz algorithm with exponential convergence.

Journal of Fourier Analysis and Applications  15(2):262  2009.

[42] M. D. Vose. A linear algorithm for generating random numbers with a given distribution. IEEE

Transactions on software engineering  17(9):972–975  1991.

[43] B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives.

In Advances in Neural Information Processing Systems  2016.

[44] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[45] D. Zhou  P. Xu  and Q. Gu. Stochastic nested variance reduced gradient descent for nonconvex

optimization. In Advances in Neural Information Processing Systems  2018.

12

,Yair Carmon
Yujia Jin
Aaron Sidford
Kevin Tian