2017,Concentration of Multilinear Functions of the Ising Model with Applications to Network Data,We prove near-tight concentration of measure for polynomial functions of the Ising model  under high temperature  improving the radius of concentration guaranteed by known results by polynomial factors in the dimension (i.e.~the number of nodes in the Ising model). We show that our results are optimal up to logarithmic factors in the dimension. We obtain our results by extending and strengthening the exchangeable-pairs approach used to prove concentration of measure in this setting by Chatterjee. We demonstrate the efficacy of such functions as statistics for testing the strength  of interactions in social networks in both synthetic and real world data.,Concentration of Multilinear Functions of the Ising

Model with Applications to Network Data

Constantinos Daskalakis ∗
EECS & CSAIL  MIT

costis@csail.mit.edu

Nishanth Dikkala∗
EECS & CSAIL  MIT

nishanthd@csail.mit.edu

Gautam Kamath∗
EECS & CSAIL  MIT
g@csail.mit.edu

Abstract

We prove near-tight concentration of measure for polynomial functions of the
Ising model under high temperature. For any degree d  we show that a degree-
d polynomial of a n-spin Ising model exhibits exponential tails that scale as
exp(−r2/d) at radius r = ˜Ωd(nd/2). Our concentration radius is optimal up to
logarithmic factors for constant d  improving known results by polynomial factors
in the number of spins. We demonstrate the efﬁcacy of polynomial functions as
statistics for testing the strength of interactions in social networks in both synthetic
and real world data.

1

Introduction

The Ising model is a fundamental probability distribution deﬁned in terms of a graph G = (V  E)
whose nodes and edges are associated with scalar parameters (θv)v∈V and (θu v){u v}∈E respectively.
The distribution samples a vector x ∈ {±1}V with probability:

p(x) = exp

θvxv +

θu vxuxv − Φ

(1)

(cid:88)

v∈V

(cid:88)

(u v)∈E

(cid:17)  
(cid:16)(cid:126)θ

(cid:16)(cid:126)θ
(cid:17)

where Φ
serves to provide normalization. Roughly speaking  there is a random variable Xv at
every node of G  and this variable may be in one of two states  or spins: up (+1) or down (−1). The
scalar parameter θv models a local ﬁeld at node v. The sign of θv represents whether this local ﬁeld
favors Xv taking the value +1  i.e. the up spin  when θv > 0  or the value −1  i.e. the down spin 
when θv < 0  and its magnitude represents the strength of the local ﬁeld. Similarly  θu v represents
the direct interaction between nodes u and v. Its sign represents whether it favors equal spins  when
θu v > 0  or opposite spins  when θu v < 0  and its magnitude corresponds to the strength of the
direct interaction. Of course  depending on the structure of G and the node and edge parameters  there
may be indirect interactions between nodes  which may overwhelm local ﬁelds or direct interactions.
Many popular models  for example  the usual ferromagnetic Ising model [Isi25  Ons44]  the
Sherrington-Kirkpatrick mean ﬁeld model [SK75] of spin glasses  and the Hopﬁeld model [Hop82]
of neural networks  the Curie-Weiss model [DCG68] all belong to the above family of distribu-
tions  with various special structures on G  the θu v’s and the θv’s. Since its introduction in

∗Authors are listed in alphabetical order.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Statistical Physics  the Ising model has found a myriad of applications in diverse research dis-
ciplines  including probability theory  Markov chain Monte Carlo  computer vision  theoretical
computer science  social network analysis  game theory  computational biology  and neuroscience;
see e.g. [LPW09  Cha05  Fel04  DMR11  GG86  Ell93  MS10] and their references. The ubiquity of
these applications motivate the problem of inferring Ising models from samples  or inferring statistical
properties of Ising models from samples. This type of problem has enjoyed much study in statistics 
machine learning  and information theory; see  e.g.  [CL68  AKN06  CT06  Cha07  RWL10  JJR11 
SW12  BGS14  Bre15  VMLC16  BK16  Bha16  BM16  MdCCU16  KM17  HKM17  DDK18].
Despite the wealth of theoretical study and practical applications of this model  outlined above  there
are still aspects of it that are poorly understood. In this work  we focus on the important topic of
concentration of measure. We are interested in studying the concentration properties of polynomial
functions f (X) of the Ising model. That is  for a random vector X sampled from p as above and a
polynomial f  we are interested in the concentration of f (X) around its expectation E[f (X)]. Since
the coordinates of X take values in {±1}  we can without loss of generality focus our attention to
multi-linear functions f.
While the theory of concentration inequalities for functions of independent random variables has
reached a high level of sophistication  proving concentration of measure for functions of dependent
random variables is signiﬁcantly harder  the main tools being martingale methods  logarithmic
Sobolev inequalities and transportation cost inequalities. One shortcoming of the latter methods is
that explicit constants are very hard or almost impossible to get. For the Ising model  in particular 
the log-Sobolev inequalities of Stroock and Zegarlinski [SZ92]  known under high temperature 2
do not give explicit constants  and it is also not clear whether they extend to systems beyond the
lattice. The high temperature regime is an interesting regime of ‘weak’ dependence where many
desirable properties related to Ising models hold. Perhaps the most important of them is that the
canonical Markov chain used to sample from these models  namely the Glauber dynamics  is fast
mixing. Although the high-temperature regime allows only ‘weak’ pairwise correlations  it is still
rich enough to encode interesting dependencies. For instance  in neuroscience  it has been seen that
weak pairwise correlations can coexist with strong correlations in the state of the population as a
whole [SBSB06].
An alternative approach  proposed recently by Chatterjee [Cha05]  is an adaptation to the Ising
model of Stein’s method of exchangeable pairs. This powerful method is well-known in probability
theory  and has been used to derive concentration inequalities with explicit constants for functions
of dependent random variables (see [MJC+14] for a recent work). Chatterjee uses this technique to
establish concentration inequalities for Lipschitz functions of the Ising model under high temperature.
While these inequalities are tight (and provide Gaussian tails) for linear functions of the Ising model 
they are unfortunately not tight for higher degree polynomials  in that the concentration radius is
off by factors that depend on the dimension n = |V |. For example  consider the function fc(X) =
i(cid:54)=j cijXiXj of an Ising model without external ﬁelds  where the cij’s are signs. Chatterjee’s
results imply that this function concentrates at radius ±O(n1.5)  but as we show this is suboptimal by
a factor of ˜Ω(
In particular  our main technical contribution is to obtain near-tight concentration inequalities for
polynomial functions of the Ising model  whose concentration radii are tight up to logarithmic factors.
A corollary of our main result (Theorem 4) is as follows:
Theorem 1. Consider any degree-d multilinear function f with coefﬁcients in [−1  1]  deﬁned on
an Ising model p without external ﬁeld in the high-temperature regime. Then there exists a constant
C = C(d) > 0 (depending only on d) such that for any r = ˜Ωd(nd/2)  we have

(cid:80)

√

n).

(cid:18)

(cid:19)

.

[|f (X) − E[f (X)]| > r] ≤ exp

Pr
X∼p

−C · r2/d
n log n

The concentration radius is tight up to logarithmic factors  and the tail bound is tight up to a
Od(1/ log n) factor in the exponent of the tail bound.

2High temperature is a widely studied regime of the Ising model where it enjoys a number of useful properties
such as decay of correlations and fast mixing of the Glauber dynamics. Throughout this paper we will take “high
temperature” to mean that Dobrushin’s conditions of weak dependence are satisﬁed. See Deﬁnition 1.

2

Our formal theorem statements for bilinear and higher degree multilinear functions appear as Theo-
rems 2 and 4 of Sections 3 and 4  respectively. Some further discussion of our results is in order:

• Under existence of external ﬁelds  it is easy to see that the above concentration does not
hold  even for bilinear functions. Motivated by our applications in Section 5 we extend the
above concentration of measure result to centered bilinear functions (where each variable
Xi appears as Xi − E[Xi] in the function) that also holds under arbitrary external ﬁelds; see
Theorem 3. We leave extensions of this result to higher degree multinear functions to the
next version of this paper.
• Moreover  notice that the tails for degree-2 functions are exponential and not Gaussian 
and this is unavoidable  and that as the degree grows the tails become heavier exponentials 
and this is also unavoidable. In particular  the tightness of our bound is justiﬁed in the
supplementary material.
• Lastly  like Chatterjee and Stroock and Zegarlinski  we prove our results under high tem-
perature. On the other hand  it is easy to construct low temperature Ising models where no
non-trivial concentration holds.3

With our theoretical understanding in hand  we proceed with an experimental evaluation of the efﬁcacy
of multilinear functions applied to hypothesis testing. Speciﬁcally  given a binary vector  we attempt
to determine whether or not it was generated by an Ising model. Our focus is on testing whether
choices in social networks can be approximated as an Ising model  a common and classical assumption
in the social sciences [Ell93  MS10]. We apply our method to both synthetic and real-world data.
On synthetic data  we investigate when our statistics are successful in detecting departures from the
Ising model. For our real-world data study  we analyze the Last.fm dataset from HetRec’11 [CBK11].
Interestingly  when considering musical preferences on a social network  we ﬁnd that the Ising model
may be more or less appropriate depending on the genre of music.

1.1 Related Work

As mentioned before  Chatterjee previously used the method of exchangeable pairs to prove variance
and concentration bounds for linear statistics of the Ising model [Cha05]. In [DDK18]  the authors
prove variance bounds for bilinear statistics. The present work improves upon this by proving
concentration rather than bounding the variance  as well as considering general degrees d rather than
just d = 2. In simultaneous work  Gheissari  Lubetzky  and Peres proved concentration bounds which
are qualitatively similar to ours  though the techniques are somewhat different [GLP17].

2 Preliminaries

We will state some preliminaries here  see the supplementary material for further preliminaries.
We deﬁne the high-temperature regime  also known as Dobrushin’s uniqueness condition – in this
paper  we will use the terms interchangeably.
Deﬁnition 1. Consider an Ising model p deﬁned on a graph G = (V  E) with |V | = n and parameter
u(cid:54)=v tanh (|θuv|) ≤ 1 − η for some η > 0. Then p is said to satisfy
vector (cid:126)θ. Suppose maxv∈V
Dobrushin’s uniqueness condition  or be in the η-high temperature regime.

(cid:80)

In some situations  we may use the parameter η implicitly and simply say the Ising model is in the
high temperature regime.
Glauber dynamics refers to the canonical Markov chain for sampling from an Ising model  see the
supplementary material for a formal deﬁnition. Glauber dynamics deﬁne a reversible  ergodic Markov
chain whose stationary distribution is identical to the corresponding Ising model. In many relevant
settings  including the high-temperature regime  the dynamics are rapidly mixing and hence offer an

the multilinear function f (X) =(cid:80)

3Consider an Ising model with no external ﬁelds  comprising two disjoint cliques of half the vertices with
inﬁnitely strong bonds; i.e. θv = 0 for all v  and θu v = ∞ if u and v belong to the same clique. Now consider
u(cid:54)∼v XuXv  wher u (cid:54)∼ v denotes that u and v are not neighbors (i.e. belong
to different cliques). It is easy to see that the maximum absolute value of f (X) is Ω(n2) and that there is no
concentration at radius better than some Ω(n2).

3

.

η

efﬁcient way to sample from Ising models. In particular  the mixing time in η-high-temperature is
tmix = n log n
We may couple two executions of the Glauber dynamics using a greedy coupling (also known as a
monotone coupling). Roughly  this couples the choices made by the runs to maximize the probability
of agreement; see the supplementary material for a formal deﬁnition. One of the key properties of
this coupling is that it satisﬁes the following contraction property:
Lemma 1. If p is an Ising model in η-high temperature  then the greedy coupling between two
executions satisﬁes the following contraction in Hamming distance:

(cid:12)(cid:12)(cid:12)(X (1)
of martingale increments  such that Si =(cid:80)i
time and K ≥ 0 be such that Pr[|Xi| ≤ K ∀ i ≤ τ ] = 1. Let vi = Var[Xi|Xi−1] and Vt =(cid:80)t

The key technical tool we use is the following concentration inequality for martingales:
Lemma 2 (Freedman’s Inequality (Proposition 2.1 in [Fre75])). Let X0  X1  . . .   Xt be a sequence
j=0 Xj forms a martingale sequence. Let τ be a stopping
i=0 vi.

(cid:105) ≤(cid:16)

0   X (2)
0 ).

1 − η
n

0   X (2)
0 )

dH (X (1)

t

dH (X (1)

  X (2)

)

t

(cid:17)t

(cid:104)

E

Then Pr[|St| ≥ r and Vt ≤ b for some t ≤ τ ] ≤ 2 exp

(cid:16)− r2

2(rK+b)

(cid:17)

.

3 Concentration of Measure for Bilinear Functions

In this section  we describe our main concentration result for bilinear functions of the Ising model.
This is not as technically involved as the result for general-degree multilinear functions  but exposes
many of the main conceptual ideas. The theorem statement is as follows:

Theorem 2. Consider any bilinear function fa(x) =(cid:80)
(cid:18)

u v auvxuxv on an Ising model p (deﬁned
on a graph G = (V  E) such that |V | = n) in η-high-temperature regime with no external ﬁeld. Let
(cid:107)a(cid:107)∞ = maxu v auv. If X ∼ p  then for any r ≥ 300(cid:107)a(cid:107)∞n log2 n/η + 2  we have

(cid:19)

Pr [|fa(X) − E [fa(X)]| ≥ r] ≤ 5 exp

−

ηr

1735(cid:107)a(cid:107)∞n log n

.

Remark 1. We note that η-high-temperature is not strictly needed for our results to hold – we only
need Hamming contraction of the “greedy coupling” (see Lemma 1). This condition implies rapid
mixing of the Glauber dynamics (in O(n log n) steps) via path coupling (Theorem 15.1 of [LPW09]).

3.1 Overview of the Technique

A well known approach to proving concentration inequalities for functions of dependent random
variables is via martingale tail bounds. For instance  Azuma’s inequality gives useful tail bounds
whenever one can bound the martingale increments (i.e.  the differences between consecutive terms
of the martingale sequence) of the underlying martingale in absolute value  without requiring any
form of independence. Such an approach is fruitful in showing concentration of linear functions on
the Ising model in high temperature. The Glauber dynamics associated with Ising models in high
temperature are fast mixing and offer a natural way to deﬁne a martingale sequence. In particular 
consider the Doob martingale corresponding to any linear function f for which we wish to show
concentration  deﬁned on the state of the dynamics at some time step t∗  i.e. f (Xt∗ ). If we choose
t∗ larger than O(n log n) then f (Xt∗ ) would be very close to a sample from p irrespective of the
starting state. We set the ﬁrst term of the martingale sequence as E[f (Xt∗ )|X0] and the last term is
simply f (Xt∗ ). By bounding the martingale increments we can show that |f (Xt∗ ) − E[f (Xt∗ )|X0]|
concentrates at the right radius with high probability. By making t∗ large enough we can argue that
E[f (Xt∗ )|X0] ≈ E[f (X)]. Also  crucially  t∗ need not be too large since the dynamics are fast
√
mixing. Hence we don’t incur too big a hit when applying Azuma’s inequality  and one can argue
that linear functions are concentrated with a radius of ˜O(
n). Crucial to this argument is the fact
that linear functions are O(1)-Lipschitz (when the entries of a are constant)  bounding the Doob
martingale differences to be O(1).
The challenge with bilinear functions is that they are O(n)-Lipschitz – a naive application of the
same approach gives a radius of concentration of ˜O(n3/2)  which albeit better than the trivial radius

4

of O(n2) is not optimal. To show stronger concentration for bilinear functions  at a high level  the
idea is to bootstrap the known fact that linear functions of the Ising model concentrate well at high
temperature.
The key insight is that  when we have a d-linear function  its Lipschitz constants are bounds on the
absolute values of certain d− 1-linear functions. In particular  this implies that the Lipschitz constants
of a bilinear function are bounds on the absolute values of certain associated linear functions. And
√
although a worst case bound on the absolute value of linear functions with bounded coefﬁcients
would be O(n)  the fact that linear functions are concentrated within a radius of ˜O(
√
n)  means
that bilinear functions are ˜O(
n)-Lipschitz in spirit. In order to exploit this intuition  we turn to
more sophisticated concentration inequalities  namely Freedman’s inequality (Lemma 2). This is
a generalization of Azuma’s inequality  which handles the case when the martingale differences
are only bounded until some stopping time (very roughly  the ﬁrst time we reach a state where the
expectation of the linear function after mixing is large). To apply Freedman’s inequality  we would
need to deﬁne a stopping time which has two properties:

1. The stopping time is larger than t∗ with high probability. Hence  with a good probability the
process doesn’t stop too early. The harm if the process stops too early (at t < t∗) is that we
will not be able to effectively decouple E [fa(Xt)|X0] from the choice of X0. t∗ is chosen
to be larger than the mixing time of the Glauber dynamics precisely because it allows us to
argue that E [fa(Xt∗ )|X0] ≈ E [fa(Xt∗ )] = E[fa(X)].
√
|Bi+1 − Bi| = O(

2. For all times i + 1 less than the stopping time  the martingale increments are bounded  i.e.

n) where {Bi}i≥0 is the martingale sequence.

√

We observe that the martingale increments corresponding to a martingale deﬁned on a bilinear
√
function have the ﬂavor of the conditional expectations of certain linear functions which can be shown
to concentrate at a radius ˜O(
n) when the process starts at its stationary distribution. This provides
us with a nice way of deﬁning the stopping time to be the ﬁrst time when one of these conditional
expectations deviates by more than Ω(
n poly log n) from the origin. More precisely  we deﬁne
K(t) of conﬁgurations xt  which is parameterized by a function fa(X) and parameter K
a set Ga
√
(which we will take to be ˜Ω(
a (Xt∗ ) conditioned
a are linear functions which arise when examining the evolution of fa over
on Xt = xt  where f v
steps of the Glauber dynamics. Ga
K(t) are the set of conﬁgurations for which all such linear functions
satisfy certain conditions  including bounded expectation and concentration around their mean. The
stopping time for our process TK is deﬁned as the ﬁrst time we have a conﬁguration which leaves
this set Ga
Lemma 3. For any t ≥ 0  for t∗ = 3tmix 
Pr [Xt /∈ Ga

K(t). We can show that the stopping time is large via the following lemma:

n)). The objects of interest are linear functions f v

K(t)] ≤ 8n exp

(cid:18)

(cid:19)

.

− K 2
8t∗

Next  we require a bound on the conditional variance of the martingale increments. This can be
shown using the property that the martingale increments are bounded up until the stopping time:
Lemma 4. Consider the Doob martingale where Bi = E[fa(Xt∗ )|Xi]. Suppose Xi ∈ Ga
Xi+1 ∈ Ga

K(i) and

K(i + 1). Then

|Bi+1 − Bi| ≤ 16K + 16n2 exp

(cid:18)

− K 2
16t∗

(cid:19)

.

With these two pieces in hand  we can apply Freedman’s inequality to bound the desired quantity.
It is worth noting that the martingale approach described above closely relates to the technique
of exchangeable pairs exposited by Chatterjee [Cha05]. When we look at differences for the
martingale sequence deﬁned using the Glauber dynamics  we end up analyzing an exchangeable
pair of the following form: sample X ∼ p from the Ising model. Take a step along the Glauber
dynamics starting from X to reach X(cid:48). (X  X(cid:48)) forms an exchangeable pair. This is precisely how
Chatterjee’s application of exchangeable pairs is set up. Chatterjee then goes on to study a function
of X and X(cid:48) which serves as a proxy for the variance of f (X) and obtains concentration results
by bounding the absolute value of this function. The deﬁnition of the function involves consider-
ing two greedily coupled runs of the Glauber dynamics just as we do in our martingale based approach.

5

√
To summarize  our proof of bilinear concentration involves showing various concentration proper-
ties for linear functions via Azuma’s inequality  showing that the martingale has ˜O(
n)-bounded
differences before our stopping time  proving that the stopping time is larger than the mixing time
with high probability  and combining these ingredients using Freedman’s inequality. Full details are
provided in the supplementary material.

3.2 Concentration Under an External Field

Under an external ﬁeld  not all bilinear functions concentrate nicely even in the high temperature
regime – in particular  they may concentrate with a radius of Θ(n1.5)  instead of O(n). As such 
we must instead consider “recentered” statistics to obtain the same radius of concentration. The
following theorem is proved in the supplementary material:
u v auv(Xu −
Theorem 3.
E[Xu])(Xv − E[Xv]) satisfy the following inequality at high temperature. There exist
absolute constants c and c(cid:48) such that  for r ≥ cn log2 n/η 
−

1. Bilinear functions on the Ising model of the form fa(X) =(cid:80)
(cid:19)
2. Bilinear functions on the Ising model of the form fa(X (1)  X (2)) = (cid:80)

u −
u )(X (1)
X (2)
v )  where X (1)  X (2) are two i.i.d samples from the Ising model  satisfy
the following inequality at high temperature. There exist absolute constants c and c(cid:48) such
that  for r ≥ cn log2 n/η 

Pr [|fa(X) − E[fa(X)]| ≥ r] ≤ 4 exp

u v auv(X (1)

v − X (2)

c(cid:48)n log n

(cid:18)

r

.

(cid:104)(cid:12)(cid:12)(cid:12)fa(X (1)  X (2)) − E[fa(X (1)  X (2))]
(cid:12)(cid:12)(cid:12) ≥ r

(cid:105) ≤ 4 exp

Pr

(cid:18)

−

r

c(cid:48)n log n

(cid:19)

.

4 Concentration of Measure for d-linear Functions

More generally  we can show concentration of measure for d-linear functions on an Ising model in
high temperature  when d ≥ 3. Again  we will focus on the setting with no external ﬁeld. Although
we will follow a recipe similar to that used for bilinear functions  the proof is more involved and
requires some new deﬁnitions and tools. The proof will proceed by induction on the degree d. Due to
the proof being more involved  for ease of exposition  we present the proof of Theorem 4 without
explicit values for constants.
Our main theorem statement is the following:

Theorem 4. Consider any degree-d multilinear function fa(x) =(cid:80)

u∈U xu on an
Ising model p (deﬁned on a graph G = (V  E) such that |V | = n) in η-high-temperature regime
with no external ﬁeld. Let (cid:107)a(cid:107)∞ = maxU⊆V :|U|=d |aU|. There exist constants C1 = C1(d) > 0 and
C2 = C2(d) > 0 depending only on d  such that if X ∼ p  then for any r ≥ C1(cid:107)a(cid:107)∞(n log2 n/η)d/2 
we have

U⊆V :|U|=d aU

(cid:81)

(cid:32)

(cid:33)

.

Pr [|fa(X) − E [fa(X)]| > r] ≤ 2 exp

−

ηr2/d

C2(cid:107)a(cid:107)2/d∞ n log n

Similar to Remark 1  our theorem statement still holds under the weaker assumption of Hamming
contraction. This bound is also tight up to polylogarithmic factors in the radius of concentration and
the exponent of the tail bound  see Remark 1 in the supplementary material.

4.1 Overview of the Technique

Our approach uses induction and is similar to the one used for bilinear functions. To show concentra-
tion for d-linear functions we will use the concentration of (d − 1)-linear functions together with
Freedman’s martingale inequality.
Consider the following process: Sample X0 ∼ p from the Ising model of interest. Starting at X0 
run the Glauber dynamics associated with p for t∗ = (d + 1)tmix steps. We will study the target

6

quantity  Pr [|fa(Xt∗ ) − E[fa(Xt∗ )|X0]| > K]  by deﬁning a martingale sequence similar to the one
in the bilinear proof. However  to bound the increments of the martingale for d-linear functions we
will require an induction hypothesis which is more involved. The reason is that with higher degree
multilinear functions (d > 2)  the argument for bounding increments of the martingale sequence runs
into multilinear terms which are a function of not just a single instance of the dynamics Xt  but also
of the conﬁguration obtained from the coupled run  X(cid:48)
t. We call such multilinear terms hybrid terms
and multilinear functions involving hybrid terms as hybrid multilinear functions henceforth. Since the
two runs (of the Glauber dynamics) are coupled greedily to maximize the probability of agreement
and they start with a small Hamming distance from each other (≤ 1)  these hybrid terms behave very
similar to the non-hybrid multilinear terms. Showing that their behavior is similar  however  requires
some supplementary statements about them which are presented in the supplementary material.
In addition to the martingale technique of Section 3  an ingredient that is crucial to the proving
concentration for d ≥ 3 is a bound on the magnitude of the (d − 1)-order marginals of the Ising
model:
Lemma 5. Consider any Ising model p at high temperature. Let d be a positive integer. We have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2

(cid:18) 4nd log n

(cid:19)d/2

.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:88)

u1 ... ud

Ep[Xu1Xu2 . . .   Xud ]

η

This is because when studying degree d ≥ 3 functions we ﬁnd ourselves having to bound expected
values of degree d − 1 multilinear functions on the Ising model. A naive bound of Od(nd−1) can
be argued for these functions but by exploiting the fact that we are in high temperature  we can
show a bound of Od(n(d−1)/2) via a coupling with the Fortuin-Kastelyn model. When d = 2 
(d − 1)-linear functions are just linear functions which are zero mean. However  for d ≥ 3  this is not
the case. Hence  we ﬁrst need to prove this desired bound on the marginals of an Ising model in high
temperature.
Further details are provided in the supplementary material.

5 Experiments

In this section  we apply our family of bilinear statistics on the Ising model to a problem of statistical
hypothesis testing. Given a single sample from a multivariate distribution  we attempt to determine
whether or not this sample was generated from an Ising model in the high-temperature regime. More
speciﬁcally  the null hypothesis is that the sample is drawn from an Ising model with a known graph
structure with a common edge parameter and a uniform node parameter (which may potentially be
known to be 0). In Section 5.1  we apply our statistics to synthetic data. In Section 5.2  we turn our
attention to the Last.fm dataset from HetRec 2011 [CBK11].
The running theme of our experimental investigation is testing the classical and common assumption
which models choices in social networks as an Ising model [Ell93  MS10]. To be more concrete 
choices in a network could include whether to buy an iPhone or an Android phone  or whether
to vote for a Republican or Democratic candidate. Such choices are naturally inﬂuenced by one’s
neighbors in the network – one may be more likely to buy an iPhone if he sees all his friends have
one  corresponding to an Ising model with positive-weight edges4 In our synthetic data study  we will
leave these choices as abstract  referring to them only as “values ” but in our Last.fm data study  these
choices will be whether or not one listens to a particular artist.
Our general algorithmic approach is as follows. Given a single multivariate sample  we ﬁrst run the
maximum pseudo-likelihood estimator (MPLE) to obtain an estimate of the model’s parameters. The
MPLE is a canonical estimator for the parameters of the Ising model  and it enjoys strong consistency
guarantees in many settings of interest [Cha07  BM16]. If the MPLE gives a large estimate of the
model’s edge parameter  this is sufﬁcient evidence to reject the null hypothesis. Otherwise  we use
Markov Chain Monte Carlo (MCMC) on a model with the MPLE parameters to determine a range
of values for our statistic. We note that  to be precise  we would need to quantify the error incurred
by the MPLE – in favor of simplicity in our exploratory investigation  we eschew this detail  and

4Note that one may also decide against buying an iPhone in this scenario  if one places high value on

individuality and uniqueness – this corresponds to negative-weight edges.

7

at this point attempt to reject the null hypothesis of the model learned by the MPLE. Our statistic
is bilinear in the Ising model  and thus enjoys the strong concentration properties explained earlier
in this paper. Note that since the Ising model will be in the high-temperature regime  the Glauber
dynamics mix rapidly  and we can efﬁciently sample from the model using MCMC. Finally  given the
range of values for the statistic determined by MCMC  we reject the null hypothesis if p ≤ 0.05.

5.1 Synthetic Data

2)

u=(i j)

We proceed with our investigation on synthetic data. Our null hypothesis is that the sample is
generated from an Ising model in the high temperature regime on the grid  with no external ﬁeld (i.e.
θu = 0 for all u) and a common (unknown) edge parameter θ (i.e.  θuv = θ iff nodes u and v are
adjacent in the grid  and 0 otherwise). For the Ising model on the grid  the critical edge parameter for
√
. In other words  we are in high-temperature if and only if θ ≤ θc 
high-temperature is θc = ln(1+
2
and we can reject the null hypothesis if the MPLE estimate ˆθ > θc.
To generate departures from the null hypothesis  we give a construction parameterized by τ ∈ [0  1].
We provide a rough description of the departures  for a precise description  see the supplemental
material. Each node x selects a random node y at Manhattan distance at most 2  and sets y’s value
to x with probability τ. The intuition behind this construction is that each individual selects a
friend or a friend-of-a-friend  and tries to convince them to take his value – he is successful with
probability τ. Selecting either a friend or a friend-of-a-friend is in line with the concept of strong
triadic closure [EK10] from the social sciences  which suggests that two individuals with a mutual
friend are likely to either already be friends (which the social network may not have knowledge of) or
become friends in the future.
An example of a sample generated from this distribution with τ = 0.04 is provided in Figure 1 of the
supplementary material  alongside a sample from the Ising model generated with the corresponding
MPLE parameters. We consider this distribution to pass the “eye test” – one can not easily distinguish
these two distributions by simply glancing at them. However  as we will see  our multilinear statistic
is able to correctly reject the null a large fraction of the time.
Our experimental process was as follows. We started with a 40 × 40 grid  corresponding to a
distribution with n = 1600 dimensions. We generated values for this grid according to the depatures
from the null described above  with some parameter τ. We then ran the MPLE estimator to obtain an
estimate for the edge parameter ˆθ  immediately rejecting the null if ˆθ > θc. Otherwise  we ran the
(cid:80)
Glauber dynamics for O(n log n) steps to generate a sample from the grid Ising model with parameter
ˆθ. We repeated this process to generate 100 samples  and for each sample  computed the value of
v=(k l):d(u v)≤2 XuXv  where d(· ·) is the Manhattan distance on
the grid. This statistic can be justiﬁed since we wish to account for the possibility of connections
between friends-of-friends of which the social network may be lacking knowledge. We then compare
with the value of the statistic Zlocal on the provided sample  and reject the null hypothesis if this
statistic corresponds to a p-value of ≤ 0.05. We repeat this for a wide range of values of τ ∈ [0  1] 
and repeat 500 times for each τ.
Our results are displayed in Figure 1 The x-axis marks the value of parameter τ  and the y-axis
indicates the fraction of repetitions in which we successfully rejected the null hypothesis. The
performance of the MPLE alone is indicated by the orange line  while the performance of our statistic
is indicated by the blue line. We ﬁnd that our statistic is able to correctly reject the null at a much
earlier point than the MPLE alone. In particular  our statistic manages to reject the null for τ ≥ 0.04 
while the MPLE requires a parameter which is an order of magnitude larger  at 0.4. As mentioned
before  in the former regime (when τ ≈ 0.04)  it appears impossible to distinguish the distribution
from a sample from the Ising model with the naked eye.

the statistic Zlocal =(cid:80)

5.2 Last.fm Dataset

We now turn our focus to the Last.fm dataset from HetRec’11 [CBK11]. This dataset consists of
data from n = 1892 users on the Last.fm online music system. On Last.fm  users can indicate
(bi-directional) friend relationships  thus constructing a social network – our dataset has m = 12717
such edges. The dataset also contains users’ listening habits – for each user we have a list of their

8

Figure 1: Power of our statistic on synthetic data.

u

sample  computed the value of the statistics Zk =(cid:80)

ﬁfty favorite artists  whose tracks they have listened to the most times. We wish to test whether users’
preference for a particular artist is distributed according to a high-temperature Ising model.
Fixing some artist a of interest  we consider the vector X (a)  where X (a)
is +1 if user u has artist
a in his favorite artists  and −1 otherwise. We wish to test the null hypothesis  whether X (a) is
distributed according to an Ising model in the high temperature regime on the known social network
graph  with common (unknown) external ﬁeld h (i.e. θu = h for all u) and edge parameter θ (i.e. 
θuv = θ iff u and v are neighbors in the graph  and 0 otherwise).
Our overall experimental process was very similar to the synthetic data case. We gathered a list of the
ten most-common favorite artists  and repeated the following process for each artist a. We consider
(cid:80)
the vector X (a) (deﬁned above) and run the MPLE estimator on it  obtaining estimates ˆh and ˆθ. We
then run MCMC to generate 100 samples from the Ising model with these parameters  and for each
v:d(u v)≤k(Xu − tanh(ˆh))(Xv − tanh(ˆh)) 
where d(· ·) is the distance on the graph  and k = 1 (the neighbor correlation statistic) or 2 (the
local correlation statistic). Motivated by our theoretical results (Theorem 3)  we consider a statistic
where the variables are recentered by their marginal expectations  as this statistic experiences sharper
concentration. We again consider k = 2 to account for the possibility of edges which are unknown to
the social network.
Strikingly  we found that the plausibility of the Ising modelling assumption varies signiﬁcantly
depending on the artist. We highlight some of our more interesting ﬁndings here  see the supplemental
material for more details. The most popular artist in the dataset was Lady Gaga  who was a favorite
artist of 611 users in the dataset. We found that X (Lady Gaga) had statistics Z1 = 9017.3 and
Z2 = 106540. The range of these statistics computed by MCMC can be seen in Figure 2 of the
supplementary material – clearly  the computed statistics fall far outside these ranges  and we can
reject the null hypothesis with p (cid:28) 0.01. Similar results held for other popular pop musicians 
including Britney Spears  Christina Aguilera  Rihanna  and Katy Perry.
However  we observed qualitatively different results for The Beatles  the fourth most popular artist 
being a favorite of 480 users. We found that X (The Beatles) had statistics Z1 = 2157.8 and Z2 =
22196. The range of these statistics computed by MCMC can be seen in Figure 3 of the supplementary
material. This time  the computed statistics fall near the center of this range  and we can not reject
the null. Similar results held for the rock band Muse.
Based on our investigation  our statistic seems to indicate that for the pop artists  the null fails to
effectively model the distribution  while it performs much better for the rock artists. We conjecture
that this may be due to the highly divisive popularity of pop artists like Lady Gaga and Britney Spears
– while some users may love these artists (and may form dense cliques within the graph)  others have
little to no interest in their music. The null would have to be expanded to accomodate heterogeneity
to model such effects. On the other hand  rock bands like The Beatles and Muse seem to be much
more uniform in their appeal: users seem to be much more homogeneous when it comes to preference
for these groups.

u

9

10-310-210-1100Model parameter value00.10.20.30.40.50.60.70.80.91Test probability of successProbability of rejecting the null with local correlations and MPLELocal correlation statisticMPLEAcknowledgments

Research was supported by NSF CCF-1617730  CCF-1650733  and ONR N00014-12-1-0999. Part
of this work was done while GK was an intern at Microsoft Research New England.

References

[AKN06] Pieter Abbeel  Daphne Koller  and Andrew Y. Ng. Learning factor graphs in polynomial
time and sample complexity. Journal of Machine Learning Research  7(Aug):1743–
1788  2006.

[BGS14] Guy Bresler  David Gamarnik  and Devavrat Shah. Structure learning of antiferromag-
netic Ising models. In Advances in Neural Information Processing Systems 27  NIPS
’14  pages 2852–2860. Curran Associates  Inc.  2014.

[Bha16] Bhaswar B. Bhattacharya. Power of graph-based two-sample tests. arXiv preprint

arXiv:1508.07530  2016.

[BK16] Guy Bresler and Mina Karzand. Learning a tree-structured Ising model in order to

make predictions. arXiv preprint arXiv:1604.06749  2016.

[BM16] Bhaswar B. Bhattacharya and Sumit Mukherjee. Inference in Ising models. Bernoulli 

2016.

[Bre15] Guy Bresler. Efﬁciently learning ising models on arbitrary graphs. In Proceedings
of the 47th Annual ACM Symposium on the Theory of Computing  STOC ’15  pages
771–782  New York  NY  USA  2015. ACM.

[CBK11] Iván Cantador  Peter Brusilovsky  and Tsvi Kuﬂik. Second workshop on information
heterogeneity and fusion in recommender systems (hetrec 2011). In Proceedings of
the 5th ACM Conference on Recommender Systems  RecSys ’11  pages 387–388  New
York  NY  USA  2011. ACM.

[Cha05] Sourav Chatterjee. Concentration Inequalities with Exchangeable Pairs. PhD thesis 

Stanford University  June 2005.

[Cha07] Sourav Chatterjee. Estimation in spin glasses: A ﬁrst step. The Annals of Statistics 

35(5):1931–1946  October 2007.

[CL68] C.K. Chow and C.N. Liu. Approximating discrete probability distributions with
dependence trees. IEEE Transactions on Information Theory  14(3):462–467  1968.

[CT06] Imre Csiszár and Zsolt Talata. Consistent estimation of the basic neighborhood of

Markov random ﬁelds. The Annals of Statistics  34(1):123–145  2006.

[DCG68] Stanley Deser  Max Chrétien  and Eugene Gross. Statistical Physics  Phase Transitions 

and Superﬂuidity. Gordon and Breach  1968.

[DDK18] Constantinos Daskalakis  Nishanth Dikkala  and Gautam Kamath. Testing Ising models.
In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms 
SODA ’18  Philadelphia  PA  USA  2018. SIAM.

[DMR11] Constantinos Daskalakis  Elchanan Mossel  and Sébastien Roch. Evolutionary trees
and the Ising model on the Bethe lattice: A proof of Steel’s conjecture. Probability
Theory and Related Fields  149(1):149–189  2011.

[EK10] David Easley and Jon Kleinberg. Networks  Crowds  and Markets: Reasoning about a

Highly Connected World. Cambridge University Press  2010.

[Ell93] Glenn Ellison. Learning  local interaction  and coordination. Econometrica  61(5):1047–

1071  1993.

[Fel04] Joseph Felsenstein. Inferring Phylogenies. Sinauer Associates Sunderland  2004.

10

[Fre75] David A. Freedman. On tail probabilities for martingales. The Annals of Probability 

3(1):100–118  1975.

[GG86] Stuart Geman and Christine Grafﬁgne. Markov random ﬁeld image models and their
In Proceedings of the International Congress of

applications to computer vision.
Mathematicians  pages 1496–1517. American Mathematical Society  1986.

[GLP17] Reza Gheissari  Eyal Lubetzky  and Yuval Peres. Concentration inequalities for poly-

nomials of contracting Ising models. arXiv preprint arXiv:1706.00121  2017.

[HKM17] Linus Hamilton  Frederic Koehler  and Ankur Moitra. Information theoretic properties
of Markov random ﬁelds  and their algorithmic applications. In Advances in Neural
Information Processing Systems 30  NIPS ’17. Curran Associates  Inc.  2017.

[Hop82] John J. Hopﬁeld. Neural networks and physical systems with emergent collective
computational abilities. Proceedings of the National Academy of Sciences  79(8):2554–
2558  1982.

[Isi25] Ernst Ising. Beitrag zur theorie des ferromagnetismus. Zeitschrift für Physik A Hadrons

and Nuclei  31(1):253–258  1925.

[JJR11] Ali Jalali  Christopher C. Johnson  and Pradeep K. Ravikumar. On learning discrete
graphical models using greedy methods. In Advances in Neural Information Processing
Systems 24  NIPS ’11  pages 1935–1943. Curran Associates  Inc.  2011.

[KM17] Adam Klivans and Raghu Meka. Learning graphical models using multiplicative
In Proceedings of the 58th Annual IEEE Symposium on Foundations of
weights.
Computer Science  FOCS ’17  Washington  DC  USA  2017. IEEE Computer Society.

[LPW09] David A. Levin  Yuval Peres  and Elizabeth L. Wilmer. Markov Chains and Mixing

Times. American Mathematical Society  2009.

[MdCCU16] Abraham Martín del Campo  Sarah Cepeda  and Caroline Uhler. Exact goodness-of-ﬁt

testing for the Ising model. Scandinavian Journal of Statistics  2016.

[MJC+14] Lester Mackey  Michael I. Jordan  Richard Y. Chen  Brendan Farrell  and Joel A. Tropp.
Matrix concentration inequalities via the method of exchangeable pairs. The Annals of
Probability  42(3):906–945  2014.

[MS10] Andrea Montanari and Amin Saberi. The spread of innovations in social networks.

Proceedings of the National Academy of Sciences  107(47):20196–20201  2010.

[Ons44] Lars Onsager. Crystal statistics. I. a two-dimensional model with an order-disorder

transition. Physical Review  65(3–4):117  1944.

[RWL10] Pradeep Ravikumar  Martin J. Wainwright  and John D. Lafferty. High-dimensional
ising model selection using (cid:96)1-regularized logistic regression. The Annals of Statistics 
38(3):1287–1319  2010.

[SBSB06] Elad Schneidman  Michael J. Berry  Ronen Segev  and William Bialek. Weak pairwise
correlations imply strongly correlated network states in a neural population. Nature 
440(7087):1007–1012  2006.

[SK75] David Sherrington and Scott Kirkpatrick. Solvable model of a spin-glass. Physical

Review Letters  35(26):1792  1975.

[SW12] Narayana P. Santhanam and Martin J. Wainwright. Information-theoretic limits of se-
lecting binary graphical models in high dimensions. IEEE Transactions on Information
Theory  58(7):4117–4134  2012.

[SZ92] Daniel W. Stroock and Boguslaw Zegarlinski. The logarithmic Sobolev inequality
for discrete spin systems on a lattice. Communications in Mathematical Physics 
149(1):175–193  1992.

11

[VMLC16] Marc Vuffray  Sidhant Misra  Andrey Lokhov  and Michael Chertkov. Interaction
screening: Efﬁcient and sample-optimal learning of Ising models. In Advances in
Neural Information Processing Systems 29  NIPS ’16  pages 2595–2603. Curran
Associates  Inc.  2016.

12

,Halid Yerebakan
Bartek Rajwa
Murat Dundar
Constantinos Daskalakis
Nishanth Dikkala
Gautam Kamath