2011,MAP Inference for Bayesian Inverse Reinforcement Learning,The difficulty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an infinite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework  we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function  and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms.,MAP Inference for

Bayesian Inverse Reinforcement Learning

Jaedeug Choi and Kee-Eung Kim
bDepartment of Computer Science

Korea Advanced Institute of Science and Technology

Daejeon 305-701  Korea

jdchoi@ai.kaist.ac.kr  kekim@cs.kaist.ac.kr

Abstract

The difﬁculty in inverse reinforcement learning (IRL) arises in choosing the best
reward function since there are typically an inﬁnite number of reward functions
that yield the given behaviour data as optimal. Using a Bayesian framework  we
address this challenge by using the maximum a posteriori (MAP) estimation for
the reward function  and show that most of the previous IRL algorithms can be
modeled into our framework. We also present a gradient method for the MAP es-
timation based on the (sub)differentiability of the posterior distribution. We show
the effectiveness of our approach by comparing the performance of the proposed
method to those of the previous algorithms.

1

Introduction

The objective of inverse reinforcement learning (IRL) is to determine the decision making agent’s
underlying reward function from its behaviour data and the model of environment [1]. The signiﬁ-
cance of IRL has emerged from problems in diverse research areas. In animal and human behaviour
studies [2]  the agent’s behaviour could be understood by the reward function since the reward func-
tion reﬂects the agent’s objectives and preferences. In robotics [3]  IRL provides a framework for
making robots learn to imitate the demonstrator’s behaviour using the inferred reward function.
In other areas related to reinforcement learning  such as neuroscience [4] and economics [5]  IRL
addresses the non-trivial problem of ﬁnding an appropriate reward function when building a com-
putational model for decision making.

In IRL  we generally assume that the agent is an expert in the problem domain and hence it be-
haves optimally in the environment. Using the Markov decision process (MDP) formalism  the IRL
problem is deﬁned as ﬁnding the reward function that the expert is optimizing given the behaviour
data of state-action histories and the environment model of state transition probabilities. In the last
decade  a number of studies have addressed IRL in a direct (reward learning) and indirect (policy
learning by inferring the reward function  i.e.  apprenticeship learning) fashions. Ng and Russell [6]
proposed a sufﬁcient and necessary condition on the reward functions that guarantees the optimality
of the expert’s policy and formulated a linear programming (LP) problem to ﬁnd the reward func-
tion from the behaviour data. Extending their work  Abbeel and Ng [7] presented an algorithm for
ﬁnding the expert’s policy from its behaviour data with a performance guarantee on the learned pol-
icy. Ratliff et al. [8] applied the structured max-margin optimization to IRL and proposed a method
for ﬁnding the reward function that maximizes the margin between the expert’s policy and all other
policies. Neu and Szepesv´ari [9] provided an algorithm for ﬁnding the policy that minimizes the
deviation from the behaviour. Their algorithm uniﬁes the direct method that minimizes a loss func-
tion of the deviation and the indirect method that ﬁnds an optimal policy from the learned reward
function using IRL. Syed and Schapire [10] proposed a method to ﬁnd a policy that improves the
expert’s policy using a game-theoretic framework. Ziebart et al. [11] adopted the principle of the

1

maximum entropy for learning the policy whose feature expectations are constrained to match those
of the expert’s behaviour. In addition  Neu and Szepesv´ari [12] provided a (non-Bayesian) uniﬁed
view for comparing the similarities and differences among previous IRL algorithms.

IRL is an inherently ill-posed problem since there may be an inﬁnite number of reward functions
that yield the expert’s policy as optimal. Previous approaches summarized above employ various
preferences on the reward function to address the non-uniqueness. For example  Ng and Russell [6]
search for the reward function that maximizes the difference in the values of the expert’s policy and
the second best policy. More recently  Ramachandran and Amir [13] presented a Bayesian approach
formulating the reward preference as the prior and the behaviour compatibility as the likelihood  and
proposed a Markov chain Monte Carlo (MCMC) algorithm to ﬁnd the posterior mean of the reward
function.

In this paper  we propose a Bayesian framework subsuming most of the non-Bayesian IRL algo-
rithms in the literature. This is achieved by searching for the maximum-a-posteriori (MAP) reward
function  in contrast to computing the posterior mean. We show that the posterior mean can be prob-
lematic for the reward inference since the loss function is integrated over the entire reward space 
even including those inconsistent with the behaviour data. Hence  the inferred reward function can
induce a policy much different from the expert’s policy. The MAP estimate  however  is more ro-
bust in the sense that the objective function (the posterior probability in our case) is evaluated on
a single reward function. In order to ﬁnd the MAP reward function  we present a gradient method
using the differentiability result of the posterior  and show the effectiveness of our approach through
experiments.

2 Preliminaries

2.1 MDPs

A Markov decision process (MDP) is deﬁned as a tuple hS  A  T  R  γ  αi: S is the ﬁnite set of
states; A is the ﬁnite set of actions; T is the state transition function where T (s  a  s′) denotes the
probability P (s′|s  a) of changing to state s′ from state s by taking action a; R is the reward function
where R(s  a) denotes the immediate reward of executing action a in state s  whose absolute value
is bounded by Rmax; γ ∈ [0  1) is the discount factor; α is the initial state distribution where
α(s) denotes the probability of starting in state s. Using matrix notations  the transition function is
denoted as an |S||A| × |S| matrix T   and the reward function is denoted as an |S||A|-dimensional
vector R.

A policy is deﬁned as a mapping π : S → A. The value of policy π is the expected discounted
return of executing the policy and deﬁned as V π = E [P∞
t=0 γtR(st  at)|α  π] where the initial
state s0 is determined according to initial state distribution α and action at is chosen by policy π
in state st. The value function of policy π for each state s is computed by V π(s) = R(s  π(s)) +
γ Ps′∈S T (s  π(s)  s′)V π(s′) such that the value of policy π is calculated by V π = Ps α(s)V π(s).
Similarly  the Q-function is deﬁned as Qπ(s  a) = R(s  a) + γ Ps′∈S T (s  a  s′)V π(s′). We can
rewrite the equations for the value function and the Q-function in matrix notations as

V π = Rπ + γT πV π  Qπ

a = Ra + γT aV π

(1)

where T π is an |S| × |S| matrix with the (s  s′) element being T (s  π(s)  s′)  T a is an |S| × |S|
matrix with the (s  s′) element being T (s  a  s′)  Rπ is an |S|-dimensional vector with the s-th
element being R(s  π(s))  Ra is an |S|-dimensional vector with the s-th element being R(s  a)  and
Qπ

a is an |S|-dimensional vector with the s-th element being Qπ(s  a).

An optimal policy π∗ maximizes the value function for all the states  and thus should satisfy
the Bellman optimality equation: π is an optimal policy if and only if for all s ∈ S  π(s) ∈
argmaxa∈A Qπ(s  a). We denote V ∗ = V π∗ and Q∗ = Qπ∗.
When the state space is large  the reward function is often linearly parameterized: R(s  a) =
Pd
: S × A → R and the weight vector w =
[w1  w2  · · ·   wd]. Each basis function φi has a corresponding basis value V π
i =
E [P∞

i=1 wiφi(s  a) with the basis functions φi

i of policy π : V π

t=0 γtφi(st  at)|α  π].

2

We also assume that the expert’s behaviour is given as the set X of M trajectories executed by
the expert’s policy πE  where the m-th trajectory is an H-step sequence of state-action pairs:
H )}. Given the set of trajectories  the value and the basis value
{(sm
of the expert’s policy πE can be empirically estimated by

2 )  · · ·   (sm

1 )  (sm

2   am

1   am

H   am

ˆV E = 1

M PM
In addition  we can empirically estimate the expert’s policy ˆπE and its state visitation frequency ˆµE
from the trajectories:

h=1 γh−1φi(sm

h=1 γh−1R(sm

m=1 PH

m=1 PH

M PM

ˆV E
i = 1

h   am

h   am

h ).

h ) 

ˆπE(s  a) = PM

h=1

m=1 PH
PM

m=1 PH

1(sm

h =s∧am

h =a)

h=1

1(sm

h =s)

 

ˆµE(s) =

1

M H PM

m=1 PH

h=1

1(sm

h =s).

In the rest of the paper  we use the notation f (R) or f (x; R) for function f in order to be explicit
that f is computed using reward function R. For example  the value function V π(s; R) denotes the
value of policy π for state s using reward function R.

2.2 Reward Optimality Condition

Ng and Russell [6] presented a necessary and sufﬁcient condition for reward function R of an MDP
to guarantee the optimality of policy π: Qπ
a (R) ≤ V π(R) for all a ∈ A. From the condition 
we obtain the following corollary (although it is a succinct reformulation of the theorem in [6]  the
proof is provided in the supplementary material).

Corollary 1 Given an MDP\R hS  A  T  γ  αi  policy π is optimal if and only if reward function R
satisﬁes

hI − (I A − γT )(I − γT π)−1Eπi R ≤ 0 

(2)

where Eπ is an |S| × |S||A| matrix with the (s  (s′  a′)) element being 1 if s = s′ and π(s′) = a′ 
and I A is an |S||A| × |S| matrix constructed by stacking the |S| × |S| identity matrix |A| times.

We refer to Equation (2) as the reward optimality condition w.r.t. policy π. Since the linear in-
equalities deﬁne the region of the reward functions that yield policy π as optimal  we refer to the
region bounded by Equation (2) as the reward optimality region w.r.t. policy π. Note that there ex-
ist inﬁnitely many reward functions in the reward optimality region even including constant reward
functions (e.g. R = c1 where c ∈ [−Rmax  Rmax]). In other words  even when we are presented
with the expert’s policy  there are inﬁnitely many reward functions to choose from  including the de-
generate ones. To resolve this non-uniqueness in solutions  IRL algorithms in the literature employ
various preferences on reward functions.

2.3 Bayesian framework for IRL (BIRL)

Ramachandran and Amir [13] proposed a Bayesian framework for IRL by encoding the reward
function preference as the prior and the optimality conﬁdence of the behaviour data as the likelihood.
We refer to their work as BIRL.

Assuming the rewards are i.i.d.  the prior in BIRL is computed by
P (R) = Qs∈S a∈A P (R(s  a)).

(3)

Various distributions can be used as the prior. For example  the uniform prior can be used if we have
no knowledge about the reward function other than its range  and a Gaussian or a Laplacian prior
can be used if we prefer rewards to be close to some speciﬁc values.

The likelihood in BIRL is deﬁned as an independent exponential distribution analogous to the soft-
max function:

P (X |R) =

M

H

Y

m=1

Y

h=1

P (am

h |sm

h   R) =

M

H

Y

m=1

Y

h=1

exp(βQ∗(sm

h   am
Pa∈A exp(βQ∗(sm

h ; R))
h   a; R))

(4)

3

)
X
|
)
5
s
(
R

 
)
1
s
(
R
(
P

0.04

0.02

0
1

0.5

R(s5)

0

0

0.2

0.4

R(s1)

0.6

0.8

1

(a)

(b)

Figure 1: (a) 5-state chain MDP. (b) Posterior for R(s1) and R(s5) of the 5-state chain MDP.

where β is a parameter that is equivalent to the inverse of temperature in the Boltzmann distribution.

The posterior over the reward function is then formulated by combining the prior and the likelihood 
using Bayes theorem:

P (R|X ) ∝ P (X |R)P (R).

(5)

BIRL uses a Markov chain Monte Carlo (MCMC) algorithm to compute the posterior mean of the
reward function.

3 MAP Inference in Bayesian IRL

In the Bayesian approach to IRL  the reward function can be determined using different estimates 
such as the posterior mean  median  or maximum-a-posterior (MAP). The posterior mean is com-
monly used since it can be shown to be optimal under the mean square error function. However 
the problem with the posterior mean in Bayesian IRL is that the error is integrated over the entire
space of reward functions  even including inﬁnitely many rewards that induce policies inconsistent
with the behaviour data. This can yield a posterior mean reward function with an optimal policy
again inconsistent with the data. On the other hand  the MAP does not involve an objective function
that is integrated over the reward function space; it is simply a point that maximizes the posterior
probability. Hence  it is more robust to inﬁnitely many inconsistent reward functions. We present a
simple example that compares the posterior mean and the MAP reward function estimation.

Consider an MDP with 5 states arranged in a chain  2 actions  and the discount factor 0.9. As shown
in Figure 1(a)  we denote the leftmost state as s1 and the rightmost state as s5. Action a1 moves to
the state on the right with probability 0.6 and to the state on the left with probability 0.4. Action a2
always moves to state s1. The true reward of each state is [0.1  0  0  0  1]  hence the optimal policy
chooses a1 in every state. Suppose that we already know R(s2)  R(s3)  and R(s4) which are all 0 
and estimate R(s1) and R(s5) from the behaviour data X which contains optimal actions for all the
states. We can compute the posterior P (R(s1)  R(s5)|X ) using Equations (3)  (4)  and (5) under the
assumption that 0 ≤ R ≤ 1 and priors P (R(s1)) being N (0.1  1)  and P (R(s5)) being N (1  1).
The reward optimality region can be also computed using Equation (2).

Figure 1(b) presents the posterior distribution of the reward function. The true reward  the MAP
reward  and the posterior mean reward are marked with the black star  the blue circle  and the red
cross  respectively. The black solid line is the boundary of the reward optimality region. Although
the prior mean is set to the true reward  the posterior mean is outside the reward optimality region.
An optimal policy for the posterior mean reward function chooses action a2 rather than action a1
in state s1  while an optimal policy for the MAP reward function is identical to the true one. The
situation gets worse when using the uniform prior. An optimal policy for the posterior mean reward
function chooses action a2 in states s1 and s2  while an optimal policy for the MAP reward function
is again identical to the true one.

In the rest of this section  we additionally show that most of the IRL algorithms in the literature can
be cast as searching for the MAP reward function in Bayesian IRL. The main insight comes from
the fact that these algorithms try to optimize an objective function consisting of a regularization term
for the preference on the reward function and an assessment term for the compatibility of the reward
function with the behaviour data. The objective function is naturally formulated as the posterior in
a Bayesian framework by encoding the regularization into the prior and the data compatibility into
the likelihood. In order to subsume different approaches used in the literature  we generalize the

4

Table 1: IRL algorithms and their equivalent f (X ; R) and prior for the Bayesian formulation. q ∈
{1  2} is for representing L1 or L2 slack penalties.

Previous algorithm

f (X ; R)

Prior

Ng and Russell’s IRL from sampled trajectories [6]

MMP without the loss function [8]

MWAL [10]

Policy matching [9]

MaxEnt [11]

fV

(fV )q

fG
fJ
fE

Uniform
Gaussian
Uniform
Uniform
Uniform

likelihood in Equation (4) to the following:

P (X |R) ∝ exp(βf (X ; R))

where β is a parameter for scaling the likelihood and f (X ; R) is a function which will be deﬁned
appropriately to encode the data compatibility assessment used in each IRL algorithm. We then have
the following result (the proof is provided in the supplementary material):

Theorem 1 IRL algorithms listed in Table 1 are equivalent to computing the MAP estimates with
the prior and the likelihood using f (X ; R) deﬁned as follows:
• fV (X ; R) = ˆV E(R) − V ∗(R)
• fJ (X ; R) = −Ps a ˆµE(s) (J(s  a; R) − ˆπE(s  a))2
where π∗(R) is an optimal policy induced by the reward function R  J(s  a; R) is a smooth
mapping from reward function R to a greedy policy such as the soft-max function  and PMaxEnt
is the distribution on the behaviour data (trajectory or path) satisfying the principle of maximum
entropy.

i i
• fG(X ; R) = mini hV π∗(R)
− ˆV E
• fE(X ; R) = log PMaxEnt(X |T   R)

i

The MAP estimation approach provides a rich framework for explaining the previous non-Bayesian
IRL algorithms in a uniﬁed manner  as well as encoding various types of a priori knowledge into the
prior distribution. Note that this framework can exploit the insights behind apprenticeship learning
algorithms even if they do not explicitly learn a reward function (e.g.  MWAL [10]).

4 A Gradient Method for Finding the MAP Reward Function

We have proposed a unifying framework for Bayesian IRL and suggested that the MAP estimate can
be a better solution to the IRL problem. We can then reformulate the IRL problem into the posterior
optimization problem  which is ﬁnding RMAP that maximizes the (log unnormalized) posterior:

RMAP = argmaxR P (R|X ) = argmaxR [log P (X |R) + log P (R)]

Before presenting a gradient method for the optimization problem  we show that the generalized
likelihood is differentiable almost everywhere.

The likelihood is deﬁned for measuring the compatibility of the reward function R with the be-
haviour data X . This is often accomplished using the optimal value function V ∗ or the optimal
Q-function Q∗ w.r.t. R. For example  the empirical value of X is compared with V ∗ [6  8]  X
is directly compared to the learned policy (e.g. the greedy policy from Q∗) [9]  or the probability
of following the trajectories in X is computing using Q∗ [13]. Thus  we generally assume that
P (X |R) = g(X   V ∗(R)) or g(X   Q∗(R)) where g is differentiable w.r.t. V ∗ or Q∗. The remain-
ing question is the differentiability of V ∗ and Q∗ w.r.t. R  which we address in the following two
theorems (The proofs are provided in the supplementary material.):

Theorem 2 V ∗(R) and Q∗(R) are convex.

Theorem 3 V ∗(R) and Q∗(R) are differentiable almost everywhere.
Theorems 2 and 3 relate to the previous work on gradient methods for IRL. Neu and Szepesv´ari [9]
showed that Q∗(R) is Lipschitz continuous  and except on a set of measure zero (almost every-
where)  it is Fr´echet differentiable by Rademacher’s theorem. We have obtained the same result

5

based on the reward optimality region  and additionally identiﬁed the condition for which V ∗(R)
and Q∗(R) are non-differentiable (refer to the proof for details). Ratliff et al. [8] used a subgra-
dient of their objective function because it involves differentiating V ∗(R). Using Theorem 3 for
computing the subgradient of their objective function yields an identical result.

Assuming a differentiable prior  we can compute the gradient of the posterior using the result in The-
orem 3 and the chain rule. If the posterior is convex  we will ﬁnd the MAP reward function. Other-
wise  as in [9]  we will obtain a locally optimal solution. In the next section  we will experimentally
show that the locally optimal solutions are nonetheless better than the posterior mean in practice.
This is due to the property that they are generally found within the reward optimality region w.r.t.
the policy consistent with the behaviour data.
The gradient method uses the update rule Rnew ← R + δt∇RP (R|X ) where δt is an appropriate
step-size (or learning rate). Since computing ∇RP (R|X ) involves computing an optimal policy
for the current reward function and a matrix inversion  caching these results helps reduce repetitive
computation. The idea is to compute the reward optimality region for checking whether we can
reuse the cached result. If Rnew is inside the reward optimality region of an already visited reward
function R′  they share the same optimal policy and hence the same ∇RV π(R) or ∇RQπ(R).
Given policy π  the reward optimality region is deﬁned by H π = I − (I A − γT )(I − γT π)−1Eπ 
and we can reuse the cached result if H π · Rnew ≤ 0. The gradient method using this idea is
presented in Algorithm 1.

Algorithm 1 Gradient method for MAP inference in Bayesian IRL
Input: MDP\R  behaviour data X   step-size sequence {δt}  number of iterations N
1: Initialize R
2: π ← solveMDP(R)
3: H π ← computeRewardOptRgn(π)
4: Π ← {hπ  H πi}
5: for t = 1 to N do
6: Rnew ← R + δt∇R P (R|X )
7:
8:
9:
10:
11:
12:
13:
14:
15: R ← Rnew
16: end for

hπ  H πi ← ﬁndRewardOptRgn(Rnew  Π)
if isEmpty(hπ  H πi) then
π ← solveMDP(Rnew)
H π ← computeRewardOptRgn(π)
Π ← Π ∪ {hπ  H πi}

if isNotInRewardOptRgn(Rnew  H π) then

end if

end if

5 Experimental Results

The ﬁrst set of experiments was conducted in N × N gridworlds [7]. The agent can move west 
east  north  or south  but with probability 0.3  it fails and moves in a random direction. The grids
are partitioned into M × M non-overlapping regions  so there are ( N
M )2 regions. The basis function
is deﬁned by a 0-1 indicator function for each region. The linearly parameterized reward function
is determined by the weight vector w sampled i.i.d. from a zero mean Gaussian prior with variance
0.1 and |wi| ≤ 1 for all i. The discount factor is set to 0.99.
We compared the performance of our gradient method to those of other IRL algorithms in the liter-
ature: Maximum Margin Planning (MMP) [8]  Maximum Entropy (MaxEnt) [11]  Policy Matching
with natural gradient (NatPM) and the plain gradient (PlainPM) [9]  and Bayesian Inverse Rein-
forcement Learning (BIRL) [13]. We executed our gradient method for ﬁnding MAP using three
different choices of the likelihood: B denotes the BIRL likelihood  and E and J denote the likelihood
with fE and fJ   respectively. For the Bayesian IRL algorithms (BIRL and MAP)  two types of the
prior are prepared: U denotes the uniform prior and G denotes the true Gaussian prior. We evaluated
the performance of the algorithms using the difference between V ∗ (the value of the expert’s policy)
and V L(the value of the optimal policy induced by the learned weight wL measured on the true
weight w∗) and the difference between w∗ and wL using L2 norm.

6

Table 2: Results in the gridworld problems.
k w ∗ − wL k2

V ∗ − V L

|S|

24 × 24

32 × 32

144
576
36
dim(w)
6.84 16.83
NatPM
3.04
6.63 16.60
PlainPM 3.77
6.05 11.98 22.11
MaxEnt
0.85
2.38
MMP
1.26
n.a.
3.27
BIRL-U
5.67
1.36
BIRL-G
0.86
n.a.
8.46 13.87
MAP-B-U 4.45
2.40
1.30
MAP-B-G 0.83
MAP-E-G 0.83
1.22
2.33
2.30
1.10
0.48
MAP-J-G

256 1024
64
8.88 21.25
3.50
5.21
9.05 17.36
7.91 15.48 25.52
0.83
3.17
n.a.
3.78
0.98
n.a.
5.68 10.50 18.21
3.17
0.94
0.76
3.13
3.11
0.65

1.62
1.53
1.51

1.61
7.89
1.71

24 × 24
144
8.97
0.67
0.60

576
36
8.74
2.49
0.51
0.15
0.33
0.60
10.74 16.32 13.72
n.a.
1.38
n.a.
2.21
0.13
1.06
0.40
0.16
0.19
0.42
0.37
0.17

0.80
0.54
0.57
0.45
0.44
0.42

32 × 32

64

256 1024
1.08 12.84 10.97
1.91
1.28
0.41
2.91
0.95
2.22
13.58 10.59
8.87
n.a.
2.24
0.35
n.a.
0.90
0.50
2.17
1.34
1.63
0.87
0.77
0.41
0.43
1.29
1.88
0.38
1.21
0.90

3

2

1

2
k

L
w
−
∗

w
k

0

20

40

60

80

L
V
−
∗
V

15

10

5

0

0

CPU time (sec)

(a)

20

40

60

80

CPU time (sec)

2
k

L
w
−
∗

w
k

5

4

3

2

1

0

200

400

600

800

20

10

L
V
−
∗
V

0

 
0

CPU time (sec)

(b)

 

BIRL
MAP−B

200

400

600

800

CPU time (sec)

Figure 2: CPU timing results of BIRL and MAP-B in 24 × 24 gridworld problem. (a) dim(w) = 36.
(b) dim(w) = 144.

We used training data with 10 trajectories of 50 time steps  collected from the simulated runs of
the expert’s policy. Table 2 shows the average performance over 10 training data. Most of the
algorithms found the weight that induces an optimal policy whose performance is as good as that
of the expert’s policy (i.e.  small V ∗ − V L) except for MMP and NatPM. The poor performance of
MMP was due to the small size in the training data  as already noted in [14]. The poor performance
of NatPM may be due to the ineffectiveness of pseudo-metric in high dimensional reward spaces 
since PlainPM was able produce good performance. Regarding the learned weights  the algorithms
using the true prior (MMP  BIRL  and the variants of MAP) found the weight close to the true one
(i.e.  small ||w∗ − wL||2). Comparing BIRL and MAP-B is especially meaningful since they share
the same prior and likelihood. The only difference was in computing the mean versus MAP from the
posterior. MAP-B was consistently better than BIRL in terms of both ||w∗ − wL||2 and V ∗ − V L.
Finally  we note that the correct prior yields small ||w∗ − wL||2 and V ∗ − V L when we compare
PlainPM  MaxEnt  BIRL-U  and MAP-B-U (uniform prior) to MAP-J-G  MAP-E-G  BIRL-G  and
MAP-B-G (Gaussian prior)  respectively.

Figure 2 compares the CPU timing results of the MCMC algorithm in BIRL and the gradient method
in MAP-B for the 24×24 gridworld with 36 and 144 basis functions. BIRL takes much longer
CPU time to converge than MAP-B since the former takes much larger number of iterations to
converge  and in addition  each iteration requires solving an MDP with a sampled reward function.
The CPU time gap gets larger as we increase the dimension of the reward function. Caching the
optimal policies and gradients sped up the gradient method by factors of 1.5 to 4.2 until convergence 
although not explicitly shown in the ﬁgure.

The second set of experiments was performed on a simpliﬁed car race problem  modiﬁed from [14].
The racetrack is shown in Figure 3. The shaded and white cells indicate the off-track and on-track
locations  respectively. The state consists of the location and velocity of the car. The velocities in the
vertical and horizontal directions are represented as 0  1  or 2  and the net velocity is computed as
the squared sum of directional velocities. The net velocity is regarded as high if greater than 2  zero
if 0  and low otherwise. The car can increase  decrease  or maintain one of the directional velocities.
The control of the car succeeds with p=0.9 if the net velocity is low  but p=0.6 if high. If the control
fails  the velocity is maintained  and if the car attempts to move outside the racetrack  it remains in
the previous location with velocity 0. The basis functions are 0-1 indicator functions for the goal
locations  off-track locations  and 3 net velocity values (zero  low  high) while the car is on track.
Hence  there are 3150 states  5 actions  and 5 basis functions. The discount factor is set to 0.99.

7

Table 3: True and learned weights in the car race problem.
Velocity while on track

Off-track

Goal

Fast expert

BIRL
MAP-B

1.00
0.96±0.02
1.00±0.00

0.00
-0.20±0.03
-0.19±0.02

0.00
-0.04±0.01
-0.03±0.01

0.00
-0.12±0.02
-0.13±0.01

0.10
0.32±0.02
0.29±0.01

Zero

Low

High

Table 4: Statistics of the policies simulated in the car race problem.

Avg. steps

to goal

20.41
32.98±6.42
24.77±1.92

Avg. steps in locations
Off-track
On-track
1.56
2.13±0.60
1.68±0.26

17.85
29.85±6.03
22.09±1.71

Avg. steps in velocity

Zero

Low

High

2.01
3.33±0.52
2.70±0.16

3.40
4.34±0.79
3.38±0.18

12.44
22.18±4.84
16.01±1.48

Fast expert

BIRL
MAP-B

We designed two experts. The slow expert prefers low velocity and avoids the off-track locations 
w = [1  −0.1  0  0.1  0]. The fast expert prefers high velocity  w = [1  0  0  0  0.1]. We compared the
posterior mean and the MAP using the prior P (w1)=N (1  1) and P (w2)=P (w3)=P (w4)=P (w5)=
N (0  1) assuming that we do not know the experts’ preference on the locations nor the velocity  but
we know the experts’ ultimate goal is to reach one of the goal locations. We used BIRL for the
posterior mean and MAP-B for the MAP estimation  hence using the identical prior and likelihood.

We used 10 training data  each consisting of 5 trajectories. We omit
the results regarding the slow expert since both BIRL and MAP-
B successfully found the weight similar with the true one  which
induced the slow expert’s policy as optimal. However for the fast
expert  MAP-B was signiﬁcantly better than BIRL.1 Table 3 shows
the true and learned weights  and Table 4 shows some statistics
characterizing the expert’s and learned policies. The policy from
BIRL tends to remain in high speed on the track for signiﬁcantly
more steps than the one from MAP-B since BIRL converged to a larger ratio of w5 to w1.

S

S

Figure 3: Racetrack.

G

G

G

G

6 Conclusion

We have argued that  when using a Bayesian framework for learning reward functions in IRL  the
MAP estimate is preferable over the posterior mean. Experimental results conﬁrmed the effec-
tiveness of our approach. We have also shown that the MAP estimation approach subsumes non-
Bayesian IRL algorithms in the literature  and allows us to incorporate various types of a priori
knowledge about the reward functions and the measurement of the compatibility with behaviour
data.

We proved that the generalized posterior is differentiable almost everywhere  and proposed a gradi-
ent method to ﬁnd a locally optimal solution to the MAP estimation. We provided the theoretical
result equivalent to the previous work on gradient methods for non-Bayesian IRL  but used a differ-
ent proof based on the reward optimality region.

Our work could be extended in a number of ways. For example  the IRL algorithm for partially
observable environments in [15] mostly relies on Ng and Russell [6]’s heuristics for MDPs  but our
work opens up new opportunities to leverage the insight behind other IRL algorithms for MDPs.

Acknowledgments

This work was supported by National Research Foundation of Korea (Grant# 2009-0069702) and the
Defense Acquisition Program Administration and the Agency for Defense Development of Korea
(Contract# UD080042AD)

1All the results in Table 4 except for the average number of steps in the off-track locations are statistically

signiﬁcant at the 95% conﬁdence level.

8

References
[1] S. Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of COLT 

1998.

[2] P. R. Montague and G. S. Berns. Neural economics and the biological substrates of valuation. Neuron 

36(2)  2002.

[3] B. D. Argall  S. Chernova  M. Veloso  and B. Browning. A survey of robot learning from demonstration.

Robotics and Autonomous Systems  57(5)  2009.

[4] Y. Niv. Reinforcement learning in the brain. Journal of Mathematical Psychology  53(3)  2009.
[5] E. Hopkins. Adaptive learning models of consumer behavior. Journal of Economic Behavior and Orga-

nization  64(3–4)  2007.

[6] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proceedings of ICML  2000.
[7] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of

ICML  2004.

[8] N. D. Ratliff  J. A. Bagnell  and M. A. Zinkevich. Maximum margin planning. In Proceedings of ICML 

2006.

[9] G. Neu and C. Szepesv´ari. Apprenticeship learning using inverse reinforcement learning and gradient

methods. In Proceedings of UAI  2007.

[10] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Proceedings of

NIPS  2008.

[11] B. D. Ziebart  A. Maas  J. A. Bagnell  and A. K. Dey. Maximum entropy inverse reinforcement learning.

In Proceedings of AAAI  2008.

[12] G. Neu and C. Szepesv´ari. Training parsers by inverse reinforcement learning. Machine Learning  77(2) 

2009.

[13] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proceedings of IJCAI  2007.
[14] A. Boularias and B. Chaib-Draa. Bootstrapping apprenticeship learning. In Proceedings of NIPS  2010.
[15] J. Choi and K. Kim. Inverse reinforcement learning in partially observable environments. In Proceedings

of IJCAI  2009.

9

,Martin Royer