2013,Lasso Screening Rules via Dual Polytope Projection,Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large  solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems  El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors  i.e.  predictors that have $0$ components in the solution vector. Then  the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form  it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper  we propose an efficient and effective screening rule via Dual Polytope Projections (DPP)  which is mainly based on the uniqueness and nonexpansiveness  of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover  we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge  there is currently no exact" screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective to identify inactive predictors than existing state-of-the-art screening rules for Lasso.",Lasso Screening Rules via Dual Polytope Projection

Jie Wang  Jiayu Zhou  Peter Wonka  Jieping Ye

Computer Science and Engineering

{jie.wang.ustc  jiayu.zhou  peter.wonka  jieping.ye}@asu.edu

Arizona State University  Tempe  AZ 85287

Abstract

Lasso is a widely used regression technique to ﬁnd sparse representations. When
the dimension of the feature space and the number of samples are extremely large 
solving the Lasso problem remains challenging. To improve the efﬁciency of solv-
ing large-scale Lasso problems  El Ghaoui and his colleagues have proposed the
SAFE rules which are able to quickly identify the inactive predictors  i.e.  predic-
tors that have 0 components in the solution vector. Then  the inactive predictors
or features can be removed from the optimization problem to reduce its scale. By
transforming the standard Lasso to its dual form  it can be shown that the inactive
predictors include the set of inactive constraints on the optimal dual solution. In
this paper  we propose an efﬁcient and effective screening rule via Dual Polytope
Projections (DPP)  which is mainly based on the uniqueness and nonexpansive-
ness of the optimal dual solution due to the fact that the feasible set in the dual
space is a convex and closed polytope. Moreover  we show that our screening rule
can be extended to identify inactive groups in group Lasso. To the best of our
knowledge  there is currently no “exact” screening rule for group Lasso. We have
evaluated our screening rule using many real data sets. Results show that our rule
is more effective in identifying inactive predictors than existing state-of-the-art
screening rules for Lasso.

Introduction

1
Data with various structures and scales comes from almost every aspect of daily life. To effectively
extract patterns in the data and build interpretable models with high prediction accuracy is always
desirable. One popular technique to identify important explanatory features is by sparse regulariza-
tion. For instance  consider the widely used (cid:96)1-regularized least squares regression problem known
as Lasso [20]. The most appealing property of Lasso is the sparsity of the solutions  which is equiv-
alent to feature selection. Suppose we have N observations and p predictors. Let y denote the N
dimensional response vector and X = [x1  x2  . . .   xp] be the N ×p feature matrix. Let λ ≥ 0 be the
regularization parameter  the Lasso problem is formulated as the following optimization problem:

1

inf
β∈(cid:60)p

(1)
Lasso has achieved great success in a wide range of applications [5  4  28  3  23] and in recent years
many algorithms have been developed to efﬁciently solve the Lasso problem [7  12  18  6  10  1  11].
However  when the dimension of feature space and the number of samples are very large  solving
the Lasso problem remains challenging because we may not even be able to load the data matrix into
main memory. The idea of a screening test proposed by El Ghaoui et al. [8] is to ﬁrst identify inactive
predictors that have 0 components in the solution and then remove them from the optimization.
Therefore  we can work on a reduced feature matrix to solve Lasso efﬁciently.
In [8]  the “SAFE” rule discards xi when
(2)
i y| is the largest parameter such that the solution is nontrivial. Tibshirani et
where λmax = maxi |xT
al. [21] proposed a set of strong rules which were more effective in identifying inactive predictors.

|xT
i y| < λ − (cid:107)xi(cid:107)2(cid:107)y(cid:107)2

λmax−λ
λmax

2(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1.

1

i y| < 2λ − λmax. However  it should be noted that the proposed
The basic version discards xi if |xT
strong rules might mistakenly discard active predictors  i.e.  predictors which have nonzero coefﬁ-
cients in the solution vector. Xiang et al. [26  25] developed a set of screening tests based on the
estimation of the optimal dual solution and they have shown that the SAFE rules are in fact a special
case of the general sphere test.
In this paper  we develop new efﬁcient and effective screening rules for the Lasso problem; our
screening rules are exact in the sense that no active predictors will be discarded. By transforming
problem (1) to its dual form  our motivation is mainly based on three geometric observations in the
dual space. First  the active predictors belong to a subset of the active constraints on the optimal dual
solution  which is a direct consequence of the KKT conditions. Second  the optimal dual solution is
in fact the projection of the scaled response vector onto the feasible set of the dual variables. Third 
because the feasible set of the dual variables is closed and convex  the projection is nonexpansive
with respect to λ [2]  which results in an effective estimation of its variation. Moreover  based on
the basic DPP rules  we propose the “Enhanced DPP” rules which are able to detect more inactive
features than DPP. We evaluate our screening rules on real data sets from many different applications.
The experimental results demonstrate that our rules are more effective in discarding inactive features
than existing state-of-the-art screening rules.
2 Screening Rules for Lasso via Dual Polytope Projections
In this section  we present the basics of the dual formulation of problem (1) including its geometric
properties (Section 2.1). Based on the geometric properties of the dual optimal  we develop the
fundamental principle in Section 2.2 (Theorem 2)  which can be used to construct screening rules
for Lasso. In section 2.3  we discuss the relation between dual optimal and LARS [7]. As a straight-
forward extension of DPP rules  we develop the sequential version of DPP (SDPP) in Section 2.4.
Moreover  we present enhanced DPP rules in Section 2.5.
2.1 Basics
Different from [26  25]  we do not assume y and all xi have unit length. We ﬁrst transform problem
(1) to its dual form (to make the paper self-contained  we provide the detailed derivation of the dual
form in the supplemental materials):

(cid:111)

2 − λ2

2 (cid:107)θ − y

λ(cid:107)2
2 :

|xT
i θ| ≤ 1  i = 1  2  . . .   p

(cid:110) 1
2(cid:107)y(cid:107)2

sup

θ

where θ is the dual variable. Since the feasible set  denoted by F   is the intersection of 2p half-
spaces  it is a closed and convex polytope. From the objective function of the dual problem (3)  it is
λ . In other words  θ∗
easy to see that the optimal dual solution θ∗ is a feasible θ which is closest to y
is the projection of y
λ onto the polytope F . Mathematically  for an arbitrary vector w and a convex
set C  if we deﬁne the projection function as

(3)

(4)

(5)

(6)

then

We know that the optimal primal and dual solutions satisfy:
y = Xβ∗ + λθ∗

and the KKT conditions for the Lasso problem (1) are

PC(w) = argmin

θ∗ = PF (y/λ) = argmin
θ∈F

(cid:107)u − w(cid:107)2 

u∈C

(cid:13)(cid:13)θ − y
(cid:13)(cid:13)2.
(cid:26)sign([β∗]i) if [β∗]i (cid:54)= 0

λ

[−1  1] if [β∗]i = 0

(θ∗)T xi ∈
where [·]k denotes the kth component.
By the KKT conditions in Eq. (7)  if the inner product (θ∗)T xi belongs to the open interval (−1  1) 
then the corresponding component [β∗]i in the solution vector β∗(λ) has to be 0. As a result  xi is
an inactive predictor and can be removed from the optimization.
On the other hand  let ∂H(xi) = {z: zT xi = 1} and H(xi)− = {z: zT xi ≤ 1} be the hyperplane
and half space determined by xi respectively. Consider the dual problem (3); constraints induced
by each xi are equivalent to requiring each feasible θ to lie inside the intersection of H(xi)− and
H(−xi)−. If |(θ∗)T xi| = 1  i.e.  either θ∗ ∈ ∂H(xi)− or θ∗ ∈ ∂H(−xi)−  we say the constraints
induced by xi are active on θ∗.

(7)

2

We deﬁne the “active” set on θ∗ as Iθ∗ = {i: |(θ∗)T xi| = 1  i ∈ I} where I = {1  2  . . .   p}.
Otherwise  if θ∗ lies between ∂H(xi) and ∂H(−xi)  i.e.  |(θ∗)T xi| < 1  we can safely remove
xi from the problem because [β∗]i = 0 according to the KKT conditions in Eq. (7). Similarly  the
“inactive” set on θ∗ is deﬁned as I θ∗ = I \ Iθ∗. Therefore  from a geometric perspective  if we
know θ∗  i.e.  the projection of y
λ onto F   the predictors in the inactive set on θ∗ can be discarded
from the optimization. It is worthwhile to mention that inactive predictors  i.e.  predictors that have
0 components in the solution  are not the same as predictors in the inactive set. In fact  by the KKT
conditions  predictors in the inactive set must be inactive predictors since they are guaranteed to
have 0 components in the solution  but the converse may not be true.
2.2 Fundamental Screening Rules via Dual Polytope Projections
Motivated by the above geometric intuitions  we next show how to ﬁnd the predictors in the inactive
set on θ∗. To emphasize the dependence on λ  let us write θ∗(λ) and β∗(λ). If we know exactly
where θ∗(λ) is  it will be trivial to ﬁnd the predictors in the inactive set. Unfortunately  in most of
the cases  we only have incomplete information about θ∗(λ) without actually solving problem (1) or
(3). Suppose we know the exact θ∗(λ(cid:48)) for a speciﬁc λ(cid:48). How can we estimate θ∗(λ(cid:48)(cid:48)) for another λ(cid:48)(cid:48)
and its inactive set? To answer this question  we start from Eq. (5); θ∗(λ) is nonexpansive because
it is a projection operator. For convenience  we cite the projection theorem in [2] as follows.
Theorem 1. Let C be a convex set  then the projection function deﬁned in Eq. (4) is continuous and
nonexpansive  i.e. 

(cid:107)PC(w2) − PC(w1)(cid:107)2 ≤ (cid:107)w2 − w1(cid:107)2  ∀w2  w1.

(8)
Given θ∗(λ(cid:48))  the next theorem shows how to estimate θ∗(λ(cid:48)(cid:48)) and its inactive set for another pa-
rameter λ(cid:48)(cid:48).
Theorem 2. For the Lasso problem  assume we are given the solution of its dual problem θ∗(λ(cid:48)) for
a speciﬁc λ(cid:48). Let λ(cid:48)(cid:48) be a nonnegative value different from λ(cid:48). Then [β∗(λ(cid:48)(cid:48))]i = 0 if

|xT
i θ∗(λ(cid:48))| < 1 − (cid:107)xi(cid:107)2(cid:107)y(cid:107)2
(9)
Proof. From the KKT conditions in Eq. (7)  we know |xT
i θ∗(λ(cid:48)(cid:48))| < 1 ⇒ [β∗(λ(cid:48)(cid:48))]i = 0. By the
dual problem (3)  θ∗(λ) is the projection of y
λ onto the feasible set F . According to the projection
theorem [2]  that is  Theorem 1  for closed convex sets  θ∗(λ) is continuous and nonexpansive  i.e. 
λ(cid:48)(cid:48) − y
(10)
λ(cid:48)

(cid:107)θ∗(λ(cid:48)(cid:48)) − θ∗(λ(cid:48))(cid:107)2 ≤(cid:13)(cid:13) y

λ(cid:48) − 1
λ(cid:48)(cid:48)

λ(cid:48)(cid:48) − 1
λ(cid:48)

Then

(cid:12)(cid:12)(cid:12)(cid:12).

(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12) 1
(cid:13)(cid:13)2 = (cid:107)y(cid:107)2
(cid:12)(cid:12) 1
(cid:12)(cid:12) + 1 − (cid:107)xi(cid:107)2(cid:107)y(cid:107)2

i θ∗(λ(cid:48))|

(cid:12)(cid:12)
(cid:12)(cid:12) 1

|xT
i θ∗(λ(cid:48)(cid:48))| ≤ |xT

i θ∗(λ(cid:48)(cid:48)) − xT

i θ∗(λ(cid:48))| + |xT

< (cid:107)xi(cid:107)2(cid:107)(θ∗(λ(cid:48)(cid:48)) − θ∗(λ(cid:48)))(cid:107)2 + 1 − (cid:107)xi(cid:107)2(cid:107)y(cid:107)2
≤ (cid:107)xi(cid:107)2(cid:107)y(cid:107)2

λ(cid:48)(cid:48) − 1
λ(cid:48)

λ(cid:48)(cid:48) − 1
λ(cid:48)
λ(cid:48)(cid:48) − 1
λ(cid:48)

(cid:12)(cid:12) = 1

(cid:12)(cid:12) 1

(cid:12)(cid:12)

(11)

y

λmax

is itself feasible. Therefore the projection of

which completes the proof.
From theorem 2  it is easy to see our rule is quite ﬂexible since every θ∗(λ(cid:48)) would result in a new
screening rule. And the smaller the gap between λ(cid:48) and λ(cid:48)(cid:48)  the more effective the screening rule is.
By “more effective”  we mean a stronger capability of identifying inactive predictors.
i y|. It is easy to verify
As an example  let us ﬁnd out θ∗(λmax). Recall that λmax = maxi |xT
.
Moreover  by noting that for ∀λ > λmax  we have |xT
i y/λ| < 1  i ∈ I  i.e.  all predictors are in the
inactive set at θ∗(λ)  we conclude that the solution to problem (1) is 0. Combining all these together
and plugging θ∗(λmax) = y
(cid:16) 1
Corollary 3. DPP: For the Lasso problem (1)  let λmax = maxi |xT
0 ∀i ∈ I. Otherwise  [β∗(λ)]i = 0 if
λ − 1

(cid:17)
i y|. If λ ≥ λmax  then [β∗]i =

(cid:12)(cid:12)(cid:12) < 1 − (cid:107)xi(cid:107)2(cid:107)y(cid:107)2

onto F is itself  i.e.  θ∗(λmax) = y

into Eq. (9)  we obtain the following screening rule.

(cid:12)(cid:12)(cid:12)xT

λmax

λmax

λmax

λmax

y

y

i

λmax

.

Clearly  DPP is most effective when λ is close to λmax. So how can we ﬁnd a new θ∗(λ(cid:48)) with
λ(cid:48) < λmax? Note that Eq. (6) is in fact a natural bridge which relates the primal and dual optimal
solutions. As long as we know β∗(λ(cid:48))  it is easy to get θ∗(λ(cid:48)) when λ is relatively small  e.g.  LARS
[7] and Homotopy [17] algorithms.

3

Table 1: Illustration of the running time for DPP screening and for solving the Lasso problem after
screening. Ts: time for screening. Tl: time for solving the Lasso problem after screening. To:
the total time. Entries of the response vector y are i.i.d. by a standard Gaussian. Columns of the
data matrix X ∈ (cid:60)1000×100000 are generated by xi = y + αz where α is a random number drawn
uniformly from [0  1]. Entries of z are i.i.d. by a standard Gaussian. λmax = 0.95 and λ/λmax=0.5.

Ts (S)
Tl (S)
To (S)

LASSO

—
—

103.314

DPP
0.035
10.250
10.285

DPP2
0.073
9.634
9.707

DPP5
0.152
8.399
8.552

DPP10
0.321
1.369
1.690

DPP20
0.648
0.121
0.769

λ(cid:48) − 1

λmax−λ
λmax

i y| < λ−(cid:107)xi(cid:107)2(cid:107)y(cid:107)2

Remark: Xiang et al. [26] developed a general sphere test which says that if θ∗ is estimated to be
inside a ball (cid:107)θ∗ − q(cid:107)2 ≤ r  then |xT
i q| < (1 − r) ⇒ [β∗]i = 0. Considering the DPP rules in
Theorem 2  it is equivalent to setting q = θ∗(λ(cid:48)) and r = | 1
λ(cid:48)(cid:48)|. Therefore  different from the
sphere test and Dome developed in [26  25] with the radius r ﬁxed at the beginning  the construction
of our DPP rules is equivalent to an “r” decreasing process. Clearly  the smaller r is  the more
effective the DPP rules will be.
Remark: Notice that  DPP is not the same as ST1 [26] and SAFE [8]  which discards the ith feature
if |xT
. From the perspective of the sphere test  the radius of ST1/SAFE
and DPP are the same. But the centers of ST1 and DPP are y/λ and y/λmax respectively  which
leads to different formulas  i.e.  Eq. (2) and Corollary 3.
2.3 DPP Rules with LARS/Homotopy Algorithms
It is well known that under mild conditions  the set {β∗(λ) : λ > 0} (also know as regularization
path [15]) is continuous piecewise linear [17  7  15]. The output of LARS or Homotopy algorithms is
in fact a sequence of values like (β∗(λ(0))  λ(0))  (β∗(λ(1))  λ(1))  . . .  where β∗(λ(i)) corresponds
to the ith breakpoint of the regularization path {β∗(λ) : λ > 0} and λ(i)s are monotonically de-
creasing. By Eq. (6)  once we get β∗(λ(i))  we can immediately compute θ∗(λ(i)). Then according
to Theorem 2  we can construct a DPP rule based on θ∗(λ(i)) and λ(i). For convenience  if the DPP
rule is built based on θ∗(λ(i))  we add the index i as sufﬁx to DPP  e.g.  DPP5 means it is developed
based on θ∗(λ(5)). It should be noted that LARS or Homotopy algorithms are very efﬁcient to ﬁnd
the ﬁrst few breakpoints of the regularization path and the corresponding parameters. For the ﬁrst
few breakpoints  the computational cost is roughly O(N p)  i.e.  linear with the size of the data ma-
trix X. In Table 1  we report both the time used for screening and the time needed to solve the Lasso
problem after screening. The Lasso solver is from the SLEP [14] package.
From Table 1  we can see that compared with the time saved by the screening rules  the time used
for screening is negligible. The efﬁciency of the Lasso solver is improved by DPP20 more than
130 times. In practice  DPP rules built on the ﬁrst few θ∗(λ(i))’s lead to more signiﬁcant perfor-
mance improvement than existing state-of-art screening tests. We will demonstrate the effectiveness
of our DPP rules in the experiment section. As another useful property of LARS/Homotopy al-
gorithms  it is worthwhile to mention that changes of the active set only happen at the breakpoints
[17  7  15]. Consequently  given the parameters corresponding to a pair of adjacent breakpoints  e.g. 
λ(i) and λ(i+1)  the active set for λ ∈ (λ(i+1)  λ(i)) is the same as λ = λ(i). Therefore  besides the
sequence of breakpoints and the associated parameters (β∗(λ(0))  λ(0))  . . . (β∗(λ(k))  λ(k)) com-
puted by LARS/Homotopy algorithms  we know the active set for ∀λ ≥ λ(k). Hence we can remove
the predictors in the inactive set from the optimization problem (1). This scheme has been embedded
in DPP rules.
Remark: Some works  e.g.  [21]  [8]  solve several Lasso problems for different parameters to
improve the screening performance. However  the DPP algorithms do not aim to solve a sequence
of Lasso problems  but just to accelerate one. The LARS/Homotopy algorithms are used to ﬁnd the
ﬁrst few breakpoints of the regularization path and the corresponding parameters  instead of solving
general Lasso problems. Thus  different from [21]  [8] who need to iteratively compute a screening
step and a Lasso step  DPP algorithms only compute one screening step and one Lasso step.
2.4 Sequential Version of DPP Rules
Motivated by the ideas of [21] and [8]  we can develop a sequential version of DPP rules. In other
words  if we are given a sequence of parameter values λ1 > λ2 > . . . > λm  we can ﬁrst apply
DPP to discard inactive predictors for the Lasso problem (1) with parameter being λ1. After solving

4

the reduced optimization problem for λ1  we obtain the exact solution β∗(λ1). Hence by Eq. (6) 
we can ﬁnd θ∗(λ1). According to Theorem 2  once we know the optimal dual solution θ∗(λ1)  we
can construct a new screening rule to identify inactive predictors for problem (1) with λ = λ2. By
repeating the above process  we obtain the sequential version of the DPP rule (SDPP).
Corollary 4. SDPP: For the Lasso problem (1)  suppose we are given a sequence of parameter
values λmax = λ0 > λ1 > . . . > λm. Then for any integer 0 ≤ k < m  we have [β∗(λk+1)]i = 0
if β∗(λk) is known and the following holds:

(cid:12)(cid:12)(cid:12) < 1 − (cid:107)xi(cid:107)2(cid:107)y(cid:107)2

(cid:16) 1

λk+1

(cid:17)

.

− 1

λk

(cid:12)(cid:12)(cid:12)xT

i

y−Xβ∗(λk)

λk

Remark: There are some other related works on screening rules  e.g.  Wu et al. [24] built screening
rules for (cid:96)1 penalized logistic regression based on the inner products between the response vector
and each predictor; Tibshirani et al. [21] developed strong rules for a set of Lasso-type problems via
the inner products between the residual and predictors; in [9]  Fan and Lv studied screening rules
for Lasso and related problems. But all of the above works may mistakenly discard predictors that
have non-zero coefﬁcients in the solution. Similar to [8  26  25]  our DPP rules are exact in the
sense that the predictors discarded by our rules are inactive predictors  i.e.  predictors that have zero
coefﬁcients in the solution.
2.5 Enhanced DPP Rules
In this section  we show how to further improve the DPP rules. From the inequality in (9)  we can
see that the larger the right hand side is  the more inactive features can be detected. From the proof
of Theorem 2  we need to make the right hand side of the inequality in (10) as small as possible. By
noting that θ∗(λ(cid:48)) = PF ( y
λ(cid:48) ) and θ∗(λ(cid:48)(cid:48)) = PF ( y
λ(cid:48)(cid:48) ) [please refer to Eq. (5)]  the inequality in (10)
is in fact a direct consequence of Theorem 1 by letting C := F   w1 := y
λ(cid:48) and w2 := y
λ(cid:48)(cid:48) .
λ(cid:48) /∈ F   i.e.  λ(cid:48) ∈ (0  λmax). It is clear that y
λ(cid:48) ) = θ∗(λ(cid:48)). Let
On the other hand  suppose y
λ(cid:48)
λ(cid:48) − θ∗(λ(cid:48))) for t ≥ 0  i.e.  θ(t) is a point lying on the ray starting from θ∗(λ(cid:48))
θ(t) = θ∗(λ(cid:48)) + t( y
λ(cid:48) − θ∗(λ(cid:48)). We can observe that PF (θ(t)) = θ∗(λ(cid:48))  i.e.  the
and pointing to the same direction as y
projection of θ(t) onto the set F is θ∗(λ(cid:48)) as well (please refer to Lemma A in the supplement for
details). By applying Theorem 1 again  we have
(cid:107)θ∗(λ(cid:48)(cid:48))−θ∗(λ(cid:48))(cid:107)2 = (cid:107)PF ( y
λ(cid:48)(cid:48) −θ∗(λ(cid:48)))(cid:107)2.
(12)
Clearly  when t = 1  the inequality in (12) reduces to the one in (10). Because the inequality in (12)
holds for all t ≥ 0  we may get a tighter bound by

λ(cid:48)(cid:48) )−PF (θ(t))(cid:107)2 ≤ (cid:107) y

λ(cid:48)(cid:48) −θ(t)(cid:107)2 = (cid:107)t( y

λ(cid:48) −θ∗(λ(cid:48)))−( y

(cid:54)= PF ( y

(cid:107)θ∗(λ(cid:48)(cid:48)) − θ∗(λ(cid:48))(cid:107)2 ≤ min
t≥0

where v1 = y
where x∗ := argmaxxi|xT
mization problem on the right hand side of the inequality (13) can be easily solved as follows:

(13)
λ(cid:48)(cid:48) − θ∗(λ(cid:48)). When λ(cid:48) = λmax  we can set v1 = sign(xT∗ y)x∗
i y| (please refer to Lemma B in the supplement for details). The mini-

λ(cid:48) − θ∗(λ(cid:48)) and v2 = y

(cid:107)tv1 − v2(cid:107)2 

(cid:40)(cid:107)v2(cid:107)2 
(cid:13)(cid:13)(cid:13)v2 − (cid:104)v1 v2(cid:105)

(cid:107)v1(cid:107)2

2

(cid:13)(cid:13)(cid:13)2

v1

 

if (cid:104)v1  v2(cid:105) < 0 
otherwise.

(cid:107)tv1 − v2(cid:107)2 = ϕ(λ(cid:48)  λ(cid:48)(cid:48)) =

min
t≥0

(14)

(15)

Similar to Theorem 2  we have the following result:
Theorem 5. For the Lasso problem  assume we are given the solution of its dual problem θ∗(λ(cid:48)) for
a speciﬁc λ(cid:48). Let λ(cid:48)(cid:48) be a nonnegative value different from λ(cid:48). Then [β∗(λ(cid:48)(cid:48))]i = 0 if

|xT
i θ∗(λ(cid:48))| < 1 − (cid:107)xi(cid:107)2ϕ(λ(cid:48)  λ(cid:48)(cid:48)).

As we explained above  the right hand side of the inequality (15) is no less than that of the inequality
(9). Thus  the enhanced DPP is able to detect more inactive features than DPP. The analogues of
Corollaries 3 and 4 can be easily derived as well.
Corollary 6. DPP∗: For the Lasso problem (1)  let λmax = maxi |xT
[β∗]i = 0 ∀i ∈ I. Otherwise  [β∗(λ)]i = 0 if the following holds:

If λ ≥ λmax  then

i y|.

(cid:12)(cid:12)(cid:12)xT

i

(cid:12)(cid:12)(cid:12) < 1 − (cid:107)xi(cid:107)2ϕ(λmax  λ).

y

λmax

Corollary 7. SDPP∗: For the Lasso problem (1)  suppose we are given a sequence of parameter
values λmax = λ0 > λ1 > . . . > λm. Then for any integer 0 ≤ k < m  we have [β∗(λk+1)]i = 0

5

if β∗(λk) is known and the following holds:
y−Xβ∗(λk)

(cid:12)(cid:12)(cid:12)xT

i

(cid:12)(cid:12)(cid:12) < 1 − (cid:107)xi(cid:107)2ϕ(λk  λk+1).

λk

To simplify notations  we denote the enhanced DPP and SDPP by DPP∗ and SDPP∗ respectively.
3 Extensions to Group Lasso
To demonstrate the ﬂexibility of DPP rules  we extend our idea to the group Lasso problem [27]:

where Xg ∈ (cid:60)N×ng is the data matrix for the gth group and p =(cid:80)G

Xgβg(cid:107)2

inf
β∈(cid:60)p

2 + λ

g=1

g=1

√

ng(cid:107)βg(cid:107)2 

problem of (16) is (see detailed derivation in the supplemental materials):

g=1 ng. The corresponding dual

2(cid:107)y −(cid:88)G

1

(cid:88)G

(cid:111)

ng  g = 1  2  . . .   G

(cid:110) 1
2(cid:107)y(cid:107)2

2 − λ2

sup

θ

Similar to the Lasso problem  the primal and dual optimal solutions of the group Lasso satisfy:

(16)

(17)

(18)

(19)

(20)

and the KKT conditions are:

g θ(cid:107)2 ≤ √

∗
g + λθ

∗

2 (cid:107)θ − y
λ(cid:107)2
2 : (cid:107)XT
(cid:88)G
(cid:40)√

y =

g=1

Xgβ

√

if β∗

g (cid:54)= 0
ng
ngu  (cid:107)u(cid:107)2 ≤ 1 if β∗

β∗
g(cid:107)β∗
g(cid:107)2
√

g = 0

(θ∗)T Xg ∈

ng  we can conclude that β∗

for g = 1  2  . . .   G. Clearly  if (cid:107)(θ∗)T Xg(cid:107)2 <
Consider problem (17). It is easy to see that the dual optimal θ∗ is the projection of y
λ onto the
feasible set. For each g  the constraint (cid:107)XT
ng conﬁnes θ to an ellipsoid which is closed
and convex. Therefore  the feasible set of the dual problem (17) is the intersection of ellipsoids and
thus closed and convex. Hence θ∗(λ) is also nonexpansive for the group lasso problem. Similar to
Theorem 2  we can readily develop the following theorem for group Lasso.
Theorem 8. For the group Lasso problem  assume we are given the solution of its dual problem
θ∗(λ(cid:48)) for a speciﬁc λ(cid:48). Let λ(cid:48)(cid:48) be a nonnegative value different from λ(cid:48). Then β∗

g θ(cid:107)2 ≤ √

g = 0.

g (λ(cid:48)(cid:48)) = 0 if

(cid:107)XT

g θ∗(λ(cid:48))(cid:107)2 <

√

(cid:12)(cid:12) 1

(cid:12)(cid:12)

ng − (cid:107)Xg(cid:107)F(cid:107)y(cid:107)2
√
g y(cid:107)2/

λ(cid:48) − 1
λ(cid:48)(cid:48)
ng  we can see that

.

y

g

y

<

λmax

λmax

λmax

λmax

√

(cid:17)

(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)XT
y−(cid:80)G

(cid:16) 1
ng.
g (λ) = 0 if the following holds:
λ − 1
(cid:16) 1

Similar to the Lasso problem  let λmax = maxg (cid:107)XT
is itself
feasible  and λmax is the largest parameter such that problem (16) has a nonzero solution. Clearly 
θ∗(λmax) = y
. Similar to DPP and SDPP  we can construct GDPP and SGDPP for group Lasso.
Corollary 9. GDPP: For the group Lasso problem (16)  let λmax = maxg (cid:107)XT
If
λ ≥ λmax  β∗

√
g y(cid:107)2/

g (λ) = 0 ∀g = 1  2  . . .   G. Otherwise  we have β∗
ng − (cid:107)Xg(cid:107)F(cid:107)y(cid:107)2

g=1 Xgβ∗

(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)XT

ng − (cid:107)Xg(cid:107)F(cid:107)y(cid:107)2

(21)
Corollary 10. SGDPP: For the group Lasso problem (16)  suppose we are given a sequence of
parameter values λmax = λ0 > λ1 > . . . > λm. For any integer 0 ≤ k < m  we have β∗
g (λk+1) =
0 if β∗(λk) is known and the following holds:
<

(22)
Remark: Similar to DPP∗  we can develop the enhanced GDPP by simply replacing the term
(cid:107)y(cid:107)2(1/λ − 1/λmax) on the right hand side of the inequality (21) with ϕ(λmax  λ). Notice that 
to compute ϕ(λmax  λ)  we set v1 = X∗(X∗)T y where X∗ = argmaxXg(cid:107)XT
ng (please
refer to Lemma C in the supplement for details). The analogs of SDPP∗  that is  SGDPP∗  can be
obtained by replacing the term (cid:107)y(cid:107)2(1/λk+1 − 1/λk) on the right hand side of the inequality (22)
with ϕ(λk  λk+1).
4 Experiments
In section 4.1  we ﬁrst evaluate the DPP and DPP∗ rules on both real and synthetic data. We then
compare the performance of DPP with Dome (see [25  26]) which achieves state-of-art performance
for the Lasso problem among exact screening rules [25]. We evaluate GDPP and SGDPP for the
group Lasso problem on three synthetic data sets in section 4.2. We are not aware of any “exact”
screening rules for the group Lasso problem at this point.

√
g y(cid:107)2/

− 1

(cid:17)

g (λk)

√

λk+1

λk

λk

.

g

6

(a) MNIST-DPP2/DPP∗2 (b) MNIST-DPP5/DPP∗5

(c) COIL-DPP2/DPP∗2

(d) COIL-DPP5/DPP∗5

Figure 1: Comparison of DPP and DPP∗ rules on the MNIST and COIL data sets.

To measure the performance of our screening rules  we compute the rejection rate  i.e.  the ratio be-
tween the number of predictors discarded by screening rules and the actual number of zero predictors
in the ground truth. Because the DPP rules are exact  i.e.  no active predictors will be mistakenly
discarded  the rejection rate will be less than one. For SAFE and Dome  it is not straightforward
to extend them to the group Lasso problem. Similarly to previous works [26]  we do not report the
computational time saved by screening because it can be easily computed from the rejection ratio.
Speciﬁcally  if the Lasso solver is linear in terms of the size of the data matrix X  a K% rejection
of the data can save K% computational time. The general experiment settings are as follows. For
each data set  after we construct the data matrix X and the response y  we run the screening rules
along a sequence of 100 values equally spaced on the λ/λmax scale from 0 to 1. We repeat the
procedure 100 times and report the average performance at each of the 100 values of λ/λmax. All
of the screening rules are implemented in Matlab. The experiments are carried out on a Intel(R)
(i7-2600) 3.4Ghz processor.
4.1 DPPs and DPP∗s for the Lasso Problem
In this experiment  we ﬁrst compare the performance of the proposed DPP rules with the enhanced
DPP rules (DPP∗) on (a) the MNIST handwritten digit data set [13]; (b) the COIL rotational image
data set [16] in Section 4.1.1. We show that the DPP∗ rules are more effective in identifying inactive
features than the DPP rules. This demonstrate our theoretical results in Section 2.5. Then we
evaluate the DPP∗/SDPP∗ rules and Dome on (c) the ADNI data set; (d) the Olivetti Faces data set
[19]; (e) Yahoo web pages data sets [22] and (f) a synthetic data set whose entries are i.i.d. by a
standard Gaussian.
4.1.1 Comparison of DPP and DPP∗
As we explain in Section 2.5  all inactive feature detected by the DPP rules can also be detected
by the DPP∗ rules. But conversely  it is not necessarily true. To demonstrate the advantage of the
DPP∗ rules  we run DPP2  DPP∗2  DPP5 and DPP∗5 on the MNIST and COIL data sets. a) The
MNIST data set contains grey images of scanned handwritten digits  including 60  000 for training
and 10  000 for testing. The dimension of each image is 28× 28. Each time  we ﬁrst randomly select
100 images for each digit (and in total we have 1000 images) and get a data matrix X ∈ (cid:60)784×1000.
Then we randomly pick an image as the response y ∈ (cid:60)784. b) The COIL data set includes 100
objects  each of which has 72 color images with 128×128 pixels. The images that belong to the same
object are taken every 5 degree by rotating the object. We use the images of object 10. Each time 
we randomly pick one of the images as the response vector y ∈ (cid:60)49152 and use all the remaining
ones to construct the data matrix X ∈ (cid:60)49152×71. The average λmax for the so cultured MNIST and
the COIL data sets are 0.837 and 0.986. Clearly  the predictors in the data sets are high correlated.
From Figure 1  we observe that DPP∗2 signiﬁcantly outperforms DPP2 for both data sets  especially
when λ/λmax is small. We also observe the same pattern for DPP5 and DPP∗5  verifying the claims
about DPP∗ made in the paper. Thus  in the following experiments  we only report the performance
of DPP∗ and the competing algorithm Dome.
4.1.2 Comparison of DPP∗/SDPP∗ and Dome
In this experiment  we compare DPP∗/SDPP∗ rules with Dome. We only report the performance of
DPP∗5 and DPP∗10 among the family of DPP∗ rules on the following four data sets.
c) The Alzheimer’s disease neuroimaging initiative (ADNI; available at www.loni.ucla.edu/ADNI)
studies the disease progression of Alzheimer’s. The ADNI data set includes 434 patients with 306
features extracted from their baseline MRI scans. Each time we randomly select 90% samples to
construct the data matrix X ∈ (cid:60)391×306. The response y is the patients’ MMSE cognitive scores
[29]. d) The Olivetti faces data set includes 400 grey scale face images of size 64× 64 for 40 people
(10 for each). Each time  we randomly take one of the images as the response vector y ∈ (cid:60)4096

7

(a) ADNI

(b) Olivetti

Figure 2: Comparison of DPP∗/SDPP∗ rules and Dome on three real data sets  Yahoo computers
data set  ADNI data set  Olivetti face data set and one synthetic data set.

(c) Yahoo-Computers

(d) Synthetic

(a) 20 groups

(b) 50 groups

(c) 100 groups

Figure 3: Performance of GDPP and SGDPP applied to three synthetic data sets.

and the data matrix X ∈ (cid:60)4096×399 is constructed by the left ones. e) The Yahoo data sets include
11 top-level categories such as Computers  Education  Health  Recreation  and Science etc. Each
category is further divided into a set of subcategories. Each time  we construct a balanced binary
classiﬁcation data set from the topic of Computers. We choose samples from one subcategory as the
positive class and randomly sample an equal number of samples from the rest of subcategories as
the negative class. The size of the data matrix is 876 × 25259 and the response vector is the binary
label of the samples. f) For the synthetic data set X ∈ (cid:60)100×5000 and the response vector y ∈ (cid:60)100 
all of the entries are i.i.d. by a standard Gaussian.
The average λmax of the above three data sets are 0.7273  0.989  0.914  and 0.371 respectively.
The predictors in ADNI  Yahoo-Computers and Olivetti data sets are highly correlated as indicated
by the average λmax. In contrast with the real data sets  the average λmax of the synthetic data is
small. As noted in [26  25]  Dome is very effective in discarding inactive features when λmax is
large. From Fig. 2  we observe that Dome performs much better on the real data sets compared to
the synthetic data. However  the proposed rules are able to identify far more inactive features than
Dome on both real and synthetic data  even for the cases in which λmax is small.
4.2 GDPPs for the Group Lasso Problem
We apply GDPPs to three synthetic data sets. The entries of data matrix X ∈ (cid:60)100×1000 and the
response vector y are generated i.i.d.
from the standard Gaussian distribution. For each of the
cases  we randomly divided X into 20  50  and 100 groups. We compare the performance of GDPP
and SGDPP along a sequence of 100 parameter values equally spaced on the λ/λmax scale. We
repeat the above procedure 100 times for each of the cases and report the average performance. The
average λmax values are 0.136  0.167  and 0.219 respectively. As shown in Fig. 3  it is expected
that SGDPP signiﬁcantly outperforms GDPP which only makes use of the information of the dual
optimal solution at a single point. For more discussions  please refer to the supplement.
5 Conclusion
In this paper  we develop new screening rules for the Lasso problem by making use of the nonex-
pansiveness of the projection operator with respect to a closed convex set. Our new methods  i.e. 
DPP rules  are able to effectively identify inactive predictors of the Lasso problem  thus greatly re-
ducing the size of the optimization problem. Moreover  we further improve DPP rules and propose
the enhanced DPP rules  that is  the DPP∗ rules  which are even more effective in discarding inactive
predictors than DPP rules. The idea of DPP and DPP∗ rules can be easily generalized to screen the
inactive groups of the group Lasso problem. Extensive experiments on both synthetic and real data
demonstrate the effectiveness of the proposed rules. Moreover  DPP and DPP∗ rules can be com-
bined with any Lasso solver as a speedup tool. In the future  we plan to generalize our idea to other
sparse formulations consisting of more general structured sparse penalties  e.g.  tree/graph Lasso.
Acknowledgments
This work was supported in part by NIH (LM010730) and NSF (IIS-0953662  CCF-1025177).

8

References
[1] S. R. Becker  E. Cand`es  and M. Grant. Templates for convex cone problems with applications to sparse

signal recovery. Technical report  Standford University  2010.

[2] D. P. Bertsekas. Convex Analysis and Optimization. Athena Scientiﬁc  2003.
[3] A. Bruckstein  D. Donoho  and M. Elad. From sparse solutions of systems of equations to sparse modeling

of signals and images. SIAM Review  51:34–81  2009.

[4] E. Cand`es. Compressive sampling. In Proceedings of the International Congress of Mathematics  2006.
[5] S. S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Review 

43:129–159  2001.

[6] D. L. Donoho and Y. Tsaig. Fast solution of l-1 norm minimization problems when the solution may be

sparse. IEEE Transactions on Information Theory  54:4789–4812  2008.

[7] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics  32:407–

499  2004.

[8] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination in sparse supervised learning. Paciﬁc

Journal of Optimization  8:667–698  2012.

[9] J. Fan and J. Lv. Sure independence screening for ultrahigh dimensional feature spaces. Journal of the

Royal Statistical Society Series B  70:849–911  2008.

[10] J. Friedman  T. Hastie  H. H¨eﬂing  and R. Tibshirani. Pathwise coordinate optimization. Annals of Applied

Statistics  1:302–332  2007.

[11] J. Friedman  T. Hastie  and R. Tibshirani. Regularization paths for generalized linear models via coordi-

nate descent. Journal of Statistical Software  33:1–22  2010.

[12] S. J. Kim  K. Koh  M. Lustig  S. Boyd  and D. Gorinevsky. An interior-point method for large scale

l1-regularized least squares. IEEE Journal of Selected Topics in Signal Processing  1:606–617  2007.

[13] Y. Lecun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

In Proceedings of the IEEE  1998.

[14] J. Liu  S. Ji  and J. Ye. SLEP: Sparse Learning with Efﬁcient Projections. Arizona State University  2009.
[15] J. Mairal and B. Yu. Complexity analysis of the lasso regularization path. In ICML  2012.
[16] S. A. Nene  S. K. Nayar  and H. Murase. Columbia object image library (coil). Technical report  No.

CUCS-006-96  Dept. Comp. Science  Columbia University  1996.

[17] M. R. Osborne  B. Presnell  and B. A. Turlach. A new approach to variable selection in least squares

problems. IMA Journal of Numerical Analysis  20:389–404  2000.

[18] M. Y. Park and T. Hastie. L1-regularized path algorithm for generalized linear models. Journal of the

Royal Statistical Society Series B  69:659–677  2007.

[19] F. Samaria and A. Harter. Parameterisation of a stochastic model for human face identiﬁcation.

Proceedings of 2nd IEEE Workshop on Applications of Computer Vision  1994.

In

[20] R. Tibshirani. Regression shringkage and selection via the lasso. Journal of the Royal Statistical Society

Series B  58:267–288  1996.

[21] R. Tibshirani  J. Bien  J. Friedman  T. Hastie  N. Simon  J. Taylor  and R. Tibshirani. Strong rules for
discarding predictors in lasso-type problems. Journal of the Royal Statistical Society Series B  74:245–
266  2012.

[22] N. Ueda and K. Saito. Parametric mixture models for multi-labeled text. Advances in neural information

processing systems  15:721–728  2002.

[23] J. Wright  Y. Ma  J. Mairal  G. Sapiro  T. Huang  and S. Yan. Sparse representation for computer vision

and pattern recognition. In Proceedings of IEEE  2010.

[24] T. T. Wu  Y. F. Chen  T. Hastie  E. Sobel  and K. Lange. Genomewide association analysis by lasso

penalized logistic regression. Bioinformatics  25:714–721  2009.

[25] Z. J. Xiang and P. J. Ramadge. Fast lasso screening tests based on correlations. In IEEE ICASSP  2012.
[26] Z. J. Xiang  H. Xu  and P. J. Ramadge. Learning sparse representation of high dimensional data on large

scale dictionaries. In NIPS  2011.

[27] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society Series B  68:49–67  2006.

[28] P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning Research 

7:2541–2563  2006.

[29] J. Zhou  L. Yuan  J. Liu  and J. Ye. A multi-task learning formulation for predicting disease progression.

In KDD  pages 814–822. ACM  2011.

9

,Jie Wang
Jiayu Zhou
Peter Wonka
Jieping Ye
George Mohler
Lijun Zhang
Zhi-Hua Zhou