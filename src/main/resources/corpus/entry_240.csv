2018,A Unified Framework for Extensive-Form Game Abstraction with Bounds,Abstraction has long been a key component in the practical solving of large-scale extensive-form games. Despite this  abstraction remains poorly understood. There have been some recent theoretical results but they have been confined to specific assumptions on abstraction structure and are specific to various disjoint types of abstraction  and specific solution concepts  for example  exact Nash equilibria or strategies with bounded immediate regret. In this paper we present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees---while maintaining comparable bounds on abstraction quality. Moreover  our framework gives an exact decomposition of abstraction error in a much broader class of games  albeit only in an ex-post sense  as our results depend on the specific strategy chosen. Nonetheless  we use this ex-post decomposition along with slightly weaker assumptions than prior work to derive generalizations of prior bounds on abstraction quality. We also show  via counterexample  that such assumptions are necessary for some games. Finally  we prove the first bounds for how $\epsilon$-Nash equilibria computed in abstractions perform in the original game. This is important because often one cannot afford to compute an exact Nash equilibrium in the abstraction. All our results apply to general-sum n-player games.,A Uniﬁed Framework for Extensive-Form Game

Abstraction with Bounds

Christian Kroer

Computer Science Department

Pittsburgh  PA 15213
ckroer@cs.cmu.edu

Tuomas Sandholm

Computer Science Department

Pittsburgh  PA 15213

sandholm@cs.cmu.edu

Abstract

Abstraction has long been a key component in the practical solving of large-scale
extensive-form games. Despite this  abstraction remains poorly understood. There
have been some recent theoretical results but they have been conﬁned to speciﬁc
assumptions on abstraction structure and are speciﬁc to various disjoint types of
abstraction  and speciﬁc solution concepts  for example  exact Nash equilibria
or strategies with bounded immediate regret. In this paper we present a uniﬁed
framework for analyzing abstractions that can express all types of abstractions
and solution concepts used in prior papers with performance guarantees—while
maintaining comparable bounds on abstraction quality. Moreover  our framework
gives an exact decomposition of abstraction error in a much broader class of games 
albeit only in an ex-post sense  as our results depend on the speciﬁc strategy
chosen. Nonetheless  we use this ex-post decomposition along with slightly weaker
assumptions than prior work to derive generalizations of prior bounds on abstraction
quality. We also show  via counterexample  that such assumptions are necessary for
some games. Finally  we prove the ﬁrst bounds for how ✏-Nash equilibria computed
in abstractions perform in the original game. This is important because often one
cannot afford to compute an exact Nash equilibrium in the abstraction. All our
results apply to general-sum n-player games.

1

Introduction

Game-theoretic equilibria have played a key role in several recent advances in the ability to construct
AIs with superhuman performance in games with imperfect information [5  9  32]. In particular these
results rely on computing an approximate Nash equilibrium [33] for the game at hand. In typical
real-world situations these games are so large that even approximate equilibria are intractable. Instead 
the dominant paradigm has been to ﬁrst construct some smaller abstraction of the game  apply an
iterative algorithm for computing a Nash equilibrium in the abstraction  and map the resulting strategy
back to the full game. This approach was used in the recent Libratus agent  which beat four top poker
pros in the game of heads-ups no-limit Texas hold’em [9] (in addition to abstraction and equilibrium
approximation the agent also utilized real-time subgame solving [8] and action abstraction reﬁnement).
Abstraction has also been used in trading-agent competitions [39] and security games [1–3].
In practice  abstractions are generated heuristically with no theoretical guarantees on solution qual-
ity [4  10  14–16  18–22  24  34  36]. Ideally  abstraction would be lossless  such that implementing
an equilibrium from the abstract game results in an equilibrium in the full game. Gilpin and Sandholm
[17] study lossless abstraction techniques for a structured class of games. Unfortunately  lossless
abstraction often leads to games that are still too large to solve. Thus  one must turn to lossy
abstraction. However  signiﬁcant abstraction pathologies (nonmonotonicities) have been shown
in games which cannot exist in single-agent settings: if an abstraction is reﬁned  the equilibrium
strategy from that new abstraction can be worse in the original game than the equilibrium strategy

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

from a coarser abstraction [37]! Lossy abstraction remains poorly understood from a theoretical
perspective. Results have been obtained only for various restricted models of abstraction. Basilico
and Gatti [3] give bounds for the special game class called patrolling security game. Sandholm
and Singh [35] provide lossy abstraction algorithms with bounds for stochastic games. Brown and
Sandholm [6]  Waugh et al. [38]  Brown and Sandholm [7]  and ˇCermák et al. [12] develop iterative
abstraction-reﬁnement schemes that have various forms of converge guarantees but they do not give
solution-quality guarantees for the original game for strategies computed in limited-size abstractions.
Results which are for extensive-form games (EFGs) are most related to this work. Lanctot et al. [31]
show that the counterfactual regret minimization algorithm (CFR) converges to an approximate NE
when run on an imperfect-recall abstraction that is a skew well-formed game (SWF) with respect to
the original game  where the error in the NE has a linear dependence on the number of information
sets. Kroer and Sandholm [27] show that Nash equilibria and strategies with bounded counterfactual
regret computed in chance-relaxed SWF (CRSWF) (a generalization of SWF that allows error in
chance outcomes) are approximate NE in the original game  with a linear dependence on game-tree
height. Kroer and Sandholm [25] show that NE computed in perfect-recall abstractions that satisfy
conditions that are similar to those in CRSWF abstractions are approximate NE in the original game
with a constant dependence on payoff error (as opposed to a linear dependence on height in Kroer and
Sandholm [27] or linear dependence on information sets in Lanctot et al. [31]). Kroer and Sandholm
[26] extend the results of Kroer and Sandholm [25] to continuous action spaces.
The results in the previous paragraph are all for disparate models of abstraction  a speciﬁc solution
concept  or speciﬁc algorithm. Yet they share a common structure on the assumptions needed in
order to obtain theoretical results. They assume that information sets (i.e.  decision points) are
aggregated into larger information sets. All pairs of information sets that are aggregated together are
compared by deﬁning a mapping between subtrees under the information sets. This mapping then
requires that the payoffs are similar  the distribution over chance outcomes is similar  and for pairs
of leaves mapped to each other  the leaves have the same sequence of information-set-action pairs
leading to them in the abstraction. Having similar payoffs and chance-outcomes under aggregated
information sets is natural. However  the requirement that information-set-action pairs are the same
for leaf nodes mapped to each other is not satisﬁed by the best heuristic abstraction algorithms used
in practice [10  14  24]. In this paper we develop an exact decomposition of the solution-quality error
that does not require any such assumption. This is the ﬁrst decomposition of solution-quality error
resulting from abstraction. This decomposition depends on several quantities that prior results did
not (owing to its more general and exact nature). We then show that by making a weaker variant of
previous assumptions  our decomposition can recover all previous solution-quality bounds. We show
via counterexample that there exist games where the assumption on information-set-action pairs is 
in a sense  necessary in order to avoid large abstraction error that is not measurable by the type of
technique presented here and in prior work.
Finally  we prove the ﬁrst bounds for how ✏-Nash equilibria computed in abstractions perform in
the original game. This is important because often one cannot afford to compute an exact Nash
equilibrium in the abstraction. All our results apply to general-sum n-player games.

2 Extensive-form games (EFGs)

An extensive-form game (EFG) is a game tree  where each node in the tree corresponds to some
history of actions taken by the players. Each node belongs to some player  and the actions available to
the player at a given node are represented by the branches. Uncertainty is modeled by having a special
player  Chance  that moves with some predeﬁned ﬁxed probability distribution over actions. EFGs
model imperfect information by having groups of nodes in information sets  where an information set
is a group of nodes all belonging to the same player such that the player cannot distinguish among
them. In the original game that we are trying to solve  we assume perfect recall  which requires that
no player forgets information they knew earlier in the game. This is a natural condition since you
generally cannot force players to forget information  and it would not be in their interest to do so.
Formally  an extensive-form game  is a tuple (H  Z  A  P  ⇡0 {Ii} {ui}). H is the set of nodes
in the game tree  corresponding to sequences (or histories) of actions. Hi is the subset of histories
belonging to Player i. Z ✓ H is the set of terminal histories  or leaves. A is the set of actions in the
game. AI denotes the set of actions available at nodes in information set I. P   the player function 
maps each non-terminal history h 2 H \ Z to {0  . . .   n}  representing the player whose turn it is to

2

move after history h. If P (h) = 0  the player is Chance. ⇡0 is a function that assigns to each h 2 H0
the probability of reaching h due to Chance (i.e.  assuming that both players play to reach h). An
information set Ii  for i 2{ 1  . . .   n}  is a partition of {h 2 H : P (h) = i}. The utility function ui
maps z 2 Z to the utility obtained by player i when the terminal history is reached.
A behavioral strategy i for a player i is a probability distribution over actions at each information
set in Ii. A strategy proﬁle  is a behavioral strategy for each player. The probability that  puts
on a 2 AI is denoted (I  a). We let ⇡(z) and ⇡(I) denote the probability of reaching z and I
respectively  if players choose actions according to . We likewise let ⇡(z|I) and ⇡( ˆI|I) denote
the reach probabilities conditioned on being at information set I. If the probability of reaching I is
zero due to players excluding i then we deﬁne . For a given strategy proﬁle  we let I!a denote the
same strategy except that I!a(I  a) = 1.
We will often quantify statements over the set of leaves or information sets that are reachable from
some given information set I belonging to Player i  sometimes conditioned on taking a speciﬁc action
a 2 AI. We let ZI DI ⇢I i be the set of leaves and information sets reachable conditioned on
being at information set I. We let ZI and DI ⇢I i be the set of leaves and information sets that are
reachable without Player i taking any further actions before reaching them. We let Z a
I and
I be deﬁned analogously but conditioned on taking action a 2 AI.
Da
As is usual we use the subscript i to denote exclusion of Player i  for example  i is the set of
behavioral strategies in  except for the strategy of Player i  and ⇡
i(z) is the probability of reaching
leaf node z disregarding actions taken by Player i  that is  assuming that Player i plays to reach z.

I  Da

I   Za

3 Game abstractions

We start by giving an intuitive description of how we model abstraction. We are given some perfect-
recall EFG  for which we would like to compute a (possibly approximate) Nash equilibrium. Instead
of solving  directly  we assume that we are given some abstraction of  called 0. Throughout
we will assume that 0 is itself an EFG  though it is allowed to be imperfect recall  unlike the
original game. The high-level idea is to compute some approximate solution to 0  and then use that
approximate solution to construct a strategy for . The type of approximate solution computed for 0
may vary. For example  computing an exact Nash equilibrium in 0 may be overkill  since what we
ultimately care about is how strong of a strategy proﬁle we get in the original game. This is especially
true when the abstraction has imperfect recall  in which case a Nash equilibrium is NP-hard to ﬁnd 
and it sufﬁces to ﬁnd a strategy with low counterfactual regret at every information set. We consider
several notions of solution to the abstract game.
Once we have an abstraction and a solution thereof  the primary question that we ask in this paper is
whether we can construct a solution to the original game that is provably near-optimal. To answer
this question we need a way to reason about the differences between the original game and the
abstract game. We do this by setting up a mapping between the real game and the abstract game:
every information set in the real game is assumed to map onto a speciﬁc abstract information set.
The strategy that we construct for the real game is such that the distribution over actions at a given
information set is constructed from the distribution over actions at the abstract information set that it
maps onto. In order to analyze the quality of the obtained strategy we propose a two-step process for
measuring differences between the real and abstract game: In the ﬁrst step we think of the original
game mapping onto an information reﬁnement of the abstraction  where the reﬁnement is the abstract
game but with some abstract information sets reﬁned into two or more new information sets. The
information reﬁnement has to be at least ﬁne-grained enough to entail perfect recall  although it
may be useful in practice to consider reﬁnement even of perfect recall information sets. We set up
measures of how different payoffs and probability distributions are in the original game versus in
the reﬁnement  where these measurements are based on how information sets and actions from the
real game are mapped onto the reﬁnement of the abstraction. In the second step  we measure the
difference between the reﬁnement and the abstract game. This is again done by measuring differences
in payoffs and probability distributions  this time between each information set in the reﬁnement
and the larger abstract information set that it was reﬁned from in the abstraction. This process is
illustrated in Figure 1. Figure 2 shows an example of how that construction might be arrived at in
practice  though note that our framework does not require the abstraction to be one that is arrived at
via game-tree modiﬁcations like this.

3

1
3

1
3

1
3

1

`

2

`
0

h
3

h
2

`
0

h
1

1

`

2

h
2

1

`

2

`
0

h

3 + ✏

`
0

h

1 + ✏

`
0

h
1

1
3

h
2

`
0

h
2

2
3

1

`

2

`
0

h
3

h
2

`
0

h
1

1

`

2

`
0

h
1

h
2

`
0

h
2

Figure 1: Abstraction example. Left: Original EFG. Right: Abstraction (which has perfect recall
in this case). Dotted red arrows denote the mapping of information sets in the original game onto
information set partitions in the abstract game. The dotted orange line in the abstract game denotes
an information set coarsening relative to ˜I.

Original game

Perfect-recall reﬁnement

Abstraction

1

`

2

`
0

h
1

1
3

h
2

`
0

h
2

1
3

1

`

2

`
0

h
3

h
2

`
0

h
1

1
3

Abstract right branch

onto middle branch

1
3

1

`

2

h
2

1

`

2

`
0

h

3 + ✏

`
0

h

1 + ✏

`
0

h
1

Coarsen Player 2
information sets

1
3

2
3

1

1

2
3

1

h
2

`
0

h
2

`

2

`
0

h
3

h
2

`

2

`
0

h
1

`
0

h
1

h
2

`
0

h
2

`

2

`
0

h
3

h
2

`
0

h
1

Figure 2: Example of how an abstraction could be constructed. First the rightmost red branch is
removed. Second the information sets for Player 2 are coarsened as shown by the red dotted line.

The step where the original game is mapped onto a reﬁnement would typically be used to model
action removal: say we have three actions a1  a2  a3 available at an information set  in the abstraction
we may want to have only a1  a2 and consider a3 as mapped onto a2. The reﬁnement step can only
model information coarsening  but is very powerful for modeling certain practical types of abstraction.
As an example  in poker research cards have typically been abstracted via information coarsening 
say treating a pair of aces and a pair of kings as the same hand in the abstraction. We can model this
in the reﬁnement step  where aces and kings would be reﬁned into two separate information sets.
We now give a formal description of our framework. As noted above  we consider abstractions that
are themselves EFGs  but we do not require abstractions to have perfect recall (the leading practical
abstractions are of imperfect recall [10  14  24]). We will use the original game to refer to some
perfect-recall game = ( H  Z  A  P  ⇡0 {Ii} {ui}) that we would like to compute a Nash equilib-
rium for. We use the abstract game to refer to some other game 0 = (H0  Z0  A0  P 0 ⇡ 00 {I0i} {u0i})
that is an abstraction of . The goal is to compute a (possibly approximate) equilibrium in the
abstraction  and map the resulting strategy proﬁle to the full game. For analytical purposes we will
also introduce an intermediary third game  which we will refer to as the perfect-recall reﬁnement
˜= ( ˜H  ˜Z  ˜A  ˜P   ˜⇡0 {˜Ii} {˜ui}). The perfect-recall reﬁnement ˜ has the same game tree as the
abstraction 0 (and thus ˜H = H0  ˜Z = Z0  ˜A = A0  ˜P = P 0  ˜⇡0 = ⇡00  and ˜u = u0)  but the
information sets must be reﬁned relative to   i.e. each information set is either intact  or partitioned
into several ﬁner information sets. Thus ˜ has a ﬁner-grained (i.e. less) abstraction than 0. ˜ is
assumed to be a perfect-recall game  unlike 0. Our deﬁnition of ˜ is analogous to that of Lanctot
et al. [31] and Kroer and Sandholm [27].
We model abstraction as a two-stage process. First  the full game is mapped onto ˜  with every
original information set I 2I i mapping onto some ˜I in ˜ via a function f : I! ˜I that maps I
surjectively onto ˜I. In Figure 1  each of the three original information sets belonging to Player 2 map
onto the same reﬁnement information set  but the leftmost original information set maps onto the left
partition  whereas the center and right information sets map onto the right partition. In the abstract
game in Figure 1  Player 2 has two subsets in ˜I: the left and right sides of their single information set.
Actions are similarly mapped with an action mapping g : A ! ˜A that maps each AI surjectively onto
˜Af (I). It is assumed that f respects the information-set tree structure by mapping Da
I surjectively
onto ˜Dg(a)
f (I). The ﬁnal part of the ﬁrst step is a way to map leaf nodes under original information sets
to leaf nodes under the corresponding abstract information set. For each information set I and action
a 2 AI  we require a surjective leaf-node mapping from the set of leaf nodes reached below I  a
before player i acts again  Za
The second step in our abstraction model captures the differences between the abstract game 0
and ˜. This is done by comparing the distribution over leaf nodes conditioned on being at a given

I   onto ˜Za0

f (I).

4

I0

I0

for each a0 in a way such that { ˜I(z0) : z0 2 ˜Z a0

˜I 2 ˜I versus the distribution conditioned on being at the corresponding abstract information set I0.
In Figure 1 this would correspond to comparing the leaf nodes under e.g. the right pair of nodes in
Player 2’s information set in the abstraction to the leaf nodes in the overall information set for Player
2. For each partition ˜I this is done with a set-valued map  ˜I that maps the set of leaf nodes ˜Z a0
˜I onto
Z0 a0
. For a given
partition ˜I  we let ˜D ˜I and ˜D ˜I be the set of descendant and child partitions  respectively  that can be
reached from ˜I.
For a strategy proﬁle 0 computed in 0 we need a way to interpret it as strategy proﬁles in . We
use the natural extension of a lifted strategy  originally developed by Sandholm and Singh [35] for
stochastic games  to EFGs. Intuitively  a lifted strategy "0 is a strategy where for any abstract
action a0  the sum of probabilities in "0 assigned to actions that map to a0 is equal to the probability
placed on a0 in 0.
Deﬁnition 1 (Strategy lifting). Given an abstract strategy proﬁle 0  a lifted strategy proﬁle is any

˜I } speciﬁes a partitioning of Z0 a0

strategy proﬁle "0 such that for all I  all a0 2 A0f (I):Pa2g1(a0) "0(I  a) = 0(f (I)  a0).

i(I) > 0; otherwise it is 0. Analogously  W 0

We use the deﬁnition of counterfactual value of an information set  introduced by Zinkevich et al. [40] 
to reason about the value of an information set under a given strategy proﬁle. The counterfactual value
of an information set I is the expected utility of the information set  assuming that all players follow
strategy proﬁle   except that Player i plays to reach I. It is deﬁned as V 
⇡(z|I)ui(z)
: I0i ! R is the corresponding function
when ⇡
for the abstract game. Note that Zinkevich et al. [40] further multiply the value by the reach
excluding i  whereas we do not. For the information set Ir that contains just the root node r  we have
i (r)  which is the value of playing the game with strategy proﬁle . We assume that at
V 
i (Ir) = V 
the root node it is not Chance’s turn to move. This is without loss of generality since we can insert
dummy player nodes above a root node belonging to Chance.
Kroer and Sandholm [25] showed that for an information set I  Vi(I) can be written as a sum over
descendant information sets

i (I) =Pz2ZI

i

V 

i (I) = Xa2AI

(I  a) XJ2Da

I

i(J|I)V 
⇡

i (J) + Xz2Za

I

⇡

i(z|I)ui(z) 

(1)

The form stated here is slightly different from the one given by Kroer and Sandholm [25]. They
assume that information sets have either only leaf nodes or only information sets immediately beneath
them  but this slightly more general statement follows easily from their proof. The value of Wi( ˜I)
can be written similarly.
We will show results for three different solution concepts that come up in practice. An ✏-Nash
equilibrium is a strategy proﬁle  such that V 
i (r)  ✏ for all players i and ˆ = (i  ˆi).
In other words  each player can gain at most ✏ by deviating to any other strategy ˆi. This is what
is computed by approaches based on ﬁrst-order methods [23  28  29]. A Nash equilibrium is an
✏-Nash equilibrium where ✏ = 0. Finally  a strategy proﬁle  has bounded counterfactual regret
if for all i  I 2I   and a 2 AI  V I!a
i (I) + r(I). Strategy proﬁles with bounded
counterfactual regret are important because regret minimization algorithms for EFGs converge by
producing strategies with low ⇡i(I)r(I) [9  11  13  30  40].

i (r)  V ˆ

(I)  V 

i

4 Measuring differences between the original game and the abstract game

Our goal is to show a decomposition of the utility difference between the original game and the
abstract game when using a lifted strategy. In order to do this  we need a way to measure differences
between the original and the reﬁned game. We measure payoff differences between nodes as

We measure leaf-node reach-probability differences conditioned on the real and abstract strategies
  0  and action a  at a given information set I versus its corresponding abstract information set-

R
i (z  ˜z) = ui(z)  ˜ui(˜z).

5

partition f (I) as follows

P

i(˜z|I  a   0) = Xz2 1(z0):z2Za

I

i(z|I)  ⇡0
⇡

i(z0|f (I)) 

for z0 2 Z0 a0

I0

.

We will also need to measure the difference in probability of reaching information set partitions 
conditioned on being at the preceding information set partition belonging to the same player 

P

i( ˜I|I  a   0) = XJ2f1( ˜I)
i(J|I  a) = 0.

i(J|I  a)  ⇡0
⇡

i( ˜I|f (I)).

Note that while the set f1( ˜I) can include information sets J that do not come after I  a  such
information sets are irrelevant since ⇡
We now prove a technical lemma that will be used as the primary tool for inductively proving that
strategies from abstractions have bounded regret.
Lemma 1. For any information set I and pair of lifted strategy proﬁles   0  assume there is a
bound (J  f (J)) such that V 
i (I)  W 0
V 

i (f (J))  (J  f (J)) for all J 2 Da

i (J)  W 0

i (z  (z)) +X˜z2 ˜Zg(a)

(I  a) Xz2Za
i (f (I))  Xa2AI
i(J|I)(J  f (J)) + X˜I2 ˜Dg(a)
+ XJ2Da
i (J)  W 0

I   a 2 AI. Then
i(˜z|I  a   0)˜ui(˜z)
P
i ( ˜I)
I and a 2 AI.
We now introduce a shorthand for denoting the utility difference attributable to differences between
a given information set I and its abstract counterpart f (I). This is the utility difference that would
arise from recursively applying Lemma 1 to information sets.

i (f (J)) = (J  f (J0)) for all J 2 Da

i( ˜I|I  a   0)W 0
P

The above holds with equality if V 

⇡
i(z|I)R

⇡

f (I)

f (I)

I

I

M(I   0

i)

def

= Xa2AI
+ XJ2Da

I

(I  a) Xz2Za

I

i(J|I)M( J    0
⇡

⇡
i(z|I)R

i (z  (z)) +X˜z2 ˜Zg(a)
i) + X˜I2 ˜Dg(a)

f (I)

f (I)

i(˜z|I  a   0)˜ui(˜z)
P
i ( ˜I)

i( ˜I|I  a   0)W 0
P

It follows from Lemma 1 that the players’ values in any lifted strategy proﬁle in the original game are
close to the players’ values of the corresponding abstract strategy proﬁle:
Lemma 2. Given any abstract strategy proﬁle 0  any lifted strategy proﬁle "0 achieves utility

W 0

i (r0) = V "0

i

(r)  M(r  "0  0

i)

Next we derive an expression for the difference between an abstract information set and any ˜I in its
partitioning. We will need a way to measure the difference between an information set I0 and any
partition ˜I. For reach probability  we let

P (˜z| ˜I 0) = ⇡0(˜z| ˜I)  Xz02 ˜I (˜z)

⇡0(z0|I0)

(2)

be the difference between the probability of arriving at ˜z conditioned on a strategy 0 and being in
partition ˜I of I0 and the probability of arriving at any leaf node z0 2 1
(˜z) conditioned on the same
strategy 0 and being in I0. For reward differences we let the utility difference between a leaf node
z0 2 ZI0 and its corresponding leaf node ˜z = 1

(3)
where  ˜I > 0 is an arbitrary scalar value that can be chosen to reﬂect the fact that we only need payoffs
to be similar in a relative sense (for example  consider two subtrees with the same payoffs except that
one subtree has all payoffs scaled by a constant; these subtrees are strategically equivalent).
These terms allow us to measure the difference between the value W 0
information set I0 and any ˜I in its partition. We let P( ˜I 0) denote this difference.

i (I0) and W 0

i ( ˜I) for any

(z0) in Z ˜I be
i (z0| ˜I) = ˜ui(˜z)   ˜Iui(z0)
R

˜I

˜I

6

Lemma 3. For any player i  abstract strategy proﬁle 0  information set I0 and any ˜I in its partition 

W 0
i ( ˜I)   ˜IW 0

i (I0) = Xz02Z0I0

⇡0(z0|I0)R

i (z0| ˜I) +X˜z2Z ˜I

5 An exact decomposition of abstraction error

P (˜z| ˜I 0)ui(z0)

def
=P( ˜I 0)

here ⇤ = (⇤i   "0

i) +XI2Ii

i)  M(r  "0  0

Our ﬁrst theorem shows that an ✏-Nash equilibrium in the abstract game maps to an ✏0-Nash equi-
librium in the original game  where ✏0 depends on the difference terms introduced in the previous
section. We say that the abstract game has a cycle if there exists a sequence of information sets
I01  . . .   I0k such that for all j 6= k there exist nodes h0j 2 I0j  h0j+1 2 I0j+1 such that h0j is an ancestor
of h0j+1  and I01 is equal to I0k. The next theorem assumes the abstract game is acyclic. This enables
induction over information sets.
Theorem 1. Given an ✏0-Nash equilibrium 0 for an acyclic abstract game  any lifted strategy proﬁle
"0 is an ✏-Nash equilibrium in the original game where ✏ = maxi2N ✏i and
✏i =✏0 +M( r  ⇤  0

⇡⇤(I) [P( f (I)  ⇤0I0!I)  P(I0I  ⇤0)]
i ) is "0 except Player i plays any best response strategy for the original game 
⇤0 = (⇤0i   0i) is such that ⇤0(I0  a0) =Pg1(a0) ⇤(I  a) where I 2 f1(I0) is chosen for each
(r)  and ⇤0I0!I is ⇤0 except that at I0 we set the strategy according to
I  i.e. ⇤0(I0  a0) =Pg1(a0) ⇤(I  a).

This theorem is the ﬁrst to show results for mapping an ✏0-Nash equilibrium in the abstract game to
an ✏-Nash equilibrium in the original game. Prior results have been for abstract strategies that are
either exact Nash equilibria [25] or with bounded counterfactual regret [27  31]. That is because all
prior proofs were based on applying a worst-case counterfactual regret bound as part of the inductive
step (which works for exact Nash equilibrium or strategies with bounded counterfactual regret but
not ✏-Nash equilibrium); our proof instead constructs an expression for W ⇤0
(r0) (i.e.  for the value
of the whole abstract game) before using the fact that 0 is an ✏-Nash equilibrium. We next show that
our framework can also measure differences for strategies with bounded counterfactual regret.
Theorem 2. For an abstract strategy proﬁle 0 with bounded counterfactual regret r(I0) at every
information set I0 2I 0  any lifted strategy proﬁle "0 is an ✏-Nash equilibrium with

I0 in order to maximize W ⇤0

i

i

✏i ✏

✏ = max
i2N

where ⇤ = (⇤i   "0

⇡⇤(I)⇥f (I)I r(f (I)) + P(f (I)  0I!⇤0)  P(I0I  0)⇤

i  XI2Ii
i) is "0 except for Player i best responding  and each 0I!⇤ is equal to 0

i)  M(r  "0  0
except that 0I!⇤(f (I)  a0) =Pa2g1(a0) ⇤(I  a) for all a0 2 Af (I).

We will show in the next sections that our two main theorems generalize prior results. In addition 
our theorems are the ﬁrst to give an exact expression for the abstraction error; the inequalities arise
only from inexactly solving the abstract game.

+M( r  ⇤  0

i)

6 Generalizing prior results

We now show that if the error in each conditional distribution over child leaves and information sets
depends only on error at Chance nodes then the exact results from the previous section subsume all
prior solution quality bounds for EFGs [25  27  31] (which also make that assumption or stronger
assumptions). For that we deﬁne measures of how well Chance outcomes are approximated in the
abstraction:

0(h  ˜a) = Xa2g1(˜a)

0(h) = X˜a2 ˜A(h)

0(h  ˜a)

Similarly for nodes in infosets belonging to Player i we have the following error

0(h  a)  0(˜h  ˜a) 
0(˜h|I) =Xh2I

7

⇡0(h|I)  ⇡0(˜h|f (I)).

I denote the set Za

In order to avoid dependence on the choice of strategy for Player i our result will measure the
worst-case loss over pure strategies for Player i. We let this set be i. We will use ~ato denote a
speciﬁc pure strategy  and we let Z~a
I such that a is the action chosen at I in ~a  and
similarly for D~a
I . In a slight abuse of notation  we let g(~a) denote the pure strategy in the abstract
game corresponding to ~awhen applying g.
Proposition 1. If an abstract strategy proﬁle 0 and a lifted strategy proﬁle "0 are such that for all
i 0( ˜I|I  a   0) = 0 then for
i  I 2I   P
all players i and  = (i  "0

i 0(z|I   0) = 0  and P

i 0(˜z|I  a   0) = 0  P
i ) we have

(I|~a) Xz2Z~a

I

⇡"0
i

(h0|I)A

0 (h0)⇡0

⇡"0
i

(z|I)R(z  (z))

i(˜z|˜h0  a0)˜ui(˜z)35

⇡"0

i)  2 max

~a2i XI2Ii
i(˜z|˜z[I]) +Xh2I Xh02H0:hvh0

M(r    0

i)  M(r  "0  0

f (I)

240(˜z[I]|I)⇡0
I X˜h2f (J)0(˜h[f (I)]|I)⇡0

+ X˜z2 ˜Zg(~a)
+ XJ2D~a
+Xh2I Xh02H0:hvh0

(h0|I)A

⇡"0
i

i(˜h|˜h[f (I)])
i(˜h|˜h0  ˜a)W 0

0 (h0)⇡0

i (f ()I)def

= Mi("0  0)

We can combine Proposition 1 with Theorem 1 to get a bound that is independent of the best-response
strategy:
Corollary 1. If 0 is an abstract ✏0-Nash equilibrium  satisﬁes the condition of Proposition 1  and
P is zero everywhere  then any lifted strategy proﬁle "0 is an ✏-Nash equilibrium where ✏ is less
than maxi2N Mi("0  0) + ✏0
This bound generalizes the bound of Sandholm and Singh [35] while simultaneously tightening their
bound.
The game class discussed by Kroer and Sandholm [25] is easily shown to satisfy the assumptions in
Proposition 1. Thus this shows a more general bound similar to that of Kroer and Sandholm [25] 
where we leave in several expectations rather than taking maxima everywhere (the result by Kroer
and Sandholm [25] required taking several maxima where we leave in the expectation because their
proof is based on upper-bounding as part of the inductive step). Therefore  Corollary 1 yields tighter
results despite also being more general.
Corollary 1 shows a result for ✏-Nash equilibrium computed in the abstraction. An analogous
corollary for abstract strategies with bounded immediate regret can easily be obtained by combining
Proposition 1 with Theorem 2.
We now show that  similar to mapping error  if the reach of leaf nodes in the original and abstract
game are the same without considering Chance moves  we can bound partitioning error with an
expression that does not depend on the best response ⇤i of Player i.
Proposition 2. If 0 is such that ⇡0
P( ˜I 0I!⇤)  P( ˜I 0)  2 max
0(˜z| ˜I  a0) Xz02 ˜I (˜z)h⇡0
+ X˜z2Z a0

0(z0|I0  a0) for all ˜I  a0  ˜z  z0 2  ˜I(˜z)  then
⇡0(z0|I0  a0)R
0 (˜z| ˜I  a0iidef

i (z0| ˜I)
= P( ˜I 0I!⇤  0)  80I!⇤

0(˜z| ˜I  a0) = ⇡0
a02AI0h Xz02Z a0
0 (z0|I0  a0))  ⇡0

⇡0

I0

˜I

This can be combined with our main theorems in order to get results for ✏-Nash equilibrium or
strategies with bounded regret where the partition error does not depend on the best response.
Corollary 2.
I0 2I
any lifted strategy "0
PI2Ii

If 0 has bounded counterfactual regret r(I0) at every information set
then
is an ✏-Nash equilibrium where ✏ = maxi2N ✏i and ✏i 

⇡⇤(I)⇥f (I)I r(f (I)) + P(f (I)  0I!⇤  0)⇤

0  satisﬁes the condition of Proposition 2  and M is zero everywhere 

8

Kroer and Sandholm [27] took maxima in several places where we left in the expectation: they take a
maximum over the decisions of Player i in ⇡⇤(I)  and they maximize over the partitions in I0. Taking
these maxima avoids dependence on ⇤. Taking these maxima could easily be done in Corollary 2 as
well. Kroer and Sandholm [27] also separate the difference in conditional distribution over leaves
into separate terms for Chance error that occurs before and after reaching I0; this potentially leads
to a looser bound than ours (and never tighter since we could combine our Corollary 2 with their
separation). An analogue to Corollary 2 but for ✏-Nash equilibrium can be obtained by combining
Theorem 1 with Proposition 2.

6.1 Neccessity of distributional similarity of reach probabilities
We now show that the style of bound given by Lanctot et al. [31] as well as our corrolaries 1 and 2
cannot generalize to games where opponents do not have the same sequence of information-set-action
pairs  or in our case the slightly weaker requirements in Propositions 1 and 2  for game nodes that
map to each other in the abstraction. The two games that we will use as counterexamples are shown
in Figure 3. From the perspective of our results  the usefulness of assuming the same sequence of
information-set-action pairs is that it implies the condition used in Propositions 1 and 2; the following
counterexamples thus also show that this assumption is a useful way to disallow bad abstractions
such as the ones presented here (although overly restrictive from a practical perspective). Contrary to
the prior results  our Theorems 1 and 2 still apply to the games below. Our two theorems would give
weak bounds commensurate with the large error in the abstract equilibrium; this error is contained in
the terms that depend on P .

1

1
4

✏

r
1

`
1

1
4

1

✏

2

r
1

2

`
1

r
1

1
2

r

1

`

2

2

2

1
2

`

1

r

2

✏

v  ✏

0  0

v  0

0 ✏

v  0

0 ✏

v  ✏

0  0

1
4

r
1

✏

1
4

2

1

`
1

1

2

`
1

Figure 3: Left: General-sum EFG with abstraction. Right: zero-sum EFG with abstraction where
Player 1 wants to minimize. Orange dashed lines denote information sets joined in the abstraction.
Bold edges denote actions taken with probability 1 in the abstracted equilibrium.
On the left in Figure 3 is a general-sum game where the two nodes belonging to Player 1 are abstracted
into a single information set. If we map ` onto ` and r onto r we get an abstraction with low payoff
error: ✏ at every node. At a high level  the idea in this counterexample is that Player 2  because
their nodes are not abstracted  can play opposite actions in the left and right subtrees  thus changing
whether Player 1 prefers going left or right. In the original game Player 1 can react to this by choosing
different actions  but not in the abstraction. Formally: Let ✏> 0. Player 2 plays the bolded edges
at nodes with non-zero probability of being reached. In the abstraction  Player 1 gets v
2 for every
strategy. In the full game  Player 1 can choose ` in the left subtree and r in the right subtree for a
payoff of v. Thus in every equilibrium where Player 2 plays according to the bolded edges (which
2 from abstracting  despite the payoff error being
includes all equilibrium reﬁnements) Player 1 loses v
arbitrarily small. If we set ✏ = 0  equilibria where Player 2 plays the bolded edges still have high
loss—despite zero payoff error. This example showed that information-set-action structure has to be
taken into account in order to get satisfying bounds in general. While the example is very simple
(and can thus easily occur in the context of a larger game)  it does exploit the fact that Player 1 utility
is discontinuous in Player 2 utility. We next show that a more intricate counterexample can avoid
relying on this discontinuity.
On the right in Figure 3 is a zero-sum game where the two bottom information sets belonging to
Player 2 have been abstracted. Consider the following abstract equilibrium: Player 1 plays the bolded
edges with probability 1  and Player 2 plays `  r with equal probability. Player 2 gets expected utility
2  but in the full game Player 2 can choose ` (r) in the left (right) information set to get utility
 ✏
2 . Thus Player 2 has a utility loss of 1
2 despite a payoff error of 0. The idea in this example is that 
1✏
because Player 1 is not abstracted  they control the distribution over nodes in Player 2’s information
set in the abstraction in a way that is inconsistent with Player 2’s original-game information sets: in
the abstraction they get an equal distribution over nodes where ` or r is the preferred action  whereas
in the original game the corresponding strategy for Player 1 means that they know exactly which
node they are at.

9

Acknowledgments
This material is based on work supported by the National Science Foundation under grants IIS-
1718457  IIS-1617590  and CCF-1733556  and the ARO under award W911NF-17-1- 0082. Christian
Kroer is supported by a Facebook Fellowship.

References
[1] A. Basak  F. Fang  T. H. Nguyen  and C. Kiekintveld. Abstraction methods for solving graph-
based security games. In International Conference on Autonomous Agents and Multiagent
Systems  pages 13–33. Springer  2016.

[2] A. Basak  F. Fang  T. H. Nguyen  and C. Kiekintveld. Combining graph contraction and strategy
generation for green security games. In International Conference on Decision and Game Theory
for Security  pages 251–271. Springer  2016.

[3] N. Basilico and N. Gatti. Automated abstractions for patrolling security games. In AAAI

Conference on Artiﬁcial Intelligence (AAAI)  2011.

[4] D. Billings  N. Burch  A. Davidson  R. Holte  J. Schaeffer  T. Schauenberg  and D. Szafron.
Approximating game-theoretic optimal strategies for full-scale poker. In Proceedings of the
International Joint Conference on Artiﬁcial Intelligence (IJCAI)  2003.

[5] M. Bowling  N. Burch  M. Johanson  and O. Tammelin. Heads-up limit hold’em poker is solved.

Science  347(6218)  Jan. 2015.

[6] N. Brown and T. Sandholm. Regret transfer and parameter optimization. In AAAI Conference

on Artiﬁcial Intelligence (AAAI)  2014.

[7] N. Brown and T. Sandholm. Simultaneous abstraction and equilibrium ﬁnding in games. In

Proceedings of the International Joint Conference on Artiﬁcial Intelligence (IJCAI)  2015.

[8] N. Brown and T. Sandholm. Safe and nested subgame solving for imperfect-information games.
In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS) 
pages 689–699  2017.

[9] N. Brown and T. Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top

professionals. Science  page eaao1733  Dec. 2017.

[10] N. Brown  S. Ganzfried  and T. Sandholm. Hierarchical abstraction  distributed equilibrium
computation  and post-processing  with application to a champion no-limit Texas Hold’em
agent. In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS) 
2015.

[11] N. Burch  M. Lanctot  D. Szafron  and R. G. Gibson. Efﬁcient Monte Carlo counterfactual regret
minimization in games with many player actions. In Proceedings of the Annual Conference on
Neural Information Processing Systems (NIPS)  pages 1880–1888  2012.

[12] J. ˇCermák  B. Bošansky  and V. Lisý. An algorithm for constructing and solving imperfect
recall abstractions of large extensive-form games. In Proceedings of the International Joint
Conference on Artiﬁcial Intelligence (IJCAI)  pages 936–942  2017.

[13] G. Farina  C. Kroer  and T. Sandholm. Regret minimization in behaviorally-constrained zero-

sum games. In International Conference on Machine Learning (ICML)  2017.

[14] S. Ganzfried and T. Sandholm. Potential-aware imperfect-recall abstraction with earth mover’s
distance in imperfect-information games. In AAAI Conference on Artiﬁcial Intelligence (AAAI) 
2014.

[15] A. Gilpin and T. Sandholm. A competitive Texas Hold’em poker player via automated abstrac-
tion and real-time equilibrium computation. In Proceedings of the National Conference on
Artiﬁcial Intelligence (AAAI)  pages 1007–1013  2006.

10

[16] A. Gilpin and T. Sandholm. Better automated abstraction techniques for imperfect information
games  with application to Texas Hold’em poker. In International Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS)  pages 1168–1175  2007.

[17] A. Gilpin and T. Sandholm. Lossless abstraction of imperfect information games. Journal of

the ACM  54(5)  2007.

[18] A. Gilpin and T. Sandholm. Expectation-based versus potential-aware automated abstraction in
imperfect information games: An experimental comparison using poker. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence (AAAI)  2008. Short paper.

[19] A. Gilpin  T. Sandholm  and T. B. Sørensen. Potential-aware automated abstraction of sequential
games  and holistic equilibrium analysis of Texas Hold’em poker. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence (AAAI)  2007.

[20] A. Gilpin  T. Sandholm  and T. B. Sørensen. A heads-up no-limit Texas Hold’em poker player:
In

Discretized betting models and automatically generated equilibrium-ﬁnding programs.
International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)  2008.

[21] J. Hawkin  R. Holte  and D. Szafron. Automated action abstraction of imperfect information

extensive-form games. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2011.

[22] J. Hawkin  R. Holte  and D. Szafron. Using sliding windows to generate action abstractions in

extensive-form games. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2012.

[23] S. Hoda  A. Gilpin  J. Peña  and T. Sandholm. Smoothing techniques for computing Nash

equilibria of sequential games. Mathematics of Operations Research  35(2)  2010.

[24] M. Johanson  N. Burch  R. Valenzano  and M. Bowling. Evaluating state-space abstractions in
extensive-form games. In International Conference on Autonomous Agents and Multi-Agent
Systems (AAMAS)  2013.

[25] C. Kroer and T. Sandholm. Extensive-form game abstraction with bounds. In Proceedings of

the ACM Conference on Economics and Computation (EC)  2014.

[26] C. Kroer and T. Sandholm. Discretization of continuous action spaces in extensive-form games.
In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)  2015.

[27] C. Kroer and T. Sandholm. Imperfect-recall abstractions with bounds in games. In Proceedings

of the ACM Conference on Economics and Computation (EC)  2016.

[28] C. Kroer  K. Waugh  F. Kılınç-Karzan  and T. Sandholm. Faster ﬁrst-order methods for extensive-
form game solving. In Proceedings of the ACM Conference on Economics and Computation
(EC)  2015.

[29] C. Kroer  K. Waugh  F. Kılınç-Karzan  and T. Sandholm. Theoretical and practical advances on
smoothing for extensive-form games. In Proceedings of the ACM Conference on Economics
and Computation (EC)  2017.

[30] M. Lanctot  K. Waugh  M. Zinkevich  and M. Bowling. Monte Carlo sampling for regret mini-
mization in extensive games. In Proceedings of the Annual Conference on Neural Information
Processing Systems (NIPS)  2009.

[31] M. Lanctot  R. Gibson  N. Burch  M. Zinkevich  and M. Bowling. No-regret learning in
extensive-form games with imperfect recall. In International Conference on Machine Learning
(ICML)  2012.

[32] M. Moravˇcík  M. Schmid  N. Burch  V. Lisý  D. Morrill  N. Bard  T. Davis  K. Waugh 
M. Johanson  and M. Bowling. Deepstack: Expert-level artiﬁcial intelligence in heads-up
no-limit poker. Science  356(6337)  May 2017.

[33] J. Nash. Equilibrium points in n-person games. Proceedings of the National Academy of

Sciences  36:48–49  1950.

11

[34] T. Sandholm. Abstraction for solving large incomplete-information games. In AAAI Conference

on Artiﬁcial Intelligence (AAAI)  2015. Senior Member Track.

[35] T. Sandholm and S. Singh. Lossy stochastic game abstraction with bounds. In Proceedings of

the ACM Conference on Electronic Commerce (EC)  2012.

[36] J. Shi and M. Littman. Abstraction methods for game theoretic poker. In CG ’00: Revised
Papers from the Second International Conference on Computers and Games  pages 333–345 
London  UK  2000. Springer-Verlag.

[37] K. Waugh. Abstraction in large extensive games. Master’s thesis  University of Alberta  2009.
[38] K. Waugh  D. Morrill  D. Bagnell  and M. Bowling. Solving games with functional regret

estimation. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2015.

[39] M. P. Wellman  D. M. Reeves  K. M. Lochner  S.-F. Cheng  and R. Suri. Approximate strategic
reasoning through hierarchical reduction of large symmetric games. In Proceedings of the
National Conference on Artiﬁcial Intelligence (AAAI)  2005.

[40] M. Zinkevich  M. Bowling  M. Johanson  and C. Piccione. Regret minimization in games
with incomplete information. In Proceedings of the Annual Conference on Neural Information
Processing Systems (NIPS)  2007.

12

,Christian Kroer
Tuomas Sandholm