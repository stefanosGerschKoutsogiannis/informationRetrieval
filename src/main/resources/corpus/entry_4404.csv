2012,Priors for Diversity in Generative Latent Variable Models,Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data  so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization  for building density models of data  and for   providing features that can be used for later discriminative tasks. A significant limitation of these models  however  is that draws from the prior are often highly redundant due to i.i.d. assumptions   on internal parameters.  For example  there is no preference in the prior of a mixture model to make components non-overlapping  or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work  we revisit these independence assumptions for probabilistic latent variable models  replacing the   underlying i.i.d.\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions  we are able to define a DPP on probability measures.  We show how to perform MAP inference   with DPP priors in latent Dirichlet allocation and in mixture models  leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction  without compromising the generative aspects of the model.,Priors for Diversity in Generative

Latent Variable Models

James Y. Zou

Ryan P. Adams

School of Engineering and Applied Sciences

School of Engineering and Applied Sciences

Harvard University

Cambridge  MA 02138

jzou@fas.harvard.edu

Harvard University

Cambridge  MA 02138

rpa@seas.harvard.edu

Abstract

Probabilistic latent variable models are one of the cornerstones of machine learn-
ing. They offer a convenient and coherent way to specify prior distributions over
unobserved structure in data  so that these unknown properties can be inferred via
posterior inference. Such models are useful for exploratory analysis and visual-
ization  for building density models of data  and for providing features that can
be used for later discriminative tasks. A signiﬁcant limitation of these models 
however  is that draws from the prior are often highly redundant due to i.i.d. as-
sumptions on internal parameters. For example  there is no preference in the prior
of a mixture model to make components non-overlapping  or in topic model to
ensure that co-occurring words only appear in a small number of topics. In this
work  we revisit these independence assumptions for probabilistic latent variable
models  replacing the underlying i.i.d. prior with a determinantal point process
(DPP). The DPP allows us to specify a preference for diversity in our latent vari-
ables using a positive deﬁnite kernel function. Using a kernel between probability
distributions  we are able to deﬁne a DPP on probability measures. We show how
to perform MAP inference with DPP priors in latent Dirichlet allocation and in
mixture models  leading to better intuition for the latent variable representation
and quantitatively improved unsupervised feature extraction  without compromis-
ing the generative aspects of the model.

1

Introduction

The probabilistic generative model is an important tool for statistical learning because it enables rich
data to be explained in terms of simpler latent structure. The discovered structure can be useful in
its own right  for explanatory purposes and visualization  or it may be useful for improving general-
ization to unseen data. In the latter case  we might think of the inferred latent structure as providing
a feature representation that summarizes complex high-dimensional interaction into a simpler form.
The core assumption behind the use of latent variables as features  however  is that the salient sta-
tistical properties discovered by unsupervised learning will be useful for discriminative tasks. This
requires that the features span the space of possible data and represent diverse characteristics that
may be important for discrimination. Diversity  however  is difﬁcult to express within the generative
framework. Most often  one builds a model where the feature representations are independent a
priori  with the hope that a good ﬁt to the data will require employing a variety of latent variables.
There is reason to think that this does not always happen in practice  and that during unsupervised
learning  model capacity is often spent improving the density around the common cases  not allo-
cating new features. For example  in a generative clustering model based on a mixture distribution 
multiple mixture components will often be used for a single “intuitive group” in the data  simply be-
cause the shape of the component’s density is not a close ﬁt to the group’s distribution. A generative

1

mixture model will happily use many of its components to closely ﬁt the density of a single group of
data  leading to a highly redundant feature representation. Similarly  when applied to a text corpus 
a topic model such as latent Dirichlet allocation [1] will place large probability mass on the same
stop words across many topics  in order to ﬁne-tune the probability assigned to the common case. In
both of these situations  we would like the latent groupings to uniquely correspond to characteristics
of the data: that a group of data should be explained by one mixture component  and that common
stop words should be one category of words among many. This intuition expresses a need for diver-
sity in the latent parameters of the model that goes beyond what is highly likely under the posterior
distribution implied by an independent prior.
In this paper  we propose a modular approach to diversity in generative probabilistic models by
replacing the independent prior on latent parameters with a determinantal point process (DPP). The
determinantal point process enables a modeler to specify a notion of similarity on the space of
interest  which in this case is a space of possible latent distributions  via a positive deﬁnite kernel.
The DPP then assigns probabilities to particular conﬁgurations of these distributions according to
the determinant of the Gram matrix. This construction naturally leads to a generative latent variable
model in which diverse sets of latent parameters are preferred over redundant sets.
The determinantal point process is a convenient statistical tool for constructing a tractable point
process with repulsive interaction. The DPP is more general than the Poisson process (see  e.g. 
[2])  which has no interaction  but more tractable than Strauss [3] and Gibbs/Markov [4] processes
(at the cost of only being able to capture anticorrelation). Hough et al. [5] provides a useful survey
of probabilistic properties of the determinantal point process  and for statistical properties  see  e.g. 
Scardicchio et al. [6] and Lavancier et al. [7]. There has also been recent interest in using the DPP
within machine learning for modeling sets of structures [8]  and for conditionally producing diverse
collections of objects [9]. The approach we propose here is different from this previous work in that
we are suggesting the use of a determinantal point process within a hierarchical model  and using it
to enforce diversity among latent variables  rather than as a mechanism for diversity across directly
observed discrete structures.

2 Diversity in Generative Latent Variable Models

In this paper we consider generic directed probabilistic latent variable models that produce distribu-
tions over a set of N data  denoted {xn}N
n=1  which live in a sample space X . Each of these data
has a latent discrete label zn  which takes a value in {1  2 ···   J}. The latent label indexes into a
set of parameters {θj}J
j=1. The parameters determined by zn then produce the data according to a
distribution f (xn | θzn ). Typically we use independent priors for the θj  here denoted by π(·)  but
the distribution over the latent indices zn may be more structured. Taken together this leads to the
generic joint distribution:

(cid:34) N(cid:89)

(cid:35) J(cid:89)

p({xn  zn}N

n=1 {θj}J

j=1) = p({zn}N

n=1)

f (xn | θzn)

π(θj).

(1)

n=1

j=1

The details of each distribution are problem-speciﬁc  but this general framework appears in many
contexts. For example  in a typical mixture model  the zn are drawn independently from a multino-
mial distribution and the θj are the component-speciﬁc parameters. In an admixture model such as
latent Dirichlet allocation (LDA) [1]  the θj may be “topics”  or distributions over words. In an ad-
mixture  the zn may share structure based on  e.g.  being words within a common set of documents.
These models are often thought of as providing a principled approach for feature extraction. At
training time  one either ﬁnds the maximum of the posterior distribution p({θj}J
n=1) or
collects samples from it  while integrating out the data-speciﬁc latent variables zn. Then when
presented with a test case x(cid:63)  one can construct a conditional distribution over the corresponding
unknown variable z(cid:63)  which is now a “feature” that might usefully summarize many related aspects
of x(cid:63). However  this interpretation of the model is suspect; we have not asked the model to make
the zn variables explanatory  except as a byproduct of improving the training likelihood. Different θj
may assign essentially identical probabilities to the same datum  resulting in ambiguous features.

j=1 |{xn}N

2

(a) Independent Points

(c) Independent Gaussians

(e) Independent Multinomials

(b) DPP Points

(d) DPP Gaussians

(f) DPP Multinomials

j=1) =(cid:81)

Figure 1: Illustrations of the determinantal point process prior. (a) 25 independent uniform draw in the unit
square; (b) a draw from a DPP with 25 points; (c) ten Gaussian distributions with means uniformly drawn on
the unit interval; (d) ten Gaussian distributions with means distributed according to a DPP using the probability
product kernel; (e) ﬁve random discrete distributions; (f) ﬁve random discrete distributions drawn from a DPP
on the simplex with the probability product kernel [10].
2.1 Measure-Valued Determinantal Point Process

In this work we propose an alternative to the independence assumption of the standard latent variable
model. Rather than specifying p({θj}J
j π(θj)  we will construct a determinantal point
process on sets of component-speciﬁc distributions {f (x| θj)}J
j=1. Via the DPP  it will be possible
for us to specify a preference for sets of distributions that have minimal overlap  as determined via a
positive-deﬁnite kernel function between distributions. In the case of the simple parametric families
for f (·) that we consider here  it is appropriate to think of the DPP as providing a “diverse” set of
parameters θ = {θj}J
j=1  where the notion of diversity is expressed entirely in terms of the resulting
probability measure on the sample space X . After MAP inference with this additional structure  the
hope is that the θj will explain substantially different regions of X — appropriately modulated by
the likelihood — and lead to improved  non-redundant feature extraction at test time.
We will use Θ to denote the space of possible θ. A realization from a point process on Θ produces
a random ﬁnite subset of Θ. To construct a determinantal point process  we ﬁrst deﬁne a positive
deﬁnite kernel on Θ  which we denote K : Θ × Θ → R. The probability density associated with a
particular ﬁnite θ ⊂ Θ is given by

p(θ ⊂ Θ) ∝ |Kθ| 

(2)
where Kθ is the |θ| × |θ| positive deﬁnite Gram matrix that results from applying K(θ  θ(cid:48)) to the
elements of θ. The eigenspectrum of the kernel on Θ must be bounded to [0  1]. The kernels we will
focus on in this paper are composed of two parts: 1) a positive deﬁnite correlation function R(θ  θ(cid:48)) 
π(θ)π(θ(cid:48))  which expresses our marginal preferences

where R(θ  θ) = 1  and 2) the “prior kernel”(cid:112)
which leads to the matrix form Kθ = Π Rθ Π  where Π = diag([(cid:112)

for some parameters over others. These are combined to form the kernel of interest:

π(θ2) ··· ]).

K(θ  θ

(cid:48)) = R(θ  θ

π(θ)π(θ(cid:48)) 

(cid:112)

π(θ1) 

(cid:48))(cid:112)

Note that if R(θ  θ(cid:48)) = 0 when θ (cid:54)= θ(cid:48)  this construction recovers the Poisson process with intensity
measure π(θ). Note also in this case that if the cardinality of θ is predetermined  then this recovers
the traditional independent prior. More interesting  however  are R(θ  θ(cid:48)) with off-diagonal structure
that induces interaction within the set. Such kernels will always induce repulsion of the points so that
diverse subsets of Θ will tend to have higher probability under the prior. See Fig. 1 for illustrations
of the difference between independent samples and the DPP for several different settings.

(3)

3

2.2 Kernels for Probability Distributions

The determinantal point process framework allows us to construct a generative model for repulsion 
but as with other kernel-based priors  we must deﬁne what “repulsion” means. A variety of positive
deﬁnite functions on probability measures have been deﬁned  but in this work we will use the proba-
bility product kernel [10]. This kernel is a natural generalization of the inner product for probability
distributions. The basic kernel has the form
(cid:48) ; ρ) =

f (x| θ)ρ f (x| θ

(cid:48))ρ dx

K(θ  θ

(cid:90)

(4)

for ρ > 0. As we require a correlation kernel  we use the normalized variant given by

(5)
This kernel has convenient closed forms for several distributions of interest  which makes it an ideal
building block for the present model.

R(θ  θ

K(θ  θ ; ρ)K(θ(cid:48)  θ(cid:48) ; ρ).

(cid:48) ; ρ) = K(θ  θ

(cid:48) ; ρ)/

X

(cid:112)

2.3 Replicated Determinantal Point Process

A property that we often desire from our prior distributions is that they have the ability to be-
come arbitrarily strong. That is  under the interpretation of a Bayesian prior as “inferences from
previously-seen data”  we would like to be able to imagine an arbitrary amount of such data and
construct a highly-informative prior when appropriate. Unfortunately  the standard determinantal
point process does not provide a knob to turn to increase its strength arbitrarily.
For example  take a DPP on a Euclidean space and consider a point t  an arbitrary unit vector w and
a small scalar . Construct two pairs of points using a δ > 1: a “near” pair {t  t + w)}  and a “far”
pair {t  t + δw}. We wish to ﬁnd some small  such that the “far” conﬁguration is arbitrarily more
likely than the “near” conﬁguration under the DPP. That is  we would like the ratio of determinants

r() =

p({t  t + δw})
p({t  t + w)})

1 − R(t  t + δw)2
1 − R(t  t + w))2  

=

(6)

to be unbounded as  approaches zero. The objective is to have a scaling parameter that can cause the
determinantal prior to be arbitrarily strong relative to the likelihood terms. If we perform a Taylor
expansion of the numerator and denominator around  = 0  we get

r() ≈ 1 − (R(t  t) + 2δw(cid:2) d
1 − (R(t  t) + 2w(cid:2) d

d˜t R(t  ˜t)(cid:3)
d˜t R(t  ˜t)(cid:3)

)

˜t=t
)

˜t=t

= δ.

(7)

We can see that  when near zero  this ratio captures the difference in distances  but not in a way
that can be rescaled to greater effect. This means that there exist ﬁnite data sets that we cannot
overwhelm with any DPP prior. To address this issue  we augment the determinantal point process
with an additional parameter λ > 0  so that the probability of a ﬁnite subset θ ⊂ Θ becomes

p(θ ⊂ Θ) ∝ |Kθ|λ.

(8)
For integer λ  it can be viewed as a set of λ identical “replicated realizations” from determinantal
point processes  leaving our generative view intact. The replicate of θ is just θλ = {λ copies of θ}
and the corresponding Kθλ is a λ|θ| × λ|θ| block diagonal matrix where each block is a replicate
of Kθ. This maps well onto the view of a prior as pseudo-data; our replicated DPP asserts that
we have seen λ previous such data sets. As in other pseudo-count priors  we do not require in
practice that λ be an integer  and under a penalized log likelihood view of MAP inference  it can be
interpreted as a parameter for increasing the effect of the determinantal penalty.

2.4 Determinantal Point Process as Regularization.

In addition to acting as a prior over distributions in the generative setting  we can also view the DPP
as a new type of “diversity” regularizer on learning. The goal is to solve
n=1) − λ ln|Kθ| 

L(θ;{xn}N

θ(cid:63) = argmin

(9)

θ⊂Θ

4

Figure 2: Schematic of DPP-LDA. We replace the standard plate notation for i.i.d topics in LDA with a “double-
struck plate” to indicate a determinantal point process.
choosing the best set of parameters θ from Θ. Here L(·) is a loss function that depends on the data
and the discrimination function  with parameters θ. From Eqn. (3) 

ln|Kθ| = ln|Rθ| +

ln π(θj).

(10)

(cid:88)

θj∈θ

n=1|θ)  then the resulting optimization is simply MAP estimation. In this
If L(·) = − ln p({xn}N
framework  we can combine the DPP penalty with any other regularizer on θ  for example the
sparsity-inducing (cid:96)1 regularizer.
In the following sections  we give empirical evidence that this
diversity improves generalization performance.

θ

3 MAP Inference
In what follows  we ﬁx the cardinality |θ|. Viewing the kernel Kθ as a function of θ  the gradient
∂θ log |Kθ| = trace(K−1
∂θ ). This allows application of general gradient-based optimization
∂Kθ
∂
algorithms for inference. In particular  we can optimize θ as a modular component within an off-
the-shelf expectation maximization (EM) algorithm. Here we examine two illustrative examples of
generative latent variable models into which we can directly plug our DPP-based prior.
Diversiﬁed Latent Dirichlet Allocation Latent Dirichlet allocation (LDA) [1] is an immensely
popular admixture model for text and  increasingly  for other kinds of data that can be treated as a
“bag of words”. LDA constructs a set of topics — distributions over the vocabulary — and asserts
that each word in the corpus is explained by one of these topics. The topic-word assignments are
unobserved  but LDA attempts to ﬁnd structure by requiring that only a small number of topics be
represented in any given document.
In the standard LDA formulation  the topics are K discrete distributions βk over a vocabulary of
size V   where βkv is the probability of topic k generating word v. There are M documents and
the mth document has Nm words. Document m has a latent multinomial distribution over topics 
denoted θm and each word in the document wmn has a topic index zmn drawn from θm. While
classical LDA uses independent Dirichlet priors for the βk  here we “diversify” latent Dirichlet
allocation by replacing this prior with a DPP. That is  we introduce a correlation kernel

R(βk  βk(cid:48)) =

 

(11)

(cid:80)V
(cid:113)(cid:80)V

(cid:113)(cid:80)V

v=1(βkv βk(cid:48)v)ρ
v=1 β2ρ

kv

v=1 β2ρ
k(cid:48)v

which approaches one as βk becomes more similar to βk(cid:48). In the application below of DPP-LDA  we
use ρ = 0.5. We use π(βk) = Dirichlet(α)  and write the resulting prior as p(β) ∝ |Kβ|. We call
this model “DPP-LDA”  and illustrate it with a graphical model in Figure 2. We use a “double-struck
plate” in the graphical model to represent the DPP  and highlight how it can be used as a drop-in
replacement for the i.i.d. assumption.
To perform MAP learning of this model  we construct a modiﬁed version of the standard variational
EM algorithm. As in variational EM for LDA  we deﬁne a factored approximation

q(θm  zm|γm  φm) = q(θm|γm)

q(zmn|φmn).

(12)

N(cid:89)

n=1

5

θmαzmnwmnβkNmMKλDPP-LDA

LDA
typical
the
to
and
it
of
is
in
that
for
you
Table 1: Top ten words from representative topics learned in LDA and DPP-LDA.

”Christianity”
jesus
matthew
prophecy
christians
church
messiah
psalm
isaiah
prophet
lord

”space”
space
nasa
astronaut
mm
mission
pilot
shuttle
military
candidates
ww

”stop words”
the
of
that
you
by
one
all
but
do
my which

and
in
at
from
some
their
with
your
who

”OS”
ﬁle
pub
usr
available
export
font
lib
directory
format
server

”politics”
ms
myers
god
president
but
package
options
dee
believe
groups

the
to
and
in
of
is
it
for
that
can

In this approximation  each document m has a Dirichlet approximation to its posterior over topics 
given by γm. φm is an N × K matrix in which the nth row  denoted φmn  is a multinomial distribu-
tion over topics for word wmn. For the current estimate of βkv  γm and φm are iteratively optimized.
See Blei et al. [1] for more details. Our extension of variational EM to include the DPP does not
require alteration of these steps.
The inclusion of the determinantal point process prior does  however  effect the maximization step.
The diversity prior introduces an additional penalty on β  so that the M-step requires solving

(cid:41)
mn ln βkv + λ ln|Kβ|

 

(13)

(cid:40) M(cid:88)

β(cid:63) = argmax

β

m=1

subject to the constraints that each row of β sum to 1. For λ = 0  this optimization procedure yields
the standard update for vanilla LDA  β(cid:63)
mn. For λ > 0 we use gradient
descent to ﬁnd a local optimal β.
Diversiﬁed Gaussian Mixture Model The mixture model is a popular model for generative clus-
tering and density estimation. Given J components  the probability of the data is given by

n=1 φmnkw(v)

m=1

φmnkw(v)

(cid:80)Nm

n=1

v=1

k=1

Nm(cid:88)
V(cid:88)
K(cid:88)
kv ∝(cid:80)M
J(cid:88)

j=1

p(xn | θ) =

χj f (xn | θj).

(14)

Typically  the θk are taken to be independent in the prior. Here we examine determinantal point
process priors for the θk in the case where the components are Gaussian.
For Gaussian mixture models  the DPP prior is particularly tractable. As in DPP-LDA  we use the
probability product kernel  which in this case also has a convenient closed form [10]. Let f1 =
N (µ1  Σ1) and f2 = N (µ2  Σ2) be two Gaussians  the product kernel is:
K(f1  f2) = (2π)(1−2ρ) D
2 ρ

2 | ˆΣ| 1
− D

(µT

2 (|Σ1||Σ2|)− ρ
1 µ1 + Σ−1

1 Σ−1

2 exp(− ρ
2 µ2 − ˆµT ˆΣˆµ))
2
In the special case of a ﬁxed  isotropic

1 µ1 + µT

2 Σ−1

2 µ2.

where ˆΣ = (Σ1 + Σ2)−1 and ˆµ = Σ−1
covariance σ2I and ρ = 1  the kernel is
K(f (·| µ)  f (·| µ

(cid:48))) =

1

(4πσ2)D/2 e

−||µ−µ

(cid:48)||2/(4σ2)

(15)

where D is the data dimensionality.
In the standard EM algorithm for Gaussian mixtures  one typically introduces latent binary variables
znj  which indicate that datum n belongs to component j. The E-step computes the responsibility
vector γ(znj) = E[znj] ∝ χjN (xn|µj  Σj). This step is identical for DPP-GMM. The update for
the component weights is also the same: χj = 1
n=1 γ(znj). The difference between this pro-
N
cedure and the standard EM approach is that the M-step for the DPP-GMM optimizes the objective
function (summarizing {µj  Σj}J
J(cid:88)

j=1 by θ for clarity):

γ(znj) [ln χj + lnN (xn|µj  Σj)] + λ ln|Kθ|

θ(cid:63) = argmax

(cid:80)N

(16)

 N(cid:88)

n=1

θ∈Θ

 .

j=1

6

Figure 3: Effect of λ on classiﬁcation error.

Figure 4: Effect of centroid distance on test error.

Closely related to DPP-GMM is DPP-K-means. The kernel acts on the set of centroids as in
Eqn. (15)  with σ2 now just a constant scaling term. Let θ = {µj} and znj be the hard assign-
ment indicator  the maximization step is:

znj||xn − µj||2 + λ ln|Kθ|

(17)

 N(cid:88)

n=1

J(cid:88)

j=1

θ(cid:63) = argmax

θ∈Θ

 .

With the product kernel  the similarity between two Gaussians decays exponentially as the distance
between their means increases. In practice  we ﬁnd that when the number of mixture components |θ|
is large  Kθ is well approximated by a sparse matrix.

4 Experiment I: diversiﬁed topic modeling.

We tested LDA and DPP-LDA on the unﬁltered 20 Newsgroup corpus  without removing any stop-
words. A common frustration with vanilla LDA is that applying LDA to unﬁltered data returns
topics that are dominated by stop-words. This frustrating phenomenon occurs even as the number of
topics is varied from K = 5 to K = 50. The ﬁrst two columns of Table 1 show the ten most frequent
words from two representative topics learned by LDA using K = 25 . Stop-words occur frequently
across all documents and thus are unhelpfully correlated with topic-speciﬁc informative keywords.
We repeated the experiments after removing a list of 669 most common stop-words. However  the
topics inferred by regular LDA are still dominated by secondary stop-words that are not informative.
DPP-LDA automatically groups common stop words into a few topics. By ﬁnding stop-word-speciﬁc
topics  the majority of the remaining topics are available for more informative words. Table 1 shows
a sample of topics learned by DPP-LDA on the unﬁltered 20 Newsgroup corpus (K = 25  λ = 104).
As we vary K or increase λ we observe robust grouping of stop-words into a few topics. High
frequency words that are common across many topics signiﬁcantly increase the similarity between
the topics  as measured by the product kernel on the β distributions. This similarity incurs a large
penalty in DPP and so the objective actively pushes the parameters of LDA away from regions where
stop words occupy large probability mass across many topics.
Features learned from DPP-LDA leads to better document classiﬁcation. It is common to use the γm 
the document speciﬁc posterior distribution over topics  as feature vectors in document classiﬁcation.
We inferred {γm train} on training documents from DPP-LDA variational EM  and then trained a
support vector machine (SVM) classiﬁer on {γm train} with the true topic labels from 20 News-
groups. On test documents  we ﬁxed the parameters α and β to the values inferred from the training
set  and used variational EM to ﬁnd MAP estimates of {γm test}. The mean test classiﬁcation accu-
racy for a range of λ values is plotted in Figure 3. The setting λ = 0 corresponds to vanilla LDA. In
each trial  we use the same training set for DPP-LDA on a range of λ values. DPP-LDA with λ = 1
consistently outperforms LDA in test classiﬁcation (p < 0.001 binomial test). Large values of λ
decrease classiﬁcation performance.

5 Experiment II: diverse clustering.

Mixture models are often a useful way to learn features for classiﬁcation. The recent work of Coates
et al. [11]  for example  shows that even simple K-means works well as a method of extracting

7

00.11101000.190.20.210.220.230.24λtest accuracy  k=20k=2505101520−0.500.511.5% increase in inter−centroid distance% change in test accuracyλ=0.1λ=0.001λ=0.01training set size K
30
500
30
1000
2000
60
150
5000
10000
300

K-means DPP K-means
34.81
43.32
52.05
61.03
66.36

36.21
44.27
52.55
61.23
66.65

gain (%)
1.4
0.95
0.50
0.20
0.29

λ
0.01
0.01
0.01
0.001
0.001

Table 2: Test classiﬁcation accuracy on CIFAR-10 dataset.

features for image labeling. In that work  K-means gave state of art results on the CIFAR-10 object
recognition task. Coates et al. achieved these results using a patch-wise procedure in which random
patches are sampled from images for training. Each patch is a 6-by-6 square  represented as a point
in a 36 dimensional vector space. Patches from the training images are combined and clustered using
K-means. Each patch is then represented by a binary K-dimensional feature vector: the kth entry
is one if the patch is closer to the centroid k than its average distance to centroids. Roughly half of
the feature entries are zero. Patches from the same image are then pooled to construct one feature
vector for the whole image. An SVM is trained on these image features to perform classiﬁcation.
We reason that DPP-K-means may produce more informative features since the cluster centroids
will repel each other into more distinct positions in pixel space. We replicated the experiments from
Coates et al.  using their publicly-available code for identical pre- and post-processing. With this
setup  λ = 0 recovers regular K-means  and reproduces the results from Coates et al. [11]. We
applied DPP-K-means to the CIFAR-10 dataset  while varying the size of the training set. For each
training set size  we ran regular K-means for a range of values of K and select the K that gives the
best test accuracy for K-means. Then we compare the performance with DPP-K-means using the
same K. For up to 10000 images in the training set  DPP-K-means leads to better test classiﬁcation
accuracy compared to the simple K-means. The comparisons are performed on matched settings:
for a given randomly sampled training set and a centroid initialization  we generate the centroids
from both K-means and DPP-K-means. The two sets of centroids were used to extract features
and train classiﬁers  which are then tested on the same test set of images. DPP-K-means consis-
tently outperforms K-means in generalization accuracy (p < 0.001 binomial test). For example 
for training set of size 1000  with k = 30  we ran 100 trials  each with an random training set and
initialization  DPP-K-means outperformed K-means in 94 trials. As expected given its role as a reg-
ularizer  improvement from DPP-K-means is more signiﬁcant for smaller training sets. For the full
CIFAR-10 with 50000 training images  DPP-K-means does not consistently outperform K-means.
Next we ask if there is a pattern between how far the DPP pushes apart the centroids and classiﬁ-
cation accuracy on the test set. Focusing on 1000 training images and k = 30  for each randomly
sampled training set and centroid initialization  we compute the mean inter-centroid distance for K-
means and DPP-K-means. We compute the test accuracy for each set of centroids. Fig. 4 bins
the relative increase in inter-centroid distance into 10 bins. For each bin  we show the 25th  50th 
and 75th percentile of changes in test accuracy. Test accuracy is maximized when the inter-centroid
distances increase by about 14% from K-means centroids  corresponding to λ = 0.01.

6 Discussion.

We have introduced a general approach to including a preference for diversity into generative proba-
bilistic models. We showed how a determinantal point process can be integrated as a modular com-
ponent into existing learning algorithms  and discussed its general role as a diversity regularizer. We
investigated two settings where diversity can be useful: learning topics from documents  and clus-
tering image patches. Plugging a DPP into latent Dirichlet allocation allows LDA to automatically
group stop-words into a few categories  enabling more informative topics in other categories. In
both document and image classiﬁcation tasks  there exists an intermediate regime of diversity (as
controlled by the hyperparameter λ) that leads to consistent improvement in accuracy when com-
pared to standard i.i.d. models. A computational bottleneck can come from inverting the M × M
kernel matrix K  where M is the number of latent distributions. However in many settings such as
LDA  M is much smaller than the data size. We expect that there are many other settings where
DPP-based diversity can be usefully introduced into a generative probabilistic model: in the emis-
sion parameters of HMM and more general time series  and as a mechanism for transfer learning.

8

References
[1] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent Dirichlet allocation. Journal of

Machine Learning Research  3:993–1022  2003.

[2] J. F. C. Kingman. Poisson Processes. Oxford University Press  Oxford  United Kingdom 

1993.

[3] David J. Strauss. A model for clustering. Biometrika  62(2):467–475  August 1975.
[4] Jesper Møller and Rasmus Plenge Waagepetersen. Statistical Inference and Simulation for
Spatial Point Processes. Monographs on Statistics and Applied Probability. Chapman and
Hall/CRC  Boca Raton  FL  2004.

[5] J. Ben Hough  Manjunath Krishnapur  Yuval Peres  and Blint Vir´ag. Determinantal processes

and independence. Probability Surveys  3:206–229  2006.

[6] Antonello Scardicchio  Chase E. Zachary  and Salvatore Torquato. Statistical properties of
determinantal point processes in high-dimensional Euclidean spaces. Physical Review E  79(4) 
2009.

[7] Fr´ed´eric Lavancier  Jesper Møller  and Ege Rubak. Statistical aspects of determinantal point

processes. http://arxiv.org/abs/1205.4818  2012.

[8] Alex Kulesza and Ben Taskar. Structured determinantal point processes. In Advanced in Neural

Information Processing Systems 23  2011.

[9] Alex Kulesza and Ben Taskar. Learning determinantal point processes. In Proceedings of the

27th Conference on Uncertainty in Artiﬁcial Intelligence  2011.

[10] Tony Jebara  Risi Kondor  and Andrew Howard. Probability product kernels. Journal of

Machine Learning Research  5:819–844  2004.

[11] Adam Coates Honglak Lee and Andrew Ng. An analysis of single-layer networks in unsu-
pervised feature learning. In Proceedings of the 14th International Conference on Artiﬁcial
Intelligence and Statistics  2011.

9

,Bernardino Romera-Paredes
Massimiliano Pontil
Medhini Narasimhan
Svetlana Lazebnik
Alexander Schwing
Deepak Pathak
Christopher Lu
Trevor Darrell
Phillip Isola
Alexei Efros