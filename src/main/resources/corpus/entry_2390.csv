2017,Group Sparse Additive Machine,A family of learning algorithms generated from additive models have attracted much attention recently for their flexibility and interpretability in high dimensional data analysis. Among them  learning models with grouped variables have shown competitive performance for prediction and variable selection. However  the previous works mainly focus on the least squares regression problem  not the classification task. Thus  it is desired to design the new additive classification model with variable selection capability for many real-world applications which focus on high-dimensional data classification. To address this challenging problem  in this paper  we investigate the classification with group sparse additive models in reproducing kernel Hilbert spaces. A novel classification method  called as \emph{group sparse additive machine} (GroupSAM)  is proposed to explore and utilize the structure information among the input variables. Generalization error bound is derived and proved by integrating the sample error analysis with empirical covering numbers and the hypothesis error estimate with the stepping stone technique. Our new bound shows that GroupSAM can achieve a satisfactory learning rate with polynomial decay. Experimental results on synthetic data and seven benchmark datasets consistently show the effectiveness of our new approach.,Group Sparse Additive Machine

Hong Chen1  Xiaoqian Wang1  Cheng Deng2  Heng Huang1∗

1 Department of Electrical and Computer Engineering  University of Pittsburgh  USA

2 School of Electronic Engineering  Xidian University  China

chenh@mail.hzau.edu.cn xqwang1991@gmail.com
chdeng@mail.xidian.edu.cn heng.huang@pitt.edu

Abstract

A family of learning algorithms generated from additive models have attracted
much attention recently for their ﬂexibility and interpretability in high dimensional
data analysis. Among them  learning models with grouped variables have shown
competitive performance for prediction and variable selection. However  the
previous works mainly focus on the least squares regression problem  not the
classiﬁcation task. Thus  it is desired to design the new additive classiﬁcation
model with variable selection capability for many real-world applications which
focus on high-dimensional data classiﬁcation. To address this challenging problem 
in this paper  we investigate the classiﬁcation with group sparse additive models
in reproducing kernel Hilbert spaces. A novel classiﬁcation method  called as
group sparse additive machine (GroupSAM)  is proposed to explore and utilize
the structure information among the input variables. Generalization error bound is
derived and proved by integrating the sample error analysis with empirical covering
numbers and the hypothesis error estimate with the stepping stone technique. Our
new bound shows that GroupSAM can achieve a satisfactory learning rate with
polynomial decay. Experimental results on synthetic data and seven benchmark
datasets consistently show the effectiveness of our new approach.

1

Introduction

The additive models based on statistical learning methods have been playing important roles for
the high-dimensional data analysis due to their well performance on prediction tasks and variable
selection (deep learning models often don’t work well when the number of training data is not
large). In essential  additive models inherit the representation ﬂexibility of nonlinear models and
the interpretability of linear models. For a learning approach under additive models  there are two
key components: the hypothesis function space and the regularizer to address certain restrictions
on estimator. Different from traditional learning methods  the hypothesis space used in additive
models is relied on the decomposition of input vector. Usually  each input vector X ∈ Rp is divided
into p parts directly [17  30  6  28] or some subgroups according to prior structural information
among input variables [27  26]. The component function is deﬁned on each decomposed input and
the hypothesis function is constructed by the sum of all component functions. Typical examples of
hypothesis space include the kernel-based function space [16  6  11] and the spline-based function
space [13  15  10  30]. Moreover  the Tikhonov regularization scheme has been used extensively for
constructing the additive models  where the regularizer is employed to control the complexity of
hypothesis space. The examples of regularizer include the kernel-norm regularization associated with
the reproducing kernel Hilbert space (RKHS) [5  6  11] and various sparse regularization [17  30  26].
More recently several group sparse additive models have been proposed to tackle the high-dimensional
regression problem due to their nice theoretical properties and empirical effectiveness [15  10 

∗Corresponding author

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

26]. However  most existing additive model based learning approaches are mainly limited to the
least squares regression problem and spline-based hypothesis spaces. Surprisingly  there is no any
algorithmic design and theoretical analysis for classiﬁcation problem with group sparse additive
models in RKHS. This paper focuses on ﬁlling in this gap on algorithmic design and learning theory
for additive models. A novel sparse classiﬁcation algorithm  called as group sparse additive machine
(GroupSAM)  is proposed under a coefﬁcient-based regularized framework  which is connected to
the linear programming support vector machine (LPSVM) [22  24]. By incorporating the grouped
variables with prior structural information and the (cid:96)2 1-norm based structured sparse regularizer  the
new GroupSAM model can conduct the nonlinear classiﬁcation and variable selection simultaneously.
Similar to the sparse additive machine (SAM) in [30]  our GroupSAM model can be efﬁciently solved
via proximal gradient descent algorithm. The main contributions of this paper can summarized in
two-fold:

• A new group sparse nonlinear classiﬁcation algorithm (GroupSAM) is proposed by extending
the previous additive regression models to the classiﬁcation setting  which contains the
LPSVM with additive kernel as its special setting. To the best of our knowledge  this is the
ﬁrst algorithmic exploration of additive classiﬁcation models with group sparsity.

• Theoretical analysis and empirical evaluations on generalization ability are presented to sup-
port the effectiveness of GroupSAM. Based on constructive analysis on the hypothesis error 
we get the estimate on the excess generalization error  which shows that our GroupSAM
model can achieve the fast convergence rate O(n−1) under mild conditions. Experimental
results demonstrate the competitive performance of GroupSAM over the related methods on
both simulated and real data.

Before ending this section  we discuss related works. In [5]  support vector machine (SVM) with
additive kernels was proposed and its classiﬁcation consistency was established. Although this
method can also be used for grouped variables  it only focuses on the kernel-norm regularizer without
addressing the sparseness for variable selection. In [30]  the SAM was proposed to deal with the
sparse representation on the orthogonal basis of hypothesis space. Despite good computation and
generalization performance  SAM does not explore the structure information of input variables and
ignores the interactions among variables. More important  different from ﬁnite splines approximation
in [30]  our approach enables us to estimate each component function directly in RKHS. As illustrated
in [20  14]  the RKHS-based method is ﬂexible and only depends on few tuning parameters  but the
commonly used spline methods need specify the number of basis functions and the sequence of knots.
It should be noticed that the group sparse additive models (GroupSpAM in [26]) also address the
sparsity on the grouped variables. However  there are key differences between GroupSAM and
GroupSpAM: 1) Hypothesis space. The component functions in our model are obtained by searching
in kernel-based data dependent hypothesis spaces  but the method in [26] uses data independent
hypothesis space (not associated with kernel). As shown in [19  18  4  25]  the data dependent
hypothesis space can provide much more adaptivity and ﬂexibility for nonlinear prediction. The
advantage of kernel-based hypothesis space for additive models is also discussed in [14]. 2) Loss
function. The hinge loss used in our classiﬁcation model is different from the least-squares loss in
[26]. 3) Optimization. Our GroupSAM only needs to construct one component function for each
variable group  but the model in [26] needs to ﬁnd the component functions for each variable in a
group. Thus  our method is usually more efﬁcient. Due to the kernel-based component function and
non-smooth hinge loss  the optimization of GroupSpAM can not be extended to our model directly. 4)
Learning theory. We establish the generalization bound of GroupSAM by the error estimate technique
with data dependent hypothesis spaces  while the error bound is not covered in [26].
Now  we present a brief summary in Table 1 to better illustrate the differences of our GroupSAM
with other methods.
The rest of this paper is organized as follows. In next section  we revisit the related classiﬁcation
formulations and propose the new GroupSAM model. Theoretical analysis on generalization error
bound is established in Section 3. In Section 4  experimental results on both simulated examples and
real data are presented and discussed. Finally  Section 5 concludes this paper.

2

Hypothesis space

Loss function
Group sparsity

Generalization bound

Table 1: Properties of different additive models.

SAM [30]

Group Lasso[27] GroupSpAM [26]

GroupSAM

data-independent data-independent data-independent data-dependent

hinge loss

least-square

least-square

hinge loss

No
Yes

Yes
No

Yes
No

Yes
Yes

2 Group sparse additive machine

In this section  we ﬁrst revisit the basic background of binary classiﬁcation and additive models  and
then introduce our new GroupSAM model.
Let Z := (X  Y) ⊂ Rp+1  where X ⊂ Rp is a compact input space and Y = {−1  1} is the set
of labels. We assume that the training samples z := {zi}n
i=1 are independently
drawn from an unknown distribution ρ on Z  where each xi ∈ X and yi ∈ {−1  1}. Let’s denote
the marginal distribution of ρ on X as ρX and denote its conditional distribution for given x ∈ X as
ρ(·|x).
For a real-valued function f : X → R  we deﬁne its induced classiﬁer as sgn(f )  where sgn(f )(x) =
1 if f (x) ≥ 0 and sgn(f )(x) = −1 if f (x) < 0. The prediction performance of f is measured by
the misclassiﬁcation error:

i=1 = {(xi  yi)}n

R(f ) = Prob{Y f (X) ≤ 0} =

Prob(Y (cid:54)= sgn(f )(x)|x)dρX .

(1)

It is well known that the minimizer of R(f ) is the Bayes rule:

fc(x) = sgn

ydρ(y|x)

= sgn

Prob(y = 1|x) − Prob(y = −1|x)

.

(cid:17)

(cid:90)
(cid:16)

X

(cid:16)(cid:90)

Y

(cid:17)

Since the Bayes rule involves the unknown distribution ρ  it can not be computed directly. In machine
learning literature  the classiﬁcation algorithm usually aims to ﬁnd a good approximation of fc by
minimizing the empirical misclassiﬁcation risk:

n(cid:88)

i=1

Rz(f ) =

1
n

I(yif (xi) ≤ 0)  

(2)

where I(A) = 1 if A is true and 0 otherwise. However  the minimization problem associated with
Rz(f ) is NP-hard due to the 0 − 1 loss I. To alleviate the computational difﬁculty  various convex
losses have been introduced to replace the 0 − 1 loss  e.g.  the hinge loss  the least square loss  and
the exponential loss [29  1  7]. Among them  the hinge loss is the most popular error metric for
classiﬁcation problem due to its nice theoretical properties. In this paper  following [5  30]  we use
the hinge loss:

(cid:96)(y  f (x)) = (1 − yf (x))+ = max{1 − yf (x)  0}

to measure the misclassiﬁcation cost.The expected and empirical risks associated with the hinge loss
are deﬁned respectively as:

E(f ) =

and

(cid:90)

Z

(1 − yf (x))+dρ(x  y)  

n(cid:88)

i=1

(1 − yif (xi))+ .

Ez(f ) =

1
n

In theory  the excess misclassiﬁcation error R(sgn(f ))−R(fc) can be bounded by the excess convex
risk E(f ) − E(fc) [29  1  7]. Therefore  the classiﬁcation algorithm usually is constructed under
structural risk minimization [22] associated with Ez(f ).

3

In this paper  we propose a novel group sparse additive machine (GroupSAM) for nonlinear clas-
siﬁcation. Let {1 ···   p} be partitioned into d groups. For each j ∈ {1  ...  d}  we set X (j) as the
grouped input space and denote f (j) : X (j) → R as the corresponding component function. Usually 
the groups can be obtained by prior knowledge [26] or be explored by considering the combinations
of input variables [11].
Let each K (j) : X (j) × X (j) → R be a Mercer kernel and let HK(j) be the corresponding RKHS
with norm (cid:107) · (cid:107)K(j). It has been proved in [5] that

with norm

j=1

(cid:110) d(cid:88)

H =

(cid:111)
f (j) : f (j) ∈ HK(j)   1 ≤ j ≤ d
f (j)(cid:111)
d(cid:88)

(cid:110) d(cid:88)

(cid:107)f (j)(cid:107)2

K(j) : f =

(cid:107)f(cid:107)2

K = inf

j=1

is an RKHS associated with the additive kernel K =(cid:80)d
(cid:110)Ez(f ) + η
d(cid:88)

For any given training set z = {(xi  yi)}n

¯fz =

f =(cid:80)d

arg min
j=1 f (j)∈H

j=1

j=1

j=1 K (j).

(cid:111)

 

τj(cid:107)f (j)(cid:107)2

K(j)

i=1  the additive model in H can be formulated as:

where η = η(n) is a positive regularization parameter and {τj} are positive bounded weights for
different variable groups.
The solution ¯fz in (3) has the following representation:

¯α(j)
z i yiK (j)(x(j)

i

  x(j))  ¯α(j)

z i ∈ R  1 ≤ i ≤ n  1 ≤ j ≤ d .

d(cid:88)

d(cid:88)

n(cid:88)

¯fz(x) =

¯fz

(j)(x(j)) =

j=1

j=1

i=1

Observe that ¯fz
¯αz
pushes us to consider the sparsity-induced penalty:

z 1 ···   ¯α(j)

(j)(x) ≡ 0 is equivalent to ¯α(j)

z (cid:107)2 = 0 for
z n)T ∈ Rn if the j-th variable group is not truly informative. This motivation

(j) = (¯α(j)

z i = 0 for all i. Hence  we expect (cid:107)¯α(j)
d(cid:88)

n(cid:88)

(cid:110) d(cid:88)

(cid:111)

τj(cid:107)α(j)(cid:107)2 : f =

α(j)
i yiK (j)(x(j)

i

 ·)

.

Ω(f ) = inf

j=1

j=1

i=1

This group sparse penalty aims at the variable selection [27] and was introduced into the additive
regression model [26].
Inspired by learning with data dependent hypothesis spaces [19]  we introduce the following hypothe-
sis spaces associated with training samples z:

Under the group sparse penalty and data dependent hypothesis space  the group sparse additive
machine (GroupSAM) can be written as:

fz = arg min
f∈Hz

(1 − yif (xi))+ + λΩ(f )

(3)

(4)

(5)

Hz =

f =

f (j) : f (j) ∈ H(j)

z

where

(cid:110)

f (j) =

H(j)
z =

α(j)
i K (j)(x(j)

i

 ·) : α(j)

 

(cid:111)
i ∈ R(cid:111)

.

(cid:111)

 

(cid:110)

d(cid:88)
n(cid:88)

j=1

i=1

(cid:110) 1

n(cid:88)

n

i=1

4

n )T and K(j)

where λ > 0 is a regularization parameter.
1  ···   α(j)
Let’s denote α(j) = (α(j)
d(cid:88)
GroupSAM in (5) can be rewritten as:
(cid:110) 1

f (j)
z =

n(cid:88)

{α(j)

z } = arg min

fz =

with

d(cid:88)
n(cid:88)
d(cid:88)
(cid:0)1 − yi

j=1

t=1

j=1

α(j)∈Rn 1≤j≤d

n

i=1

j=1

i = (K (j)(x(j)

1   x(j)

i ) ···   K (j)(x(j)

n   x(j)

i ))T . The

z t K (j)(x(j)
α(j)

t

 ·)  

i )T α(j)(cid:1)

(K(j)

d(cid:88)

j=1

+ + λ

(cid:111)

.

τj(cid:107)α(j)(cid:107)2

(6)

The formulation (6) transforms the function-based learning problem (5) into a coefﬁcient-based
learning problem in a ﬁnite dimensional vector space. The solution of (5) is spanned naturally by
the kernelized functions {K (j)(·  x(j)
i ))}  rather than B-Spline basis functions [30]. When d = 1 
our GroupSAM model degenerates to the special case which includes the LPSVM loss and the
sparsity regularization term. Compared with LPSVM [22  24] and SVM with additive kernels [5]  our
GroupSAM model imposes the sparsity on variable groups to improve the prediction interpretation of
additive classiﬁcation model.
For given {τj}  the optimization problem of GroupSAM can be computed efﬁciently via an acceler-
ated proximal gradient descent algorithm developed in [30]. Due to space limitation  we don’t recall
the optimization algorithm here again.

3 Generalization error bound
In this section  we will derive the estimate on the excess misclassiﬁcation error R(sgn(fz)) − R(fc).
Before providing the main theoretical result  we introduce some necessary assumptions for learning
theory analysis.
Assumption A. The intrinsic distribution ρ on Z := X × Y satisﬁes the Tsybakov noise condition
with exponent 0 ≤ q ≤ ∞. That is to say  for some q ∈ [0 ∞) and ∆ > 0 

(cid:16){x ∈ X : |Prob(y = 1|x) − Prob(y = −1|x)| ≤ ∆t}(cid:17) ≤ tq ∀t > 0.

(7)

ρX

The Tsybakov noise condition was proposed in [21] and has been used extensively for theoretical
analysis of classiﬁcation algorithms [24  7  23  20]. Indeed  (7) holds with exponent q = 0 for any
distribution and with q = ∞ for well separated classes.
Now we introduce the empirical covering numbers [8] to measure the capacity of hypothesis space.
Deﬁnition 1 Let F be a set of functions on Z with u = {ui}k
i=1 ⊂ Z. Deﬁne the (cid:96)2-empirical
2 . The covering number of F with (cid:96)2-empirical
metric is deﬁned as N2(F  ε) = supn∈N supu∈X n N2 u(F  ε)  where

t=1(f (ut) − g(ut))2(cid:9) 1
(cid:80)k

n

metric as (cid:96)2 u(f  g) =(cid:8) 1
(cid:110)

N2 u(F  ε) = inf

l(cid:91)

{f ∈ F : (cid:96)2 u(f  fi) ≤ ε}(cid:111)

.

l ∈ N : ∃{fi}l

i=1 ⊂ F s. t. F =

Let Br = {f ∈ HK : (cid:107)f(cid:107)K ≤ r} and B(j)

Assumption B. Assume that κ = (cid:80)d

r = {f (j) ∈ HK(j) : (cid:107)f (j)(cid:107)K(j) ≤ r}.
j=1 supx(j)

(cid:112)K (j)(x(j)  x(j)) < ∞ and for some s ∈

(0  2)  cs > 0 

i=1

1   ε) ≤ csε−s  ∀ε > 0  j ∈ {1  ...  d}.
It has been asserted in [6] that under Assumption B the following holds:

log N2(B(j)

log N2(B1  ε) ≤ csd1+sε−s  ∀ε > 0.

5

It is worthy noticing that the empirical covering number has been studied extensively in learning
theory literatures [8  20]. Detailed examples have been provided in Theorem 2 of [19]  Lemma 3 of
[18]  and Examples 1  2 of [9]. The capacity condition of additive assumption space just depends
on the dimension of subspace X (j). When K (j) ∈ C ν(X (j) × X (j)) for every j ∈ {1 ···   d}  the
theoretical analysis in [19] assures that Assumption B holds true for:

 2d0

d0+2ν  
2d0
d0+ν  
d0
ν  

s =

ν ∈ (0  1];
ν ∈ [1  1 + d0/2];
ν ∈ (1 + d0/2 ∞).

Here d0 denotes the maximum dimension among {X (j)}.
With respect to (3)  we introduce the data-free regularized function fη deﬁned by:

fη =

arg min
j=1 f (j)∈H
Inspired by the analysis in [6]  we deﬁne:

f =(cid:80)d

(cid:111)

.

τj(cid:107)f (j)(cid:107)2

K(j)

d(cid:88)

(cid:110)E(f ) + η
d(cid:88)

j=1

D(η) = E(fη) − E(fc) + η

τj(cid:107)f (j)
η (cid:107)2

K(j)

(8)

(9)

j=1

as the approximation error  which reﬂects the learning ability of hypothesis space H under Tikhonov
regularization scheme.
The following approximation condition has been studied and used extensively for classiﬁcation
problems  such as [3  7  24  23]. Please see Examples 3 and 4 in [3] for the explicit version for Soblov
kernel and Gaussian kernel induced reproducing kernel Hilbert space.
Assumption C. There exists an exponent β ∈ (0  1) and a positive constant cβ such that:

D(η) ≤ cβηβ ∀η > 0.

Now we introduce our main theoretical result on the generalization bound as follows.

Theorem 1 Let 0 < min
(5) for 0 < θ ≤ min{ 2−s
such that

j

τj ≤ max
τj ≤ c0 < ∞ and Assumptions A-C hold true. Take λ = n−θ in
2−2β}. For any δ ∈ (0  1)  there exists a constant C independent of n  δ

2s   3+5β

j

R(sgn(fz)) − R(fc) ≤ C log(3/δ)n−ϑ

with conﬁdence 1 − δ  where

(cid:110) q + 1

q + 2

ϑ = min

 

β(2θ + 1)

2β + 2

 

(q + 1)(2 − s − 2sθ)

4 + 2q + sq

 

3 + 5β + 2βθ − 2θ

4 + 4β

(cid:111)

.

Theorem 1 demonstrates that GroupSAM in (5) can achieve the convergence rate with polynomial
decay under mild conditions in hypothesis function space. When q → ∞  β → 1  and each
K (j) ∈ C∞  the error decay rate of GroupSAM can arbitrarily close to O(n− min{1  1+2θ
4 }). Hence 
the fast convergence rate O(n−1) can be obtained under proper selections on parameters. To verify
the optimal bound  we need provide the lower bound for the excess misclassiﬁcation error. This is
beyond the main focus of this paper and we leave it for future study.
Additionally  the consistency of GroupSAM can be guaranteed with the increasing number of training
samples.
Corollary 1 Under conditions in Theorem 1  there holds R(sgn(fz)) − R(fc) → 0 as n → ∞.
To better understand our theoretical result  we compare it with the related works as below:

6

1) Compared with group sparse additive models. Although the asymptotic theory of group sparse
additive models has been well studied in [15  10  26]  all of them only consider the regression task un-
der the mean square error criterion and basis function expansion. Due to the kernel-based component
function and non-smooth hinge loss  the previous analysis cannot be extended to GroupSAM directly.
2) Compared with classiﬁcation with additive models. In [30]  the convergence rate is presented for
sparse additive machine (SAM)  where the input space X is divided into p subspaces directly without
considering the interactions among variables. Different to the sparsity on variable groups in this
paper  SAM is based on the sparse representation of orthonormal basis similar with [15]. In [5]  the
consistency of SVM with additive kernel is established  where the kernel-norm regularizer is used.
However  the sparsity on variables and the learning rate are not investigated in previous articles.
3) Compared with the related analysis techniques. While the analysis technique used here is inspired
from [24  23]  it is the ﬁrst exploration for additive classiﬁcation model with group sparsity. In
particular  the hypothesis error analysis develops the stepping stone technique from the (cid:96)1-norm
regularizer to the group sparse (cid:96)2 1-norm regularizer. Our analysis technique also can be applied to
other additive models. For example  we can extend the shrunk additive regression model in [11] to
the sparse classiﬁcation setting and investigate its generalization bound by the current technique.
Proof sketches of Theorem 1
To get tight error estimate  we introduce the clipping operator π(f )(x) = max{−1  min{f (x)  1}} 
which has been widely used in learning theory literatures  such as [7  20  24  23]. Since R(sgn(fz))−
R(fc) can be bounded by E(π(fz)) − E(fc)  we focus on bounding the excess convex risk.
Using fη as the intermediate function  we can obtain the following error decomposition.

Proposition 1 For fz deﬁned in (5)  there holds

where D(η) is deﬁned in (9) 

and

R(sgn(fz)) − R(fc) ≤ E(π(fz)) − E(fc) ≤ E1 + E2 + E3 + D(η) 

E1 = E(π(fz)) − E(fc) −(cid:0)Ez(π(fz)) − Ez(fc)(cid:1) 
E2 = Ez(fη) − Ez(fc) −(cid:0)Ez(fη) − E(fc)(cid:1) 
(cid:1).

E3 = Ez(π(fz)) + λΩ(fz) −(cid:0)Ez(fη) + η

d(cid:88)

τj(cid:107)f (j)
η (cid:107)2

K(j)

j=1

and Ez(fη) + λ(cid:80)d

In learning theory literature  E1 + E2 is called as the sample error and E3 is named as the hypothesis
error. Detailed proofs for these error terms are provided in the supplementary materials.
The upper bound of hypothesis error demonstrates that the divergence induced from regularization
and hypothesis space tends to zero as n → ∞ under proper selected parameters. To estimate the
hypothesis error E3  we choose ¯fz as the stepping stone function to bridge Ez(π(fz)) + λΩ(fz)
K(j). The proof is inspired from the stepping stone technique for
support vector machine classiﬁcation [24]. Notice that our analysis is associated with the (cid:96)2 1-norm
regularizer while the previous analysis just focuses on the (cid:96)1-norm regularization.
The error term E1 reﬂects the divergence between the expected excess risk E(π(fz)) − E(fc) and
the empirical excess risk Ez(π(fz)) − Ez(fc). Since fz involves any given z = {(xi  yi)}n
i=1  we
introduce the concentration inequality in [23] to bound E1. We also bound the error term E2 in terms
of the one-side Bernstein inequality [7].

η (cid:107)2
j=1 τj(cid:107)f (j)

4 Experiments

To evaluate the performance of our proposed GroupSAM model  we compare our model with the
following methods: SVM (linear SVM with (cid:96)2-norm regularization)  L1SVM (linear SVM with (cid:96)1-
norm regularization)  GaussianSVM (nonlinear SVM using Gaussian kernel)  SAM (Sparse Additive
Machine) [30]  and GroupSpAM (Group Sparse Additive Models) [26] which is adapted to the
classiﬁcation setting.

7

Table 2: Classiﬁcation accuracy comparison on the synthetic data. The upper half shows the results
with 24 features groups  while the lower half corresponds to the results with 300 feature groups. The
table shows the average classiﬁcation accuracy and the standard deviation in 2-fold cross validation.

SVM

SAM

GaussianSVM L1SVM

GroupSpAM GroupSAM
σ = 0.8 0.943±0.011 0.935±0.028 0.925±0.035 0.895±0.021 0.880±0.021 0.953±0.018
σ = 0.85 0.943±0.004 0.938±0.011 0.938±0.004 0.783±0.088 0.868±0.178 0.945±0.000
σ = 0.9 0.935±0.014 0.925± 0.007 0.938±0.011 0.853± 0.117 0.883±0.011 0.945±0.007
σ = 0.8 0.975±0.035 0.975±0.035 0.975±0.035 0.700±0.071 0.275±0.106 1.000±0.000
σ = 0.85 0.975±0.035 0.975±0.035 0.975±0.035 0.600±0.141 0.953±0.004 1.000±0.000
σ = 0.9 0.975±0.035 0.975±0.035 0.975±0.035 0.525±0.035 0.983±0.004 1.000±0.000

As for evaluation metric  we calculate the classiﬁcation accuracy  i.e.  percentage of correctly labeled
samples in the prediction. In comparison  we adopt 2-fold cross validation and report the average
performance of each method.
We implement SVM  L1SVM and GaussianSVM using the LIBSVM toolbox [2]. We determine the
hyper-parameter of all models  i.e.  parameter C of SVM  L1SVM and GaussianSVM  parameter
λ of SAM  parameter λ of GroupSpAM  parameter λ in Eq. (6) of GroupSAM  in the range of
{10−3  10−2 
. . .   103}. We tune the hyper-parameters via 2-fold cross validation on the training
data and report the best parameter w.r.t. classiﬁcation accuracy of each method. In the accelerated
proximal gradient descent algorithm for both SAM and GroupSAM  we set µ = 0.5  and the number
of maximum iterations as 2000.

4.1 Performance comparison on synthetic data

We ﬁrst examine the classiﬁcation performance on the synthetic data as a sanity check. Our synthetic
data is randomly generated as a mixture of Gaussian distributions. In each class  data points are
sampled i.i.d. from a multivariate Gaussian distribution with the covariance being σI  with I as
the identity matrix. This setting indicates independent covariates of the data. We set the number of
classes to be 4  the number of samples to be 400  and the number of dimensions to be 24. We set
the value of σ in the range of {0.8  0.85  0.9} respectively. Following the experimental setup in
[31]  we make three replicates for each feature in the data to form 24 feature groups (each group
has three replicated features). We randomly pick 6 feature groups to generate the data such that we
can evaluate the capability of GroupSAM in identifying truly useful feature groups. To make the
classiﬁcation task more challenging  we add random noise drawn from uniform distribution U(0  θ)
where θ is 0.8 times the maximum value in the data. In addition  we test on a high-dimensional case
by generating 300 feature groups (e.g.  a total of 900 features) with 40 samples in a similar approach.
We summarize the classiﬁcation performance comparison on the synthetic data in Table 2. From
the experimental results we notice that GroupSAM outperforms other approaches under all settings.
This comparison veriﬁes the validity of our method. We can see that GroupSAM signiﬁcantly
improves the performance of SAM  which shows that the incorporation of group information is
indeed beneﬁcial for classiﬁcation. Moreover  we can notices the superiority of GroupSAM over
GroupSpAM  which illustrates that our GroupSAM model is more suitable for classiciation. We also
present the comparison of feature groups in Table 3. For illustration purpose  we use the case with 24
feature groups as an example. Table 3 shows that the feature groups identiﬁed by GroupSAM are
exactly the same as the ground truth feature groups used for synthetic data generation. Such results
further demonstrate the effectiveness of GroupSAM method  from which we know GroupSAM is
able to select the truly informative feature groups thus improve the classiﬁcation performance.

4.2 Performance comparison on benchmark data

In this subsection  we use 7 benchmark data from UCI repository [12] to compare the classiﬁcation
performance of different methods. The 7 benchmark data includes: Ecoli  Indians Diabetes  Breast
Cancer  Stock  Balance Scale  Contraceptive Method Choice (CMC) and Fertility. Similar to the
settings in synthetic data  we construct feature groups by replicating each feature for 3 times. In each

8

Table 3: Comparison between the true feature group ID (used for data generation) and the selected
feature group ID by our GroupSAM method on the synthetic data. Order of the true feature group ID
does not represent the order of importance.

True Feature Group IDs

Selected Feature Group IDs via GroupSAM

σ = 0.8
σ = 0.85
σ = 0.9

2 3 4 8 10 17
1 5 10 12 17 21
2 6 7 9 12 22

3 10 17 8 2 4
5 12 17 21 1 10
6 22 7 9 2 12

Table 4: Classiﬁcation accuracy comparison on the benchmark data. The table shows the average
classiﬁcation accuracy and the standard deviation in 2-fold cross validation.

SVM

0.815±0.054
0.651±0.000

SAM

GaussianSVM L1SVM
GroupSpAM GroupSAM
0.818±0.049 0.711±0.051 0.816±0.039 0.771±0.009 0.839±0.028
0.652±0.002 0.638±0.018 0.652±0.000 0.643±0.004 0.660±0.013

Ecoli
Indians
Diabetes
Breast
0.965±0.017 0.833±0.008 0.833±0.224 0.958±0.027 0.966±0.014
0.968±0.017
Cancer
0.913±0.001
0.911±0.002 0.873±0.001 0.617±0.005 0.875±0.005 0.917±0.005
Stock
Balance
0.864± 0.003 0.869±0.004 0.870±0.003 0.763±0.194 0.848±0.003 0.893±0.003
Scale
CMC 0.420± 0.011 0.445±0.015 0.437±0.014 0.427±0.000 0.433±0.003 0.456±0.003
Fertility 0.880± 0.000 0.880±0.000 0.750±0.184 0.860±0.028 0.780±0.000 0.880±0.000

feature group  we add random noise drawn from uniform distribution U(0  θ) where θ is 0.3 times the
maximum value in each data.
We display the comparison results in Table 4. We ﬁnd that GroupSAM performs equal or better than
the compared methods in all benchmark datasets. Compared with SVM and L1SVM  our method
uses additive model to incorporate nonlinearity thus is more appropriate to ﬁnd the complex decision
boundary. Moreover  the comparison with Gaussian SVM and SAM illustrates that by involving
the group information in classiﬁcation  GroupSAM makes better use of the structure information
among features such that the classiﬁcation ability can be enhanced. Compared with GroupSpAM  our
GroupSAM model is proposed in data dependent hypothesis spaces and employs hinge loss in the
objective  thus is more suitable for classiﬁcation.

5 Conclusion

In this paper  we proposed a novel group sparse additive machine (GroupSAM) by incorporating the
group sparsity into the additive classiﬁcation model in reproducing kernel Hilbert space. By develop-
ing the error analysis technique with data dependent hypothesis space  we obtain the generalization
error bound of the proposed GroupSAM  which demonstrates our model can achieve satisfactory
learning rate under mild conditions. Experimental results on both synthetic and real-world benchmark
datasets validate the algorithmic effectiveness and support our learning theory analysis. In the future 
it is interesting to investigate the learning performance of robust group sparse additive machines with
loss functions induced by quantile regression [6  14].

Acknowledgments

This work was partially supported by U.S. NSF-IIS 1302675  NSF-IIS 1344152  NSF-DBI 1356628 
NSF-IIS 1619308  NSF-IIS 1633753  NIH AG049371. Hong Chen was partially supported by
National Natural Science Foundation of China (NSFC) 11671161. We are grateful to the anonymous
NIPS reviewers for the insightful comments.

9

References
[1] P. L. Bartlett  M. I. Jordan  and J. D. Mcauliffe. Convexity  classiﬁcation and risk bounds. J.

Amer. Statist. Assoc.  101(473):138–156  2006.

[2] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology  2(27):1–27  2011.

[3] D. R. Chen  Q. Wu  Y. Ying  and D. X. Zhou. Support vector machine soft margin classiﬁers:

error analysis. J. Mach. Learn. Res.  5:1143–1175  2004.

[4] H. Chen  Z. Pan  L. Li  and Y. Tang. Learning rates of coefﬁcient-based regularized classiﬁer

for density level detection. Neural Comput.  25(4):1107–1121  2013.

[5] A. Christmann and R. Hable. Consistency of support vector machines using additive kernels for

additive models. Comput. Stat. Data Anal.  56:854–873  2012.

[6] A. Christmann and D. X. Zhou. Learning rates for the risk of kernel based quantile regression

estimators in additive models. Anal. Appl.  14(3):449–477  2016.

[7] F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge

Univ. Press  Cambridge  U.K.  2007.

[8] D. Edmunds and H. Triebel. Function Spaces  Entropy Numbers  Differential Operators.

Cambridge Univ. Press  Cambridge  U.K.  1996.

[9] Z. Guo and D. X. Zhou. Concentration estimates for learning with unbounded sampling. Adv.

Comput. Math.  38(1):207–223  2013.

[10] J. Huang  J. Horowitz  and F. Wei. Variable selection in nonparametric additive models. Ann.

Statist.  38(4):2282–2313  2010.

[11] K. Kandasamy and Y. Yu. Additive approximation in high dimensional nonparametric regression

via the salsa. In ICML  2016.

[12] M. Lichman. UCI machine learning repository  2013.

[13] Y. Lin and H. H. Zhang. Component selection and smoothing in smoothing spline analysis of

variance models. Ann. Statist.  34(5):2272–2297  2006.

[14] S. Lv  H. Lin  H. Lian  and J. Huang. Oracle inequalities for sparse additive quantile regression

in reproducing kernel hilbert space. Ann. Statist.  preprint  2017.

[15] L. Meier  S. van de Geer  and P. Buehlmann. High-dimensional additive modeling. Ann. Statist. 

37(6B):3779–3821  2009.

[16] G. Raskutti  M. Wainwright  and B. Yu. Minimax-optimal rates for sparse additive models over

kernel classes via convex programming. J. Mach. Learn. Res.  13:389–427  2012.

[17] P. Ravikumar  J. Lafferty  H. Liu  and L. Wasserman. Sparse additive models. J. Royal. Statist.

Soc B.  71:1009–1030  2009.

[18] L. Shi. Learning theory estimates for coefﬁcient-based regularized regression. Appl. Comput.

Harmon. Anal.  34(2):252–265  2013.

[19] L. Shi  Y. Feng  and D. X. Zhou. Concentration estimates for learning with (cid:96)1-regularizer and

data dependent hypothesis spaces. Appl. Comput. Harmon. Anal.  31(2):286–302  2011.

[20] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.

[21] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statis.  32:135–

166  2004.

[22] V. Vapnik. Statistical Learning Theory. John Wiley and Sons  1998.

10

[23] Q. Wu  Y. Ying  and D. X. Zhou. Multi-kernel regularized classﬁers. J. Complexity  23:108–134 

2007.

[24] Q. Wu and D. X. Zhou. Svm soft margin classiﬁers: linear programming versus quadratic

programming. Neural Comput.  17:1160–1187  2005.

[25] L. Yang  S. Lv  and J. Wang. Model-free variable selection in reproducing kernel hilbert space.

J. Mach. Learn. Res.  17:1–24  2016.

[26] J. Yin  X. Chen  and E. Xing. Group sparse additive models. In ICML  2012.

[27] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variabels. J.

Royal. Statist. Soc B.  68(1):49–67  2006.

[28] M. Yuan and D. X. Zhou. Minimax optimal rates of estimation in high dimensional additive

models. Ann. Statist.  44(6):2564–2593  2016.

[29] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk

minimization. Ann. Statist.  32:56–85  2004.

[30] T. Zhao and H. Liu. Sparse additive machine. In AISTATS  2012.

[31] L. W. Zhong and J. T. Kwok. Efﬁcient sparse modeling with automatic feature grouping. In

ICML  2011.

11

,Daniel Zoran
Dilip Krishnan
José Bento
Bill Freeman
Hong Chen
Xiaoqian Wang
Cheng Deng
Heng Huang