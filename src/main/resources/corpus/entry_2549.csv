2012,Imitation Learning by Coaching,Imitation Learning has been shown to be successful in solving many challenging real-world problems. Some recent approaches give strong performance guarantees by training the policy iteratively. However  it is important to note that these guarantees depend on   how well the policy we found can imitate the oracle on the training data.  When there is a substantial difference between the oracle's ability  and the learner's policy space  we may fail to find a policy that has low error on the training set. In such cases  we propose to use a coach that demonstrates easy-to-learn actions for the learner  and gradually approaches the oracle. By a reduction of learning by demonstration to online learning   we prove that coaching can yield a lower regret bound than using the oracle. We apply our algorithm to a novel cost-sensitive dynamic feature selection problem  a hard decision problem that considers a user-specified accuracy-cost trade-off.  Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods.,Imitation Learning by Coaching

He He Hal Daumé III

Department of Computer Science

University of Maryland
College Park  MD 20740

{hhe hal}@cs.umd.edu

Jason Eisner

Department of Computer Science

Johns Hopkins University

Baltimore  MD 21218
jason@cs.jhu.edu

Abstract

Imitation Learning has been shown to be successful in solving many challenging
real-world problems. Some recent approaches give strong performance guaran-
tees by training the policy iteratively. However  it is important to note that these
guarantees depend on how well the policy we found can imitate the oracle on the
training data. When there is a substantial difference between the oracle’s abil-
ity and the learner’s policy space  we may fail to ﬁnd a policy that has low error
on the training set. In such cases  we propose to use a coach that demonstrates
easy-to-learn actions for the learner and gradually approaches the oracle. By a
reduction of learning by demonstration to online learning  we prove that coach-
ing can yield a lower regret bound than using the oracle. We apply our algorithm
to cost-sensitive dynamic feature selection  a hard decision problem that consid-
ers a user-speciﬁed accuracy-cost trade-off. Experimental results on UCI datasets
show that our method outperforms state-of-the-art imitation learning methods in
dynamic feature selection and two static feature selection methods.

1

Introduction

Imitation learning has been successfully applied to a variety of applications [1  2]. The standard
approach is to use supervised learning algorithms and minimize a surrogate loss with respect to
an oracle. However  this method ignores the difference between distributions of states induced by
executing the oracle’s policy and the learner’s  thus has a quadratic loss in the task horizon T . A
recent approach called Dataset Aggregation [3] (DAgger) yields a loss linear in T by iteratively
training the policy in states induced by all previously learned policies. Its theoretical guarantees
are relative to performance of the policy that best mimics the oracle on the training data. In difﬁcult
decision-making problems  however  it can be hard to ﬁnd a good policy that has a low training error 
since the oracle’s policy may resides in a space that is not imitable in the learner’s policy space. For
instance  the task loss function can be highly non-convex in the learner’s parameter space and very
different from the surrogate loss.
When the optimal action is hard to achieve  we propose to coach the learner with easy-to-learn
actions and let it gradually approach the oracle (Section 3). A coach trains the learner iteratively in a
fashion similar to DAgger. At each iteration it demonstrates actions that the learner’s current policy
prefers and have a small task loss. The coach becomes harsher by showing more oracle actions
as the learner makes progress. Intuitively  this allows the learner to move towards a better action
without much effort. Thus our algorithm achieves the best action gradually instead of aiming at an
impractical goal from the beginning. We analyze our algorithm by a reduction to online learning
and show that our approach achieves a lower regret bound than DAgger that uses the oracle action
(Section 3.1). Our method is also related to direct loss minimization [4] for structured prediction
and methods of selecting oracle translations in machine translation [5  6] (Section 5).

1

Our approach is motivated by a formulation of budgeted learning as a sequential decision-making
problem [7  8] (Section 4).
In this setting  features are acquired at a cost  such as computation
time and experiment expense. In dynamic feature selection  we would like to sequentially select a
subset of features for each instance at test time according to a user-speciﬁed accuracy-cost trade-off.
Experimental results show that coaching has a more stable training curve and achieves lower task
loss than state-of-the-art imitation learning algorithms.
Our major contribution is a meta-algorithm for hard imitation learning tasks where the available
policy space is not adequate for imitating the oracle. Our main theoretical result is Theorem 4 which
states that coaching as a smooth transition from the learner to the oracle have a lower regret bound
than only using the oracle.

2 Background

|s  a). We denote dt
Es∼dt

In a sequential decision-making problem  we have a set of states S  a set of actions A and a policy
space Π. An agent follows a policy π : S → A that determines which action to take in a given
state. After taking action a in state s  the environment responds by some immediate loss L(s  a).
We assume L(s  a) is bounded in [0  1]. The agent is then taken to the next state s(cid:48) according to the
loss of π is J(π) = (cid:80)T
transition probability P (s(cid:48)
π the state distribution at time t after executing π from
time 1 to t− 1  and dπ the average state distribution of states over T steps. Then the T -step expected
[L(s  π(s)] = T Es∼dπ [L(s  π(s))]. A trajectory is a complete
sequence of (cid:104)s  a  L(s  a)(cid:105) tuples from the starting state to a goal state. Our goal is to learn a policy
π ∈ Π that minimizes the task loss J(π). We assume that Π is a closed  bounded and non-empty
convex set in Euclidean space; a policy π can be parameterized by a vector w ∈ Rd.
In imitation learning  we deﬁne an oracle that executes policy π∗ and demonstrates actions a∗
s =
L(s  a) in state s. The learner only attempts to imitate the oracle’s behavior without any
arg min
notion of the task loss function. Thus minimizing the task loss is reduced to minimizing a surrogate
loss with respect to the oracle’s policy.

a∈A

t=1

π

2.1

Imitation by Classiﬁcation

A typical approach to imitation learning is to use the oracle’s trajectories as supervised data and learn
a policy (multiclass classiﬁer) that predicts the oracle action under distribution of states induced by
running the oracle’s policy. At each step t  we collect a training example (st  π∗(st))  where π∗(st)
is the oracle’s action (class label) in state st. Let (cid:96)(s  π  π∗(s)) denote the surrogate loss of executing
π in state s with respect to π∗(s). This can be any convex loss function used for training the classiﬁer 
for example  hinge loss in SVM. Using any standard supervised learning algorithm  we can learn a
policy

ˆπ = arg min

π∈Π

Es∼dπ∗ [(cid:96)(s  π  π∗(s))].

(1)

We then bound J(ˆπ) based on how well the learner imitates the oracle. Assuming (cid:96)(s  π  π∗(s)) is
an upper bound on the 0-1 loss and L(s  a) is bounded in [0 1]  Ross and Bagnell [9] have shown
that:
Theorem 1. Let Es∼dπ∗ [(cid:96)(s  ˆπ  π∗(s))] =   then J(ˆπ) ≤ J(π∗) + T 2.
One drawback of the supervised approach is that it ignores the fact that the state distribution is
different for the oracle and the learner. When the learner cannot mimic the oracle perfectly (i.e.
classiﬁcation error occurs)  the wrong action will change the following state distribution. Thus the
learned policy is not able to handle situations where the learner follows a wrong path that is never
chosen by the oracle  hence the quadratically increasing loss. In fact in the worst case  performance
can approach random guessing  even for arbitrarily small  [10].
Ross et al. [3] generalized Theorem 1 to any policy that has  surrogate loss under its own state
distribution  i.e. Es∼dπ [(cid:96)(s  π  π∗(s))] = . Let Qπ(cid:48)
t (s  π) denote the t-step loss of executing π in
the initial state and then running π(cid:48). We have the following:
T−t+1(s  π∗) ≤ u for all action a  t ∈ {1  2  . . .   T}  then
Theorem 2. If Qπ∗
J(π) ≤ J(π∗) + uT .

T−t+1(s  π) − Qπ∗

2

It basically says that when π chooses a different action from π∗ at time step t  if the cumulative cost
due to this error is bounded by u  then the relative task loss is O(uT ).

2.2 Dataset Aggregation

The above problem of insufﬁcient exploration can be alleviated by iteratively learning a policy
trained under states visited by both the oracle and the learner. For example  during training one
can use a “mixture oracle” that at times takes an action given by the previous learned policy [11].
Alternatively  at each iteration one can learn a policy from trajectories generated by all previous
policies [3].
In its simplest form  the Dataset Aggregation (DAgger) algorithm [3] works as follows. Let
sπ denote a state visited by executing π.
In the ﬁrst iteration  we collect a training set D1 =
{(sπ∗   π∗(sπ∗ ))} from the oracle (π1 = π∗) and learn a policy π2. This is the same as the super-
vised approach to imitation. In iteration i  we collect trajectories by executing the previous policy
πi and form the training set Di by labeling sπi with the oracle action π∗(sπi); πi+1 is then learned
on D1
Thus we can obtain a policy that performs well under its own induced state distribution.

(cid:83) . . .Di. Intuitively  this enables the learner to make up for past failures to mimic the oracle.

2.3 Reduction to Online Learning
[(cid:96)(s  π  π∗(s))] denote the expected surrogate loss of executing π in states dis-
Let (cid:96)i(π) = Es∼dπi
tributed according to dπi. In an online learning setting  in iteration i an algorithm executes policy πi
and observes loss (cid:96)i(πi). It then provides a different policy πi+1 in the next iteration and observes
(cid:96)i+1(πi+1). A no-regret algorithm guarantees that in N iterations

N(cid:88)

i=1

1
N

N(cid:88)

i=1

(cid:96)i(πi) − min
π∈Π

1
N

(cid:96)i(π) ≤ γN

(2)

π∈Π

(cid:80)i

and limN→∞ γN = 0.
Assuming a strongly convex loss function  Follow-The-Leader is a simple no-regret algorithm. In
each iteration it picks the policy that works best so far: πi+1 = arg min
j=1 (cid:96)j(π). Similarly 
in DAgger at iteration i we choose the policy that has the minimum surrogate loss on all previous
data. Thus it can be interpreted as Follow-The-Leader where trajectories collected in each iteration
are treated as one online-learning example.
Assume that (cid:96)(s  π  π∗(s)) is a strongly convex loss in π upper bounding the 0-1 loss.
We denote the sequence of
N =
[(cid:96)(s  π  π∗(s))] be the minimum loss we can achieve in the policy space
minπ∈Π
Π. In the inﬁnite sample per iteration case  following proofs in [3] we have:
T−t+1(s  π∗) ≤ u  there exists
Theorem 3. For DAgger  if N is O(uT log T ) and Qπ∗
a policy π ∈ π1:N s.t. J(π) ≤ J(π∗) + uT N + O(1).
This theorem holds for any no-regret online learning algorithm and can be generalized to the ﬁnite
sample case as well.

learned policies π1  π2  . . .   πN by π1:N .

T−t+1(s  π)−Qπ∗

(cid:80)N

1
N

Es∼dπi

i=1

Let

3

Imitation by Coaching

An oracle can be hard to imitate in two ways. First  the learning policy space is far from the space
that the oracle policy lies in  meaning that the learner only has limited learning ability. Second 
the environment information known by the oracle cannot be sufﬁciently inferred from the state 
meaning that the learner does not have access to good learning resources. In the online learning
setting  a too-good oracle may result in adversarially varying loss functions over iterations from the
learner’s perspective. This may cause violent changes during policy updating. These difﬁculties
result in a substantial gap between the oracle’s performance and the best performance achievable in
the policy space Π (i.e. a large N in Theorem 3).

3

Algorithm 1 DAgger by Coaching
Initialize D ← ∅
Initialize π1 ← π∗
for i = 1 to N do
Sample T -step trajectories using πi
Collect coaching dataset Di = {(sπi  arg max
a∈A
Aggregate datasets D ← D
Train policy πi+1 on D

end for
Return best πi evaluated on validation set

(cid:83)

Di

λi · scoreπi(sπi  a) − L(sπi  a))}

To address this problem  we deﬁne a coach in place of the oracle. To better instruct the learner  a
coach should demonstrate actions that are not much worse than the oracle action but are easier to
achieve within the learner’s ability. The lower an action’s task loss is  the closer it is to the oracle
action. The higher an action is ranked by the learner’s current policy  the more it is preferred by the
learner  thus easier to learn. Therefore  similar to [6]  we deﬁne a hope action that combines the task
loss and the score of the learner’s current policy. Let scoreπi(s  a) be a measure of how likely πi
chooses action a in state s. We deﬁne ˜πi by

˜πi(s) = arg max

a∈A

λi · scoreπi(s  a) − L(s  a)

(3)

where λi is a nonnegative parameter specifying how close the coach is to the oracle. In the ﬁrst
iteration  we set λ1 = 0 as the learner has not learned any model yet. Algorithm 1 shows the
training process. Our intuition is that when the learner has difﬁculty performing the optimal action 
the coach should lower the goal properly and let the learner gradually achieving the original goal in
a more stable way.

i=1

(cid:80)N

T−t+1(s  π)−Qπ∗

[(cid:96)(s  π  ˜πi(s))] denote the expected surrogate loss with respect to ˜πi. We denote
˜(cid:96)i(π) the minimum loss of the best policy in hindsight with respect to hope
T−t+1(s  π∗) ≤

3.1 Theoretical Analysis
Let ˜(cid:96)i(π) = Es∼dπi
˜N = 1
N minπ∈Π
actions. The main result of this paper is the following theorem:
Theorem 4. For DAgger with coaching  if N is O(uT log T ) and Qπ∗
u  there exists a policy π ∈ π1:N s.t. J(π) ≤ J(π∗) + uT ˜N + O(1).
It is important to note that both the DAgger theorem and the coaching theorem provide a relative
guarantee. They depend on whether we can ﬁnd a policy that has small training error in each Follow-
The-Leader step. However  in practice  for hard learning tasks DAgger may fail to ﬁnd such a good
policy. Through coaching  we can always adjust λ to create a more learnable oracle policy space 
thus get a relatively good policy that has small training error  at the price of running a few more
iterations.
To prove this theorem  we ﬁrst derive a regret bound for coaching  and then follows the proofs of
DAgger.
We consider a policy π parameterized by a vector w ∈ Rd. Let φ : S × A → Rd be a feature map
describing the state. The predicted action is
(4)

wT φ(s  a)

ˆaπ s = arg max

a∈A

and the hope action is

˜aπ s = arg max

(5)
We assume that the loss function (cid:96) : Rd → R is a convex upper bound of the 0-1 loss. Further  it
can be written as (cid:96)(s  π  π∗(s)) = f (wT φ(s  π(s))  π∗(s)) for a function f : R → R and a feature
vector (cid:107)φ(s  a)(cid:107)2 ≤ R. We assume that f is twice differentiable and convex in wT φ(s  π(s))  which
is common for most loss functions used by supervised classiﬁcation methods.

λ · wT φ(s  a) − L(s  a).

a∈A

4

It has been shown that given a strongly convex loss function (cid:96)  Follow-The-Leader has O(log N )
regret [12  13]. More speciﬁcally  given the above assumptions we have:
Theorem 5. Let D = maxw1 w2∈Rd (cid:107)w1 − w2(cid:107)2 be the diameter of the convex set Rd. For some
b  m > 0  assume that for all w ∈ Rd  we have |f(cid:48)(wT φ(s  a))| ≤ b and |f(cid:48)(cid:48)(wT φ(s  a))| ≥ m.
Then Follow-The-Leader on functions (cid:96) have the following regret:

N(cid:88)

i=1

N(cid:88)

i=1

(cid:96)i(πi) − min
π∈Π

(cid:96)i(π) ≤

(cid:96)i(πi) −

(cid:96)i(πi+1)

≤

log

+ 1

N(cid:88)

i=1
2nb2
m

(cid:20)

i=1

N(cid:88)
(cid:18) DRmN
(cid:80)N

b

i=1

˜(cid:96)i(π).

i=1

(cid:19)

(cid:21)

˜(cid:96)i(πi+1).

To analyze the regret using surrogate loss with respect to hope actions  we use the following lemma:

Lemma 1. (cid:80)N
(cid:80)N
Proof. We prove inductively that(cid:80)N
˜(cid:96)i(π) ≤
˜(cid:96)i(πi+1) ≤ minπ∈Π
When N = 1  by Follow-The-Leader we have π2 = arg min
π∈Π

i=1 (cid:96)i(πi) − minπ∈Π

(cid:80)N

i=1

i=1

i=1 (cid:96)i(πi) −

(cid:80)N

N(cid:88)

Assume correctness for N − 1  then
˜(cid:96)i(πi+1) ≤ min
N−1(cid:88)
π∈Π

i=1

≤

i=1

N−1(cid:88)

i=1

˜(cid:96)i(π) + ˜(cid:96)N (πN +1)

˜(cid:96)1(π)  thus ˜(cid:96)1(π2) = minπ∈Π

˜(cid:96)1(π).

(inductive assumption)

N(cid:88)

i=1

˜(cid:96)i(πN +1) + ˜(cid:96)N (πN +1) = min
π∈Π

˜(cid:96)i(π)

(cid:80)N

π∈Π

The last equality is due to the fact that πN +1 = arg min

˜(cid:96)i(π).

i=1

i=1

˜(cid:96)i(π).

(cid:80)N

To see how learning from ˜πi allows us to approaching π∗  we derive the regret bound of

(cid:80)N
i=1 (cid:96)i(πi) − minπ∈Π
Theorem 6. Assume that wi is upper bounded by C  i.e. for all i (cid:107)wi(cid:107)2 ≤ C  (cid:107)φ(s  a)(cid:107)2 ≤ R and
|L(s  a) − L(s  a(cid:48))| ≥  for some action a  a(cid:48)
∈ A. Assume λi is non-increasing and deﬁne nλ as
. Let (cid:96)max be an upper bound on the loss  i.e. for all i 
the largest n < N such that λnλ ≥
(cid:96)i(s  πi  π∗(s)) ≤ (cid:96)max. We have
N(cid:88)

N(cid:88)

(cid:19)

2RC

(cid:21)

(cid:20)



(cid:96)i(πi) − min
π∈Π

i=1

i=1

˜(cid:96)i(π) ≤ 2(cid:96)maxnλ +

2nb2
m

log

+ 1

Proof. Given Lemma 1  we only need to bound the RHS  which can be written as

(cid:18) DRmN
(cid:33)

b

(cid:32) N(cid:88)

i=1

(cid:33)
(cid:96)i(πi) − ˜(cid:96)i(πi)

+

(cid:32) N(cid:88)

i=1

˜(cid:96)i(πi) − ˜(cid:96)i(πi+1)

.

(6)

To bound the ﬁrst term  we consider a binary action space A = {1 −1} for clarity. The proof can
be extended to the general case in a straightforward manner.
Note that in states where a∗
s = ˜aπ s  (cid:96)(s  π  π∗(s)) = (cid:96)(s  π  ˜π(s)). Thus we only need to consider
situations where a∗
s (cid:54)= ˜aπ s:

(cid:104)

(cid:96)i(πi) − ˜(cid:96)i(πi)
(cid:104)

= Es∼dπi
+Es∼dπi

((cid:96)i(s  πi −1) − (cid:96)i(s  πi  1)) 1{s : ˜aπi s=1 a∗
((cid:96)i(s  πi  1) − (cid:96)i(s  πi −1)) 1{s:˜aπi s=−1 a∗

s =−1}

s =1}

5

(cid:105)
(cid:105)

In the binary case  we deﬁne ∆L(s) = L(s  1) − L(s −1) and ∆φ(s) = φ(s  1) − φ(s −1).
˜aπi s = 1 and a∗
Case 1
˜aπi s = 1 implies λiwT
∆L(s) ∈ (0  λiwT
ˆaπi = 1. Therefore we have
p(a∗

s = −1 implies ∆L(s) > 0. Together we have
i ∆φ(s) ≥ 0 since λi > 0  which implies

s = −1.
i ∆φ(s) ≥ ∆L(s) and a∗

i ∆φ(s)]. From this we know that wT

(cid:19)

(cid:18)
(cid:16)

= p

s = −1  ˜aπi s = 1  ˆaπi s = 1)

= p(˜aπi s = 1|a∗
s = −1  ˆaπi s = 1)p(ˆaπi  s = 1)p(a∗
(cid:17)

∆L(s)
wT
i ∆φ(s)


s = −1)
i ∆φ(s) ≥ 0) · p(∆L(s) > 0)
λi ≥
  we have

(cid:16)
· p(wT
· 1 · 1 = p

λi ≥
λi ≥

≤ p

(cid:17)

2RC

2RC



Let nλ be the largest n < N such that λi ≥



2RC

(cid:104)

Es∼dπi

N(cid:88)

i=1

((cid:96)i(s  πi −1) − (cid:96)i(s  πi  1)) 1{s : ˜aπi s=1 a∗

s =−1}

≤ (cid:96)maxnλ

(cid:105)

For example  let λi decrease exponentially  e.g.  λi = λ0e−i.

If λ0 <

eN
2RC

  Then nλ =

2λ0RC



(cid:101).
˜aπi s = −1 and a∗

s = 1. This is symmetrical to Case 1. Similar arguments yield the same

(cid:100)log
Case 2
bound.
In the online learning setting  imitating the coach is to obsearve the loss ˜(cid:96)i(πi) and learn a policy
˜(cid:96)j(π) at iteration i. This is indeed equivalent to Follow-The-Leader except
πi+1 = arg min
that we replaced the loss function. Thus Theorem 5 gives the bound of the second term.

(cid:80)i

π∈Π

j=1

(cid:20)

(cid:18) DRmN

(cid:19)

(cid:21)

+ 1

.

Equation 6 is then bounded by 2(cid:96)maxnλ +

2nb2
m

log

b

N(cid:88)

i=1

Now we can prove Theorem 4. Consider the best policy in π1:N   we have

min
π∈π1:N

Es∼dπ [(cid:96)(s  π  π∗(s))] ≤

1
N

Es∼dπi

[(cid:96)(s  πi  π∗(s))]

≤ ˜N +

2(cid:96)maxnλ

N

+

2nb2
mN

log

(cid:20)

(cid:18) DRmN

(cid:19)

(cid:21)

+ 1

b

When N is Ω(T log T )  the regret is O(1/T ). Applying Theorem 2 completes the proof.

4 Experiments

We apply imitation learning to a novel dynamic feature selection problem. We consider the setting
where a pretrained model (data classiﬁer) on a complete feature set is given and each feature has a
known cost. At test time  we would like to dynamically select a subset of features for each instance
and be able to explicitly specify the accuracy-cost trade-off. This can be naturally framed as a
sequential decision-making problem. The state includes all features selected so far. The action space
includes a set of non-selected features and the stop action. At each time step  the policy decides
whether to stop acquiring features and make a prediction; if not  which feature(s) to purchase next.
Achieving an accuracy-cost trade-off corresponds to ﬁnding the optimal policy minimizing a loss
function. We deﬁne the loss function as a combination of accuracy and cost:

L(s  a) = α · cost(s) − margin(a)

(7)

6

(a) Reward of DAgger and Coaching.

(b) Radar dataset.

(c) Digit dataset.

(d) Segmentation dataset.

Figure 1: 1(a) shows reward versus cost of DAgger and Coaching over 15 iterations on the digit
dataset with α = 0.5. 1(b) to 1(d) show accuracy versus cost on the three datasets. For DAgger and
Coaching  we show results when α = 0  0.1  0.25  0.5  1.0  1.5  2.

where margin(a) denote the margin of classifying the instance after action a; cost(s) denote the
user-deﬁned cost of all selected features in the current state s; and α is a user-speciﬁed trade-off
parameter. Since we consider feature selection for each single instance here  the average margin
reﬂects accuracy on the whole datasets.

4.1 Dynamic Feature Selection by Imitation Learning

Ideally  an oracle should lead to a subset of features having the maximum reward. However  we
have too large a state space to exhaustedly search for the optimal subset of features. In addition 
the oracle action may not be unique since the optimal subset of features do not have to be selected
in a ﬁxed order. We address this problem by using a forward-selection oracle. Given a state s  the
oracle iterates through the action space and calculates each action’s loss; it then chooses the action
that leads to the minimum immediate loss in the current state. We deﬁne φ(st  a) as a concatenation
of the current feature vector and a meta-feature vector that provides information about previous
classiﬁcation results and cost.
In most cases  our oracle can achieve high accuracy with rather small cost. Considering a linear
classiﬁer  as the oracle already knows the correct class label of an instance  it can simply choose 
for example  a positive feature that has a positive weight to correctly classify a positive instance. In
addition  at the start state even when φ(s0  a) are almost the same for all instances  the oracle may
tend to choose features that favor the instance’s class. This makes the oracle’s behavior very hard to
imitate. In the next section we show that in this case coaching achieves better results than using an
oracle.

7

0.260.280.300.320.340.360.38averagecostperexample0.400.450.500.55rewardDAggerCoaching0.00.20.40.60.81.0averagecostperexample0.600.650.700.750.800.850.900.951.00accuracy|w|/costForwardDAggerCoachingOracle0.00.20.40.60.81.0averagecostperexample0.40.50.60.70.80.9accuracy|w|/costForwardDAggerCoachingOracle0.00.20.40.60.81.0averagecostperexample0.600.650.700.750.800.850.90accuracy|w|/costForwardDAggerCoachingOracle4.2 Experimental Results

We perform experiments on three UCI datasets (radar signal  digit recognition  image segmentation).
Random costs are assigned to features. We ﬁrst compare the learning curve of DAgger and Coaching
over 15 iterations on the digit dataset with α = 0.5 in Figure 1(a). We can see that DAgger makes
a big improvement in the second iteration  while Coaching takes smaller steps but achieves higher
reward gradually. In addition  the reward of Coaching changes smoothly and grows stably  which
means coaching avoids drastic change of the policy.
To test the effect of dynamic selection  we compare our results with DAgger and two static fea-
ture selection baselines that sequentially add features according to a ranked list. The ﬁrst baseline
(denoted by Forward) ranks features according to the standard forward feature selection algorithm
without any notion of the cost. The second baseline (denoted by |w|/cost) uses a cost-sensitive
ranking scheme based on |w|/cost  the weight of a feature divided by its cost. Therefore  features
having high scores are expected to be cost-efﬁcient. We give the results in Figure 1(b) to 1(d). To
get results of our dynamic feature selection algorithm at different costs  we set α in the loss function
to be 0.0  0.1  0.25  0.5  1.0  1.5  2.0 and use the best policy evaluated on the development set for
each α. For coaching  we set λ2 = 1 and decrease it by e−1 in each iteration. First  we can see that
dynamically selecting features for each instance signiﬁcantly improves the accuracy at a small cost.
Sometimes  it even achieves higher accuracy than using all features. Second  we notice that there is
a substantial gap between the learned policy’s performance and the oracle’s  however  in almost all
settings Coaching achieves higher reward  i.e. higher accuracy at a lower cost as shown in the ﬁg-
ures. Through coaching  we can reduce the gap by taking small steps towards the oracle. However 
the learned policy is still much worse compared to the oracle’s policy. This is because coaching
is still inherently limited by the insufﬁcient policy space  which can be ﬁxed by using expensive
kernels and nonlinear policies.

5 Related Work

The idea of using hope action is similar to what Chiang et al. [6] and Liang et al. [5] have used
for selecting oracle translations in machine translation. They maximized a linear combination of the
BLEU score (similar to negative task loss in our case) and the model score to ﬁnd good translations
that are easier to train against. More recently  McAllester et al. [4] deﬁned the direct label that
combines model score and task loss from a different view: they showed that using a perceptron-like
training methods and update towards the direct label is equivalent to perform gradient descent on
the task loss.
Coaching is also similar to proximal methods in online learning [14  15]. They avoid large changes
during updating and achieve the original goal gradually. In proximal regularization  we want the
updated parameter vector to stay close to the previous one. Do et al. [14] showed that solving the
original learning problem through a sequence of modiﬁed optimization tasks whose objectives have
greater curvature can achieve a lower regret bound.

6 Conclusion and Future Work

In this paper  we consider the situation in imitation learning where an oracle’s performance is far
from what is achievable in the learning space. We propose a coaching algorithm that lets the learner
target at easier goals ﬁrst and gradually approaches the oracle. We show that coaching has a lower
regret bound both theoretically and empirically. In the future  we are interested in formally deﬁning
the hardness of a problem so that we know exactly in which cases coaching is more suitable than
DAgger. Another direction is to develop a similar coaching process in online convex optimization by
optimizing a sequence of approximating functions. We are also interested in applying coaching to
more complex structured prediction problems in natural language processing and computer vision.

References
[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML 

2004.

8

[2] M. Veloso B. D. Argall  S. Chernova and B. Browning. A survey of robot learning from

demonstration. 2009.

[3] Stéphane. Ross  Geoffrey J. Gordon  and J. Andrew. Bagnell. A reduction of imitation learning

and structured prediction to no-regret online learning. In AISTATS  2011.

[4] D. McAllester  T. Hazan  and J. Keshet. Direct loss minimization for structured prediction. In

NIPS  2010.

[5] D. Klein P. Liang  A. Bouchard-Ct and B. Taskar. An end-to-end discriminative approach to

machine translation. In ACL  2006.

[6] D. Chiang  Y. Marton  and P. Resnik. Online large-margin training of syntactic and structural

translation features. In EMNLP  2008.

[7] R. Busa-Fekete D. Benbouzid and B. Kégl. Fast classiﬁcation using space decision dags. In

ICML  2012.

[8] P. Preux G. Dulac-Arnold  L. Denoyer and P. Gallinari. Datum-wise classiﬁcation: a sequential

approach to sparsity. In ECML  2011.

[9] Stéphane Ross and J. Andrew Bagnell. Efﬁcient reductions for imitation learning. In AISTATS 

2010.

[10] Kääriäinen. Lower bounds for reductions. In Atomic Learning Workshop  2006.
[11] Hal Daumé III  John Langford  and Daniel Marcu. Search-based structured prediction. Ma-

chine Learning Journal (MLJ)  2009.

[12] Elad Hazan  Adam Kalai  Satyen Kale  and Amit Agarwal. Logarithmic regret algorithms for

online convex optimization. In COLT  pages 499–513  2006.

[13] Sham M. Kakade and Shai Shalev-shwartz. Mind the duality gap: Logarithmic regret algo-

rithms for online optimization. In NIPS  2008.

[14] Q. Le C. B. Do and C.S. Foo. Proximal regularization for online and batch learning. In ICML 

2009.

[15] H Brendan Mcmahan. Follow-the-regularized-leader and mirror descent : Equivalence theo-

rems and l1 regularization. JMLR  15:525–533  2011.

9

,Roger Frigola
Fredrik Lindsten
Thomas Schön
Carl Edward Rasmussen
Nagarajan Natarajan
Prateek Jain