2019,Towards a Zero-One Law for Column Subset Selection,There are a number of approximation algorithms for NP-hard versions of low rank approximation  such as finding a rank-$k$ matrix $B$ minimizing the sum of absolute values of differences to a given $n$-by-$n$ matrix $A$  $\min_{\textrm{rank-}k~B}\|A-B\|_1$  or more generally finding a rank-$k$ matrix $B$ which minimizes the sum of $p$-th powers of absolute values of differences  $\min_{\textrm{rank-}k~B}\|A-B\|_p^p$. Many of these algorithms are linear time columns subset selection algorithms  
returning a subset of $\poly(k \log n)$ columns whose cost is no more than a $\poly(k)$ factor larger than the cost of the best rank-$k$ matrix. 
The above error measures are special cases of the following general entrywise
low rank approximation problem: given an arbitrary function $g:\mathbb{R} \rightarrow \mathbb{R}_{\geq 0}$  find a rank-$k$ matrix $B$ which minimizes $\|A-B\|_g = \sum_{i j}g(A_{i j}-B_{i j})$. A natural question is which functions $g$ admit efficient approximation algorithms? Indeed  this is a central question of recent work studying generalized low rank models. In this work we give approximation algorithms for {\it every} function $g$ which is approximately monotone and satisfies an approximate triangle inequality  and we show both of these conditions are necessary. Further  our algorithm is efficient if the function $g$ admits an efficient approximate regression algorithm. Our approximation algorithms handle functions which are not even scale-invariant  such as the Huber loss function  which we show have very different structural properties than $\ell_p$-norms  e.g.  one can show the lack of scale-invariance causes any column subset selection algorithm to provably require a $\sqrt{\log n}$ factor larger number of columns than $\ell_p$-norms; nevertheless we design the first efficient column subset selection algorithms for such error measures.,Towards a Zero-One Law for Column Subset

Selection

Zhao Song∗

University of Washington

magic.linuxkde@gmail.com

David P. Woodruff∗

Carnegie Mellon University
dwoodruf@cs.cmu.edu

Peilin Zhong∗

Columbia University

pz2225@columbia.edu

Abstract

There are a number of approximation algorithms for NP-hard versions of low
rank approximation  such as ﬁnding a rank-k matrix B minimizing the sum of
absolute values of differences to a given n-by-n matrix A  minrank-k B (cid:107)A − B(cid:107)1 
or more generally ﬁnding a rank-k matrix B which minimizes the sum of p-th
powers of absolute values of differences  minrank-k B (cid:107)A − B(cid:107)p
p. Many of these
algorithms are linear time columns subset selection algorithms  returning a subset
of poly(k log n) columns whose cost is no more than a poly(k) factor larger
than the cost of the best rank-k matrix. The above error measures are special
cases of the following general entrywise low rank approximation problem: given
an arbitrary function g : R → R≥0  ﬁnd a rank-k matrix B which minimizes
(cid:107)A − B(cid:107)g =(cid:80)i j g(Ai j − Bi j). A natural question is which functions g admit
efﬁcient approximation algorithms? Indeed  this is a central question of recent
work studying generalized low rank models. In this work we give approximation
algorithms for every function g which is approximately monotone and satisﬁes an
approximate triangle inequality  and we show both of these conditions are necessary.
Further  our algorithm is efﬁcient if the function g admits an efﬁcient approximate
regression algorithm. Our approximation algorithms handle functions which are
not even scale-invariant  such as the Huber loss function  which we show have
very different structural properties than (cid:96)p-norms  e.g.  one can show the lack of
scale-invariance causes any column subset selection algorithm to provably require
a √log n factor larger number of columns than (cid:96)p-norms; nevertheless we design
the ﬁrst efﬁcient column subset selection algorithms for such error measures.

1

Introduction

A well-studied problem in machine learning and numerical linear algebra  with applications to recom-
mendation systems  text mining  and computer vision  is that of computing a low-rank approximation
of a matrix. Such approximations reveal low-dimensional structure  provide a compact way of storing
a matrix  and can quickly be applied to a vector.
A commonly used version of the problem is to compute a near optimal low-rank approximation with
respect to the Frobenius norm. That is  given an n×n input matrix A and an accuracy parameter  > 0 
output a rank-k matrix B with large probability so that (cid:107)A − B(cid:107)2
F   where for
a matrix C  (cid:107)C(cid:107)2
i j is its squared Frobenius norm  and Ak = argminrank-k B(cid:107)A − B(cid:107)F .
Ak can be computed exactly using the singular value decomposition (SVD)  but takes O(n3) time in
practice and nω time in theory  where ω ≈ 2.373 is the exponent of matrix multiplication [1  2  3  4].
Sárlos [5] showed how to achieve the above guarantee with constant probability in (cid:101)O(nnz(A)· k/) +
n · poly(k/) time  where nnz(A) denotes the number of non-zero entries of A. This was improved

F =(cid:80)i j C 2

F ≤ (1 + )(cid:107)A − Ak(cid:107)2

∗equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

in [6  7  8  9  10] using sparse random projections in O(nnz(A)) + n · poly(k/) time. Large sparse
datasets in recommendation systems are common  such as the Bookcrossing (100K × 300K with
106 observations) [11] and Yelp datasets (40K × 10K with 105 observations) [12]  and this is a
substantial improvement over the SVD.
Robust Low Rank Approximation. To understand the role of the Frobenius norm in the algorithms
above  we recall a standard motivation for this error measure. Suppose one has n data points in a k-
dimensional subspace of Rd  where k (cid:28) d. We can write these points as the rows of an n×d matrix A∗
which has rank k. The matrix A∗ is often called the ground truth matrix. In a number of settings  due to
measurement noise or other kinds of noise  we only observe the matrix A = A∗ +∆  where each entry
of the noise matrix ∆ ∈ Rn×n is an i.i.d. random variable from a certain mean-zero noise distribution
D. One method for approximately recovering A∗ from A is maximum likelihood estimation. Here
one tries to ﬁnd a matrix B maximizing the log-likelihood: maxrank-k B(cid:80)i j log p(Ai j − Bi j) 
where p(·) is the probability density function of the underlying noise distribution D. For example 
when the noise distribution is Gaussian with mean zero and variance σ2  denoted by N (0  σ2)  then
the optimization problem is maxrank-k B(cid:80)i j(cid:16)log(1/√2πσ2) − (Ai j − Bi j)2/(2σ2)(cid:17)   which is

equivalent to solving the Frobenius norm loss low rank approximation problem deﬁned above.
The Frobenius norm loss  while having nice statistical properties for Gaussian noise  is well-known
to be sensitive to outliers. Applying the same maximum likelihood framework above to other
kinds of noise distributions results in minimizing other kinds of loss functions. In general  if the
density function of the underlying noise D is p(z) = c · e−g(z)  where c is a normalization constant 
then the maximum likelihood estimation problem for this noise distribution becomes the following
generalized entry-wise loss low rank approximation problem: minrank-k B(cid:80)i j g(Ai j − Bi j) =
minrank-k B (cid:107)A − B(cid:107)g  which is a central topic of recent work on generalized low-rank models [13].
For example  when the noise is Laplacian  the entrywise (cid:96)1 loss is the maximum likelihood estimation 
which is also robust to sparse outliers. A natural setting is when the noise is a mixture of small
Gaussian noise and sparse outliers; this noise distribution is referred to as the Huber density. In this
case the Huber loss function gives the maximum likelihood estimate [13]  where the Huber function
[14] is deﬁned to be: g(x) = x2/(2τ ) if |x| < τ /2  and g(x) = |x| − τ /2 if |x| ≥ τ. Another nice
property of the Huber error measure is that it is differentiable everywhere  unlike the (cid:96)1-norm  yet
still enjoys the robustness properties as one moves away from the origin  making it less sensitive to
outliers than the (cid:96)2-norm. There are many other kinds of loss functions  known as M-estimators [15] 
which are widely used as loss functions in robust statistics [16].
Although several speciﬁc cases have been studied  such as entry-wise (cid:96)p loss [17  18  19  20  21] 
weighted entry-wise (cid:96)2 loss [22]  and cascaded (cid:96)p((cid:96)2) loss [23  24]  the landscape of general entry-
wise loss functions remains elusive. There are no results known for any loss function which is
not scale-invariant  much less any kind of characterization of which loss functions admit efﬁcient
algorithms. This is despite the importance of these loss functions; we refer the reader to [13] for a
survey of generalized low rank models. This motivates the main question in our work:

Question 1.1 (General Loss Functions). For a given approximation factor α > 1  which functions g
allow for efﬁcient low-rank approximation algorithms? Formally  given an n × d matrix A  can we
ﬁnd a rank-k matrix B for which (cid:107)A − B(cid:107)g ≤ α minrank−k B(cid:48) (cid:107)A − B(cid:48)
(cid:107)g  where for a matrix C 
(cid:107)C(cid:107)g =(cid:80)i∈[n] j∈[d] g(Ci j)? What if we also allow B to have rank poly(k log n)?
For Question 1.1  one has g(x) = |x|p for p-norms  and note the Huber loss function also ﬁts into this
framework. Allowing B to have slightly larger rank than k  namely  poly(k log n)  is often sufﬁcient
for applications as it still allows for the space savings and computational gains outlined above. These
are referred to as bicriteria approximations and are the focus of our work.
Notation. Before we present our results  let us brieﬂy introduce the notation. For n ∈ Z≥0  let [n]
denote the set {1  2 ···   n}. Let A ∈ Rn×m. Ai and Aj denote the ith column and the jth row of A
respectively. Let P ⊆ [m]  Q ⊆ [n]. AP denotes the matrix which is composed by the columns of A
with column indices in P . Similarly  AQ denotes the matrix composed by the rows of A with row
indices in Q. Let S be a set and s ∈ Z≥0. We use(cid:0)S
s(cid:1) to denote the set of all the size-s subsets of S.

2

Table 1: Example functions satisfying both structural properties.

|x| ≤ τ
|x| > τ

g(x)
τ (|x| − τ /2)
|x|p/p

(cid:26) x2/2
2((cid:112)1 + x2/2 − 1)
(cid:26) τ 2/6 · (1 − (1 − (x/τ )2)3)

x2/(2 + 2x2)

τ 2 (|x|/τ − log(1 + |x|/τ ))

HUBER
(cid:96)p (p ≥ 1)
(cid:96)1 − (cid:96)2
GEMAN-MCCLURE
“FAIR"

TUKEY

CAUCHY
QUANTILE (τ ∈ (0  1))

1.1 Our Results

atig t

O(t)
O(tp−1)
O(t)
O(t)
O(t)

O(t)

O(t)

1

max

mong

1

1
1
1
1

1

(cid:16) τ

1
1−τ   1−τ

τ

(cid:17)

|x| ≤ τ
|x| > τ

τ 2/6

(cid:26) τ x

τ 2/2 · log(1 + (x/τ )2)
x ≥ 0
(τ − 1)x x < 0

We studied low rank approximation with respect to general error measures. Our algorithm is a
column subset selection algorithm  returning a small subset of columns which span a good low rank
approximation. Column subset selection has the beneﬁt of preserving sparsity and interpretability  as
described above.
We give a “zero-one law” for such column subset selection problems. We describe two properties on
the function g that we need to obtain our low rank approximation algorithms. We also show that if
we are missing any one of the properties  then we can ﬁnd an example function g for which there is
no good column subset selection (see Appendix B).
Since we obtain column subset selection algorithms for a wide class of functions  our algorithms
must necessarily be bicriteria and have approximation factor at least poly(k). Indeed  a special case
of our class of functions includes entrywise (cid:96)1-low rank approximation  for which it was shown in
Theorem G.27 of [18] that any subset of poly(k) columns incurs an approximation error of at least
kΩ(1). We also show that for the entrywise Huber-low rank approximation  already for k = 1  √log n
columns are needed to obtain any constant factor approximation  thus showing that for some of the
functions we consider  a dependence on n in our column subset size is necessary.
We note that previously for almost all such functions  it was not known how to obtain any non-trivial
approximation factor with any sublinear number of columns.

1.1.1 A Zero-One Law

We ﬁrst state three general properties  the ﬁrst two of which are structural properties and are necessary
and sufﬁcient for obtaining a good approximation from a small subset of columns. The third property
is needed for efﬁcient running time.
Approximate triangle inequality. For t ∈ Z>0  we say a function g(x) : R → R≥0 satisﬁes the
atig t-approximate triangle inequality if for any x1  x2 ···   xt ∈ R  g ((cid:80) xi) ≤ atig t ·(cid:80) g(xi).
Monotone property. For any parameter mong ≥ 1  we say function g(x) : R → R≥0 is mong-
monotone if for any x  y ∈ R with 0 ≤ |x| ≤ |y|  we have g(x) ≤ mong ·g(y).
Many functions including most M-estimators [15] and the quantile function [26] satisfy the above
two properties. See Table 1 for several examples. We refer the reader to the supplementary  namely
Appendix B  for the necessity of these two properties. Our next property is not structural  but rather
states that if the loss function has an efﬁcient regression algorithm  then that sufﬁces to efﬁciently
ﬁnd a small subset of columns spanning a good low rank approximation.
Regression property. We say function g(x) : R → R≥0 has the (regg d Treg g n d m)-regression
property if the following holds: given two matrices A ∈ Rn×d and B ∈ Rn×m  for each i ∈ [m]  let
OPTi denote minx∈Rd (cid:107)Ax − Bi(cid:107)g. There is an algorithm that runs in Treg g n d m time and outputs
a matrix X(cid:48)
i − Bi(cid:107)g ≤ regg d · OPTi ∀i ∈ [m] and outputs a vector of

∈ Rd×m such that (cid:107)AX(cid:48)

3

estimated regression costs v ∈ Rd such that OPTi ≤ vi ≤ regg d · OPTi ∀i ∈ [m]. The success
probability is at least 1 − 1/ poly(nm).
Some functions for which regression itself is non-trivial are e.g.  the (cid:96)0-loss function and Tukey
function. The (cid:96)0-loss function corresponds to the nearest codeword problem over the reals and has
slightly better than an O(d)-approximation ([27  28]  see also [20]). For the Tukey function  [29]
shows that Tukey regression is NP-hard  and it also gives approximation algorithms. For discussion
on regression solvers  we refer the reader to Appendix C.

Zero-one law (sufﬁcient conditions): For any function  as long as the above general three proper-
ties hold  we can provide an efﬁcient algorithm  as our following main theorem shows.
Theorem 1.2. Given a matrix A ∈ Rn×n  let k ≥ 1  k(cid:48) = 2k + 1. Let g : R → R≥0 denote a
function satisfying the atig k(cid:48)-approximate triangle inequality  the mong-monotone property   and
the (regg k(cid:48) Treg g n k(cid:48) n)-regression property. Let OPT = minrank −k A(cid:48) (cid:107)A(cid:48)
− A(cid:107)g. There is an
algorithm that runs in (cid:101)O(n + Treg g n k(cid:48) n) time and outputs a set S ⊆ [n] with |S| = O(k log n)
such that with probability at least 0.99 

X∈R|S|×n (cid:107)ASX − A(cid:107)g ≤ atig k(cid:48) · mong · regg k(cid:48) ·O(k log k) · OPT .

min

Although the input matrix A in the above statement is a square matrix  it is straightforward to extend
the result to the rectangular case. By the above theorem  we can obtain a good subset of columns. To
further get a low rank matrix B which is a good low rank approximation to A  it is sufﬁcient to take
an additional Treg g n |S| n time to solve the regression problem.
Zero-one law (necessary conditions):
In Appendix B.1  we show how to construct a monotone
function without approximate triangle inequality such that it is not possible to obtain a good low rank
approximation by selecting a small subset of columns.
In Appendix B.2  we discuss a function which has the approximate triangle inequality but is not
monotone. We show that for some matrices  there is no small subset of columns which can give a
good low rank approximation for such loss function.

1.1.2 Lower Bound on the Number of Columns

One may wonder if the log n blowup in rank is necessary in our theorem. We show some dependence
on n is necessary by showing that for the important Huber loss function  at least √log n columns are
required in order to obtain a constant factor approximation for k = 1:

Theorem 1.3. Let H(x) denote the following function: H(x) =(cid:26)x2 
For any n ≥ 1  there is a matrix A ∈ Rn×n such that  if we select o(√log n) columns to ﬁt the entire
matrix  there is no O(1)-approximation  i.e.  for any subset S ⊆ [n] with |S| = o(√log n) 

if |x| < 1;
if |x| ≥ 1.

|x| 

min

X∈R|S|×n (cid:107)ASX − A(cid:107)H ≥ ω(1) · min

rank −1 A(cid:48) (cid:107)A

(cid:48)

− A(cid:107)H .

Notice that the above function H(x) is always a constant approximation to the Huber function (see
Table 1) with τ = 1. Thus  the hardness also holds for the Huber function. For more discussion on
our lower bound  we refer the reader to Appendix D.

1.2 Overview of our Approach and Related Work

Low Rank Approximation for General Functions. A natural approach to low rank approximation
is “column subset selection”  which has been extensively studied in numerical linear algebra [30  31 
32  33  34  35  36  37  18  38]. One can take the column subset selection algorithm for (cid:96)p-low rank
approximation in [19] and try to adapt it to general loss functions. Namely  their argument shows that
for any matrix A ∈ Rn×n there exists a subset S of k columns of A  denoted by AS ∈ Rn×k  for
p; we
which there exists a k × n matrix V for which (cid:107)ASV − A(cid:107)p
(cid:107)p
refer the reader to Theorem 3 of [19]. Given the existence of such a subset S  a natural next idea is to
then sample a set T of k columns of A uniformly at random. It is then likely the case that if we look

p ≤ (k + 1)p minrank-k B(cid:48) (cid:107)A − B(cid:48)

4

(cid:48)

(cid:107)p
p.

j(cid:107)p+(cid:13)(cid:13)A∗

i = A∗

(2(k + 1)/n) · min

rank-k B(cid:48) (cid:107)A − B

(cid:107)∆s(cid:107)p(cid:107)p = (cid:107)∆j(cid:107)p  and it follows that (cid:107)Aj − As

at a random column Ai  (1) with probability 1/(k + 1)  i is not among the subset S of k columns out
of the k + 1 columns T ∪ {i} deﬁning the optimal rank-k approximation to the submatrix AT∪{i} 
and (2) with probability at least 1/2  the best rank-k approximation to AT∪{i} has cost at most
(1)
Indeed  (1) follows from T ∪ {i} being a uniformly random subset of k + 1 columns  while (2)
follows from a Markov bound. The argument in Theorem 7 of [19] is then able to “prune” a 1/(k + 1)
fraction of columns (this can be optimized to a constant fraction) in expectation  by “covering” them
with the random set T . Recursing on the remaining columns  this procedure stops after k log n
iterations  giving a column subset of size O(k2 log n) (which can be optimized to O(k log n)) and an
O(k)-approximation.
The proof in [19] of the existence of a subset S of k columns of A spanning a (k + 1)-approximation
above is quite general  and one might suspect it generalizes to a large class of error functions. Suppose 
for example  that k = 1. The idea there is to write A = A∗ +∆  where A∗ = U ·V is the optimal rank-
1 (cid:96)p-low rank approximation to A. One then “normalizes” by the error  deﬁning (cid:101)A∗
i /(cid:107)∆i(cid:107)p
and letting s be such that (cid:107)(cid:101)A∗
s(cid:107)p is largest. The rank-1 subset S is then just As. Note that since (cid:101)A∗
j for every j (cid:54)= s as αj · (cid:101)A∗
s(cid:107)p is largest  one can write (cid:101)A∗
has rank-1 and (cid:107)(cid:101)A∗
s for |αj| ≤ 1. The
fact that |αj| ≤ 1 is crucial; indeed  consider what happens when we try to “approximate” Aj by
j − Asαj(cid:107)∆j(cid:107)p/(cid:107)∆s(cid:107)p(cid:13)(cid:13)p =
As· αj(cid:107)∆j(cid:107)p
. Then (cid:107)Aj − Asαj(cid:107)∆j(cid:107)p/(cid:107)∆s(cid:107)p(cid:107)p ≤ (cid:107)Aj−A∗
(cid:107)∆s(cid:107)p
s + ∆s)αj(cid:107)∆j(cid:107)p/(cid:107)∆s(cid:107)p(cid:13)(cid:13)p = (cid:107)∆j(cid:107)p +(cid:107)∆sαj(cid:107)∆j(cid:107)p/(cid:107)∆s(cid:107)p(cid:107)p   and since the
(cid:107)∆j(cid:107)p +(cid:13)(cid:13)A∗
j − (A∗

(cid:107)∆j(cid:107)p
(cid:107)∆s(cid:107)p(cid:107)p. So far 
p-norm is monotonically increasing and αj ≤ 1  the latter is at most (cid:107)∆j(cid:107)p + (cid:107)∆s
all we have used about the p-norm is the monotone increasing property  so one could hope that the
argument could be generalized to a much wider class of functions.
(cid:107)∆j(cid:107)p
However  at this point the proof uses that the p-norm has scale-invariance  and so (cid:107)∆s
(cid:107)∆s(cid:107)p(cid:107)p =
αj(cid:107)∆j(cid:107)p
(cid:107)∆s(cid:107)p (cid:107)p ≤ 2(cid:107)∆j(cid:107)p  giving an overall
(cid:107)∆j(cid:107)p · (cid:107) ∆s
2-approximation (recall k = 1). But what would happen for a general  not necessarily scale-invariant
(cid:107)∆j(cid:107)g
function g? We need to bound (cid:107)∆s
(cid:107)∆s(cid:107)g (cid:107)g. If we could bound this by O((cid:107)∆j(cid:107)g)  we would
obtain the same conclusion as before  up to constant factors. Consider  though  the “reverse Huber
function”: g(x) = x2 if x ≥ 1 and g(x) = |x| for x ≤ 1. Suppose that ∆s and ∆j were just
1-dimensional vectors  i.e.  real numbers  so we need to bound g(∆sg(∆j)/g(∆s)) by O(g(∆j)).
Suppose ∆s = 1. Then g(∆s) = 1 and g(∆sg(∆j)/g(∆s)) = g(g(∆j)) and if ∆j = n  then
g(g(∆j)) = n4 = g(∆j)2  much larger than the O(g(∆j)) we were aiming for.
Maybe the analysis can be slightly changed to correct for these normalization issues? This is not
the case  as we show that unlike for (cid:96)p-low rank approximation  for the reverse Huber function
there is no subset of 2 columns of A obtaining better than an n1/4-approximation factor. (See
Section D.2 for more details). Further  the lack of scale invariance not only breaks the argument
in [19]  it shows that combinatorially such functions g behave very differently than (cid:96)p-norms. We
show more generally there exist functions  in particular the Huber function  for which one needs
to choose Ω(√log n) columns to obtain a constant factor approximation; we describe this more
below. Perhaps more surprisingly  we show a subset of O(log n) columns sufﬁce to obtain a constant
factor approximation to the best rank-1 approximation for any function g(x) which is approximately
monotone and has the approximate triangle inequality  the latter implying for any constant C > 0 and
any x ∈ R≥0  g(Cx) = O(g(x)). For k > 1  these conditions become: (1) g(x) is monotone non-
decreasing in x  (2) g(x) is within a poly(k) factor of g(−x)  and (3) for any real number x ∈ R≥0 
g(O(kx)) ≤ poly(k) · g(x). We show it is possible to obtain an O(k2 log k) approximation with
O(k log n) columns. We give the intuition and main lemma statements for our result in Section 2 
deferring proofs to the supplementary material.
Even for (cid:96)p-low rank approximation  our algorithms slightly improve and correct a minor error in
[19] which claims in Theorem 7 an O(k)-approximation with O(k log n) columns for (cid:96)p-low rank
approximation. However  their algorithm actually gives an O(k log n)-approximation with O(k log n)
columns. In [19] it was argued that one expects to pay a cost of O(k/n) · minrank-k B(cid:48) (cid:107)A − B(cid:48)
p per
(cid:107)p
column as in (1)  and since each column is only counted in one iteration  summing over the columns
gives O(k) · minrank-k B(cid:48) (cid:107)A − B(cid:48)
(cid:107)p total cost. The issue is that the value of n is changing in each

5

Algorithm 1 Low rank approximation algorithm for general functions
1: procedure GENERALFUNCTIONLOWRANKAPPROX(A ∈ Rn×n  k ∈ Z≥1  g : R → R≥0)

Initialization: T0 ← [n]  i ← 1  r ← 0
for |Ti−1| ≥ 1000k do

for j = 1 → log n do

i

Sample S(j)
Solve the regg 2k-approximate regression minx∈R2k (cid:107)AS(j)
  and let v(j)

2k (cid:1) uniformly at random

i t be the regg 2k-estimated regression cost

from(cid:0)Ti−1

i

x − At(cid:107)g for each t ∈
(cid:46) See Section 1.1.1 for

Ti−1 \ S(j)
regression property

i

|/20 largest value in {v(j)

i t(cid:48) | t(cid:48)

∈ Ti−1 \

2:
3:
4:
5:
6:

7:

8:
9:
10:

S(j)
i }}

i

i

i t is the bottom |Ti−1 \ S(j)
v(j)
i t

i ← {t | v(j)
R(j)
i ←(cid:80)t∈R(j)
c(j)
end for
← arg minj∈[log n](cid:110)c(j)
i (cid:111)
j∗
  Ri ← R(j∗
Si ← S(j∗
  Ti ← Ti−1\ (Si ∪ Ri)
r ← i
i ← i + 1
end for

)

)

i

i

11:
12:
13:
14:
15:
16: end procedure

return S = Tr ∪(cid:83)i∈[r] Si

(cid:46) It is easy to see r ≤ O(log n) from the above procedure

iteration  so if in the i-th iteration it is ni  then we could pay ni · O(k/ni)· minrank-k B(cid:48) (cid:107)A− B(cid:48)
(cid:107)p =
O(k) · minrank-k B(cid:48) (cid:107)A − B(cid:48)
(cid:107)p in each of O(log n) iterations  giving O(k log n) approximation ratio.
In contrast  our algorithm achieves an O(k log k) approximation ratio for (cid:96)p-low rank approximation
as a special case  which gives the ﬁrst O(1) approximation in nearly linear time for any constant k
for (cid:96)p norms. Our analysis is ﬁner in that we show not only do we expect to pay a cost of O(k/ni) ·
minrank-k B(cid:48) (cid:107)A − B(cid:48)
p per column in iteration i  we pay O(k/ni) times the cost of the best rank-k
(cid:107)p
approximation to A after the most costly n/k columns have been removed; thus we pay O(k/ni)
times a residual cost with the top n/k columns removed. This ultimately implies any column’s cost
can contribute in at most O(log k) of O(log n) recursive calls  replacing an O(log n) factor with
an O(log k) factor in the approximation ratio. This also gives the ﬁrst poly(k)-approximation for
(cid:96)0-low rank approximation  studied in [20]  improving the O(k2 log(n/k))-approximation there to
O(k2 log k) and giving the ﬁrst constant approximation for constant k.

2 Algorithm for General Loss Low Rank Approximation

x − At(cid:107)g for all t ∈ Ti−1 \ S(j)

Our algorithm is presented in Algorithm 1. First  let us brieﬂy analyze the running time. Consider
ﬁxed i ∈ [r]  j ∈ [log n]. Sampling S(j)
takes O(k) time. Solving regg 2k-approximate regression
| ≤ Treg g n 2k+1 n time.
minx (cid:107)AS(j)
Since ﬁnding |Ti−1 \ S(j)
can be computed
in O(n) time. Thus the inner loop takes O(n + Treg g n 2k+1 n) time. Since r = O(log n)  the total
running time over all i  j is O((n + Treg g n 2k+1 n) log2 n). In the remainder of the section  we will
sketch the proof of the correctness. For the missing proofs  we refer the reader to Appendix A.

|/20 smallest element can be done in O(n) time  R(j)

takes Treg g n 2k |Ti−1\S(j)

i

i

i

i

i

i

2.1 Properties of Uniform Column Sampling
Let us ﬁrst introduce some useful notation. Consider a rank-k matrix M∗
H ⊆ [m]  let RM∗ (H) ⊆ H be a set such that
∗
)Q

RM∗ (H) = arg max

P :P⊆H(cid:26)(cid:12)(cid:12)(cid:12)det(cid:16)(M

P(cid:17)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12) |P| = |Q| = rank(M

∈ Rn×m. For a set
H )  Q ⊆ [n](cid:27) .

∗

6

where det(C) denotes the determinant of a square matrix C. Notice that in the above formula 
the maximum is over all possible choices of P and Q while RM∗ (H) only takes the value of the
corresponding P . By Cramer’s rule  if we use a linear combination of the columns of M∗
RM∗ (H)
to express any column of M∗
H  the absolute value of every ﬁtting coefﬁcient will be at most 1. For
example  consider a rank k matrix M∗
∈ Rn×(k+1) and H = [k + 1]. Let P ⊆ [k + 1]  Q ⊆
[n] |P| = |Q| = k be such that | det((M∗)Q
P )| is maximized. Since M∗ has rank k  we know
P ) (cid:54)= 0 and thus the columns of M∗
det((M∗)Q
P are independent. Let i ∈ [k + 1] \ P . Then the linear
equation M∗
P x = M∗
i is feasible and there is a unique solution x. Furthermore  by Cramer’s rule
det((M∗
P )| ≥ | det((M∗)Q
. Since | det((M∗)Q
xj =

[k+1]\{j})|  we have (cid:107)x(cid:107)∞ ≤ 1.

det((M∗)Q
P )

)Q
[k+1]\{j})

H to express M∗

Consider an arbitrary matrix M ∈ Rn×m. We can write M = M∗ + N  where M∗
∈ Rn×m is an
arbitrary rank-k matrix  and N ∈ Rn×m is the residual matrix. The following lemma shows that  if
we randomly choose a subset H ⊆ [m] of 2k columns  and we randomly look at another column i 
then with constant probability  the absolute values of all the coefﬁcients of using a linear combination
of the columns of M∗
i are at most 1  and furthermore  if we use the same coefﬁcients
to use columns of MH to ﬁt Mi  then the ﬁtting cost is proportional to (cid:107)NH(cid:107)g + (cid:107)Ni(cid:107)g.
Lemma 2.1. Given a matrix M ∈ Rn×m and a parameter k ≥ 1 
let M∗
∈ Rn×m
be an arbitrary rank-k matrix. Let N = M − M∗. Let H ⊆ [m] be a uniformly ran-
dom subset of [m]  and let i denote a uniformly random index sampled from [m]\H. Then
(I) Pr [i /∈ RM∗ (H ∪ {i})] ≥ 1/2; (II) If i /∈ RM∗ (H ∪ {i})  then there exist |H| coefﬁcients
i =(cid:80)|H|
α1  α2 ···   α|H| for which M∗
H )j ∀j ∈ [|H|] |αj| ≤ 1  and minx∈R|H| (cid:107)MH x −
Mi(cid:107)g ≤ atig |H|+1 · mong ·(cid:16)(cid:107)Ni(cid:107)g +(cid:80)|H|

j=1 αj(M∗
j=1 (cid:107)(NH )j(cid:107)g(cid:17) .

Notice that part (II) of the above lemma does not depend on any randomness of H or i. By applying
part (I) of the above lemma  it is enough to prove that if we randomly choose a subset H of 2k
columns  there is a constant fraction of columns that each column M∗
i can be expressed by a linear
combination of columns in M∗
H   and the absolute values of all the ﬁtting coefﬁcients are at most 1.
Because of Cramer’s rule  it thus sufﬁces to prove the following lemma.
Lemma 2.2.

Pr
H∼([m]

2k )(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)(cid:26)i(cid:12)(cid:12)(cid:12)(cid:12) i ∈ [m] \ H  i (cid:54)∈ RM∗ (H ∪ {i})(cid:27)(cid:12)(cid:12)(cid:12)(cid:12) ≥ (m − 2k)/4(cid:21) ≥ 1/4.

2.2 Correctness of the Algorithm
We write the input matrix A as A∗ + ∆  where A∗

∈ Rn×n is the best rank-k approximation to A 
and ∆ ∈ Rn×n is the residual matrix with respect to A∗. Then (cid:107)∆(cid:107)g =(cid:80)n
i=1 (cid:107)∆i(cid:107)g is the optimal
cost. As shown in Algorithm 1  our approach iteratively eliminates all the columns. In each iteration 
we sample a subset of columns  and use these columns to ﬁt other columns. We drop a constant
fraction of columns which have a good ﬁtting cost. Suppose the indices of the columns surviving
after the i-th outer iteration are Ti = {ti 1  ti 2 ···   ti mi} ⊆ [n]. Without loss of generality  we can
assume (cid:107)∆ti 1(cid:107)g ≥ (cid:107)∆ti 2(cid:107)g ≥ ··· ≥ (cid:107)∆ti mi(cid:107)g. The following claim shows that if we randomly
sample 2k column indices H from Ti  then the cost of ∆H will not be large.
100k (cid:107)∆ti j(cid:107)g(cid:105) ≥ 19
mi(cid:80)mi
Claim 2.3. If |Ti| = mi ≥ 1000k  PrH∼(Ti
By an averaging argument  in the following claim  we can show that there is a constant fraction of
columns in Ti whose optimal cost is also small.
100k (cid:107)∆ti j(cid:48)(cid:107)g(cid:27)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1

2k)(cid:104)(cid:80)j∈H (cid:107)∆j(cid:107)g ≤ 400 k
Claim 2.4. If |Ti| = mi ≥ 1000k (cid:12)(cid:12)(cid:12)(cid:12)(cid:26)ti j(cid:12)(cid:12)(cid:12)(cid:12) ti j ∈ Ti (cid:107)∆ti j(cid:107)g ≥ 20
mi(cid:80)mi

By combining Lemma 2.2  part (II) of Lemma 2.1 with the above two claims  it is sufﬁcient to prove
the following core lemma. It says that if we randomly choose a subset of 2k columns from Ti  then
we can ﬁt a constant fraction of the columns from Ti with a small cost.

5 mi.

20 .

mi

j=

j(cid:48)=

mi

7

Lemma 2.5. If |Ti| = mi ≥ 1000k 

Pr
H∼(Ti

2k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


j(cid:12)(cid:12)(cid:12)(cid:12) j ∈ Ti  min

x∈R|H| (cid:107)AH x − Aj(cid:107)g ≤ C1 ·

1
mi ·

mi(cid:88)j(cid:48)=

mi
100k

(cid:107)∆ti j(cid:48)(cid:107)g

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
20

≥

mi ≥

1
5

 

where C1 = 500 · k · atig |S|+1 · mong .
Let us brieﬂy explain why the above lemma is enough to prove the correctness of our algorithm. For
each column j ∈ [m]  either the column j is in Tr and is selected by the end of the algorithm  or
∃i < r such that j ∈ Ti\Ti+1. If j ∈ Ti\Ti+1  then by the above lemma  we can show that with high
probability  minx (cid:107)ASi+1x − Aj(cid:107)g ≤ O(C1(cid:107)∆(cid:107)1/|Ti|). Thus  minX (cid:107)ASi+1 X − ATi\Ti+1(cid:107)g ≤
O(C1(cid:107)∆(cid:107)1). It directly gives a O(rC1) = O(C1 log n) approximation. For the detailed proof of
Theorem 1.2  we refer the reader to Appendix A.

3 Experiments

We show that with the Huber loss low rank approximation  it is possible to outperform the SVD and
entrywise (cid:96)1-low rank approximation on certain noise distributions. Even bi-criteria solutions can
work very well. This motivates our study of general entry-wise loss functions.
Suppose the noise of the input matrix is a mixture of small Gaussian noise and sparse outliers.
Consider an extreme case: the data matrix A ∈ Rn×n is a block diagonal matrix which contains three
blocks: one block has size n1 × n1 (n1 = Θ(n)) which has uniformly small noise (every entry is
Θ(1/√n))  another block has only one entry which is a large outlier (with value Θ(n0.8))  and the
third matrix is the ground truth matrix with size n3 × n3 (n3 = Θ(n0.6)) where the absolute value of
each entry is at least 1/no(1) and at most no(1). If we apply Frobenius norm rank-1 approximation 
then since (n0.8)2 > (n0.6)2 · no(1) and (n0.8)2 > n2 · (1/√n)2  we can only learn the large outlier.
If we apply entry-wise (cid:96)1 norm rank-1 approximation  then since n2 · 1/√n > (n0.6)2 · no(1) and
n2 · 1/√n > n0.8  we can only learn the uniformly small noise. But if we apply Huber loss rank-1

approximation  then we can learn the ground truth matrix.
A natural question is: can bi-criteria Huber loss low rank approximation also learn the ground truth
matrix under certain noise distributions? We did experiments to answer this question.
Parameters. In each iteration  we choose 2k columns to ﬁt the remaining columns  and we drop half
of the columns with smallest regression cost. In each iteration  we repeat 20 times to ﬁnd the best 2k
columns. At the end  if there are at most 4k columns remaining  we ﬁnish our algorithm. We choose
to optimize the Huber loss function  i.e.  f (x) = 1
Data. We evaluate our algorithms on several input data matrix A ∈ Rn×n sizes  for n ∈
{200  300  400  500}. For rank-1 bi-criteria solutions  the output rank is given in Table 2.

2 x2 for x ≤ 1  and f (x) = |x| − 1

2 for x > 1.

Table 2: The output rank of our algorithm for different input sizes and for k = 1.

n

Output rank

200
12

300
12

400
14

500
14

A is constructed as a block diagonal matrix with three blocks. The ﬁrst block has size 4
5 n. It
contains many copies of k(cid:48) different columns where k(cid:48) is equal to the output rank corresponding
to n (see Table 2). The entry of a column is uniformly drawn from {−5/√n  5/√n}. The second
block is the ground truth matrix. It is generated by 1/√k(cid:48) · U · V (cid:62) where U  V ∈ Rn×k(cid:48)
are two
× k(cid:48) diagonal matrix where each diagonal
i.i.d. random Gaussian matrices. The last block is a size k(cid:48)
entry is a sparse outlier with magnitude of absolute value 5 · n0.8.
Experimental Results. We compare our algorithm with Frobenius norm low rank approximation
and entry-wise (cid:96)1 loss low rank approximation algorithms [18]. To make it comparable  we set the
target rank of previous algorithms to be the output rank of our algorithm. In Figure 1  we can see that
the ground truth matrix is well covered by our Huber loss low rank approximation. In Figure 2  we
show that our algorithm indeed gives a good solution with respect to the Huber loss.

5 n × 4

8

(a)

(c)

(b)

(d)

Figure 1: The input data has size 500 × 500. The color indicates the logarithmic magnitude of the
absolute value of each entry. (a) is the input matrix. It contains 3 blocks on its diagonal. The top-left
one has uniformly small noise. The central one is the ground truth. The bottom-right one contains
sparse outliers. Each block has rank 14. So the rank of the input matrix is 3 × 14 = 42. (b) is the
entry-wise (cid:96)1 loss rank-14 approximation given by [18]. As shown above  it mainly covers the small
noise  but loses the information of the ground truth. (c) is the Frobenius norm rank-14 approximation
given by the top 14 singular vectors. As shown in the ﬁgure  it mainly covers the outliers. However  it
loses the information of the ground truth. (d) is the rank-1 bi-criteria solution given by our algorithm.
As we can see  it can cover the ground truth matrix quite well.

Figure 2: The Huber loss given by different algorithms. The red bar is for the entrywise (cid:96)1 low
rank approximation algorithm [18]. The green bar is for traditional PCA. The blue bar is for our
algorithm. For input size n = 200  300  all the algorithms output rank-12 approximations. For input
size n = 400  500  all the algorithms output rank-14 approximations.

Acknowledgments. David P. Woodruff was supported in part by Ofﬁce of Naval Research (ONR)
grant N00014- 18-1-2562. Part of this work was done while he was visiting the Simons Institute
for the Theory of Computing. Peilin Zhong was supported in part by NSF grants (CCF-1703925 
CCF-1421161  CCF-1714818  CCF-1617955 and CCF-1740833)  Simons Foundation (#491119 to
Alexandr Andoni)  Google Research Award and a Google Ph.D. fellowship. Part of this work was
done while Zhao Song and Peilin Zhong were interns at IBM Research - Almaden and while Zhao
Song was visiting the Simons Institute for the Theory of Computing.

9

References
[1] Volker Strassen. Gaussian elimination is not optimal. Numerische Mathematik  13(4):354–356 

1969.

[2] Don Coppersmith and Shmuel Winograd. Matrix multiplication via arithmetic progressions.
In Proceedings of the nineteenth annual ACM symposium on Theory of computing  pages 1–6.
ACM  1987.

[3] Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In
Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC)  pages
887–898. ACM  2012.

[4] François Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings of the 39th
international symposium on symbolic and algebraic computation  pages 296–303. ACM  2014.

[5] Tamás Sarlós. Improved approximation algorithms for large matrices via random projections.
In 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)   21-24 October
2006  Berkeley  California  USA  Proceedings  pages 143–152  2006.

[6] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference  STOC’13  Palo Alto  CA 
USA  June 1-4  2013  pages 81–90. https://arxiv.org/pdf/1207.6365  2013.

[7] Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the forty-ﬁfth annual ACM
symposium on Theory of computing  pages 91–100. ACM  https://arxiv.org/pdf/1210.
3135  2013.

[8] Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In 2013 IEEE 54th Annual Symposium on Foundations of Computer
Science (FOCS)  pages 117–126. IEEE  https://arxiv.org/pdf/1211.1002  2013.

[9] Jean Bourgain  Sjoerd Dirksen  and Jelani Nelson. Toward a uniﬁed theory of sparse dimen-
sionality reduction in euclidean space. In Proceedings of the Forty-Seventh Annual ACM on
Symposium on Theory of Computing  STOC 2015  Portland  OR  USA  June 14-17  2015  pages
499–508  2015.

[10] Michael B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities.

In
Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA)  Arlington  VA  USA  January 10-12  2016  pages 278–287  2016.

[11] Cai-Nicolas Ziegler  Sean M McNee  Joseph A Konstan  and Georg Lausen.

Improving
recommendation lists through topic diversiﬁcation. In Proceedings of the 14th international
conference on World Wide Web  pages 22–32. ACM  2005.

[12] Yelp. Yelp dataset. http://www.yelp.com/dataset_challenge  2014.

[13] Madeleine Udell  Corinne Horn  Reza Zadeh  Stephen Boyd  et al. Generalized low rank models.

Foundations and Trends R(cid:13) in Machine Learning  9(1):1–118  2016.

[14] Peter J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics 

35(1):73–101  1964.

[15] Zhengyou Zhang. Parameter estimation techniques: A tutorial with application to conic ﬁtting.

Image and vision Computing  15(1):59–76  1997.

[16] Frank R Hampel  Elvezio M Ronchetti  Peter J Rousseeuw  and Werner A Stahel. Robust
statistics: the approach based on inﬂuence functions  volume 196. John Wiley & Sons  2011.

[17] Emmanuel J Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component

analysis? Journal of the ACM (JACM)  58(3):11  2011.

10

[18] Zhao Song  David P Woodruff  and Peilin Zhong. Low rank approximation with entrywise
(cid:96)1-norm error. In Proceedings of the 49th Annual Symposium on the Theory of Computing
(STOC). ACM  https://arxiv.org/pdf/1611.00898  2017.

[19] Flavio Chierichetti  Sreenivas Gollapudi  Ravi Kumar  Silvio Lattanzi  Rina Panigrahy  and
In ICML. arXiv preprint

David P Woodruff. Algorithms for (cid:96)p low rank approximation.
arXiv:1705.06730  2017.

[20] Karl Bringmann  Pavel Kolev  and David P. Woodruff. Approximation algorithms for (cid:96)0-low
rank approximation. In Advances in Neural Information Processing Systems (NIPS)  pages
6651–6662  2017.

[21] Frank Ban  Vijay Bhattiprolu  Karl Bringmann  Pavel Kolev  Euiwoong Lee  and David P.

Woodruff. A PTAS for (cid:96)p-low rank approximation. In SODA  2019.

[22] Ilya Razenshteyn  Zhao Song  and David P Woodruff. Weighted low rank approximations with
provable guarantees. In Proceedings of the 48th Annual Symposium on the Theory of Computing
(STOC)  2016.

[23] Amit Deshpande  Kasturi R. Varadarajan  Madhur Tulsiani  and Nisheeth K. Vishnoi. Algo-

rithms and hardness for subspace approximation. CoRR  abs/0912.1403  2009.

[24] Kenneth L Clarkson and David P Woodruff. Input sparsity and hardness for robust subspace
approximation. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science
(FOCS)  pages 310–329. IEEE  https://arxiv.org/pdf/1510.06073  2015.

[25] Zhao Song  Ruosong Wang  Lin F Yang  Hongyang Zhang  and Peilin Zhong. Efﬁcient

symmetric norm regression via linear sketching. arXiv preprint arXiv:1910.01788  2019.

[26] Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the

Econometric Society  pages 33–50  1978.

[27] Piotr Berman and Marek Karpinski. Approximating minimum unsatisﬁability of linear equations.
In Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms 
January 6-8  2002  San Francisco  CA  USA.  pages 514–516  2002.

[28] Noga Alon  Rina Panigrahy  and Sergey Yekhanin. Deterministic approximation algorithms for

the nearest codeword problem. In Algebraic Methods in Computational Complexity  2009.

[29] Kenneth L. Clarkson  Ruosong Wang  and David P. Woodruff. Dimensionality reduction for

tukey regression. In ICML  2019.

[30] Petros Drineas  Michael W. Mahoney  and S. Muthukrishnan. Subspace sampling and relative-
error matrix approximation: Column-row-based methods. In Algorithms - ESA 2006  14th
Annual European Symposium  Zurich  Switzerland  September 11-13  2006  Proceedings  pages
304–314  2006.

[31] Petros Drineas  Michael W. Mahoney  and S. Muthukrishnan. Subspace sampling and relative-
error matrix approximation: Column-based methods. In Approximation  Randomization  and
Combinatorial Optimization. Algorithms and Techniques  9th International Workshop on Ap-
proximation Algorithms for Combinatorial Optimization Problems  APPROX 2006 and 10th
International Workshop on Randomization and Computation  RANDOM 2006  Barcelona  Spain 
August 28-30 2006  Proceedings  pages 316–326  2006.

[32] Petros Drineas  Michael W. Mahoney  and S. Muthukrishnan. Relative-error CUR matrix

decompositions. SIAM J. Matrix Analysis Applications  30(2):844–881  2008.

[33] Christos Boutsidis  Michael W Mahoney  and Petros Drineas. An improved approximation
algorithm for the column subset selection problem. In Proceedings of the twentieth Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 968–977. Society for Industrial
and Applied Mathematics  https://arxiv.org/pdf/0812.4293  2009.

11

[34] Christos Boutsidis  Petros Drineas  and Malik Magdon-Ismail. Near optimal column-based
matrix reconstruction. In IEEE 52nd Annual Symposium on Foundations of Computer Science
(FOCS)  2011  Palm Springs  CA  USA  October 22-25  2011  pages 305–314. https://arxiv.
org/pdf/1103.0995  2011.

[35] Ahmed K Farahat  Ahmed Elgohary  Ali Ghodsi  and Mohamed S Kamel. Distributed column
subset selection on mapreduce. In 2013 IEEE 13th International Conference on Data Mining
(ICDM)  pages 171–180. IEEE  2013.

[36] Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions. In Proceedings
of the 46th Annual ACM Symposium on Theory of Computing (STOC)  pages 353–362. ACM 
https://arxiv.org/pdf/1405.7910  2014.

[37] Yining Wang and Aarti Singh. Column subset selection with missing data via active sampling.
In The 18th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages
1033–1041  2015.

[38] Zhao Song  David P Woodruff  and Peilin Zhong. Relative error tensor low rank approximation.

In SODA. https://arxiv.org/pdf/1704.08246  2019.

[39] Jiyan Yang  Xiangrui Meng  and Michael W. Mahoney. Quantile regression for large-scale

applications. SIAM J. Scientiﬁc Computing  36(5)  2014.

[40] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery
guarantees for one-hidden-layer neural networks. In ICML. https://arxiv.org/pdf/1706.
03175.pdf  2017.

[41] Peter Bartlett  Dave Helmbold  and Phil Long. Gradient descent with identity initialization
In International Conference on

efﬁciently learns positive deﬁnite linear transformations.
Machine Learning  pages 520–529  2018.

[42] Surbhi Goel  Adam Klivans  and Raghu Meka. Learning one convolutional layer with overlap-

ping patches. In ICML. arXiv preprint arXiv:1802.02547  2018.

[43] Kenneth L Clarkson and David P Woodruff. Sketching for m-estimators: A uniﬁed approach
to robust regression. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA)  pages 921–939. SIAM  2015.

[44] David P. Woodruff and Qin Zhang. Subspace embeddings and (cid:96)p-regression using exponential
random variables. In COLT 2013 - The 26th Annual Conference on Learning Theory  June
12-14  2013  Princeton University  NJ  USA  pages 546–567  2013.

12

,Aditi Raghunathan
Prateek Jain
Ravishankar Krishnawamy
Zhao Song
David Woodruff
Peilin Zhong