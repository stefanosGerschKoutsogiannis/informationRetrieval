2019,Optimal Sparsity-Sensitive Bounds for  Distributed Mean Estimation,We consider the problem of estimating the mean of a set of vectors  which are stored in a distributed system. This is a fundamental task with applications in distributed SGD and many other distributed problems  where communication is a main bottleneck for scaling up computations. We propose a new sparsity-aware algorithm  which improves previous results both theoretically and empirically. The communication cost of our algorithm is characterized by Hoyer's measure of sparseness.  Moreover  we prove that the communication cost of our algorithm is information-theoretic optimal up to a constant factor in all sparseness regime. We have also conducted experimental studies  which demonstrate the advantages of our method and confirm our theoretical findings.,Optimal Sparsity-Sensitive Bounds for Distributed

Mean Estimation

Zengfeng Huang

School of Data Science

Fudan University

huangzf@fudan.edu.cn

Yilei Wang

Department of CSE

HKUST

ywanggq@cse.ust.hk

Ziyue Huang

Department of CSE

HKUST

zhuangbq@cse.ust.hk

Department of CSE

Ke Yi

HKUST

yike@cse.ust.hk

Abstract

We consider the problem of estimating the mean of a set of vectors  which are
stored in a distributed system. This is a fundamental task with applications in
distributed SGD and many other distributed problems  where communication is a
main bottleneck for scaling up computations. We propose a new sparsity-aware
algorithm  which improves previous results both theoretically and empirically.
The communication cost of our algorithm is characterized by Hoyer’s measure of
sparseness. Moreover  we prove that the communication cost of our algorithm is
information-theoretic optimal up to a constant factor in all sparseness regime. We
have also conducted experimental studies  which demonstrate the advantages of
our method and conﬁrm our theoretical ﬁndings.

1

Introduction

(cid:80)n

Consider a distributed system with n nodes  called clients  each of which holds a d-dimensional
vector Xi ∈ Rd. The goal of distributed mean estimation (DME) is to estimate the mean of these
vectors  i.e.  X := 1
i=1 Xi  subject to a constraint on the communication cost (i.e. the total
n
number of bits transmitted by all clients).
DME is a fundamental task in distributed machine learning and optimization problems [3  10  18 
14  12]. For example  gradient aggregation in distributed stochastic gradient decent (SGD) is a form
of DME. In the standard synchronous implementation  in each round  clients evaluate their local
gradients with respect to local mini-batches and communicate them to a central server; the server then
computes the mean of all these gradients  which is used to update the model parameters. It is widely
observed that the communication cost of gradient exchange has become a signiﬁcant bottleneck for
scaling up distributed training [5  19  24]. Therefore  communication-efﬁcient gradient aggregation
has received lots of attention recently [1  2  15  23  26]. DME is also a critical subproblem in many
other applications such as the distributed implementation of Lloyd’s algorithm for K-means clustering
[16] and power iteration for computing eigenvectors [21].
However  the communication complexity of this fundamental problem has not been fully understood 
especially when the input is sparse or skew. In this paper  we provide a tight connection between
communication complexity and input sparsity. Speciﬁcally  we propose a new sparsity-aware lossy
compression scheme  which reduces the communication cost both theoretically and empirically.
We also prove that the communication cost of our method is information-theoretic optimal up to a
constant factor in all sparsity regime  under Hoyer’s measure of sparseness [9].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(cid:80)n

1.1 Problem deﬁnition and notation
The problem setting in this paper is the same as in [20]. Each client i holds a private vector Xi ∈ Rd
and transmits messages only to the central sever according to some protocol; at the end  the sever
outputs an estimate for the mean X = 1
i=1 Xi based on all the messages it has received. The
n
communication cost of a protocol is measured by the total number of bits exchanged between clients
and the sever. Let ˆX denote the estimated mean and we wish to minimize the mean square error
(MSE) of the estimate  i.e.  E = E (cid:107) ˆX − X(cid:107)2
2  under a certain communication budget. Note that the
problem considered here is non-stochastic  i.e.  the input vectors are arbitrary or even adversarial.
This is different from distributed statistical estimation [27  8  4]  where the inputs are i.i.d samples
from some distribution and the goal is to estimate the parameters of the underlying distribution. In
particular  the expectation in the above deﬁnition of MSE is only over the randomness of the protocol.
2 be
the sum of squared (cid:96)2-norms of the input vectors and F0 be the total number of non-zero entries in
all the input vectors. We will always use d to denote the dimensionality of input vectors and n for the
number of clients.

i=1 (cid:107)Xi(cid:107)1  i.e.  the sum of (cid:96)1-norms of input vectors; let F2 :=(cid:80)n

We deﬁne F1 :=(cid:80)n

i=1 (cid:107)Xi(cid:107)2

1.2 Previous results

Naively sending all vectors to the sever needs ndr bits of communication  where r is the number
of bits to represent a ﬂoating point number. In [20]  several methods to save communication are
proposed. The best of them uses O(nd) bits of communication while achieving an MSE of F2/n2.
Their algorithm ﬁrst applies stochastic quantization and then encodes the quantized vectors by entropy
encoding schemes such as arithmetic coding. Moreover  it is also proved that  in the worst case  this
cost is optimal for one-round protocols. Similar bounds are also obtained in [2  11]. One major
limitation of the methods in [20] is that they cannot exploit the sparseness in the inputs due to the
nature of their quantization and encoding methods. In many distributed learning scenarios  the input
vectors can be very sparse or skew  i.e.  a large fraction of the entries can be zero or close to zero.
The sparsity can be caused by either data unbalance (large entries occur in a few clients) or feature
unbalance (large entries occur in a few dimensions). QSGD of [2] works well in practice for sparse
data  but doesn’t have an upper bound on the cost that is parameterized by input sparsity: to achieve
an MSE of F2/n2  the cost is still O(nd) bits (Theorem 3.2  Corollary 3.3 in [2]).
Intuitively  one could drop entries with small absolute values without affecting the MSE too much.
Gradient sparsiﬁcation utilizes this idea  which has been successfully applied in distributed gradient
compression [19  1  15  22  23]. However  such methods either do not have optimal sparsity-sensitive
theoretical guarantees or work only under strong sparsity assumptions.
There are various sparsity notions  but it is currently not clear which notion best characterizes the
inherent complexity of DME. To get meaningful sparsity-sensitive bounds  it is essential to identify
an appropriate sparsity notion for DME. In this paper  we propose to use a modiﬁed notion of Hoyer’s
to measure the sparseness of vectors [9]. For a d-dimensional vector X  its sparseness is deﬁned as
(cid:107)X(cid:107)2
.1 Since our inputs can be viewed as an nd-dimensional vector  the global sparseness is deﬁned
d(cid:107)X(cid:107)2
nd ≤ s ≤ 1; s = 1 (densest) iff all entries are non-zero and have equal
as s := F 2
absolute values  and s = 1
nd (sparsest) iff the input contains a single non-zero entry. Wangni et al.
[23] obtain a sparsity-aware bound based on a different sparsity notion  but our result implies theirs
and can be much better for some inputs (see the supplementary for details).
1.3 Our contributions
First  we propose a sparsity-sensitive compression method that provably exploits the sparseness
of the input. Speciﬁcally  to achieve an MSE of E ≤ F2
n2   our protocol only needs to transmit

s + 1(cid:1) bits (ignoring some lower order terms)  where s is the sparseness of the input
s + 1(cid:1) ≤ 1 when s ≤ 1  this is always no worse than nd (the cost of

C ≈ nds log(cid:0) 1
deﬁned earlier. Since s log(cid:0) 1

[20]) and can be much smaller on sparse inputs  i.e.  when s (cid:28) 1.
Secondly  we prove that  for any sparseness s ≤ 1  the communication cost of our protocol is
optimal  up to a constant factor. Speciﬁcally  for any s ≤ 1  we construct a family of inputs with

1 /ndF2. Note that 1

1

2

1The original Hoyer’s measure is the ratio between the (cid:96)1 and (cid:96)2 norm  normalized to the range [0  1].

2

n2 on this family must incur
sparseness equal to s  and prove that any protocol achieving an MSE of F2
s ) bits of communication in expectation for some inputs in this family. This lower bound
Ω(nds log 1
holds for multi-round protocols in the broadcasting model (where each message can be seen by all
clients). As observed in [20]  any lower bound for distributed statistical mean estimation can be
translated to a lower bound for the DME problem. However  current lower bounds in this area do not
sufﬁce to obtain tight sparsity-sensitive bounds for DME.
Finally  we complement our theoretical ﬁndings with experimental studies. Empirical results show
that  under the same communication bandwidth  our proposed method has a much lower MSE 
especially on sparse inputs  which veriﬁes our theoretical analyses. Moreover  as a subroutine  our
protocol outperforms previous approaches consistently in various distributed learning tasks  e.g. 
Lloyd’s algorithm for K-means clustering and power iteration.
2 Sparsity-Sensitive DME Protocol
Overview of our techniques. Algorithms in [20] apply k-level stochastic quantization and then
encode the quantized vectors using variable length coding. Speciﬁcally  for each Xi  the client divides
] into k− 1 intervals of equal length  and then identiﬁes the interval containing
the range [X min
each Xij and rounds it either to the left point or the right point of the corresponding interval with
probability depending on its distance to the end points. After quantization  the vector can be viewed
as a string of length d over an alphabet of size k  which is then compressed using arithmetic coding.
QSGD is similar  but encodes signs separately and uses the Elias coding method.
Since the sparseness depends on the (cid:96)1 norm F1  our quantization step size depends on F1 as
in Wang et al. [22]. In addition to F1 quantization  our protocol has the following algorithmic
ingredients. 1) All clients in our protocol use the same interval size in stochastic quantization. This
means that the number of levels may vary for different clients  as opposed to all previous methods 
where all clients use a ﬁxed number of levels. This is another major difference in our quantization
step  which is necessary to get communication bounds in terms of global sparseness. 2) As in
QSGD  we encode the sign of each entry separately and only quantize the absolute values  which
can be conveniently implemented by a scaling and rounding procedure. 3) Instead of encoding the
quantized vectors directly with entropy coding methods  we ﬁrst convert each integer vector into
a bit string using a one-to-one map: for any integer vector v = (v1  v2 ···   vd)  the length of its
corresponding bit string is d +(cid:107)v(cid:107)1 − 1  among which the number of 1’s is exactly d− 1. 4) We then
apply efﬁcient coding methods  e.g.  arithmetic coding  to encode the entire bit string using roughly

  X max

i

i

log(cid:0)d+(cid:107)v(cid:107)1

d

(cid:1) ≈ (cid:107)v(cid:107)1 log d+(cid:107)v(cid:107)1

(cid:107)v(cid:107)1

bits.

Scaling and Rounding. We ﬁrst introduce the scaling and rounding procedure (Algorithm 1) 
which is essentially equivalent to stochastic quantization (for the absolute values only). The next
Lemma summarizes the key properties of SaR  the proof of which is in the supplementary.

Algorithm 1 Scaling and Rounding (SaR)
input v ∈ Rd and a scaling factor F
1: u = 1
2: Randomized rounding: for j = 1 ···   d

F · v

ˆuj =

3: return ˆu

(cid:26)(cid:98)uj(cid:99) + 1  with probability uj − (cid:98)uj(cid:99)

(cid:98)uj(cid:99)  

otherwise.

Lemma 2.1. Let ˆv = F ˆu  then E[ˆv] = v and E[(cid:107)ˆv − v(cid:107)2

Let ˆui be the output for Xi and ˆXi = F ˆui. At the end  the server uses ˆX =(cid:80)n

In our protocol  we apply Algorithm 1 on each Xi with F = F1/C  where C is a tunable parameter.
ˆXi/n as the

2] ≤ F(cid:107)v(cid:107)1. Moreover  E[|ˆvi|] = |vi|.

i=1

estimate for the mean  then by Lemma 2.1  the MSE is

E = E (cid:107) ˆX − X(cid:107)2

2 =

1
n2

(cid:107)Xi(cid:107)1 =

F 2
1
Cn2 .

(1)

n(cid:88)

i=1

E[(cid:107) ˆXi − Xi(cid:107)2

2] ≤ F
n2

3

n(cid:88)

i=1

w

w

and decoding time O(d).

Constant-weight binary sequence. The Hamming weight of a length-d binary sequence v is
denoted by w(v) = |{vi : vi = 1}|. Constant-weight binary codes C(d  w) is the set of all length-d

(cid:1)  the number of bits to represent a sequence in C is at least
(cid:1)(cid:101). There exists efﬁcient encoding methods  such as arithmetic coding or its variants [17]  to
(cid:1)(cid:101)  which has encoding

sequences with weight w. Since |C| =(cid:0) d
(cid:100)log(cid:0) d
encode sequences in C using binary strings of length very close to (cid:100)log(cid:0) d
negative integer vector v by w(v) =(cid:80)d

Constant-weight non-negative integer vector coding. Denote the weight of a length-d non-
i=1 vi. Constant-weight integer codes I(d  w) is the set of
length-d non-negative integer vectors with weight w. In our protocol  we map each v ∈ I(d  w) to a
binary sequence f (v) ∈ C(d + w − 1  d − 1) as follows. For i = 1  2 ···   d − 1 we write vi ‘0’s
and one ‘1’  and in the end we write vd ‘0’s. It is an (d + w − 1  d − 1) constant-weight binary code.
One can also verify that f is a one-to-one and onto map from I(d  w) to C(d + w − 1  d − 1). By
applying encoding methods for C mentioned above  we have the following lemma.

Lemma 2.2. Sequences in I(d  w) can be encoded losslessly by (cid:100)log(cid:0)d+w−1

(cid:1)(cid:101)-bit binary strings 

w

d−1

with encoding and decoding time O(d + w).

2.1 The Protocol

d−1

F1 =(cid:80)
(cid:100)log(cid:0)d+wi−1

(cid:1)(cid:101)-bit string (Lemma 2.2) and sends it to the sever. The client also sends the value

We are now ready to describe our sparsity-sensitive DME protocol.
1. (Initialization) Clients and the server determine the scaling factor F to be used in Algorithm 1 and
we will use F = F1/C for some C ≤ nd. To compute F1  each client i sends (cid:107)Xi(cid:107)1 to the server
using r bits  where r is the number of bits to represent ﬂoating points. Then the server computes
i (cid:107)Xi(cid:107)1 and broadcasts it to all the clients. This step use 2r bits of communication per client.
2. (Quantization) Client i runs SaR(Xi  F1/C) (Algorithm 1) and get an integer vector ˆui. The
absolute value and sign of each entry in ˆui will be encoded separately. Let vi = (|ˆui1| ···  |ˆuid|).
(Encoding) Note that vi ∈ I(d  wi)  where wi = w(vi). Client i encodes vi using a
3.
∆wi = wi − (cid:98)C(cid:107)Xi(cid:107)1/F1(cid:99) with log(2d + 1) bits 2  as wi is needed for decoding vi. 3
4. (Sending the signs) Let si be a binary sequence indicating the signs of non-zero entries in ˆui.
Client i simply sends this sequence with di bits of communication  where di is the number of non-zero
values in vi. Moreover  we can apply constant-weight coding to compress this sequence.
5. (Decoding) The server decodes vi  which contains the absolute values of ˆui. Given the signs of
its the non-zero entries si  the server is now able to recover ˆui losslessly. It ﬁnally computes the
estimated mean ˆX = 1
n
The correctness of the protocol readily follows from Lemma 2.1 and (1): E[ ˆX] = X and E[(cid:107) ˆX −
X(cid:107)2
Cn2 . Below we analyze its communication cost. By part 2 of Lemma 2.1  we have

2] ≤ F 2

ˆXi = F1
Cn

(cid:80)

i ˆui.

i

= C.
Because of the observation di ≤ wi  we have the expected total communication cost is at most

F1

F1

F1

. Therefore  E[(cid:80)n

E[wi] =(cid:80)

C|Xij|

= C(cid:107)Xi(cid:107)1

C(cid:107)Xi(cid:107)1

1

(cid:80)
j E[|ˆuij|] =(cid:80)
(cid:34) n(cid:88)
(cid:34) n(cid:88)

i=1

E

j

(cid:54) E

i=1

(cid:18)

i=1 wi] =(cid:80)n
(cid:19)(cid:35)

i=1

(cid:18)d + wi − 1
(cid:19)

+ di

d − 1

2r + log(2d + 1) + log

(cid:19)(cid:35)

(cid:18) d

wi

wi log

+ 1

+ O(C + nr + n log d).

2Clearly  ∆wi is an integer and |∆wi| (cid:54) d. One can also use universal code such as Elias gamma code [7]

3One can also use entropy coding to encode vi  but it is unclear whether such methods achieve the same

to reduce the bits of transmitting ∆wi.

theoretical guarantee as ours.

4

i=1

i=1

+ 1

(cid:34)

E

wi log

+ 1

wi

(cid:54) E

(

(cid:19)(cid:35)

(cid:19)(cid:35)

n(cid:88)

(cid:18) d

(cid:18) nd(cid:80)n

From the concavity of the function x log( 1

x + 1) on R>0 and Jensen’s inequality  we have

(cid:34) n(cid:88)
C + 1(cid:1) + O(C + nr + n log d) bits of communication  where r is the number of bits to
C log(cid:0) nd
s + 1(cid:1) +

i=1 wi
Therefore  we get the following theorem  and by setting C = F 2
Theorem 2.3. For any C ≤ nd  there exists a DME protocol that achieves an MSE of F 2

represent a ﬂoating point.
Corollary 2.4. There exists a DME protocol that achieves an MSE of F2
O(nds + nr + n log d) bits  where s = F 2

n2 using nds · log(cid:0) 1

1 /ndF2 is the Hoyer’s measure of sparseness of the inputs.

1 /F2  the next corollary follows.

(cid:18) nd

Cn2 with

(cid:54) C log

wi) log

+ 1

.

C

(cid:19)

1

Remark. The authors of [20] discuss how to use client or coordinate sampling to obtain a trade-off
between MSE and communication. Their analysis shows that  to achieve an MSE of F2/pn2  the
communication cost is O(pnd) bits (ignoring low order terms)  where 0 ≤ p ≤ 1 is the sampling
probability. Applying our sparsity-sensitive protocol on the sampled clients or coordinates  we can
achieve the same MSE with a communication of O(pnds log( 1
s + 1)) bits  which will never be worse
and can be much better on inputs with small spareness s.
We also would like to point out that  our algorithm can also be run without the synchronization round.
For this setting  we can derive a communication bound for each client by simply setting n = 1 in
Corollary 2.4  although s in the bound will become the local sparsity of the client when doing so.
Local sparsity bound is worse than global sparsity when there is data unbalance  but the bound is still
better than prior work as long as there is dimension unbalance across different clients. This is also
veriﬁed in our experimental results below.
Since the sparseness depends on the (cid:96)1 norm F1  the key to getting a sparsity-sensitive bound is to
understand the connection between F1 and the MSE-communication trade-off. So our quantization
step size depends on F1  which is one of the main differences in the quantization step compared with
[20  2  23]. Wang et al. [22] also use F1 quantization  but only consider 1-level quantization and
doesn’t specify an appropriate encoding method to achieve an optimal sparsity-sensitive bound. Our
1 /n2C  where C ≤ nd
is a tunable parameter; and if we set C = F 2
1 /F2  the MSE and communication cost are F2/n2 and
nds log(1/s + 1) respectively  as claimed earlier. Having C as a tunable parameter gives us a better
control on the cost of the protocol; our result in fact implies the MSE-communication trade-off of
[22] (and could be much better) but not vice versa. Wang et al. [22] prove that their algorithm can
compress a vector X ∈ Rd using kr bits (where r is the number of bits to represents ﬂoating points
1 /k; ours algorithm (the special case when n = 1) can
and k is a tuning parameter) with MSE F 2
compress X using C log( d
1 /C. By setting C = k  we achieve the
same MSE while the cost is k log(d/k) bits (and it is trivial to make it be k min(log(d/k)  r)). When
k = Θ(d)  the cost is O(k) bits versus O(kr).

C + 1(cid:1) bits of communication to achieve an MSE of F 2

protocol uses C log(cid:0) nd

C + 1) bits with MSE at least F 2

3 Lower Bound

In this section  we show the optimality of Theorem 2.3 by proving the following lower bound.
Theorem 3.1. For any n ≤ C ≤ nd
2   there exists a family of inputs  all of which have F1 = F2 = C 
such that any randomized protocol solving the DME problem on this family in the broadcast model
with an MSE of F 2

4n2C must communicate at least C

2 log nd

2C bits.

1

This theorem immediately leads to the following corollary  which means that our sparsity-sensitive
protocol is optimal (up to a constant factor) for all sparseness 1
Corollary 3.2. For any sparseness 1
2   there exists a family of inputs  all of which have
sparseness s  such that any randomized protocol solving the DME problem on this family in the
broadcast model with an MSE of F2

4n2 must communicate at least of nds

d ≤ s ≤ 1
2.

d ≤ s ≤ 1

2 log( 1

2s ) bits.

Proof. Note that on the family of inputs used in the proof of Theorem 3.1 (presented shortly)  we
have s = F 2
2   we obtain a
ndF2

n2 . Since this family exists for any n ≤ C ≤ nd

nd and E = F 2

4n2C = F2

= C

1

1

5

d ≤ s ≤ 1
family with sparseness s for any 1
can be rewritten as claimed in the corollary.

2. Then  the MSE and communication cost in Theorem 3.1

The rest of this section is devoted to the proof of Theorem 3.1. To prove lower bounds for
randomized protocols  the standard tool is Yao’s Minimax Principle [25]. We will deﬁne an
input distribution D for DME. Suppose there is a randomized algorithm AR with worst case
(for any possible input) MSE M and expected cost T   where R is the randomness used in
the algorithm. Now  if we sample input X ∼ D  then EREX∼D[MSE of AR(X)] ≤ M and
EREX∼D[Cost of AR(X)] ≤ T . By Markov’s inequality  PrR [EX∼D[ MSE of AR(X)] ≤ 4M ] ≥
0.75 and PrR [EX∼D[Cost of AR(X)] ≤ 2T ] ≥ 0.5. Then  with positive probability  the two
events happen simultaneously. In other words  there exists some ﬁxed randomness r such that
EX∼D[ MSE of Ar(X)] ≤ 4M and EX∼D[Cost of Ar(X)] ≤ 2T where Ar is now simply a deter-
ministic algorithm. That means if there is a randomized algorithm with worst case MSE M and
expected cost T   then there exist a deterministic algorithm with MSE 4M and expected cost 2T w.r.t.
any input distribution D.

C ) bits.

Minimax Principle. From the above argument  it is sufﬁcient to prove that  for some input dis-
tribution D  any deterministic protocol with MSE at most Θ(F 2
1 /n2C) must incur an expected
communication cost of Ω(C log nd
Input distribution. For any ﬁxed n ≤ C ≤ nd/2  we deﬁne the hard distribution D for our
problem as follows. Each Xi is divided into t = C/n blocks  each of size b = nd/C. In this
section  we use xij ∈ Rb to denote the jth block in Xi. In D  each block xij is uniformly sampled
from b-dimensional standard basis vectors  i.e. Pr[xij = ek] = 1/b for each 1 ≤ k ≤ b  and the
distribution of xij are independent across all i and j. Note that any input sampled from D has (cid:96)1
norm exactly C.
Let Π be any deterministic protocol with MSE bounded by F 2
1 /4n2C = C/4n2 w.r.t. the input
distribution D. We next prove a lower bound of the expected communication cost of Π w.r.t. D. Let
X1  X2 ···   Xn be a random input sampled from D and Π(X1 ···   Xn) be the transcript of the
protocol given the input  i.e.  the concatenation of all messages  which is a random variable. When
there is no confusion  we will omit the input and use Π to denote the random transcript; and π ∼ Π
means π is chosen according to the distribution of Π(X1 ···   Xn) .
Since the protocol is deterministic  any particular transcript π corresponds to a deterministic set of
inputs Rπ  i.e.  all inputs in Rπ generate the same transcript π under the protocol Π. Hence  all inputs
in Rπ share the same output  denoted as Y π. Note each input belongs to a unique Rπ  and thus all
Rπ corresponds to a partition of all possible inputs. It is well-known Rπ is a combinatorial rectangle 
i.e.  Rπ = B1 × ··· × Bn  where each Bi ⊆ {0  1}d is some subset of all possible inputs of the ith
client.
Deﬁnition 1. Deﬁne Dπ as the conditional distribution of X1  X2 ···   Xn (sampled from D) condi-
tioned on the event [X1  X2 ···   Xn] ∈ Rπ.
Let X = 1
i=1 Xi. By the property of conditional expectation  we have the following Lemma.
n
Lemma 3.3. We assume Π has an MSE of C
4n2 .
Deﬁnition 2. Suppose [X1 ···   Xn] ∼ Dπ  then for every i and j  the distribution of xij is still a
ijk = PrDπ [xij = ek] for
distribution over b-dimensional basis vectors. For each i  j  we deﬁne pπ

(cid:2)(cid:107)X − Y π(cid:107)2(cid:3) ≤ C

4n2   then Eπ∼ΠE[X1 ···  Xn]∼Dπ

(cid:80)n

k ∈ [b]  where(cid:80)b

k=1 pπ

ijk = 1.

The next lemma is crucial to our argument  the proof of which can be found in the supplementary.
Lemma 3.4. For any π and let Y π be its output  we have

t(cid:88)

b(cid:88)

n(cid:88)

ijk(1 − pπ
[pπ

ijk)] ≤ n2 · E[X1 ···  Xn]∼Dπ

(cid:2)(cid:107)X − Y π(cid:107)2(cid:3) .

j=1

k=1

i=1

Here we introduce some basic notations from information theory [6]. For any random variable
X  H(X) is the standard Shannon Entropy of X. For any random variables X  Y  Z  we use

6

H(X|Y ) = EY [H(X|Y = y)] to denote the conditional entropy of X given Y   and I(X; Y |Z) =
H(X|Z)− H(X|Y  Z) to denote the conditional mutual information between X and Y given Z. We
know the average encoding length of a random transcript Π  i.e.  the expected communication cost  is
lower bounded by its entropy H(Π). By the non-negativity of (conditional) entropy  we have

H(Π) = I(X1 ···   Xn; Π) + H(Π|X1 ···   Xn) ≥ I(X1 ···   Xn; Π).

(2)

Next we prove a lower bound on I(X1 ···   Xn; Π). We will need the following property.
Lemma 3.5. Let X  Y  Z be three random variables such that X and Y are independent  then
I(X  Y ; Z) ≥ I(X; Z) + I(Y ; Z).
Lemma 3.6. I(X1 ···   Xn; Π) ≥ C

2 log nd
2C .

Proof. Since the input distribution is independent across different blocks and clients  by Lemma 3.5 

we have I(X1 ···   Xn; Π) ≥(cid:80)t

(cid:80)n
I(X1 ···   Xn; Π) ≥ t(cid:88)
n(cid:88)

j=1

i=1 I(Xij; Π). Thus 

H(Xij) − t(cid:88)
n(cid:88)
 t(cid:88)
n(cid:88)

− Eπ∼Π

j=1

i=1

nd
C

j=1

i=1

j=1

i=1

= C log

H(Xij | Π)

  

H(Xij | Π = π)

(3)

ij = min(pπ

ij  1 − pπ

ij) (see Deﬁnition 2)  then
≥ (1 − qπ
  which

ij) log

1−qπ

1

ij

ij log 1
qπ
ij

pπ
ijk log

qπ
ijk log

qπ
ijk) log

1
pπ
ijk

(cid:33)
(cid:33)
(cid:80)n
(cid:80)t
Eπ∼Π[(cid:80)t

1
qπ
ijk

j=1

qπ
ijk] log

i=1

i=1

j=1

j=1

k=1

. So 

Eπ∼Π

≥ pπ

n(cid:88)

ij log 1
qπ
ij

ij log 1
pπ
ij

≤ Eπ∼Π

C . Let qπ

 t(cid:88)

H(Xij | Π = π)

 = Eπ∼Π

where we use H(Xij) = log b = log nd
ij ≤ 0.5. It can be veriﬁed by elementary calculus that qπ
qπ
 t(cid:88)
(cid:32)
implies that qπ
b(cid:88)
n(cid:88)
 t(cid:88)
(cid:32)
b(cid:88)
n(cid:88)
(
t(cid:88)
b(cid:88)
n(cid:88)
n(cid:88)
t(cid:88)
b(cid:88)
 t(cid:88)
 ≤ 2Eπ∼Π
 t(cid:88)

ijk ≤ 0.5 and by Lemma 3.3 and 3.4  we have
n(cid:88)
n(cid:88)

 t(cid:88)

≤ Eπ∼Π[

≤ Eπ∼Π

n(cid:88)

b(cid:88)

= 2Eπ∼Π

Eπ∼Π

qπ
ijk

k=1

k=1

k=1

j=1

j=1

j=1

j=1

i=1

i=1

i=1

i=1

b(cid:88)
b(cid:88)

k=1

ijk(1 − qπ
[qπ

ijk)]

ijk(1 − pπ
[pπ

ijk)]

k=1 qπ
ijk]
where the last two inequalities is from Jensen’s inequality (since x log(1/x) is concave on R>0).
Since each qπ

k=1

j=1

j=1

i=1



k=1 qπ
ijk

(cid:80)b

i=1

i=1

nd

nd

(cid:80)b
(cid:80)n

 ≤ C

.

2

ln 2 . Thus g(x) attains its maximum at x = nd

j=1

i=1

k=1

x   which is concave on R>0 and its derivative is g(cid:48)(x) =
21/ ln 2 . Moreover  g(x) is monoton-
21/ ln 2   we have

21/ ln 2 . Since we assume C

2 ≤ nd

4 <

nd

nd

x − 1

(cid:104)(cid:80)t

Consider the function g(x) = x log nd
log nd
(cid:80)n
ically increasing for 0 < x ≤
i=1 H(Xij | Π = π)
Eπ∼Π
I(X1 ···   Xn; Π) ≥ C log
This ﬁnishes the proof of the Lemma.

j=1

nd
C

(cid:105) ≤ C

2 log 2nd
− C
2

C . By (3)  we prove
nd
C

2nd
C

C
2

log

log

=

− C
2

=

C
2

log

nd
2C

.

7

(a) sparseness = 0.60

(b) sparseness = 0.36

(c) sparseness = 0.15

(d) sparseness = 0.06

Figure 1: Communication-MSE trade-off on the synthetic dataset generated from t-distribution. The
x-axis is the average number of bits sent for each dimension  and the y-axis is log(MSE).

By (2)  we prove that the expected communication cost of Π is at least C
Theorem 3.1 follows from the minimax principle.

2 log nd

2C bits w.r.t. D. Then

4 Experiments

We have conducted experiments comparing our DME protocol with the variable length coding method
(the best in [20]) and the methods in [2  23] on their MSE-communication trade-off  as well as the
performance in distributed learning tasks that use DME as a subroutine  including K-means clustering
and power iteration. The algorithm in [22] doesn’t specify an appropriate encoding method and
directly sends ﬂoating points  and thus the cost is worse than that of [2  23].

4.1 DME

In the ﬁrst set of experiments  we compare our new protocol with that of [2  23  20] on the DME
problem directly  in terms of the MSE-communication trade-off. To see how the performances of the
protocols are affected by the sparseness of the input  we generated synthetic datasets with varying
spareness. Speciﬁcally  we generated 16 vectors  each held by a different client. Each vector has
10000 dimensions  whose values are generated independently from student’s t-distribution. This data
set has an empirical sparseness of 0.60  and the results are shown in Figure 1(a).
We used two ways to create sparser data. First  we scaled up the data on each nodes by a different
factor  which is also generated from t-distribution. This resulted in a data set with sparseness 0.36 
and the experimental results are shown in Figure 1(b)  which conﬁrms the effectiveness of using a
global quantization step size when data is unbalance across clients. Second  we randomly chose 30%
and 10% of the dimensions and set the rest to 0. This resulted in two data sets with sparseness 0.15
and 0.06  respectively  and the experimental results are shown in Figure 1(c) and Figure 1(d). These
results render that the sparser and/or less balance (across clients) the data is  the higher performance
gain our new protocol has. The same phenomenon is also observed in the next two tasks.
In Figure 1 (a)(c)(d)  the data sets used do not have data unbalance across different clients (meaning
the coordination round is effectively useless)  and the results are still better than previous methods.

4.2 Distributed K-Means

We then test the performances for distributed K-means. In each iteration of the distributed K-means
algorithm  the server broadcasts the current centroids of the clusters to all clients. Each client updates
the centroids based on its local data  and then sends back the updated centroids to the server. The
server then computes the average of these centroids for each cluster. This is exactly K instances of
the DME problem  except that average should be weighted by the cluster size at each client. Thus 
we ﬁrst scale up the centroids by the cluster size  and then apply the DME protocols.
We used the MNIST [13] data set  uniformly or non-uniformly distributed across 10 clients. The
number of clusters and iterations is set to 10 and 30 respectively. The results are shown in Figure 2 
where we used different values of k (quantization level) for Suresh et al.’s algorithm  k = 32 for less
communication and k = 512 for less error  and other methods are tuned to achieve the same objective.
The results show that with the same ﬁnal objective  our algorithm has less communication cost.

8

246810bits per dimension6420246810log(MSE)Alistarh et al.Suresh et al.Wangni et al.Ours246810bits per dimension7.55.02.50.02.55.07.510.0log(MSE)Alistarh et al.Suresh et al.Wangni et al.Ours246810bits per non-zero entry5.02.50.02.55.07.510.0log(MSE)Alistarh et al.Suresh et al.Wangni et al.Ours2.55.07.510.012.515.017.5bits per non-zero entry10.07.55.02.50.02.55.07.510.0log(MSE)Alistarh et al.Suresh et al.Wangni et al.Ours(a) uniform

(b) uniform

(c) non-uniform

(d) non-uniform

Figure 2: Distributed K-Means on MNIST dataset distributed among 10 workers. The x-axis is the
average number of bits sent for each dimension  accumulated over the iterations  and the y-axis is the
objective function value of K-Means. In (a) and (b) data is uniform distributed  while in (c) and (d)
data is non-uniform distributed  every worker has 1000  4000  7000  10000 or 13000 images.

(a) uniform

(b) uniform

(c) non-uniform

(d) non-uniform

Figure 3: Distributed power iteration on MNIST dataset distributed among 100 workers. The x-axis is
the averaged number of bits sent for each dimension  which scales linearly to the number of iterations 
and the y-axis is the (cid:96)2 distance between the current estimate of eigenvector and the ground-truth
eigenvector. In (a) and (b) data is uniform distributed  while in (c) and (d) data is non-uniform
distributed  every worker has 100  400  700  1000 or 1300 images.

4.3 Distributed Power Iteration

The second learning task we tested is the distributed power iteration algorithm. The number of clients
is set to 100 and the number of iterations is set to 15. In this algorithm  the server broadcasts the
current estimate of the eigenvector to all clients  then each client updates the eigenvector based on
one power iteration on its local data  and sends back the compressed eigenvector to the server. The
server updates the current estimate of eigenvector with the average of all the received eigenvectors.
The results on the MNIST data set are reported in Figure 3  where we used different values of k
(quantization level) for Suresh et al.’s algorithm  k = 32 for less communication and k = 512 for less
error  and other methods are tuned to achieve the same error. It also shows that our DME protocol
uses less communication to achieve the same error.

Acknowledgments

Zengfeng Huang is partially supported by National Natural Science Foundation of China (Grant
No. 61802069)  Shanghai Sailing Program (Grant No. 18YF1401200) and Shanghai Science and
Technology Commission (Grant No. 17JC1420200). Ziyue Huang  Yilei Wang  and Ke Yi are
supported by HKRGC under grants 16200415  16202317  and 16201318.

References
[1] A. F. Aji and K. Heaﬁeld. Sparse communication for distributed gradient descent. In Proceedings
of the 2017 Conference on Empirical Methods in Natural Language Processing  pages 440–445 
2017.

[2] D. Alistarh  D. Grubic  J. Li  R. Tomioka  and M. Vojnovic. Qsgd: Communication-efﬁcient
sgd via gradient quantization and encoding. In Advances in Neural Information Processing
Systems  pages 1709–1720  2017.

9

01020304050Total bits per dimension19.820.020.220.420.620.821.0Objectivecomm:50.9 bits  obj:19.89  Alistarh et al.comm:52.8 bits  obj:19.88  Suresh et al.comm:184.3 bits  obj:19.88  Wangni et al.comm:46.8 bits  obj:19.88  Ours020406080100120140Total bits per dimension19.820.020.220.420.620.821.0Objectivecomm:168.2 bits  obj:19.72  Alistarh et al.comm:162.4 bits  obj:19.72  Suresh et al.comm:398.2 bits  obj:19.72  Wangni et al.comm:128.6 bits  obj:19.72  Ours0510152025303540Total bits per dimension19.820.020.220.420.620.821.0Objectivecomm:48.7 bits  obj:19.88  Alistarh et al.comm:49.9 bits  obj:19.88  Suresh et al.comm:174.5 bits  obj:19.88  Wangni et al.comm:36.4 bits  obj:19.88  Ours01020304050Total bits per dimension19.820.020.220.420.620.821.0Objectivecomm:62.1 bits  obj:19.72  Alistarh et al.comm:62.5 bits  obj:19.72  Suresh et al.comm:224.9 bits  obj:19.72  Wangni et al.comm:45.9 bits  obj:19.72  Ours0510152025Total bits per dimension0.00.10.20.30.40.50.6Errorcomm:24.2 bits  err:0.040  Alistarh et al.comm:34.7 bits  err:0.040  Suresh et al.comm:77.4 bits  err:0.041  Wangni et al.comm:23.4 bits  err:0.040  Ours01020304050607080Total bits per dimension0.00.10.20.30.40.50.6Errorcomm:88.7 bits  err:0.002  Alistarh et al.comm:94.4 bits  err:0.002  Suresh et al.comm:230.1 bits  err:0.002  Wangni et al.comm:74.8 bits  err:0.002  Ours0510152025Total bits per dimension0.00.10.20.30.40.50.6Errorcomm:26.6 bits  err:0.040  Alistarh et al.comm:36.5 bits  err:0.040  Suresh et al.comm:93.1 bits  err:0.040  Wangni et al.comm:20.6 bits  err:0.040  Ours01020304050607080Total bits per dimension0.00.10.20.30.40.50.6Errorcomm:94.4 bits  err:0.002  Alistarh et al.comm:102.0 bits  err:0.002  Suresh et al.comm:238.8 bits  err:0.002  Wangni et al.comm:69.6 bits  err:0.002  Ours[3] S. Boyd  N. Parikh  E. Chu  B. Peleato  J. Eckstein  et al. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in
Machine learning  3(1):1–122  2011.

[4] M. Braverman  A. Garg  T. Ma  H. L. Nguyen  and D. P. Woodruff. Communication lower
bounds for statistical estimation problems via a distributed data processing inequality.
In
Proceedings of the forty-eighth annual ACM symposium on Theory of Computing  pages 1011–
1020. ACM  2016.

[5] T. M. Chilimbi  Y. Suzue  J. Apacible  and K. Kalyanaraman. Project adam: Building an
efﬁcient and scalable deep learning training system. In OSDI  volume 14  pages 571–582  2014.

[6] T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons  2006.

[7] P. Elias. Universal codeword sets and representations of the integers. IEEE transactions on

information theory  21(2):194–203  1975.

[8] A. Garg  T. Ma  and H. Nguyen. On communication cost of distributed statistical estimation
and dimensionality. In Advances in Neural Information Processing Systems  pages 2726–2734 
2014.

[9] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine

learning research  5(Nov):1457–1469  2004.

[10] M. Jaggi  V. Smith  M. Takác  J. Terhorst  S. Krishnan  T. Hofmann  and M. I. Jordan.
Communication-efﬁcient distributed dual coordinate ascent. In Advances in neural information
processing systems  pages 3068–3076  2014.

[11] J. Koneˇcn`y and P. Richtárik. Randomized distributed mean estimation: Accuracy vs communi-

cation. Frontiers in Applied Mathematics and Statistics  4:62  2018.

[12] G. Lan  S. Lee  and Y. Zhou. Communication-efﬁcient algorithms for decentralized and

stochastic optimization. Mathematical Programming  2018.

[13] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[14] J. D. Lee  Q. Lin  T. Ma  and T. Yang. Distributed stochastic variance reduced gradient
methods by sampling extra data with replacement. The Journal of Machine Learning Research 
18(1):4404–4446  2017.

[15] Y. Lin  S. Han  H. Mao  Y. Wang  and B. Dally. Deep gradient compression: Reducing the
communication bandwidth for distributed training. In International Conference on Learning
Representations  2018.

[16] S. Lloyd. Least squares quantization in pcm.

28(2):129–137  1982.

IEEE transactions on information theory 

[17] T. V. Ramabadran. A coding scheme for m-out-of-n codes. IEEE Transactions on Communica-

tions  38(8):1156–1163  1990.

[18] K. Scaman  F. Bach  S. Bubeck  Y. T. Lee  and L. Massoulié. Optimal algorithms for smooth and
strongly convex distributed optimization in networks. In International Conference on Machine
Learning  pages 3027–3036  2017.

[19] N. Strom. Scalable distributed dnn training using commodity gpu cloud computing. In Sixteenth

Annual Conference of the International Speech Communication Association  2015.

[20] A. T. Suresh  F. X. Yu  S. Kumar  and H. B. McMahan. Distributed mean estimation with

limited communication. ICML  2017.

[21] L. N. Trefethen and D. Bau III. Numerical linear algebra  volume 50. Siam  1997.

10

[22] H. Wang  S. Sievert  S. Liu  Z. Charles  D. Papailiopoulos  and S. Wright. Atomo:
Communication-efﬁcient learning via atomic sparsiﬁcation. In Advances in Neural Information
Processing Systems  pages 9850–9861  2018.

[23] J. Wangni  J. Wang  J. Liu  and T. Zhang. Gradient sparsiﬁcation for communication-efﬁcient

distributed optimization. In Advances in Neural Information Processing Systems 31. 2018.

[24] W. Wen  C. Xu  F. Yan  C. Wu  Y. Wang  Y. Chen  and H. Li. Terngrad: Ternary gradients
to reduce communication in distributed deep learning. In Advances in neural information
processing systems  pages 1509–1519  2017.

[25] A. C. Yao. Probabilistic computations: Toward a uniﬁed measure of complexity. In 18th Annual

Symposium on Foundations of Computer Science (FOCS)  pages 222—-227. IEEE  1977.

[26] M. Yu  Z. Lin  K. Narra  S. Li  Y. Li  N. S. Kim  A. Schwing  M. Annavaram  and S. Avestimehr.
Gradiveq: Vector quantization for bandwidth-efﬁcient gradient aggregation in distributed cnn
training. In Advances in Neural Information Processing Systems 31. 2018.

[27] Y. Zhang  J. Duchi  M. I. Jordan  and M. J. Wainwright. Information-theoretic lower bounds
for distributed statistical estimation with communication constraints. In Advances in Neural
Information Processing Systems  pages 2328–2336  2013.

11

,Neil Gallagher
Austin Talbot
Kafui Dzirasa
Lawrence Carin
David Carlson
zengfeng Huang
Ziyue Huang
Yilei WANG
Ke Yi