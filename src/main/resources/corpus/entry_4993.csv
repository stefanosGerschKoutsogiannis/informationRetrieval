2013,Sequential Transfer in Multi-armed Bandit with Finite Set of Models,Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance  most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning  notably in the multi-arm bandit framework  where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks.  We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.,Sequential Transfer in Multi-armed Bandit

with Finite Set of Models

Mohammad Gheshlaghi Azar ⇤
School of Computer Science

CMU

Alessandro Lazaric †

INRIA Lille - Nord Europe

Team SequeL

Emma Brunskill ⇤

School of Computer Science

CMU

Abstract

Learning from prior tasks and transferring that experience to improve future per-
formance is critical for building lifelong learning agents. Although results in su-
pervised and reinforcement learning show that transfer may signiﬁcantly improve
the learning performance  most of the literature on transfer is focused on batch
learning tasks. In this paper we study the problem of sequential transfer in online
learning  notably in the multi–armed bandit framework  where the objective is to
minimize the total regret over a sequence of tasks by transferring knowledge from
prior tasks. We introduce a novel bandit algorithm based on a method-of-moments
approach for estimating the possible tasks and derive regret bounds for it.

1

Introduction

Learning from prior tasks and transferring that experience to improve future performance is a key
aspect of intelligence  and is critical for building lifelong learning agents. Recently  multi-task
and transfer learning received much attention in the supervised and reinforcement learning (RL)
setting with both empirical and theoretical encouraging results (see recent surveys by Pan and Yang 
2010; Lazaric  2011). Most of these works focused on scenarios where the tasks are batch learning
problems  in which a training set is directly provided to the learner. On the other hand  the online
learning setting (Cesa-Bianchi and Lugosi  2006)  where the learner is presented with samples in
a sequential fashion  has been rarely considered (see Mann and Choe (2012); Taylor (2009) for
examples in RL and Sec. E of Azar et al. (2013) for a discussion on related settings).
The multi–armed bandit (MAB) (Robbins  1952) is a simple yet powerful framework formalizing
the online learning with partial feedback problem  which encompasses a large number of applica-
tions  such as clinical trials  web advertisements and adaptive routing. In this paper we take a step
towards understanding and providing formal bounds on transfer in stochastic MABs. We focus on a
sequential transfer scenario where an (online) learner is acting in a series of tasks drawn from a sta-
tionary distribution over a ﬁnite set of MABs. The learning problem  within each task  can be seen
as a standard MAB problem with a ﬁxed number of steps. Prior to learning  the model parameters
of each bandit problem are not known to the learner  nor does it know the distribution probability
over the bandit problems. Also  we assume that the learner is not provided with the identity of the
tasks throughout the learning. To act efﬁciently in this setting  it is crucial to deﬁne a mechanism
for transferring knowledge across tasks. In fact  the learner may encounter the same bandit prob-
lem over and over throughout the learning  and an efﬁcient algorithm should be able to leverage
the knowledge obtained in previous tasks  when it is presented with the same problem again. To
address this problem one can transfer the estimates of all the possible models from prior tasks to
the current one. Once these models are accurately estimated  we show that an extension of the UCB
algorithm (Auer et al.  2002) is able to efﬁciently exploit this prior knowledge and reduce the regret
through tasks (Sec. 3).

⇤{mazar ebrun}@cs.cmu.edu
†alessandro.lazaric@inria.fr

1

The main contributions of this paper are two-fold: (i) we introduce the tUCB algorithm which trans-
fers the model estimates across the tasks and uses this knowledge to achieve a better performance
than UCB. We also prove that the new algorithm is guaranteed to perform as well as UCB in early
episodes  thus avoiding any negative transfer effect  and then to approach the performance of the
ideal case when the models are all known in advance (Sec. 4.4). (ii) To estimate the models we rely
on a new variant of method of moments  robust tensor power method (RTP) (Anandkumar et al. 
2013  2012b) and extend it to the multi-task bandit setting1:we prove that RTP provides a consistent
estimate of the means of all arms (for all models) as long as they are pulled at least three times
per task and prove sample complexity bounds for it (Sec. 4.2). Finally  we report some preliminary
results on synthetic data conﬁrming the theoretical ﬁndings (Sec. 5). An extended version of this
paper containing proofs and additional comments is available in (Azar et al.  2013).
2 Preliminaries
We consider a stochastic MAB problem deﬁned by a set of arms A = {1  . . .   K}  where each arm
i 2A is characterized by a distribution ⌫i and the samples (rewards) observed from each arm are
independent and identically distributed. We focus on the setting where there exists a set of models
⇥= {✓ = (⌫1  . . .  ⌫ K)}  |⇥| = m  which contains all the possible bandit problems. We denote the
mean of an arm i  the best arm  and the best value of a model ✓ 2 ⇥ respectively by µi(✓)  i⇤(✓) 
µ⇤(✓). We deﬁne the arm gap of an arm i for a model ✓ as i(✓) = µ⇤(✓)  µi(✓)  while the model
gap for an arm i between two models ✓ and ✓0 is deﬁned as i(✓  ✓0) = |µi(✓)  µi(✓0)|. We also
assume that arm rewards are bounded in [0  1]. We consider the sequential transfer setting where at
each episode j the learner interacts with a task ¯✓j  drawn from a distribution ⇢ over ⇥  for n steps.
The objective is to minimize the (pseudo-)regret RJ over J episodes measured as the difference
between the rewards obtained by pulling i⇤(¯✓j) and those achieved by the learner:

RJ =

JXj=1

Rj
n =

JXj=1Xi6=i⇤

i ni(¯✓j) 
T j

(1)

i X 2

where T j
i n is the number of pulls to arm i after n steps of episode j. We also introduce some
tensor notation. Let X 2 RK be a random realization of the rewards of all arms from a ran-
dom model. All the realizations are i.i.d. conditional on a model ¯✓ and E[X|✓ = ¯✓] = µ(✓) 
where the i-th component of µ(✓) 2 RK is [µ(✓)]i = µi(✓). Given realizations X 1   X 2 and
X 3  we deﬁne the second moment matrix M2 = E[X 1 ⌦ X 2] such that [M 2]i j = E[X 1
j ] and
i X 2
l ]. Since
the third moment tensor M3 = E[X 1 ⌦ X 2 ⌦ X 3] such that [M2]i j l = E[X 1
j X 3
the realizations are conditionally independent  we have that  for every ✓ 2 ⇥  E[X 1 ⌦ X 2|✓] =
E[X 1|✓] ⌦ E[X 2|✓] = µ(✓) ⌦ µ(✓) and this allows us to rewrite the second and third moments as
M2 =P✓ ⇢(✓)µ(✓)⌦2  M3 =P✓ ⇢(✓)µ(✓)⌦3  where v⌦p = v ⌦ v ⌦··· v is the p-th tensor power.
Let A be a 3rd order member of the tensor product of the Euclidean space RK (as M3)  then we de-
ﬁne the multilinear map as follows. For a set of three matrices {Vi 2 RK⇥m}1i3   the (i1  i2  i3)
entry in the 3-way array representation of A(V1  V2  V3) 2 Rm⇥m⇥m is [A(V1  V2  V3)]i1 i2 i3 :=
P1j1 j2 j3n Aj1 j2 j3[V1]j1 i1[V2]j2 i2[V3]j3 i3. We also use different norms: the Euclidean norm
k·k ; the Frobenius norm k·k F ; the matrix max-norm kAkmax = maxij |[A]ij|.
3 Multi-arm Bandit with Finite Models
Before considering the transfer problem  we
show that a simple variation to UCB allows us
to effectively exploit the knowledge of ⇥ and
obtain a signiﬁcant reduction in the regret. The
mUCB (model-UCB) algorithm in Fig. 1 takes
as input a set of models ⇥ including the current
(unknown) model ¯✓. At each step t  the algo-
rithm computes a subset ⇥t ✓ ⇥ containing
only the models whose means µi(✓) are com-
patible with the current estimates ˆµi t of the means µi(¯✓) of the current model  obtained averaging

Require: Set of models ⇥  number of steps n
Build ⇥t = {✓ : 8i |µi(✓)  ˆµi t| "i t}
Select ✓t = arg max✓2⇥t µ⇤(✓)
Pull arm It = i⇤(✓t)
Observe sample xIt and update

for t = 1  . . .   n do

end for

Figure 1: The mUCB algorithm.

1Notice that estimating the models involves solving a latent variable model estimation problem  for which

RTP is the state-of-the-art.

2

Ti t pulls  and their uncertainty "i t (see Eq. 2 for an explicit deﬁnition of this term). Notice that it
is enough that one arm does not satisfy the compatibility condition to discard a model ✓. Among
all the models in ⇥t  mUCB ﬁrst selects the model with the largest optimal value and then it pulls
its corresponding optimal arm. This choice is coherent with the optimism in the face of uncertainty
principle used in UCB-based algorithms  since mUCB always pulls the optimal arm corresponding
to the optimistic model compatible with the current estimates ˆµi t. We show that mUCB incurs a
regret which is never worse than UCB and it is often signiﬁcantly smaller.
We denote the set of arms which are optimal for at least a model in a set ⇥0 as A⇤(⇥0) = {i 2A :
9✓ 2 ⇥0 : i⇤(✓) = i}. The set of models for which the arms in A0 are optimal is ⇥(A0) = {✓ 2 ⇥:
9i 2A 0 : i⇤(✓) = i}. The set of optimistic models for a given model ¯✓ is ⇥+ = {✓ 2 ⇥: µ⇤(✓) 
µ⇤(¯✓)}  and their corresponding optimal arms A+ = A⇤(⇥+). The following theorem bounds the
expected regret (similar bounds hold in high probability). The lemmas and proofs (using standard
tools from the bandit literature) are available in Sec. B of Azar et al. (2013).
Theorem 1. If mUCB is run with  = 1/n  a set of m models ⇥ such that the ¯✓ 2 ⇥ and

"i t =qlog(mn2/)/(2Ti t1) 
2i(¯✓) logmn3
min✓2⇥+ i i(✓  ¯✓)2  K + Xi2A+

where Ti t1 is the number of pulls to arm i at the beginning of step t  then its expected regret is

E[Rn]  K + Xi2A+

2 logmn3
where A+ = A⇤(⇥+) is the set of arms which are optimal for at least one optimistic model ⇥+ and
⇥+ i = {✓ 2 ⇥+ : i⇤(✓) = i} is the set of optimistic models for which i is the optimal arm.
Remark (comparison to UCB). The UCB algorithm incurs a regret

min✓2⇥+ i i(✓  ¯✓)

(3)

(2)

 

We see that mUCB displays two major improvements. The regret in Eq. 3 can be written as

E[Rn(UCB)]  O⇣Xi2A

log n

log n

i(¯✓)⌘  O⇣K
min✓2⇥+ i i(✓  ¯✓)⌘  O⇣|A+|

mini i(¯✓)⌘.
mini min✓2⇥+ i i(✓  ¯✓)⌘.

log n

log n

E[Rn(mUCB)]  O⇣Xi2A+

This result suggests that mUCB tends to discard all the models in ⇥+ from the most optimistic
down to the actual model ¯✓ which  with high-probability  is never discarded. As a result  even if
other models are still in ⇥t  the optimal arm of ¯✓ is pulled until the end. This signiﬁcantly reduces
the set of arms which are actually pulled by mUCB and the previous bound only depends on the
number of arms in A+  which is |A+||A ⇤(⇥)| K. Furthermore  for all arms i  the minimum
gap min✓2⇥+ i i(✓  ¯✓) is guaranteed to be larger than the arm gap i(¯✓) (see Lem. 4 in Sec. B
of Azar et al. (2013))  thus further improving the performance of mUCB w.r.t. UCB.

4 Online Transfer with Unknown Models

We now consider the case when the set of models is unknown and the regret is cumulated over
multiple tasks drawn from ⇢ (Eq. 1). We introduce tUCB (transfer-UCB) which transfers estimates
of ⇥  whose accuracy is improved through episodes using a method-of-moments approach.

4.1 The transfer-UCB Bandit Algorithm

Fig. 2 outlines the structure of our online transfer bandit algorithm tUCB (transfer-UCB). The al-
gorithm uses two sub-algorithms  the bandit algorithm umUCB (uncertain model-UCB)  whose ob-
jective is to minimize the regret at each episode  and RTP (robust tensor power method) which at
each episode j computes an estimate {ˆµj
i (✓)} of the arm means of all the models. The bandit al-
gorithm umUCB in Fig. 3 is an extension of the mUCB algorithm. It ﬁrst computes a set of models
⇥j
t whose means ˆµi(✓) are compatible with the current estimates ˆµi t. However  unlike the case
where the exact models are available  here the models themselves are estimated and the uncertainty
"j in their means (provided as input to umUCB) is taken into account in the deﬁnition of ⇥j
t. Once

3

Require: number of arms K  number of

models m  constant C(✓).
Initialize estimated models ⇥1 =
{ˆµ1
for j = 1  2  . . .   J do

i (✓)}i ✓  samples R 2 RJ⇥K⇥n
Run Rj = umUCB(⇥j  n)
Run ⇥j+1 = RTP(R  m  K  j  )

end for

Require: set of models ⇥j  num. steps n

Pull each arm three times
for t = 3K + 1  . . .   n do
t = {✓ : 8i |ˆµj
t (i; ✓) = min(ˆµj

Build ⇥j
Compute Bj
Compute ✓j
t = arg max✓2⇥j
Pull arm It = arg maxi Bj
Observe sample R(It  Ti t) = xIt and update

i (✓)  ˆµi t| "i t + "j}

t (i; ✓j
t )

i (✓) + "j)  (ˆµi t + "i t) 

maxi Bj

t (i; ✓)

t

end for
return Samples R

Figure 2: The tUCB algorithm.

Figure 3: The umUCB algorithm.

Require: samples R 2 Rj⇥n  number of models m and arms K  episode j
Estimate the second and third momentcM2 andcM3 using the reward samples from R (Eq. 4)
Compute bD 2 Rm⇥m and bU 2 RK⇥m (m largest eigenvalues and eigenvectors ofcM2 resp.)
Compute the whitening mappingcW = bUbD1/2 and the tensor bT = cM3(cW  cW  cW )
Plug bT in Alg. 1 of Anandkumar et al. (2012b) and compute eigen-vectors/values {bv(✓)}  {b(✓)}
Computebµj(✓) =b(✓)(cW T)+bv(✓) for all ✓ 2 ⇥
return ⇥j+1 = {bµj(✓) : ✓ 2 ⇥}

Figure 4: The robust tensor power (RTP) method (Anandkumar et al.  2012b).

the active set is computed  the algorithm computes an upper-conﬁdence bound on the value of each
arm i for each model ✓ and returns the best arm for the most optimistic model. Unlike in mUCB 
due to the uncertainty over the model estimates  a model ✓ might have more than one optimal arm 
and an upper-conﬁdence bound on the mean of the arms ˆµi(✓) + "j is used together with the upper-
conﬁdence bound ˆµi t + "i t  which is directly derived from the samples observed so far from arm
i. This guarantees that the B-values are always consistent with the samples generated from the ac-
tual model ¯✓j. Once umUCB terminates  RTP (Fig. 4) updates the estimates of the model means
i (✓)}i 2 RK using the samples obtained from each arm i. At the beginning of each task
umUCB pulls all the arms 3 times  since RTP needs at least 3 samples from each arm to accurately
estimate the 2nd and 3rd moments (Anandkumar et al.  2012b). More precisely  RTP uses all the
reward samples generated up to episode j to estimate the 2nd and 3rd moments (see Sec. 2) as

bµj(✓) = {ˆµj

µ1l ⌦ µ2l 

and

l=1

µ1l ⌦ µ2l ⌦ µ3l 

(4)

l=1

cM3 = j1Xj

cM2 = j1Xj

where the vectors µ1l  µ2l  µ3l 2 RK are obtained by dividing the T l
i in episode l in three batches and taking their average (e.g.  [µ1l]i is the average of the ﬁrst T l
samples).2 Since µ1l  µ2l  µ3l are independent estimates of µ(¯✓l)  cM2 and cM3 are consistent esti-
mates of the second and third moments M2 and M3. RTP relies on the fact that the model means
µ(✓) can be recovered from the spectral decomposition of the symmetric tensor T = M3(W  W  W ) 
where W is a whitening matrix for M2  i.e.  M2(W  W ) = Im⇥m (see Sec. 2 for the deﬁni-
tion of the mapping A(V1  V2  V3)). Anandkumar et al. (2012b) (Thm. 4.3) have shown that un-
der some mild assumption (see later Assumption 1) the model means {µ(✓)}  can be obtained as
µ(✓) = (✓)Bv(✓)  where ((✓)  v(✓)) is a pair of eigenvector/eigenvalue for the tensor T and

i n samples observed from arm
i n/3

B := (W T)+.Thus the RTP algorithm estimates the eigenvectorsbv(✓) and the eigenvaluesb(✓)  of
the m ⇥ m ⇥ m tensor bT := cM3(cW  cW  cW ).3 Oncebv(✓) andb(✓) are computed  the estimated
mean vectorbµj(✓) is obtained by the inverse transformationbµj(✓) = b(✓)bBbv(✓)  where bB is the
pseudo inverse ofcW T(for a detailed description of RTP algorithm see Anandkumar et al.  2012b).
3The matrixcW 2 RK⇥m is such that cM2(cW  cW ) = Im⇥m  i.e. cW is the whitening matrix of cM2. In
generalcW is not unique. Here  we choosecW = bUbD1/2  where bD 2 Rm⇥m is a diagonal matrix consisting
of the m largest eigenvalues ofcM2 and bU 2 RK⇥m has the corresponding eigenvectors as its columns.

i n  the empirical mean of arm i at the end of episode l.

2Notice that 1/3([µ1l]i + [µ2l]i + [µ1l]i) = ˆµl

4

4.2 Sample Complexity of the Robust Tensor Power Method

  max := max2⌃M2

umUCB requires as input "j  i.e.  the uncertainty of the model estimates. Therefore we need sam-
ple complexity bounds on the accuracy of {ˆµi(✓)} computed by RTP. The performance of RTP is
directly affected by the error of the estimates cM2 and cM3 w.r.t. the true moments. In Thm. 2 we
prove that  as the number of tasks j grows  this error rapidly decreases with the rate ofp1/j. This
result provides us with an upper-bound on the error "j needed for building the conﬁdence intervals
in umUCB. The following deﬁnition and assumption are required for our result.
Deﬁnition 1. Let ⌃M2 = {1  2  . . .   m} be the set of m largest eigenvalues of the matrix M2.
Deﬁne min := min2⌃M2
 and max := max✓ (✓). Deﬁne the minimum
gap between the distinct eigenvalues of M2 as  := mini6=l(|i  l|).
Assumption 1. The mean vectors {µ(✓)}✓ are linear independent and ⇢(✓) > 0 for all ✓ 2 ⇥.
We now state our main result which is in the form of a high probability bound on the estimation
error of mean reward vector of every model ✓ 2 ⇥.
Theorem 2. Pick  2 (0  1). Let C(⇥) := C3maxpmax/3
min (max/ + 1/min + 1/max) 
where C3 > 0 is a universal constant. Then under Assumption 1 there exist constants C4 > 0 and a
permutation ⇡ on ⇥  such that for all ✓ 2 ⇥  we have w.p. 1  
kµ(✓) bµj(⇡(✓))k  "j   C(⇥)K2.5m2q log(K/)
after
Remark (computation of C(⇥)). As illustrated in Fig. 3  umUCB relies on the estimatesbµj(✓) and

on their accuracy "j. Although the bound reported in Thm. 2 provides an upper conﬁdence bound
on the error of the estimates  it contains terms which are not computable in general (e.g.  min). In
practice  C(⇥) should be considered as a parameter of the algorithm.This is not dissimilar from the
parameter usually introduced in the deﬁnition of "i t in front of the square-root term in UCB.

j  C4m5K6 log(K/)

min(min )23

min2

(5)

min

.

j

4.3 Regret Analysis of umUCB

i (✓) + "j < ˆµj

⇤(✓) = {i 2A : @i0  ˆµj

+(✓; ¯✓j) = {i 2A j
+(¯✓j) = {✓ 2 ⇥: Aj

We now analyze the regret of umUCB when an estimated set of models ⇥j is provided as input. At
episode j  for each model ✓ we deﬁne the set of non-dominated arms (i.e.  potentially optimal arms)
as Aj
i0(✓)  "j}. Among the non-dominated arms  when the
actual model is ¯✓j  the set of optimistic arms is Aj
i (✓) + "j  µ⇤(¯✓j)}.
⇤(✓) : ˆµj
+(✓; ¯✓j) 6= ;}. In some cases 
As a result  the set of optimistic models is ⇥j
because of the uncertainty in the model estimates  unlike in mUCB  not all the models ✓ 6= ¯✓j can be
discarded  not even at the end of a very long episode. Among the optimistic models  the set of models
that cannot be discarded is deﬁned ase⇥j
i (✓)µi(¯✓j)|
"j}. Finally  when we want to apply the previous deﬁnitions to a set of models ⇥0 instead of single
model we have  e.g.  Aj
The proof of the following results are available in Sec. D of Azar et al. (2013)  here we only report
the number of pulls  and the corresponding regret bound.
Corollary 1. If at episode j umUCB is run with "i t as in Eq. 2 and "j as in Eq. 5 with a parameter
0 = /2K  then for any arm i 2A   i 6= i⇤(¯✓j) is pulled Ti n times such that

⇤(⇥0; ¯✓j) =S✓2⇥0 Aj

+(¯✓j) = {✓ 2 ⇥j

+(¯✓j) : 8i 2A j

+(✓; ¯✓j) |ˆµj

⇤(✓; ¯✓j).

i +(¯✓j) = {✓ 2 ⇥j

w.p. 1    where ⇥j
among their optimistic non-dominated arms bi(✓; ¯✓j) = i(✓  ¯✓j)/2"j  Aj
1 = Aj
+(e⇥j
+(¯✓j); ¯✓j) (i.e.  set of arms only proposed by models that can be discarded)  and Aj
Aj
+(e⇥j
+(¯✓j); ¯✓j) (i.e.  set of arms only proposed by models that cannot be discarded).
Aj

+(¯✓j) : i 2A +(✓; ¯✓j)} is the set of models for which i is
+(¯✓j); ¯✓j)
2 =

+(⇥j

5

Ti n  min⇢ 2 log2mKn2/
Ti n  2 log2mKn2//(i(¯✓j)2) + 1

i(¯✓j)2

Ti n = 0

 

2 min✓2⇥j

log2mKn2/
i +(¯✓j )bi(✓; ¯✓j)2 + 1 if i 2A j

1

if i 2A j
2
otherwise

8>>>><>>>>:

The previous corollary states that arms which cannot be optimal for any optimistic model (i.e. 
the optimistic non-dominated arms) are never pulled by umUCB  which focuses only on arms in
+(¯✓j); ¯✓j). Among these arms  those that may help to remove a model from the active set
+(⇥j
i 2A j
(i.e.  i 2A j
1) are potentially pulled less than UCB  while the remaining arms  which are optimal for
the models that cannot be discarded (i.e.  i 2A j
2)  are simply pulled according to a UCB strategy.
Similar to mUCB  umUCB ﬁrst pulls the arms that are more optimistic until either the active set ⇥j
t
changes or they are no longer optimistic (because of the evidence from the actual samples). We are
now ready to derive the per-episode regret of umUCB.
Theorem 3. If umUCB is run for n steps on the set of models ⇥j estimated by RTP after j episodes
with  = 1/n  and the actual model is ¯✓j  then its expected regret (w.r.t. the random realization in
episode j and conditional on ¯✓j) is

n]  K +Xi2Aj
E[Rj
+ Xi2Aj

log2mKn3 min⇢2/i(¯✓j)2  1/2 min✓2⇥j
2 log2mKn3/i(¯✓j).

2

1

i +(¯✓j )bi(✓; ¯✓j)2i(¯✓j)

Remark (negative transfer). The transfer of knowledge introduces a bias in the learning process
which is often beneﬁcial. Nonetheless  in many cases transfer may result in a bias towards wrong
solutions and a worse learning performance  a phenomenon often referred to as negative transfer.
The ﬁrst interesting aspect of the previous theorem is that umUCB is guaranteed to never perform
worse than UCB itself. This implies that tUCB never suffers from negative transfer  even when the
set ⇥j contains highly uncertain models and might bias umUCB to pull suboptimal arms.
Remark (improvement over UCB). In Sec. 3 we showed that mUCB exploits the knowledge of ⇥
to focus on a restricted set of arms which are pulled less than UCB. In umUCB this improvement is
not as clear  since the models in ⇥ are not known but are estimated online through episodes. Yet 
similar to mUCB  umUCB has the two main sources of potential improvement w.r.t.
to UCB. As
illustrated by the regret bound in Thm. 3  umUCB focuses on arms in Aj
1 [A j
2 which is potentially
a smaller set than A. Furthermore  the number of pulls to arms in Aj
1 is smaller than for UCB
whenever the estimated model gap bi(✓; ¯✓j) is bigger than i(¯✓j). Eventually  umUCB reaches
the same performance (and improvement over UCB) as mUCB when j is big enough. In fact  the
+(¯✓j) ⌘ ⇥+(¯✓j)) and all the
set of optimistic models reduces to the one used in mUCB (i.e.  ⇥j
optimistic models have only optimal arms (i.e.  for any ✓ 2 ⇥+ the set of non-dominated optimistic
arms is A+(✓; ¯✓j) = {i⇤(✓)})  which corresponds to Aj
2 ⌘{ i⇤(¯✓j)}  which
matches the condition of mUCB. For instance  for any model ✓  in order to have A⇤(✓) = {i⇤(✓)} 
for any arm i 6= i⇤(✓) we need that ˆµj
j 

i⇤(✓)(✓)  "j. Thus after
mini i(✓)2 + 1.

i (✓) + "j  ˆµj
2C(⇥)

1 ⌘A ⇤(⇥+(¯✓j)) and Aj

min

min
¯✓2⇥

✓2⇥+(¯✓)

episodes  all the optimistic models have only one optimal arm independently from the actual identity
of the model ¯✓j. Although this condition may seem restrictive  in practice umUCB starts improving
over UCB much earlier  as illustrated in the numerical simulation in Sec. 5.

4.4 Regret Analysis of tUCB

Given the previous results  we derive the bound on the cumulative regret over J episodes (Eq. 1).
Theorem 4. If tUCB is run over J episodes of n steps in which the tasks ¯✓j are drawn from a ﬁxed
distribution ⇢ over a set of models ⇥  then its cumulative regret is

min⇢ 2 log2mKn2/

i(¯✓j)2

 

j=1 Xi2Aj

1

RJ  JK + XJ
+ XJ
j=1 Xi2Aj

2

2 log2mKn2/

i(¯✓j)

 

log2mKn2/
i +(¯✓j )bj

2 min✓2⇥j

i (✓; ¯✓j)2i(¯✓j)

w.p. 1   w.r.t. the randomization over tasks and the realizations of the arms in each episode.

6

d
r
a
w
e
R

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

m1 m2 m3 m4 m5

m1 m2 m3 m4 m5

m1 m2 m3 m4 m5

m1 m2 m3 m4 m5

m1 m2 m3 m4 m5

m1 m2 m3 m4 m5

m1 m2 m3 m4 m5

Models

l

y
t
i
x
e
p
m
o
C

30

25

20

15

10

5

0
 
0

 

UCB
UCB+
mUCB
tUCB

1000

2000

3000

4000

5000

Number of Tasks (J)

Figure 5: Set of models ⇥.

Figure 6: Complexity over tasks.

t

e
r
g
e
R

350

300

250

200

150

100

50
 
0

UCB
UCB+
mUCB
tUCB (J=1000)
tUCB (J=2000)
tUCB (J=5000)

 

5000

10000

15000

Number of Steps (n)

Figure 7: Regret of UCB  UCB+  mUCB  and
tUCB (avg. over episodes) vs episode length.

t

e
r
g
e
R
 
e
d
o
s
p
e
−
r
e
P

i

350

300

250

200

150

100

 
0

 

UCB
UCB+
mUCB
tUCB

1000

2000

3000

4000

5000

Number of Tasks (J)

Figure 8: Per-episode regret of tUCB.

This result immediately follows from Thm. 3 and it shows a linear dependency on the number of
episodes J. This dependency is the price to pay for not knowing the identity of the current task ¯✓j.
If the task was revealed at the beginning of the task  a bandit algorithm could simply cluster all the
samples coming from the same task and incur a much smaller cumulative regret with a logarithmic
dependency on episodes and steps  i.e.  log(nJ). Nonetheless  as discussed in the previous section 
the cumulative regret of tUCB is never worse than for UCB and as the number of tasks increases it
approaches the performance of mUCB  which fully exploits the prior knowledge of ⇥.

5 Numerical Simulations

In this section we report preliminary results of tUCB on synthetic data. The objective is to illustrate
and support the previous theoretical ﬁndings. We deﬁne a set ⇥ of m = 5 MAB problems with
K = 7 arms each  whose means {µi(✓)}i ✓ are reported in Fig. 5 (see Sect. F in Azar et al. (2013)
for the actual values)  where each model has a different color and squares correspond to optimal arms
(e.g.  arm 2 is optimal for model ✓2). This set of models is chosen to be challenging and illustrate
some interesting cases useful to understand the functioning of the algorithm.4 Models ✓1 and ✓2
only differ in their optimal arms and this makes it difﬁcult to distinguish them. For arm 3 (which is
optimal for model ✓3 and thus potentially selected by mUCB)  all the models share exactly the same
mean value. This implies that no model can be discarded by pulling it. Although this might suggest
that mUCB gets stuck in pulling arm 3  we showed in Thm. 1 that this is not the case. Models ✓1
and ✓5 are challenging for UCB since they have small minimum gap. Only 5 out of the 7 arms are
actually optimal for a model in ⇥. Thus  we also report the performance of UCB+ which  under the
assumption that ⇥ is known  immediately discards all the arms which are not optimal (i /2A ⇤) and
performs UCB on the remaining arms. The model distribution is uniform  i.e.  ⇢(✓) = 1/m.
Before discussing the transfer results  we compare UCB  UCB+  and mUCB  to illustrate the ad-
vantage of the prior knowledge of ⇥ w.r.t. UCB. Fig. 7 reports the per-episode regret of the three

4Notice that although ⇥ satisﬁes Assumption 1  the smallest singular value min = 0.0039 and  =

0.0038  thus making the estimation of the models difﬁcult.

7

algorithms for episodes of different length n (the performance of tUCB is discussed later). The re-
sults are averaged over all the models in ⇥ and over 200 runs each. All the algorithms use the same
conﬁdence bound "i t. The performance of mUCB is signiﬁcantly better than both UCB  and UCB+ 
thus showing that mUCB makes an efﬁcient use of the prior of knowledge of ⇥. Furthermore  in
Fig. 6 the horizontal lines correspond to the value of the regret bounds up to the n dependent terms
and constants5 for the different models in ⇥ averaged w.r.t. ⇢ for the three algorithms (the actual
values for the different models are in the supplementary material). These values show that the im-
provement observed in practice is accurately predicated by the upper-bounds derived in Thm. 1.
We now move to analyze the performance of tUCB. In Fig. 8 we show how the per-episode regret
changes through episodes for a transfer problem with J = 5000 tasks of length n = 5000. In
tUCB we used "j as in Eq. 5 with C(⇥) = 2. As discussed in Thm. 3  UCB and mUCB deﬁne
the boundaries of the performance of tUCB. In fact  at the beginning tUCB selects arms according
to a UCB strategy  since no prior information about the models ⇥ is available. On the other hand 
as more tasks are observed  tUCB is able to transfer the knowledge acquired through episodes and
build an increasingly accurate estimate of the models  thus approaching the behavior of mUCB. This
is also conﬁrmed by Fig. 6 where we show how the complexity of tUCB changes through episodes.
In both cases (regret and complexity) we see that tUCB does not reach the same performance of
mUCB. This is due to the fact that some models have relatively small gaps and thus the number of
episodes to have an accurate enough estimate of the models to reach the performance of mUCB is
much larger than 5000 (see also the Remarks of Thm. 3). Since the ﬁnal objective is to achieve a
small global regret (Eq. 1)  in Fig. 7 we report the cumulative regret averaged over the total number
of tasks (J) for different values of J and n. Again  this graph shows that tUCB outperforms UCB
and that it tends to approach the performance of mUCB as J increases  for any value of n.
6 Conclusions and Open Questions
In this paper we introduce the transfer problem in the multi–armed bandit framework when a tasks
are drawn from a ﬁnite set of bandit problems. We ﬁrst introduced the bandit algorithm mUCB
and we showed that it is able to leverage the prior knowledge on the set of bandit problems ⇥ and
reduce the regret w.r.t. UCB. When the set of models is unknown we deﬁne a method-of-moments
variant (RTP) which consistently estimates the means of the models in ⇥ from the samples collected
through episodes. This knowledge is then transferred to umUCB which performs no worse than UCB
and tends to approach the performance of mUCB. For these algorithms we derive regret bounds  and
we show preliminary numerical simulations. To the best of our knowledge  this is the ﬁrst work
studying the problem of transfer in multi-armed bandit. It opens a series of interesting directions 
including whether explicit model identiﬁcation can improve our transfer regret.
Optimality of tUCB. At each episode  tUCB transfers the knowledge about ⇥ acquired from previous
tasks to achieve a small per-episode regret using umUCB. Although this strategy guarantees that the
per-episode regret of tUCB is never worse than UCB  it may not be the optimal strategy in terms of
the cumulative regret through episodes. In fact  if J is large  it could be preferable to run a model
identiﬁcation algorithm instead of umUCB in earlier episodes so as to improve the quality of the
estimates ˆµi(✓). Although such an algorithm would incur a much larger regret in earlier tasks (up
to linear)  it could approach the performance of mUCB in later episodes much faster than done by
tUCB. This trade-off between identiﬁcation of the models and transfer of knowledge may suggest
that different algorithms than tUCB are possible.
Unknown model-set size. In some problems the size of model set m is not known to the learner and
needs to be estimated. This problem can be addressed by estimating the rank of matrix M2 which
equals to m (Kleibergen and Paap  2006). We also note that one can relax the assumption that ⇢(✓)
needs to be positive (see Assumption 1) by using the estimated model size as opposed to m  since
M2 depends not on the means of models with ⇢(✓) = 0.
Acknowledgments. This research was supported by the National Science Foundation (NSF award #SBE-
0836012). We would like to thank Sham Kakade and Animashree Anandkumar for valuable discussions. A.
Lazaric would like to acknowledge the support of the Ministry of Higher Education and Research  Nord-Pas-
de-Calais Regional Council and FEDER through the’ Contrat de Projets Etat Region (CPER) 2007-2013’  and
the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement 231495
(project CompLACS).

5For instance  for UCB we computePi 1/i.

8

References
Agarwal  A.  Dudík  M.  Kale  S.  Langford  J.  and Schapire  R. E. (2012). Contextual bandit learn-
ing with predictable rewards. In Proceedings of the 15th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS’12).

Anandkumar  A.  Foster  D. P.  Hsu  D.  Kakade  S.  and Liu  Y.-K. (2012a). A spectral algorithm for
latent dirichlet allocation. In Proceedings of Advances in Neural Information Processing Systems
25 (NIPS’12)  pages 926–934.

Anandkumar  A.  Ge  R.  Hsu  D.  and Kakade  S. M. (2013). A tensor spectral approach to learning

mixed membership community models. Journal of Machine Learning Research  1:65.

Anandkumar  A.  Ge  R.  Hsu  D.  Kakade  S. M.  and Telgarsky  M. (2012b). Tensor decompositions

for learning latent variable models. CoRR  abs/1210.7559.

Anandkumar  A.  Hsu  D.  and Kakade  S. M. (2012c). A method of moments for mixture models
and hidden markov models. In Proceeding of the 25th Annual Conference on Learning Theory
(COLT’12)  volume 23  pages 33.1–33.34.

Auer  P.  Cesa-Bianchi  N.  and Fischer  P. (2002). Finite-time analysis of the multi-armed bandit

problem. Machine Learning  47:235–256.

Azar  M. G.  Lazaric  A.  and Brunskill  E. (2013). Sequential transfer in multi-armed bandit with

ﬁnite set of models. CoRR  abs/1307.6887.

Cavallanti  G.  Cesa-Bianchi  N.  and Gentile  C. (2010). Linear algorithms for online multitask

classiﬁcation. Journal of Machine Learning Research  11:2901–2934.

Cesa-Bianchi  N. and Lugosi  G. (2006). Prediction  Learning  and Games. Cambridge University

Press.

Dekel  O.  Long  P. M.  and Singer  Y. (2006). Online multitask learning. In Proceedings of the 19th

Annual Conference on Learning Theory (COLT’06)  pages 453–467.

Garivier  A. and Moulines  E. (2011). On upper-conﬁdence bound policies for switching bandit
problems. In Proceedings of the 22nd international conference on Algorithmic learning theory 
ALT’11  pages 174–188  Berlin  Heidelberg. Springer-Verlag.

Kleibergen  F. and Paap  R. (2006). Generalized reduced rank tests using the singular value decom-

position. Journal of Econometrics  133(1):97–126.

Langford  J. and Zhang  T. (2007). The epoch-greedy algorithm for multi-armed bandits with side
information. In Proceedings of Advances in Neural Information Processing Systems 20 (NIPS’07).
Lazaric  A. (2011). Transfer in reinforcement learning: a framework and a survey. In Wiering  M.

and van Otterlo  M.  editors  Reinforcement Learning: State of the Art. Springer.

Lugosi  G.  Papaspiliopoulos  O.  and Stoltz  G. (2009). Online multi-task learning with hard con-

straints. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT’09).

Mann  T. A. and Choe  Y. (2012). Directed exploration in reinforcement learning with trans-
ferred knowledge. In Proceedings of the Tenth European Workshop on Reinforcement Learning
(EWRL’12).

Pan  S. J. and Yang  Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge

and Data Engineering  22(10):1345–1359.

Robbins  H. (1952). Some aspects of the sequential design of experiments. Bulletin of the AMS 

58:527–535.

Saha  A.  Rai  P.  Daumé III  H.  and Venkatasubramanian  S. (2011). Online learning of multiple
tasks and their relationships. In Proceedings of the 14th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS’11)  Ft. Lauderdale  Florida.

Stewart  G. W. and Sun  J.-g. (1990). Matrix perturbation theory. Academic press.
Taylor  M. E. (2009). Transfer in Reinforcement Learning Domains. Springer-Verlag.
Wedin  P. (1972). Perturbation bounds in connection with singular value decomposition. BIT Nu-

merical Mathematics  12(1):99–111.

9

,Mohammad Gheshlaghi azar
Alessandro Lazaric
Emma Brunskill
William Hoiles
Mihaela van der Schaar
Sagie Benaim
Lior Wolf