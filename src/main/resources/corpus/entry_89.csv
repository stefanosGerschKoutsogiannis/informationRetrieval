2015,A Generalization of Submodular Cover via the Diminishing Return Property on the Integer Lattice,We consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice. We are motivated by real scenarios in machine learning that cannot be captured by (traditional) submodular set functions.  We show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm.  Our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy. The running time of our algorithm is roughly $O(n\log (nr) \log{r})$  where $n$ is the size of the ground set and $r$ is the maximum value of a coordinate.  The dependency on $r$ is exponentially better than the naive reduction algorithms. Several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms  while the running time is several orders of magnitude faster.,A Generalization of Submodular Cover via the

Diminishing Return Property on the Integer Lattice

Tasuku Soma

The University of Tokyo

tasuku soma@mist.i.u-tokyo.ac.jp

Yuichi Yoshida

National Institute of Informatics  and

Preferred Infrastructure  Inc.
yyoshida@nii.ac.jp

Abstract

We consider a generalization of the submodular cover problem based on the con-
cept of diminishing return property on the integer lattice. We are motivated by
real scenarios in machine learning that cannot be captured by (traditional) sub-
modular set functions. We show that the generalized submodular cover problem
can be applied to various problems and devise a bicriteria approximation algo-
rithm. Our algorithm is guaranteed to output a log-factor approximate solution
that satisﬁes the constraints with the desired accuracy. The running time of our
algorithm is roughly O(n log(nr) log r)  where n is the size of the ground set and
r is the maximum value of a coordinate. The dependency on r is exponentially
better than the naive reduction algorithms. Several experiments on real and artiﬁ-
cial datasets demonstrate that the solution quality of our algorithm is comparable
to naive algorithms  while the running time is several orders of magnitude faster.

Introduction

1
A function f : 2S → R+ is called submodular if f (X) + f (Y ) ≥ f (X ∪ Y ) + f (X ∩ Y ) for
all X  Y ⊆ S  where S is a ﬁnite ground set. An equivalent and more intuitive deﬁnition is by
the diminishing return property: f (X ∪ {s}) − f (X) ≥ f (Y ∪ {s}) − f (Y ) for all X ⊆ Y and
s ∈ S \ Y . In the last decade  the optimization of a submodular function has attracted particular
interest in the machine learning community. One reason of this is that many real-world models
naturally admit the diminishing return property. For example  document summarization [12  13] 
inﬂuence maximization in viral marketing [7]  and sensor placement [10] can be described with the
concept of submodularity  and efﬁcient algorithms have been devised by exploiting submodularity
(for further details  refer to [8]).
A variety of proposed models in machine learning [4  13  18] boil down to the submodular cover
problem [21]; for given monotone and nonnegative submodular functions f  c : 2S → R+  and
α > 0  we are to

subject to f (X) ≥ α.

minimize c(X)

(1)
Intuitively  c(X) and f (X) represent the cost and the quality of a solution  respectively. The objec-
tive of this problem is to ﬁnd X of minimum cost with the worst quality guarantee α. Although this
problem is NP-hard since it generalizes the set cover problem  a simple greedy algorithm achieves
tight log-factor approximation and it practically performs very well.
The aforementioned submodular models are based on the submodularity of a set function  a function
deﬁned on 2S. However  we often encounter problems that cannot be captured by a set function. Let
us give two examples:

Sensor Placement: Let us consider the following sensor placement scenario. Suppose that we
have several types of sensors with various energy levels. We assume a simple trade-off between

1

information gain and cost. Sensors of a high energy level can collect a considerable amount of
information  but we have to pay a high cost for placing them. Sensors of a low energy level can
be placed at a low cost  but they can only gather limited information. In this scenario  we want to
decide which type of sensor should be placed at each spot  rather than just deciding whether to place
a sensor or not. Such a scenario is beyond the existing models based on submodular set functions.

Optimal Budget Allocation: A similar situation also arises in the optimal budget allocation prob-
lem [2]. In this problem  we want to allocate budget among ad sources so that (at least) a certain
number of customers is inﬂuenced while minimizing the total budget. Again  we have to decide
how much budget should be set aside for each ad source  and hence set functions cannot capture the
problem.
We note that a function f : 2S → R+ can be seen as a function deﬁned on a Boolean hypercube
{0  1}S. Then  the above real scenarios prompt us to generalize the submodularity and the diminish-
ing return property to functions deﬁned on the integer lattice ZS
+. The most natural generalization
of the diminishing return property to a function f : ZS

+ → R+ is the following inequality:

(2)
for x ≤ y and s ∈ S  where χs is the s-th unit vector. If f satisﬁes (2)  then f also satisﬁes the
following lattice submodular inequality:

f (x + χs) − f (x) ≥ f (y + χs) − f (y)

f (x) + f (y) ≥ f (x ∨ y) + f (x ∧ y)

(3)
+  where ∨ and ∧ are the coordinate-wise max and min operations  respectively.
for all x  y ∈ ZS
While the submodularity and the diminishing return property are equivalent for set functions  this
is not the case for functions over the integer lattice; the diminishing return property (2) is stronger
than the lattice submodular inequality (3). We say that f is lattice submodular if f satisﬁes (3) 
and if f further satisﬁes (2) we say that f is diminishing return submodular (DR-submodular for
short). One might feel that the DR-submodularity (2) is too restrictive. However  considering the
fact that the diminishing return is more crucial in applications  we may regard the DR-submodularity
(2) as the most natural generalization of the submodularity  at least for applications mentioned so
far [17  6]. For example  under a natural condition  the objective function in the optimal budget al-
location satisﬁes (2) [17]. The DR-submodularity was also considered in the context of submodular
welfare [6].
In this paper  we consider the following generalization of the submodular cover problem for set
+ → R+  a subadditive function
functions: Given a monotone DR-submodular function f : ZS
c : ZS
0 ≤ x ≤ r1 
(4)
where we say that c is subadditive if c(x + y) ≤ c(x) + c(y) for all x  y ∈ ZS
+. We call problem (4)
the DR-submodular cover problem. This problem encompasses problems that boil down to the sub-
modular cover problem for set functions and their generalizations to the integer lattice. Furthermore 
the cost function c is generalized to a subadditive function. In particular  we note that two examples
given above can be rephrased using this problem (see Section 4 for details).
If c is also monotone DR-submodular  one can reduce the problem (4) to the set version (1) (for
technical details  see Section 3.1). The problem of this naive reduction is that it only yields a
pseudo-polynomial time algorithm; the running time depends on r rather than log r. Since r can be
huge in many practical settings (e.g.  the maximum energy level of a sensor)  even linear dependence
on r could make an algorithm impractical. Furthermore  for a general subadditive function c  this
naive reduction does not work.

+ → R+  α > 0  and r ∈ Z+  we are to

subject to f (x) ≥ α 

minimize c(x)

1.1 Our Contribution

For the problem (4)  we devise a bicriteria approximation algorithm based on the decreasing thresh-
old technique of [3]. More precisely  our algorithm takes the additional parameters 0 <   δ < 1. The
output x ∈ ZS
1 + log d
β
times the optimum and f (x) ≥ (1 − δ)α  where ρ is the curvature of c (see Section 3 for the def-
inition)  d = maxs f (χs) is the maximum value of f over all standard unit vectors  and β is the
minimum value of the positive increments of f in the feasible region.

+ of our algorithm is guaranteed to satisfy that c(x) is at most (1 + 3)ρ

(cid:16)

(cid:17)

2

Running Time (dependency on r): An important feature of our algorithm is that the running
time depends on the bit length of r only polynomially whereas the naive reduction algorithms de-
pend on it exponentially as mentioned above. More precisely  the running time of our algorithm is
log r)  which is polynomial in the input size  whereas the naive algorithm is only
O( n
psuedo-polynomial time algorithm. In fact  our experiments using real and synthetic datasets show
that our algorithm is considerably faster than naive algorithms. Furthermore  in terms of the objec-
tive value (that is  the cost of the output)  our algorithm also exhibits comparable performance.

 log nrcmax
δcmin

Approximation Guarantee: Our approximation guarantee on the cost is almost tight. Note that
the DR submodular cover problem (4) includes the set cover problem  in which we are given a
collection of sets  and we want to ﬁnd a minimum number of sets that covers all the elements. In
our context  S corresponds to the collection of sets  the cost c is the number of chosen sets  and f
is the number of covered elements. It is known that we cannot obtain an o(log m)-approximation
unless P (cid:54)= NP  where m is the number of elements [16]. However  since for the set cover problem
we have ρ = 1  d = O(m)  and β = 1  our approximation guarantee is O(log m).

1.2 Related Work

Our result can be compared with several results in the literature for the submodular cover problem
for set functions. It is shown by Wolsey [21] that if c(X) = |X|  a simple greedy algorithm yields
β )-approximation  which coincides with our approximation ratio except for the (1 + 3)
(1 + log d
factor. Note that ρ = 1 when c(X) = |X|  or more generally  when c is modular. Recently  Wan
et al. [20] discussed a slightly different setting  in which c is also submodular and both f and c
are integer valued. They proved that the greedy algorithm achieves ρH(d)-approximation  where
H(d) = 1+1/2+···+1/d is the d-th harmonic number. Again  their ratio asymptotically coincides
with our approximation ratio (Note that β ≥ 1 when f is integer valued).
Another common submodular-based model in machine learning is in the form of the submodular
maximization problem: Given a monotone submodular set function f : {0  1}S → R+ and a feasible
set P ⊆ [0  1]S (e.g.  a matroid polytope or a knapsack polytope)  we want to maximize f (x) subject
to x ∈ P ∩ {0  1}S. Such models can be widely found in various tasks as already described. We
note that the submodular cover problem and the submodular maximization problem are somewhat
dual to each other. Indeed  Iyer and Bilmes [5] showed that a bicriteria algorithm of one of these
problems yields a bicriteria algorithm for the other. Being parallel to our setting  generalizing the
submodular maximization problem to the integer lattice ZS
+ is a natural question. In this direction 
Soma et al. [17] considered the maximization of lattice submodular functions (not necessarily being
DR-submodular) and devised a constant-factor approximation pseudo-polynomial time algorithm.
We note that our result is not implied by [17] via the duality of [5]. In fact  such reduction only
yields a pseudo-polynomial time algorithm.

1.3 Organization of This Paper

The rest of this paper is organized as follows: Section 2 sets the mathematical basics of submod-
ular functions over the integer lattice. Section 3 describes our algorithm and the statement of our
main theorem. In Section 4  we show various experimental results using real and artiﬁcial datasets.
Section 5 sketches the proof of the main theorem. Finally  we conclude the paper in Section 6.

2 Preliminaries
Let S be a ﬁnite set. For each s ∈ S  we denote the s-th unit vector by χs; that is  χs(t) = 1
if t = s  otherwise χs(t) = 0. A function f : ZS → R is said to be lattice submodular if
f (x) + f (y) ≥ f (x ∨ y) + f (x ∧ y) for all x  y ∈ ZS. A function f is monotone if f (x) ≥ f (y)
for all x  y ∈ ZS with x ≥ y. For x  y ∈ ZS and a function f : ZS → R  we denote f (y |
x) := f (y + x) − f (x). A function f is diminishing return submodular (or DR-submodular) if
f (x + χs) − f (x) ≥ f (y + χs) − f (y) for each x ≤ y ∈ ZS and s ∈ S. For a DR-submodular
function f  one can immediately check that f (kχs | x) ≥ f (kχs | y) for arbitrary x ≤ y  s ∈ S 
and k ∈ Z+. A function f is subadditive if f (x + y) ≤ f (x) + f (y) for x  y ∈ ZS. For each
x ∈ ZS

+  we deﬁne {x} to be the multiset in which each s ∈ S is contained x(s) times.

3

In [17]  a lattice submodular function f : ZS → R is said to have the diminishing return property if
f is coordinate-wise concave: f (x + 2χs) − f (x + χs) ≤ f (x + χs) − f (x) for each x ∈ ZS and
s ∈ S. We note that our deﬁnition is consistent with [17]. Formally  we have the following lemma 
whose proof can be found in Appendix.
Lemma 2.1. A function f : ZS → R is DR-submodular if and only if f is lattice submodular and
coordinate-wise concave.

Lemma 2.2. For a monotone DR-submodular function f  f (x) − f (y) ≤(cid:80)

The following is fundamental for a monotone DR-submodular function. A proof is placed in Ap-
pendix due to the limitation of space.
s∈{x} f (χs | y) for

arbitrary x  y ∈ ZS.

3 Algorithm for the DR-submodular Cover

+ → R+ be a monotone DR-submodular
Recall the DR-submodular cover problem (4). Let f : ZS
+ → R+ be a subadditive cost function. The objective is to minimize c(x)
function and let c : ZS
subject to f (x) ≥ α and 0 ≤ x ≤ r1  where α > 0 and r ∈ Z+ are the given constants. Without
loss of generality  we can assume that max{f (x) : 0 ≤ x ≤ r1} = α (otherwise  we can consider

(cid:98)f (x) := min{f (x)  α} instead of f). Furthermore  we can assume c(x) > 0 for any x ∈ ZS

+.

A pseudocode description of our algorithm is presented in Algorithm 1. The algorithm can be viewed
as a modiﬁed version of the greedy algorithm and works as follows: We start with the initial solution
x = 0 and increase each coordinate of x gradually. To determine the amount of increments  the
algorithm maintains a threshold θ that is initialized to be sufﬁciently large enough. For each s ∈ S 
the algorithm ﬁnds the largest integer step size 0 < k ≤ r − x(s) such that the marginal cost-gain
ratio f (kχs|x)
is above the threshold θ. If such k exists  the algorithm updates x to x + kχs. After
repeating this for each s ∈ S  the algorithm decreases the threshold θ by a factor of (1 − ). If x
becomes feasible  the algorithm returns the current x. Even if x does not become feasible  the ﬁnal
x satisﬁes f (x) ≥ (1 − δ)α if we iterate until θ gets sufﬁciently small.

kc(χs)

+ → R+  c : ZS

c(χs)  cmax ← max
s∈S

+ → R+  r ∈ N  α > 0   > 0  δ > 0.

Algorithm 1 Decreasing Threshold for the DR-Submodular Cover Problem
Input: f : ZS
Output: 0 ≤ x ≤ r1 such that f (x) ≥ α.
1: x ← 0  d ← max
f (χs)  cmin ← min
s∈S
s∈S
ncmaxr d; θ ← θ(1 − )) do
; θ ≥ δ
2: for (θ = d
for all s ∈ S do
cmin
3:
4:
5:
6:
7: return x
Before we claim the theorem  we need to deﬁne several parameters on f and c. Let β := min{f (χs |
x) : s ∈ S  x ∈ ZS
+  f (χs | x) > 0} and d := maxs f (χs). Let cmax := maxs c(χs) and
cmin := mins c(χs). Deﬁne the curvature of c to be

Find maximum integer 0 < k ≤ r − x(s) such that f (kχs|x)
If such k exists then x ← x + kχs.
If f (x) ≥ α then break the outer for loop.

kc(χs) ≥ θ with binary search.

c(χs)

c(x∗)
Deﬁnition 3.1. For γ ≥ 1 and 0 < δ < 1  a vector x ∈ ZS
solution if c(x) ≤ γ · c(x∗)  f (x) ≥ (1 − δ)α  and 0 ≤ x ≤ r1.

x∗:optimal solution

ρ :=

min

s∈{x∗} c(χs)

.

(5)

+ is a (γ  δ)-bicriteria approximate

Our main theorem is described below. We sketch the proof in Section 5.
Theorem 3.2. Algorithm 1 outputs a

(1 + 3)ρ

  δ

1 + log d
β

-bicriteria approximate solution

(cid:16) n

(cid:17)

in O

 log nrcmax
δcmin

log r

(cid:80)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

time.

4

3.1 Discussion
Integer-valued Case. Let us make a simple remark on the case that f is integer valued. Without
loss of generality  we can assume α ∈ Z+. Then  Algorithm 1 always returns a feasible solution for
any 0 < δ < 1/α. Therefore  our algorithm can be easily modiﬁed to an approximation algorithm
if f is integer valued.

c(χs)

c(χs|r1−χs)

+ → R+ is deﬁned as κ := 1 − mins∈S

Deﬁnition of Curvature. Several authors [5  19] use a different notion of curvature called the
total curvature  whose natural extension for a function over the integer lattice is as follows: The
total curvature κ of c : ZS
. Note that κ = 0
if c is modular  while ρ = 1 if c is modular. For example  Iyer and Bilmes [5] devised a bicriteria
approximation algorithm whose approximation guarantee is roughly O((1 − κ)−1 log β
d ).
Let us investigate the relation between ρ and κ for DR-submodular functions. One can show that
1 − κ ≤ ρ ≤ (1 − κ)−1 (see Lemma E.1 in Appendix)  which means that our bound in terms of ρ
is tighter than one in terms of (1 − κ)−1.
Comparison to Naive Reduction Algorithm.
If c is also a monotone DR-submodular function 
one can reduce (4) to the set version (1) as follows. For each s ∈ S  create r copies of s and let
˜S be the set of these copies. For ˜X ⊆ ˜S  deﬁne x ˜X ∈ ZS
+ be the integral vector such that x ˜X (s)
is the number of copies of s contained in ˜X. Then  ˜f ( ˜X) := f (x ˜X ) is submodular. Similarly 
˜c( ˜X) := c(x ˜X ) is also submodular if c is a DR-submodular function. Therefore we may apply a
standard greedy algorithm of [20  21] to the reduced problem and this is exactly what Greedy does
in our experiment (see Section 4). However  this straightforward reduction only yields a pseudo-
polynomial time algorithm since | ˜S| = nr; even if the original algorithm was linear  the resulting
algorithm would require O(nr) time. Indeed this difference is not negligible since r can be quite
large in practical applications  as illustrated by our experimental evaluation.

Lazy Evaluation. We ﬁnally note that we can combine the lazy evaluation technique [11  14] 
which signiﬁcantly reduces runtime in practice  with our algorithm. Speciﬁcally  we ﬁrst push all
the elements in S to a max-based priority queue. Here  the key of an element s ∈ S is f (χs)
c(χs) . Then
the inner loop of Algorithm 1 is modiﬁed as follows: Instead of checking all the elements in S 
we pop elements whose keys are at least θ. For each popped element s ∈ S  we ﬁnd k such that
0 < k ≤ r − x(s) with f (kχs|x)
kc(χs) ≥ θ with binary search. If there is such k  we update x with
x + kχs. Finally  we push s again with the key f (χs|x)
The correctness of this technique is obvious because of the DR-submodularity of f. In particular 
the key of each element s ∈ S in the queue is always at least f (χs|x)
c(χs)   where x is the current vector.
Hence  we never miss s ∈ S with f (kχs|x)

if x(s) < r.

c(χs)

kc(χs) ≥ θ.

4 Experiments

4.1 Experimental Setting

We conducted experiments on a Linux server with an Intel Xeon E5-2690 (2.90 GHz) processor and
256 GB of main memory. The experiments required  at most  4 GB of memory. All the algorithms
were implemented in C++ and compiled with g++ 4.6.3.
In our experiments  the cost function c : ZS

+ → R+ is always chosen as c(x) = (cid:107)x(cid:107)1 :=
+ → R+ be a submodular function and α be the worst quality guarantee.

s∈S x(s). Let f : ZS

(cid:80)

We implemented the following four methods:

• Decreasing-threshold is our method with the lazy evaluation technique. We chose δ =

0.01 as stated otherwise.

• Greedy is a method in which  starting from x = 0  we iteratively increment x(s) for s ∈ S
that maximizes f (x + χs) − f (x) until we get f (x) ≥ α. We also implemented the lazy
evaluation technique [11].

5

• Degree is a method in which we assign x(s) a value proportional to the marginal f (χs)−
f (0)  where (cid:107)x(cid:107)1 is determined by binary search so that f (x) ≥ α. Precisely speaking 
x(s) is approximately proportional to the marginal since x(s) must be an integer.
• Uniform is a method that returns k1 for minimum k ∈ Z+ such that f (k1) ≥ α.

We use the following real-world and synthetic datasets to conﬁrm the accuracy and efﬁciency of our
method against other methods. We set r = 100  000 for both problems.

Sensor placement. We used a dataset acquired by running simulations on a 129-vertex sensor
network used in Battle of the Water Sensor Networks (BWSN) [15]. We used the “bwsn-utilities” [1]
program to simulate 3000 random injection events to this network for a duration of 96 hours. Let S
and E be the set of the 129 sensors in the network and the set of the 3000 events  respectively. For
each sensor s ∈ S and event e ∈ E  a value z(s  e) is provided  which denotes the time  in minutes 
the pollution has reached s after the injection time.1
We deﬁne a function f : ZS
+ be a vector  where we regard x(s) as
the energy level of the sensor s. Suppose that when the pollution reaches a sensor s  the probability
that we can detect it is 1 − (1 − p)x(s)  where p = 0.0001. In other words  by spending unit energy 
we obtain an extra chance of detecting the pollution with probability p. For each event e ∈ E  let se
be the ﬁrst sensor where the pollution is detected in that injection event. Note that se is a random
variable. Let z∞ = max

+ → R+ as follows: Let x ∈ ZS

e∈E s∈S

z(s  e). Then  we deﬁne f as follows:
[z∞ − z(se  e)] 

f (x) = E
e∈E

E
se

where z(se  e) is deﬁned as z∞ when there is no sensor that managed to detect the pollution. In-
[z∞ − z(se  e)] expresses how much time we managed to save in the event e
tuitively speaking  E
se
on average. Then  we take the average over all the events. A similar function was also used in [11]
to measure the performance of a sensor allocation although they only considered the case p = 1.
This corresponds to the case that by spending unit energy at a sensor s  we can always detect the
pollution that has reached s. We note that f (x) is DR-submodular (see Lemma F.1 for the proof).

Budget allocation problem.
In order to observe the behavior of our algorithm for large-scale
instances  we created a synthetic instance of the budget allocation problem [2  17] as follows: The
instance can be represented as a bipartite graph (S  T ; E)  where S is a set of 5 000 vertices and T
is a set of 50 000 vertices. We regard a vertex in S as an ad source  and a vertex in T as a person.
Then  we ﬁx the degrees of vertices in S so that their distribution obeys the power law of γ := 2.5;
that is  the fraction of ad sources with out-degree d is proportional to d−γ. For a vertex s ∈ S of
the supposed degree d  we choose d vertices in T uniformly at random and connect them to s with
edges. We deﬁne a function f : ZS

(cid:16)
(cid:88)
1 − (cid:89)
+ → R+ as

f (x) =

(1 − p)x(s)(cid:17)

t∈T

s∈Γ(t)

 

(6)

where Γ(t) is the set of vertices connected to t and p = 0.0001. Here  we suppose that  by investing
a unit cost to an ad source s ∈ S  we have an extra chance of inﬂuencing a person t ∈ T with
s ∈ Γ(t) with probability p. Then  f (x) can be seen as the expected number of people inﬂuenced
by ad sources. We note that f is known to be a monotone DR-submodular function [17].

4.2 Experimental Results
Figure 1 illustrates the obtained objective value (cid:107)x(cid:107)1 for various choices of the worst quality guar-
antee α on each dataset. We chose  = 0.01 in Decreasing threshold. We can observe that De-
creasing threshold attains almost the same objective value as Greedy  and it outperforms Degree
and Uniform.
Figure 2 illustrates the runtime for various choices of the worst quality guarantee α on each dataset.
We chose  = 0.01 in Decreasing threshold. We can observe that the runtime growth of Decreas-
ing threshold is signiﬁcantly slower than that of Greedy.

1Although three other values are provided  they showed similar empirical results and we omit them.

6

(a) Sensor placement (BWSN)

(a) Sensor placement (BWSN)

(a) Relative cost increase

(b) Budget allocation (synthetic)

(b) Budget allocation (synthetic)

(b) Runtime

Figure 1: Objective values

Figure 2: Runtime

Figure 3: Effect of 

Figures 3(a) and 3(b) show the relative increase of the objective value and the runtime  respectively 
of our method against Greedy on the BWSN dataset. We can observe that the relative increase of the
objective value gets smaller as α increases. This phenomenon can be well explained by considering
the extreme case that α = max f (r1). In this case  we need to choose x = r1 anyway in order to
achieve the worst quality guarantee  and the order of increasing coordinates of x does not matter.
Also  we can see that the empirical runtime grows as a function of 1
   which matches our theoretical
bound.

5 Proof of Theorem 3.2

In this section  we outline the proof of the main theorem. Proofs of some minor claims can be found
in Appendix.
First  we introduce a notation. Let us assume that x is updated L times in the algorithm. Let xi be
the variable x after the i-th update (i = 0  . . .   L). Note that x0 = 0 and xL is the ﬁnal output of
the algorithm. Let si ∈ S and ki ∈ Z+ be the pair used in the i-th update for i = 1  . . .   L; that is 
xi = xi−1 + kiχsi for i = 1  . . .   L. Let µ0 := 0 and µi := kic(χsi )
f (kiχsi|xi−1) for i = 1  . . .   L. Let
ˆµ0 := 0 and ˆµi := θ−1
for i = 1  . . .   L  where θi is the threshold value on the i-th update. Note that

ˆµi−1 ≤ ˆµi for i = 1  . . .   L. Let x∗ be an optimal solution such that ρ · c(x∗) =(cid:80)

s∈{x∗} c(χs).
We regard that in the i-th update  the elements of {x∗} are charged by the value of µi(f (χs |
xi−1) − f (χs | xi)). Then  the total charge on {x∗} is deﬁned as

i

T (x  f ) :=

µi(f (χs | xi−1) − f (χs | xi)).

(cid:88)

L(cid:88)

s∈{x∗}

i=1

Claim 5.1. Let us ﬁx 1 ≤ i ≤ L arbitrary and let θ be the threshold value on the i-th update. Then 

f (kiχsi | xi−1)

kic(χsi)

≥ θ

and

f (χs | xi−1)

c(χs)

≤ θ
1 − 

(s ∈ S).

Eliminating θ from the inequalities in Claim 5.1  we obtain

kic(χsi)

f (kiχsi | xi−1)

≤ 1
1 − 

c(χs)

f (χs | xi−1)

(i = 1  . . .   L 

s ∈ S)

(7)

7

050010001500200025003000®050001000015000200002500030000Objective valueUniformDecreasing thresholdDegreeGreedy05000100001500020000®0.00.51.01.52.02.5Objective value1e8GreedyDecreasing thresholdDegreeUniform050010001500200025003000®10-210-1100101102103104time (s)UniformDecreasing thresholdDegreeGreedy05000100001500020000®10-210-1100101102103104time (s)GreedyDecreasing thresholdDegreeUniform050010001500200025003000®010-310-210-1100101102103Relative increase of the objective value1.00.10.010.0010.0001050010001500200025003000®10-1100101102103104time (s)1.00.10.010.0010.0001Greedy1− µi for i = 1  . . .   L.

Furthermore  we have µi ≤ ˆµi ≤ 1
Claim 5.2. c(x) ≤ 1
Claim 5.3. For each s ∈ {x∗}  the total charge on s is at most
Proof. Let us ﬁx s ∈ {x∗} and let l be the minimum i such that f (χs | xi) = 0. By (7)  we have

1− (1 + log(d/β))c(χs).

1− T (x  f ).

1

µi =

kic(χsi )

f (kiχsi | xi−1)

≤ 1
1 − 

·

c(χs)

f (χs | xi−1)

.

(i = 1  . . .   l)

µi(f (χs | xi−1) − f (χs | xi)) =

µi(f (χs | xi−1) − f (χs | xi)) + µlf (χs | xl−1)

l−1(cid:88)

i=1

(cid:17)

f (χs | xl−1)
f (χs | xl−1)

+

Then  we have

L(cid:88)

c(χs)

c(χs)

i=1

(cid:16) l−1(cid:88)
(cid:16)
(cid:16)
(cid:16)

1 +

c(χs)

1 +

c(χs)

1 + log

(f (χs | xi−1) − f (χs | xi))

(cid:16)
l−1(cid:88)
l−1(cid:88)

i=1

i=1

(cid:17)(cid:17)

f (χs | xi−1)
1 − f (χs | xi)
f (χs | xi−1)
(cid:17)
f (χs | xi−1)
(cid:17) ≤ 1
f (χs | xi)

log
f (χs | x0)
f (χs | xl−1)

1 − 

i=1

≤ 1
1 − 

≤ 1
1 − 

≤ 1
1 − 
1
1 − 

=

Proof of Theorem 3.2. Combining these claims  we have
c(x) ≤ 1
1 − 

(1 − )2 ·

· T (x  f ) ≤

1 + log

d
β

1

c(χs) ≤ (1 + 3) ·

(cid:18)

1 + log

(cid:16)
(cid:19)
· (cid:88)

s∈{x∗}

(since 1 − 1/x ≤ log x for x ≥ 1)

c(χs)

(cid:17)

d
β

(cid:18)

1 + log

(cid:19)

d
β

· ρc(x∗).

Thus  x is an approximate solution with the desired ratio.
Let us see that x approximately satisﬁes the constraint; that is  f (x) ≥ (1 − δ)α. We will now
consider a slightly modiﬁed version of the algorithm; in the modiﬁed algorithm  the threshold is
updated until f (x) = α. Let x(cid:48) be the output of the modiﬁed algorithm. Then  we have

f (x(cid:48)) − f (x) ≤ (cid:88)

f (χs | x) ≤ (cid:88)

s∈{x(cid:48)}

s∈{x(cid:48)}

δc(χs)
cmaxnr

d ≤ δd ≤ δα

The third inequality holds since c(χs) ≤ cmax and |{x(cid:48)}| ≤ nr. Thus f (x) ≥ (1 − δ)α.

6 Conclusions

In this paper  motivated by real scenarios in machine learning  we generalized the submodular cover
problem via the diminishing return property over the integer lattice. We proposed a bicriteria ap-
proximation algorithm with the following properties: (i) The approximation ratio to the cost almost
matches the one guaranteed by the greedy algorithm [21] and is almost tight in general. (ii) We can
satisfy the worst solution quality with the desired accuracy. (iii) The running time of our algorithm
is roughly O(n log n log r). The dependency on r is exponentially better than that of the greedy al-
gorithm. We conﬁrmed by experiment that compared with the greedy algorithm  the solution quality
of our algorithm is almost the same and the runtime is several orders of magnitude faster.

Acknowledgments

The ﬁrst author is supported by JSPS Grant-in-Aid for JSPS Fellows. The second author is supported
by JSPS Grant-in-Aid for Young Scientists (B) (No. 26730009)  MEXT Grant-in-Aid for Scientiﬁc
Research on Innovative Areas (24106003)  and JST  ERATO  Kawarabayashi Large Graph Project.
The authors thank Satoru Iwata and Yuji Nakatsukasa for reading a draft of this paper.

8

References
[1] http://www.water-simulation.com/wsp/about/bwsn/.
[2] N. Alon  I. Gamzu  and M. Tennenholtz. Optimizing budget allocation among channels and

inﬂuencers. In Proc. of WWW  pages 381–388  2012.

[3] A. Badanidiyuru and J. Vondr´ak. Fast algorithms for maximizing submodular functions. In

Proc. of SODA  pages 1497–1514  2014.

[4] Y. Chen  H. Shioi  C. A. F. Montesinos  L. P. Koh  S. Wich  and A. Krause. Active detection

via adaptive submodularity. In Proc. of ICML  pages 55–63  2014.

[5] R. Iyer and J. Bilmes. Submodular optimization with submodular cover and submodular knap-

sack constraints. In Proc. of NIPS  pages 2436–2444  2013.

[6] M. Kapralov  I. Post  and J. Vondrak. Online submodular welfare maximization: Greedy is

optimal. In Proc. of SODA  pages 1216–1225  2012.

[7] D. Kempe  J. Kleinberg  and E. Tardos. Maximizing the spread of inﬂuence through a social

network. In Proc. of KDD  pages 137–146  2003.

[8] A. Krause and D. Golovin. Submodular function maximization.

In Tractability: Practical

Approaches to Hard Problems  pages 71–104. Cambridge University Press  2014.

[9] A. Krause and J. Leskovec. Efﬁcient sensor placement optimization for securing large water
distribution networks. Journal of Water Resources Planning and Management  134(6):516–
526  2008.

[10] A. Krause  A. Singh  and C. Guestrin. Near-optimal sensor placements in gaussian processes:
Theory  efﬁcient algorithms and empirical studies. The Journal of Machine Learning Research 
9:235–284  2008.

[11] J. Leskovec  A. Krause  C. Guestrin  C. Faloutsos  J. VanBriesen  and N. Glance. Cost-effective

outbreak detection in networks. In Proc. of KDD  pages 420–429  2007.

[12] H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submod-
ular functions. In Proceedings of the Annual Conference of the North American Chapter of the
Association for Computational Linguistics  pages 912–920  2010.

[13] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In Proc.

of NAACL  pages 510–520  2011.

[14] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Opti-

mization Techniques  Lecture Notes in Control and Information Sciences  7:234–243  1978.

[15] A. Ostfeld  J. G. Uber  E. Salomons  J. W. Berry  W. E. Hart  C. A. Phillips  J.-P. Watson 
G. Dorini  P. Jonkergouw  Z. Kapelan  F. di Pierro  S.-T. Khu  D. Savic  D. Eliades  M. Polycar-
pou  S. R. Ghimire  B. D. Barkdoll  R. Gueli  J. J. Huang  E. A. McBean  W. James  A. Krause 
J. Leskovec  S. Isovitsch  J. Xu  C. Guestrin  J. VanBriesen  M. Small  P. Fischbeck  A. Preis 
M. Propato  O. Piller  G. B. Trachtman  Z. Y. Wu  and T. Walski. The battle of the water
sensor networks (BWSN): A design challenge for engineers and algorithms. Journal of Water
Resources Planning and Management  134(6):556–568  2008.

[16] R. Raz and S. Safra. A sub-constant error-probability low-degree test  and a sub-constant

error-probability PCP characterization of NP. In Proc. of STOC  pages 475–484  1997.

[17] T. Soma  N. Kakimura  K. Inaba  and K. Kawarabayashi. Optimal budget allocation: Theoret-

ical guarantee and efﬁcient algorithm. In Proc. of ICML  2014.

[18] H. O. Song  R. Girshick  S. Jegelka  J. Mairal  Z. Harchaoui  and T. Darrell. On learning to

localize objects with minimal supervision. In Proc. of ICML  2014.

[19] M. Sviridenko  J. Vondr´ak  and J. Ward. Optimal approximation for submodular and super-

modular optimization with bounded curvature. In Proc. of SODA  pages 1134–1148  2015.

[20] P.-J. Wan  D.-Z. Du  P. Pardalos  and W. Wu. Greedy approximations for minimum submodular
cover with submodular cost. Computational Optimization and Applications  45(2):463–474 
2009.

[21] L. A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem.

Combinatorica  2(4):385–393  1982.

9

,Wojciech Samek
Duncan Blythe
Klaus-Robert Müller
Motoaki Kawanabe
Deeparnab Chakrabarty
Prateek Jain
Pravesh Kothari
Tasuku Soma
Yuichi Yoshida