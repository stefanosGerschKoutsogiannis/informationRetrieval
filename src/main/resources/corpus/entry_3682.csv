2017,Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem,We study minimax strategies for the online prediction problem with expert advice. It has been conjectured that a simple adversary strategy  called COMB  is near optimal in this game for any number of experts. Our results and new insights make progress in this direction by showing that  up to a small additive term  COMB is minimax optimal in the finite-time three expert problem. In addition  we provide for this setting a new near minimax optimal COMB-based learner. Prior to this work  in this problem  learners obtaining the optimal multiplicative constant in their regret rate were known only when $K=2$ or $K\rightarrow\infty$. We characterize  when $K=3$  the regret of the game scaling as $\sqrt{8/(9\pi)T}\pm \log(T)^2$ which gives for the first time the optimal constant in the leading ($\sqrt{T}$) term of the regret.,Near Minimax Optimal Players for the Finite-Time

3-Expert Prediction Problem

Yasin Abbasi-Yadkori

Adobe Research

Peter L. Bartlett

UC Berkeley

Victor Gabillon

Queensland University of Technology

Abstract

We study minimax strategies for the online prediction problem with expert advice.
It has been conjectured that a simple adversary strategy  called COMB  is near
optimal in this game for any number of experts. Our results and new insights make
progress in this direction by showing that  up to a small additive term  COMB is
minimax optimal in the ﬁnite-time three expert problem. In addition  we provide
for this setting a new near minimax optimal COMB-based learner. Prior to this
work  in this problem  learners obtaining the optimal multiplicative constant in
their regret rate were known only when K = 2 or K → ∞. We characterize  when
K = 3  the regret of the game scaling as�8/(9π)T ± log(T )2 which gives for
the ﬁrst time the optimal constant in the leading (√T ) term of the regret.

1

Introduction

This paper studies the online prediction problem with expert advice. This is a fundamental problem
of machine learning that has been studied for decades  going back at least to the work of Hannan [12]
(see [4] for a survey). As it studies prediction under adversarial data the designed algorithms are
known to be robust and are commonly used as building blocks of more complicated machine learning
algorithms with numerous applications. Thus  elucidating the yet unknown optimal strategies has the
potential to signiﬁcantly improve the performance of these higher level algorithms  in addition to
providing insight into a classic prediction problem. The problem is a repeated two-player zero-sum
game between an adversary and a learner. At each of the T rounds  the adversary decides the
quality/gain of K experts’ advice  while simultaneously the learner decides to follow the advice of
one of the experts. The objective of the adversary is to maximize the regret of the learner  deﬁned as
the difference between the total gain of the learner and the total gain of the best ﬁxed expert.
Open Problems and our Main Results. Previously this game has been solved asymptotically as
both T and K tend to ∞: asymptotically the upper bound on the performance of the state-of-the-
art Multiplicative Weights Algorithm (MWA) for the learner matches the optimal multiplicative
constant of the asymptotic minimax optimal regret rate�(T /2) log K [3]. However  for ﬁnite K 
al. [10] proved a matching lower bound�(T /2) log K on the regret of the classic version of MWA 
Cover [5] proved that the value of the game is of order of�T /(2π) when K = 2  meaning that the

regret of a MWA learner is 47% larger that the optimal learner in this case. Therefore the question of
optimality remains open for non-asymptotic K which are the typical cases in applications.
In studying a related setting with K = 3  where T is sampled from a geometric distribution with
parameter δ  Gravin et al. [9] conjectured that  for any K  a simple adversary strategy  called
the COMB adversary  is asymptotically optimal (T → ∞  or when δ → 0)  and also excessively
competitive for ﬁnite-time ﬁxed T . The COMB strategy sorts the experts based on their cumulative
gains and  with probability one half  assigns gain one to each expert in an odd position and gain zero

this asymptotic quantity actually overestimates the ﬁnite-time value of the game. Moreover  Gravin et

additionally showing that the optimal learner does not belong an extended MWA family. Already 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

to each expert in an even position. With probability one half  the zeros and ones are swapped. The
simplicity and elegance of this strategy  combined with its almost optimal performance makes it very
appealing and calls for a more extensive study of its properties.
Our results and new insights make progress in this direction by showing that  for any ﬁxed T and up to
small additive terms  COMB is minimax optimal in the ﬁnite-time three expert problem. Additionally
and with similar guarantees  we provide for this setting a new near minimax optimal COMB-based
learner. For K = 3  the regret of a MWA learner is 39% larger than our new optimal learner1. In
this paper we also characterize  when K = 3  the regret of the game as�8/(9π)T ± log(T )2 which
gives for the ﬁrst time the optimal constant in the leading (√T ) term of the regret. Note that the
state-of-the-art non-asymptotic lower bound in [15] on the value of this problem is non informative
as the lower bound for the case of K = 3 is a negative quantity.
Related Works and Challenges. For the case of K = 3  Gravin et al. [9] proved the exact minimax
optimality of a COMB-related adversary in the geometrical setting  i.e. where T is not ﬁxed in advance
but rather sampled from a geometric distribution with parameter δ. However the connection between
the geometrical setting and the original ﬁnite-time setting is not well understood  even asymptotically
(possibly due to the large variance of geometric distributions with small δ). Addressing this issue  in
Section 7 of [8]  Gravin et al. formulate the “Finite vs Geometric Regret” conjecture which states
that the value of the game in the geometrical setting  Vα  and the value of the game in the ﬁnite-time
setting  VT   verify VT = 2√π Vα=1/T . We resolve here the conjecture for K = 3.
Analyzing the ﬁnite-time expert problem raises new challenges compared to the geometric setting. In
the geometric setting  at any time (round) t of the game  the expected number of remaining rounds
before the end of the game is constant (does not depend on the current time t). This simpliﬁes the
problem to the point that  when K = 3  there exists an exactly minimax optimal adversary that
ignores the time t and the parameter δ. As noted in [9]  and noticeable from solving exactly small
instances of the game with a computer  in the ﬁnite-time case  the exact optimal adversary seems to
depend in a complex manner on time and state. It is therefore natural to compromise for a simpler
adversary that is optimal up to a small additive error term. Actually  based on the observation of the
restricted computer-based solutions  the additive error term of COMB seems to vanish with larger T .
Tightly controlling the errors made by COMB is a new challenge with respect to [9]  where the
solution to the optimality equations led directly to the exact optimal adversary. The existence of such
equations in the geometric setting crucially relies on the fact that the value-to-go of a given policy in
a given state does not depend on the current time t (because geometric distributions are memoryless).
To control the errors in the ﬁnite-time setting  our new approach solves the game by backward
induction showing the approximate greediness of COMB with respect to itself (read Section 2.1 for
an overview of our new proof techniques and their organization). We use a novel exchangeability
property  new connections to random walks and a close relation that we develop between COMB and
a TWIN-COMB strategy. Additional connections with new related optimal strategies and random
walks are used to compute the value of the game (Theorem 2). We discuss in Section 6 how our new
techniques have more potential to extend to an arbitrary number of arms  than those of [9].
Additionally  we show how the approximate greediness of COMB with respect to itself is key to
proving that a learner based directly on the COMB adversary is itself quasi-minimax-optimal. This is
the ﬁrst work to extend to the approximate case  approaches used to designed exactly optimal players
in related works. In [2] a probability matching learner is proven optimal under the assumption that the
adversary is limited to a ﬁxed cumulative loss for the best expert. In [14] and [1]  the optimal learner
relies on estimating the value-to-go of the game through rollouts of the optimal adversary’s plays.
The results in these papers were limited to games where the optimal adversary was only playing
canonical unit vector while our result holds for general gain vectors. Note also that a probability
matching learner is optimal in [9].
Notation: Let [a : b] = {a  a + 1  . . .   b} with a  b ∈ N  a ≤ b  and [a] = [1 : a]. For a vector
w ∈ Rn  n ∈ N  �w�∞
= maxk∈[n]|wk|. A vector indexed by both a time t and a speciﬁc
element index k is wt k. An undiscounted Markov Decision Process (MDP) [13  16] M is a 4-tuple
�S A  r  p�. S is the state space  A is the set of actions  r : S × A → R is the reward function  and
the transition model p(·|s  a) gives the probability distribution over the next state when action a is
taken in state s. A state is denoted by s or st if it is taken at time t. An action is denoted by a or at.

1[19] also provides an upper-bound that is suboptimal when K = 3 even after optimization of its parameters.

2

2 The Game

We consider a game  composed of T rounds  between two players  called a learner and an adversary.
At each time/round t the learner chooses an index It ∈ [K] from a distribution pt on the K arms.
Simultaneously  the adversary assigns a binary gain to each of the arms/experts  possibly at random
from a distribution ˙At  and we denote the vector of these gains by gt ∈ {0  1}K. The adversary and
the learner then observe It and gt. For simplicity we use the notation g[t] = (gs)s=1 ... t. The value
of one realization of such a game is the cumulative regret deﬁned as

RT =�����

T�t=1

gt�����∞

−

T�t=1

gt It .

A state s ∈ S = (N ∪ {0})K is a K-dimensional vector such that the k-th element is the cumulative
sum of gains dealt by the adversary on arm k before the current time t. Here the state does not include
t but is typically denoted for a speciﬁc time t as st and computed as st =�t−1
t�=1 gt�. This deﬁnition
is motivated by the fact that there exist minimax strategies for both players that rely solely on the
state and time information as opposed to the complete history of plays  g[t] ∪ I[t]. In state s  the set
of leading experts  i.e.  those with maximum cumulative gain  is X(s) = {k ∈ [K] : sk = �s�∞}.
We use π to denote the (possibly non-stationary) strategy/policy used by the adversary  i.e.  for any
input state s and time t it outputs the gain distribution π(s  t) played by the adversary at time t in
state s. Similarly we use ¯p to denote the strategy of the learner. As the state depends only on the
adversary plays  we can sample a state s at time t from π.
Given an adversary π and a learner ¯p 
is
¯p π = Eg[T ]∼π I[T ]∼ ¯p [RT ] . The learner tries to minimize the expected regret while the adversary
V T
tries to maximize it. The value of the game is the minimax value VT deﬁned by

the expected regret of

the game  V T

¯p π 

VT = min

¯p

max

π

V T
¯p π = max

π

min

¯p

V T
¯p π.

In this work  we are interested in the search for optimal minimax strategies  which are adversary
strategies π∗ such that VT = min ¯p V T

¯p π∗ and learner strategies ¯p∗  such that VT = maxπ V T

¯p∗ π.

2.1 Summary of our Approach to Obtain the Near Greediness of COMB

Most of our material is new. First  Section 3 recalls that Gravin et al. [9] have shown that the search
for the optimal adversary π∗ can be restricted to the ﬁnite family of balanced strategies (deﬁned in
the next section). When K = 3  the action space of a balanced adversary is limited to seven stochastic
actions (gain distributions)  denoted by ˙B3 = { ˙W  ˙C  ˙V  ˙1  ˙2 {} {123}} (see Section 5.1 for their
description). The COMB adversary repeats the gain distribution ˙C at each time and in any state.
In Section 4 we provide an explicit formulation of the problem as ﬁnding π∗ inside an MDP with
a speciﬁc reward function. Interestingly  we observe that another adversary  which we call TWIN-
COMB and denote by πW  which repeats the distribution ˙W  has the same value as πC (Section 5.1).
To control the errors made by COMB  the proof uses a novel and intriguing exchangeability property
(Section 5.2). This exchangeability property holds thanks to the surprising role played by the TWIN-
COMB strategy. For any distributions ˙A ∈ ˙B3 there exists a distribution ˙D  mixture of ˙C and ˙W 
such that for almost all states  playing ˙A and then ˙D is the same as playing ˙W and then ˙A in terms of
the expected reward and the probabilities over the next states after these two steps. Using Bellman
operators  this can be concisely written as: for any (value) function f : S −→ R  in (almost) any
state s  we have that [T ˙A[T ˙Df ]](s) = [T ˙W[T ˙Af ]](s). We solve the MDP with a backward induction
in time from t = T . We show that playing ˙C at time t is almost greedy with respect to playing πC in
later rounds t� > t. The greedy error is deﬁned as the difference of expected reward between always
playing πC and playing the best (greedy) ﬁrst action before playing COMB. Bounding how these
errors accumulate through the rounds relates the value of COMB to the value of π∗ (Lemma 16).
To illustrate the main ideas  let us ﬁrst make two simplifying (but unrealistic) assumptions at time t:
COMB has been proven greedy w.r.t. itself in rounds t� > t and the exchangeability holds in all states.
Then we would argue at time t that by the exchangeability property  instead of optimizing the greedy

3

˙A ˙C . . . ˙C  we can study the optimizer of max ˙A∈ ˙B3

˙W ˙A ˙C . . . ˙C. Then
action w.r.t. COMB as max ˙A∈ ˙B3
we use the induction property to conclude that ˙C is the solution of the previous optimization problem.
Unfortunately  the exchangeability property does not hold in one speciﬁc state denoted by sα. What
saves us though is that we can directly compute the error of greediﬁcation of any gain distribution
with respect to COMB in sα and show that it diminishes exponentially fast as T − t  the number of
rounds remaining  increases (Lemma 7). This helps us to control how the errors accumulate during
the induction. From one given state st �= sα at time t  ﬁrst  we use the exchangeability property once
when trying to assess the ‘quality’ of an action ˙A as a greedy action w.r.t. COMB. This leads us to
consider the quality of playing ˙A in possibly several new states {st+1} at time t + 1 reached following
TWIN-COMB in s. We use our exchangeability property repeatedly  starting from the state st until a
subsequent state reaches sα  say at time tα  where we can substitute the exponentially decreasing
greedy error computed at this time tα in sα. Here the subsequent states are the states reached after
having played TWIN-COMB repetitively starting from the state st. If sα is never reached we use
the fact that COMB is an optimal action everywhere else in the last round. The problem is then to
determine at which time tα  starting from any state at time t and following a TWIN-COMB strategy 
we hit sα for the ﬁrst time. This is translated into a classical gambler’s ruin problem  which concerns
the hitting times of a simple random walk (Section 5.3). Similarly the value of the game is computed
using the study of the expected number of equalizations of a simple random walk (Theorem 5.1).

3 Solving for the Adversary Directly

In this section  we recall the results from [9] that  for arbitrary K  permit us to directly search for the
minimax optimal adversary in the restricted set of balanced adversaries while ignoring the learner.
Deﬁnition 1. A gain distribution ˙A is balanced if there exists a constant c ˙A  the mean gain of ˙A  such
that ∀k ∈ [K]  c ˙A = Eg| ˙A [gk]. A balanced adversary uses exclusively balanced gain distributions.
Lemma 1 (Claim 5 in [9]). There exists a minimax optimal balanced adversary.

Use B to denote the set of all balanced strategies and ˙B to denote the set of all balanced gain
distributions. Interestingly  as demonstrated in [9]  a balanced adversary π inﬂicts the same regret
on every learner: If π ∈ B  then ∃V π
T . (See Lemma 10) Therefore  given an
adversary strategy π  we can deﬁne the value-to-go V π
t0 (s) associated with π from time t0 in state s 

¯p π = V π

V π
t0 (s) = E

sT +1 �sT +1�∞ −

st+1 ∼ P (.|st  π(st  t)  st0 = s).

T ∈ R : ∀ ¯p  V T
st�cπ(st t)�  

T�t=t0

E

Another reduction comes from the fact that the set of balanced gain distributions can be seen as a
convex combination of a ﬁnite set of balanced distributions [9  Claim 2 and 3]. We call this limited
set the atomic gain distributions. Therefore the search for π∗ can be limited to this set. The set of
convex combinations of the m distributions ˙A1  . . . ˙Am is denoted by Δ( ˙A1  . . . ˙Am).

4 Reformulation as a Markovian Decision Problem

In this section we formulate  for arbitrary K  the maximization problem over balanced adversaries
as an undiscounted MDP problem �S A  r  p�. The state space S was deﬁned in Section 2 and the
action space is the set of atomic balanced distributions as discussed in Section 3. The transition
model is deﬁned by p(.|s  ˙D)  which is a probability distribution over states given the current state
s and a balanced distribution over gains ˙D. In this model  the transition dynamics are deterministic
and entirely controlled by the adversary’s action choices. However  the adversary is forced to choose
stochastic actions (balanced gain distributions). The maximization problem can therefore also be
thought of as designing a balanced random walk on states so as to maximize a sum of rewards (that
are yet to be deﬁned). First  we deﬁne P ˙A the transition probability operator with respect to a gain
distribution ˙A. Given function f : S −→ R  P ˙A returns

[P ˙Af ](s) = E[f (s�)|s� ∼ p(.|s  ˙A)] = E
g∼s  ˙A

[f (s + g)].

g is sampled in s according to ˙A. Given ˙A in s  the per-step regret is denoted by r ˙A(s) and deﬁned as

r ˙A(s) = E

s�|s  ˙A �s��∞ − �s�∞ − c ˙A.

4

t=t0

t0 (s) = �T

Given an adversary strategy π  starting in s at time t0 
E�rπ(· t)(st) | st+1 ∼ p(.|st  π(st  t)  st0 = s)�. The action-value function of π
¯V π
at (s  ˙D) and t is the expected sum of rewards received by starting from s  taking action ˙D  and then
t (st  ˙D) = E [�T
following π: ¯Qπ
t�=t r ˙At (st) | ˙A0 = ˙D  st+1 ∼ p(·|st  ˙At)  ˙At+1 = π(st+1  t + 1)].
The Bellman operator of ˙A  T ˙A  is [T ˙Af ](s) = r ˙A(s) + [P ˙Af ](s). with [Tπ(s t)
t (s).
This per-step regret  r ˙A(s)  depends on s and ˙A and not on the time step t. Removing the time
from the picture permits a simpliﬁed view of the problem that leads to a natural formulation of the
exchangeability property that is independent of the time t. Crucially  this decomposition of the regret
into per-step regrets is such that maximizing ¯V π
t0 (s) over adversaries π is equivalent  for all time t0
t0 (s) (Lemma 2).
and s  to maximizing over adversaries the original value of the game  the regret V π
t0 (s) = ¯V π
Lemma 2. For any adversary strategy π and any state s and time t0  V π
.
t0 (s) + �s�∞

the cumulative per-step regret is

¯V π
t+1](s) = ¯V π

The proof of Lemma 2 is in Section 8. In the following  our focus will be on maximizing ¯V π
t (s) in
any state s. We now show some basic properties of the per-step regret that holds for an arbitrary
number of experts K and discuss their implications. The proofs are in Section 9.
Lemma 3. Let ˙A ∈ ˙B  for all s  t   we have 0 ≤ r ˙A(s) ≤ 1. Furthermore if |X(s)|= 1  r ˙A(s) = 0.
Lemma 3 shows that a state s in which the reward is not zero contains at least two equal leading
experts  |X(s)|> 1. Therefore the goal of maximizing the reward can be rephrased into ﬁnding a
policy that visits the states with |X(s)|> 1 as often as possible  while still taking into account that the
per-step reward increases with |X(s)|. The set of states with |X(s)|> 1 is called the ‘reward wall’.
Lemma 4. In any state s  with |X(s)|= 2  for any balanced gain distribution ˙D such that with
probability one exactly one of the leading expert receives a gain of 1  r ˙D(s) = max ˙A∈ ˙B r ˙A(s).
5 The Case of K = 3

5.1 Notations in the 3-Experts Case  the COMB and the TWIN-COMB Adversaries

First we deﬁne the state space in the 3-expert case. The experts are sorted with respect to their
cumulative gains and are named in decreasing order  the leading expert  the middle expert and the
lagging expert. As mentioned in [9]  in our search for the minimax optimal adversary  it is sufﬁcient
for any K to describe our state only using dij that denote the difference between the cumulative gains
of consecutive sorted experts i and j = i + 1. Here  i denotes the expert with ith largest cumulative
gains  and hence dij ≥ 0 for all i < j. Therefore one notation for a state  that will be used throughout
this section  is s = (x  y) = (d12  d23). We distinguish four types of states C1  C2  C3  C4 as
detailed below in Figure 1. In the same ﬁgure  in the center  the states are represented on a 2d-grid.
C4 contains only the state denoted sα = (0  0).

s ∈ C1  d12 > 0  d23 > 0
s ∈ C2  d12 = 0  d23 > 0
s ∈ C3  d12 > 0  d23 = 0
s ∈ C4  d12 = 0  d23 = 0

Atomic ˙A
{1}{23}
{2}{13}
{3}{12}
{1}{2}{3}
{12}{13}{23}

Symbol

˙W
˙C
˙V
˙1
˙2

c ˙A
1/2
1/2
1/2
1/3
2/3

Figure 1: 4 types of states (left)  their location on the 2d grid of states (center) and 5 atomic ˙A (right)

Concerning the action space  the gain distributions use brackets. The group of arms in the same bracket
receive gains together and each group receive gains with equal probability. For instance  {1}{2}{3}
exclusively deals a gain to expert 1 (leading expert) with probability 1/3  expert 2 (middle expert)
with probability 1/3  and expert 3 (lagging expert) with probability 1/3  whereas {1}{23} means
dealing a gain to expert 1 alone with probability 1/2 and experts 2 and 3 together with probability 1/2.
As discussed in Section 3  we are searching for a π∗ using mixtures of atomic balanced distributions.
When K = 3 there are seven atomic distributions  denoted by ˙B3 = { ˙V  ˙1  ˙2  ˙C  ˙W {} {123}}
and described in Figure 1 (right). Moreover  in Figure 2  we report in detail—in a table (left) and

5

d12d234333211121112111Reward Walls

C1
C2
C3
C4

r ˙C(s)

0
1/2
0
1/2

Distribution of next
state s� ∼ p(·|s  ˙C)
with s = (x  y)
P (s� = (x−1  y+1)) = P (s� = (x+1  y−1)) = .5
P (s� = (x + 1  y)) = P (s� = (x + 1  y − 1)) = .5
P (s� = (x  y + 1)) = P (s� = (x − 1  y + 1)) = .5
P (s� = (x  y + 1)) = P (s� = (x + 1  y)) = .5

Figure 2: The per-step regret and transition probabilities of the gain distribution ˙C

an illustration (right) on the 2-D state grid—the properties of the COMB gain distribution ˙C. The
remaining atomic distributions are similarly reported in the appendix in Figures 5 to 8.
In the case of three experts  the COMB distribution is simply playing {2}{13} in any state. We use
˙W to denote the strategy that plays {1}{23} in any state and refer to it as the TWIN-COMB strategy.
The COMB and TWIN-COMB strategies (as opposed to the distributions) repeat their respective gain
distributions in any state and any time. They are respectively denoted πC  πW. The Lemma 5 shows
that the COMB strategy πC  the TWIN-COMB strategy πW and therefore any mixture of both  have
the same expected cumulative per-step regret. The proof is reported to Section 11.
Lemma 5. For all states s at time t  we have ¯V πC

(s) = ¯V πW

t

(s).

t

5.2 The Exchangeability Property
Lemma 6. Let ˙A ∈ ˙B3  there exists ˙D ∈ Δ( ˙C  ˙W) such that for any s �= sα  and for any f : S −→ R 

[T ˙A[T ˙Df ]](s) = [T ˙W[T ˙Af ]](s).

Proof. If ˙A = ˙W  ˙A = {} or ˙A = {123}  use ˙D = ˙W. If ˙A = ˙C  use Lemma 11 and 12.
Case 1. ˙A = ˙V: ˙V is equal to ˙C in C3 ∪ C4 and if s� ∼ p(.|s  ˙W) with s ∈ C3 then s� ∈ C3 ∪ C4.
So when s ∈ C3 we reuse the case ˙A = ˙C above. When s ∈ C1 ∪ C2  we consider two cases.
Case 1.1. s �= (0  1): We choose ˙D = ˙W which is {1}{23}. If s� ∼ p(.|s  ˙V) with s ∈ C2 then
s� ∈ C2. Similarly  if s� ∼ p(.|s  ˙V) with s ∈ C1 then s� ∈ C1 ∪ C3. Moreover ˙D modiﬁes
similarly the coordinates (d12  d23) of s ∈ C1 and s ∈ C3. Therefore the effect in terms of transition
probability and reward of ˙D is the same whether it is done before or after the actions chosen by ˙V. If
s� ∼ p(.|s  ˙D) with s ∈ C1 ∪ C2 then s� ∈ C1 ∪ C2. Moreover ˙V modiﬁes similarly the coordinates
(d12  d23) of s ∈ C1 and s ∈ C2. Therefore the effect in terms of the transition probability of ˙V is
the same whether it is done before or after the action ˙D. In terms of reward  notice that in the states
s ∈ C1 ∪ C2  ˙V has 0 per-step regret and using ˙V does not make s� leave or enter the reward wall.
Case 1.2 st = (0  1): We can chose ˙D = ˙W. One can check from the tables in Figures 7 and 8 that
exchangebility holds. Additionally we provide an illustration of the exchangeability equality in the
2d-grid in Figure 1. The starting state s = (0  1)  is graphically represented by . We show on the
grid the effect of the gain distribution ˙V (in dashed red) followed (left picture) or preceded (right
picture) by the gain distribution ˙D (in plain blue). The illustration shows that ˙V· ˙D and ˙D· ˙V lead to
the same ﬁnal states (
) with equal probabilities. The rewards are displayed on top of the pictures.
Their color corresponds to the actions  the probabilities are in italic  and the rewards are in roman.

Case 2 & 3. ˙A = ˙1 & ˙A = ˙2: The proof is similar and is reported in Section 12 of the appendix.

6

4.52.5.53.5.5.51.5.5001/21/2d12d235.3 Approximate Greediness of COMB  Minimax Players and Regret

The greedy error of the gain distribution ˙D in state s at time t is
¯QπC
t (s  ˙A) − ¯QπC

� ˙D
s t = max
˙A∈ ˙B3

t (s  ˙D).

t = maxs∈S � ˙D

6� 1
2�T−t.
sα t ≤ 1

Let � ˙D
s t denote the maximum greedy error of the gain distribution ˙D at time t. The
COMB greedy error in sα is controlled by the following lemma proved in Section 13.1. Missing
proofs from this section are in the appendix in Section 13.2.
Lemma 7. For any t ∈ [T ] and gain distribution ˙D ∈ { ˙W  ˙C  ˙V  ˙1}  � ˙D
The following proposition shows how we can index the states
in the 2d-grid as a one dimensional line over which the TWIN-
COMB strategy behaves very similarly to a simple random walk.
Figure 3 (top) illustrates this random walk on the 2d-grid and
the indexing scheme (the yellow stickers).
Proposition 1. Index a state s = (x  y) by is = x + 2y ir-
respective of the time. Then for any state s �= sα  and s� ∼
p(·|s  ˙W) we have that P (is� = is−1) = P (is� = is +1) = 1
2 .
Consider a random walk that starts from state s0 = s and is gen-
erated by the TWIN-COMB strategy  st+1 ∼ p(.|st  ˙W). Deﬁne
the random variable Tα s = min{t ∈ N∪{0} : st = sα}. This
random variable is the number of steps of the random walk be-
fore hitting sα for the ﬁrst time. Then  let Pα(s  t) be the proba-
bility that sα is reached after t steps: Pα(s  t) = P (Tα s = t).
Lemma 8 controls the COMB greedy error in st in relation to
Pα(s  t). Lemma 9 derives a state-independent upper-bound for Pα(s  t).
Lemma 8. For any time t ∈ [T ] and state s 
T�t�=t

Figure 3: Numbering TWIN-COMB
(top) & πG random walks (bottom)

6� 1
2�T−t�

Pα(s  t� − t)

� ˙C
s t ≤

1

.

Proof. If s = sα  this is a direct application of Lemma 7 as Pα(sα  t�) = 0 for t� > 0.
When s �= sα  the following proof is by induction.
Initialization: Let t = T . At the last round only the last per-step regret matters (for all states s 
¯QπC
t (s  ˙D) = r ˙D(s)). As s �= sα  s is such that |X(s)|≤ 2 then r ˙D(s) = max ˙A∈ ˙B r ˙A(s) because of
Lemma 4 and Lemma 3. Therefore the statement holds.
Induction: Let t < T . We assume the statement is true at time t + 1. We distinguish two cases.
For all gain distributions ˙D ∈ ˙B3 
¯V πC
t+2]](s)

¯V πC
t+2]](s) = [T ˙W

(b)
= [T ˙W[T ˙D

t+1(.  ˙D)](s)

(a)
= [T ˙D[T ˙E

t (s  ˙D)

¯QπC

¯QπC

(c)

≥ [T ˙W max
˙A∈ ˙B3

(d)

≥ max
˙A∈ ˙B3

[T ˙W

(b)
= max
˙A∈ ˙B3

(e)
= max
˙A∈ ˙B3

¯QπC
t (s  ˙A) −

¯QπC
t (s  ˙A) −

1

T�t1=t+1
6� 1
2�T−t1
T�t1=t+1
6� 1
2�T−t1
T�t1=t+1
6� 1
2�T−t1
T�t1=t

1

1

Pα(s  t1 − t)

7

¯QπC
t+1(.  ˙A)](s) −

[P ˙WPα(.  t1 − t − 1)

1

6� 1
2�T−t1

](s)

¯QπC
t+1(.  ˙A)](s) −

[P ˙WPα(.  t1 − t − 1)](s)

[P ˙WPα(.  t1 − t − 1)](s)

d12d2343333211112111121111.5.5.5.5.5.5.5.5 12343456256784789106d12d2343333.5.5112340where in (a) ˙E is any distribution in Δ( ˙C  ˙W) and this step holds because of Lemma 5  (b) holds
because of the exchangeability property of Lemma 6  (c) is true by induction and monotonicity
of Bellman operator  in (d) the max operators change from being speciﬁc to any next state s� at
time t + 1 to being just one max operator that has to choose a single optimal gain distribution in
state s at time t  (e) holds by deﬁnition as for any t2  (here the last equality holds because s �= sα)
[P ˙WPα(.  t2)](s) = Es�∼p(.|s  ˙W)[Pα(s�  t2)] = Es�∼p(.|s  ˙W)[P (Tα s� = t2)] = Pα(s  t2 + 1).
Lemma 9. For t > 0 and any s 

Pα(s  t) ≤

2

t� 2

π

.

Proof. Using the connection between the TWIN-COMB strategy and a simple random walk in
Proposition 1  a formula can be found for Pα(s  t) from the classical “Gambler’s ruin” problem 
where one wants to know the probability that the gambler reaches ruin (here state sα) at any time
t given an initial capital in dollars (here is as deﬁned in Proposition 1). The gambler has an equal
probability to win or lose one dollar at each round and has no upper bound on his capital during the
game. Using [7] (Chapter XIV  Equation 4.14) or [18] we have Pα(s  t) = is
binomial coefﬁcient is 0 if t and is are not of the same parity. The technical Lemma 14 completes the
proof.

t� t
2 �2−t  where the

t+is

We now state our main result  connecting the value of the COMB adversary to the value of the game.
Theorem 1. Let K = 3  the regret of COMB strategies against any learner ¯p  min ¯p V T
¯p πC  satisﬁes

min

¯p

¯p πC ≥ VT − 12 log2 (T + 1) .
V T

We also characterize the minimax regret of the game.
Theorem 2. Let K = 3  for even T   we have that

����VT −� T + 2

T /2 + 1� T /2 + 1

3 ∗ 2T ���� ≤ 12 log2(T + 1) 

with� T + 2

T /2 + 1� T /2 + 1

3 ∗ 2T ∼� 8T

9π

.

In Figure 4 we introduce a COMB-based learner that is denoted by ¯pC. Here a state is represented
by a vector of 3 integers. The three arms/experts are ordered as (1) (2) (3)  breaking ties arbitrarily.
We connect the value of the COMB-based learner to the
value of the game.
Theorem 3. Let K = 3  the regret of COMB-based
learner against any adversary π  maxπ V T

pt (1)(s) = V πC
(s)
pt (2)(s) = V πC
(s)
pt (3)(s) = 1 − pt (1)(s) − pt (2)(s)

t+1(s+e(1))−V πC
t+1(s+e(2))−V πC

¯pC π  satisﬁes

t

t

max

π

¯pC π ≤ VT + 36 log2 (T + 1) .
V T

Similarly to [2] and [14]  this strategy can be efﬁciently
computed using rollouts/simulations from the COMB adversary in order to estimate the value V πC
of πC in s at time t.

t

(s)

Figure 4: A COMB learner  ¯pC

6 Discussion and Future Work

The main objective is to generalize our new proof techniques to higher dimensions. In our case  the
MDP formulation and all the results in Section 4 already holds for general K. Interestingly  Lemma 3
and 4 show that the COMB distribution is the balanced distribution with highest per-step regret in all
the states s such that |X(s)|≤ 2  for arbitrary K. Then assuming an ideal exchangeability property
that gives max ˙A∈ ˙B ˙A ˙C . . . ˙C = max ˙A∈ ˙B ˙C ˙C . . . ˙C ˙A  a distribution would be greedy w.r.t the COMB
strategy at an early round of the game if it maximizes the per-step regret at the last round of the
game. The COMB policy speciﬁcally tends to visit almost exclusively states |X(s)|≤ 2  states where
COMB itself is the maximizer of the per-step regret (Lemma 3). This would give that COMB is greedy
w.r.t. itself and therefore optimal. To obtain this result for larger K  we will need to extend the
exchangeability property to higher K and therefore understand how the COMB and TWIN-COMB
families extend to higher dimensions. One could also borrow ideas from the link with pde approaches
made in [6].

8

Acknowledgements
We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the Australian
Research Council through an Australian Laureate Fellowship (FL110100281) and through the Aus-
tralian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS).
We would like to thank Nate Eldredge for pointing us to the results in [18] and Wouter Koolen for
pointing us at [19]!

References
[1] Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries. In

Advances in Neural Information Processing Systems (NIPS)  pages 1–9  2010.

[2] Jacob Abernethy  Manfred K. Warmuth  and Joel Yellin. Optimal strategies from random walks.

In 21st Annual Conference on Learning Theory (COLT)  pages 437–446  2008.

[3] Nicolò Cesa-Bianchi  Yoav Freund  David Haussler  David P. Helmbold  Robert E. Schapire  and
Manfred K. Warmuth. How to use expert advice. Journal of the ACM (JACM)  44(3):427–485 
1997.

[4] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  learning  and games. Cambridge university

press  2006.

[5] Thomas M. Cover. Behavior of sequential predictors of binary sequences. In 4th Prague
Conference on Information Theory  Statistical Decision Functions  Random Processes  pages
263–272  1965.

[6] Nadeja Drenska. A pde approach to mixed strategies prediction with expert advice.

http://www.gtcenter.org/Downloads/Conf/Drenska2708.pdf. (Extended abstract).

[7] Willliam Feller. An Introduction to Probability Theory and its Applications  volume 2. John

Wiley & Sons  2008.

[8] Nick Gravin  Yuval Peres  and Balasubramanian Sivan. Towards optimal algorithms for predic-

tion with expert advice. In arXiv preprint arXiv:1603.04981  2014.

[9] Nick Gravin  Yuval Peres  and Balasubramanian Sivan. Towards optimal algorithms for predic-
tion with expert advice. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA)  pages 528–547  2016.

[10] Nick Gravin  Yuval Peres  and Balasubramanian Sivan. Tight Lower Bounds for Multiplicative
Weights Algorithmic Families. In 44th International Colloquium on Automata  Languages  and
Programming (ICALP)  volume 80  pages 48:1–48:14  2017.

[11] Charles Miller Grinstead and James Laurie Snell.

Mathematical Soc.  2012.

Introduction to probability. American

[12] James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of

Games  3:97–139  1957.

[13] Ronald A. Howard. Dynamic Programming and Markov Processes. The MIT Press  Cambridge 

MA  1960.

[14] Haipeng Luo and Robert E. Schapire. Towards minimax online learning with unknown time
horizon. In Proceedings of The 31st International Conference on Machine Learning (ICML) 
pages 226–234  2014.

[15] Francesco Orabona and Dávid Pál. Optimal non-asymptotic lower bound on the minimax regret

of learning with expert advice. arXiv preprint arXiv:1511.02176  2015.

[16] Martin L. Puterman. Markov Decision Processes. Wiley  New York  1994.

[17] Pantelimon Stanica. Good lower and upper bounds on binomial coefﬁcients. Journal of

Inequalities in Pure and Applied Mathematics  2(3):30  2001.

9

[18] Remco van der Hofstad and Michael Keane. An elementary proof of the hitting time theorem.

The American Mathematical Monthly  115(8):753–756  2008.

[19] Vladimir Vovk. A game of prediction with expert advice. Journal of Computer and System

Sciences (JCSS)  56(2):153–173  1998.

10

,Vikas Sindhwani
Tara Sainath
Sanjiv Kumar
Yasin Abbasi Yadkori
Peter Bartlett
Victor Gabillon
Sindy Löwe
Peter O'Connor
Bastiaan Veeling