2016,Scaled Least Squares Estimator for GLMs in Large-Scale Problems,We study the problem of efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations $n$ is much larger than the number of predictors $p$  i.e. $n\gg p \gg 1$. We show that in GLMs with random (not necessarily Gaussian) design  the GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients. Using this relation  we design an algorithm that achieves the same accuracy as the maximum likelihood estimator (MLE)  through iterations that  attain up to a cubic convergence rate  and that are cheaper than  any batch optimization algorithm by at least a factor of $\mathcal{O}(p)$. We provide theoretical guarantees for our algorithm  and analyze the convergence behavior in terms of data dimensions. % Finally  we demonstrate the performance of  our algorithm through extensive numerical studies  on large-scale real and synthetic datasets  and show that it achieves the highest performance compared to several other widely used optimization algorithms.,Scaled Least Squares Estimator for GLMs

in Large-Scale Problems

Murat A. Erdogdu

Department of Statistics

Stanford University

erdogdu@stanford.edu

Mohsen Bayati

Graduate School of Business

Stanford University

bayati@stanford.edu

Lee H. Dicker

Department of Statistics and Biostatistics

Rutgers University and Amazon ⇤
ldicker@stat.rutgers.edu

Abstract

We study the problem of efﬁciently estimating the coefﬁcients of generalized linear
models (GLMs) in the large-scale setting where the number of observations n is
much larger than the number of predictors p  i.e. n  p  1. We show that in
GLMs with random (not necessarily Gaussian) design  the GLM coefﬁcients are
approximately proportional to the corresponding ordinary least squares (OLS) coef-
ﬁcients. Using this relation  we design an algorithm that achieves the same accuracy
as the maximum likelihood estimator (MLE) through iterations that attain up to a
cubic convergence rate  and that are cheaper than any batch optimization algorithm
by at least a factor of O(p). We provide theoretical guarantees for our algorithm 
and analyze the convergence behavior in terms of data dimensions. Finally  we
demonstrate the performance of our algorithm through extensive numerical studies
on large-scale real and synthetic datasets  and show that it achieves the highest
performance compared to several other widely used optimization algorithms.

Introduction

1
We consider the problem of efﬁciently estimating the coefﬁcients of generalized linear models (GLMs)
when the number of observations n is much larger than the dimension of the coefﬁcient vector p 
(n  p  1). GLMs play a crucial role in numerous machine learning and statistics problems  and
provide a miscellaneous framework for many regression and classiﬁcation tasks. Celebrated examples
include ordinary least squares  logistic regression  multinomial regression and many applications
involving graphical models [MN89  WJ08  KF09].
The standard approach to estimating the regression coefﬁcients in a GLM is the maximum likelihood
method. Under standard assumptions on the link function  the maximum likelihood estimator (MLE)
can be written as the solution to a convex minimization problem [MN89]. Due to the non-linear
structure of the MLE problem  the resulting optimization task requires iterative methods. The most
commonly used optimization technique for computing the MLE is the Newton-Raphson method 
which may be viewed as a reweighted least squares algorithm [MN89]. This method uses a second
order approximation to beneﬁt from the curvature of the log-likelihood and achieves locally quadratic
convergence. A drawback of this approach is its excessive per-iteration cost of O(np2). To remedy
this  Hessian-free Krylov sub-space based methods such as conjugate gradient and minimal residual
are used  but the resulting direction is imprecise [HS52  PS75  Mar10]. On the other hand  ﬁrst order

⇤Work conducted while at Rutgers University

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

approximation yields the gradient descent algorithm  which attains a linear convergence rate with
O(np) per-iteration cost. Although its convergence rate is slow compared to that of second order
methods  its modest per-iteration cost makes it practical for large-scale problems. In the regime
n  p  another popular optimization technique is the class of Quasi-Newton methods [Bis95  Nes04] 
which can attain a per-iteration cost of O(np)  and the convergence rate is locally super-linear; a
well-known member of this class of methods is the BFGS algorithm [Nes04]. There are recent studies
that exploit the special structure of GLMs [Erd15]  and achieve near-quadratic convergence with a
per-iteration cost of O (np)  and an additional cost of covariance estimation.
In this paper  we take an alternative approach to ﬁtting GLMs  based on an identity that is well-known
in some areas of statistics  but appears to have received relatively little attention for its computational
implications in large scale problems. Let glm denote the GLM regression coefﬁcients  and let ols
denote the corresponding ordinary least squares (OLS) coefﬁcients (this notation will be deﬁned
more precisely in Section 2). Then  under certain random predictor (design) models 

glm / ols.

(1)
For logistic regression with Gaussian design (which is equivalent to Fisher’s discriminant analysis) 
(1) was noted by Fisher in the 1930s [Fis36]; a more general formulation for models with Gaussian
design is given in [Bri82]. The relationship (1) suggests that if the constant of proportionality is
known  then glm can be estimated by computing the OLS estimator  which may be substantially
simpler than ﬁnding the MLE for the original GLM. Our work in this paper builds on this idea.
Our contributions can be summarized as follows.

1. We show that glm is approximately proportional to ols in random design GLMs  regardless

of the predictor distribution. That is  we prove

glm  c ⇥ ols1 . 1

p

  for some c 2 R.

3. For random design GLMs with sub-Gaussian predictors  we show that

2. We design a computationally efﬁcient estimator for glm by ﬁrst estimating the OLS co-
efﬁcients  and then estimating the proportionality constant c . We refer to the resulting
estimator as the Scaled Least Squares (SLS) estimator and denote it by ˆ sls. After estimating
the OLS coefﬁcients  the second step of our algorithm involves ﬁnding a root of a real valued
function; this can be accomplished using iterative methods with up to a cubic convergence
rate and only O(n) per-iteration cost. This is cheaper than the classical batch methods
mentioned above by at least a factor of O(p).
+r

n/ max{log(n)  p}
This bound characterizes the performance of the proposed estimator in terms of data dimen-
sions  and justiﬁes the use of the algorithm in the regime n  p  1.
MLE (using several well-known implementations)  on a variety of large-scale datasets.

4. We study the statistical and computational performance of ˆ sls  and compare it to that of the

 ˆ sls  glm1

. 1
p

p

.

The rest of the paper is organized as follows: Section 1.1 surveys the related work and Section 2
introduces the required background and the notation. In Section 3  we provide the intuition behind the
relationship (1)  which are based on exact calculations for GLMs with Gaussian design. In Section 4 
we propose our algorithm and discuss its computational properties. Section 5 provides a thorough
comparison between the proposed algorithm and other existing methods. Theoretical results may be
found in Section 6. Finally  we conclude with a brief discussion in Section 7.

1.1 Related work
As mentioned in Section 1  the relationship (1) is well-known in several forms in statistics. Brillinger
[Bri82] derived (1) for models with Gaussian predictors. Li & Duan [LD89] studied model misspeci-
ﬁcation problems in statistics and derived (1) when the predictor distribution has linear conditional
means (this is a slight generalization of Gaussian predictors). More recently  Stein’s lemma [BEM13]
and the relationship (1) has been revisited in the context of compressed sensing [PV15  TAH15] 
where it has been shown that the standard lasso estimator may be very effective when used in models

2

where the relationship between the expected response and the signal is nonlinear  and the predictors
(i.e. the design or sensing matrix) are Gaussian. A common theme for all of this previous work is that
it focuses solely on settings where (1) holds exactly and the predictors are Gaussian (or  in [LD89] 
nearly Gaussian). Two key novelties of the present paper are (i) our focus on the computational
beneﬁts following from (1) for large scale problems with n  p  1; and (ii) our rigorous analysis
of models with non-Gaussian predictors  where (1) is shown to be approximately valid.
2 Preliminaries and notation
We assume a random design setting  where the observed data consists of n random iid pairs (y1  x1) 
(y2  x2)  . . .  (yn  xn); yi 2 R is the response variable and xi = (xi1  . . .   xip)T 2 Rp is the vector
of predictors or covariates. We focus on problems where ﬁtting a GLM is desirable  but we do not
need to assume that (yi  xi) are actually drawn from the corresponding statistical model (i.e. we
allow for model misspeciﬁcation).
The MLE for GLMs with canonical link is deﬁned by

ˆmle = argmax
2Rp

1
n

nXi=1

yihxi  i  (hxi  i).

(2)

where h· ·i denotes the Euclidean inner-product on Rp  and is a sufﬁciently smooth convex function.
The GLM coefﬁcients glm are deﬁned by taking the population average in (2):

glm = argmax
2Rp

E [yihxi  i  (hxi  i)] .

(3)

While we make no assumptions on beyond smoothness  note that if is the cumulant generating
function for yi | xi  then we recover the standard GLM with canonical link and regression parameters
glm [MN89]. Examples of GLMs in this form include logistic regression  with (w) = log{1+ew};
Poisson regression  with (w) = ew; and linear regression (least squares)  with (w) = w2/2.
Our objective is to ﬁnd a computationally efﬁcient estimator for glm. The alternative estima-
tor for glm proposed in this paper is related to the OLS coefﬁcient vector  which is deﬁned by
i ]1E [xiyi]; the corresponding OLS estimator is ˆols := (XT X)1XT y  where
ols := E[xixT
X = (x1  . . .   xn)T is the n ⇥ p design matrix and y = (y1  . . .   yn)T 2 Rn.
Additionally  throughout the text we let [m] ={1  2  ...  m}  for positive integers m  and we denote
the size of a set S by |S|. The m-th derivative of a function g : R ! R is denoted by g(m). For
a vector u 2 Rp and a n ⇥ p matrix U  we let kukq and kUkq denote the `q-vector and -operator
norms  respectively. If S ✓ [n]  let US denote the |S|⇥ p matrix obtained from U by extracting the
rows that are indexed by S. For a symmetric matrix M 2 Rp⇥p  max(M) and min(M) denote the
maximum and minimum eigenvalues  respectively. ⇢k(M) denotes the condition number of M with
respect to k-norm. We denote by Nq the q-variate normal distribution.

3 OLS is equivalent to GLM up to a scalar factor
To motivate our methodology  we assume in this section that the covariates are multivariate normal 
as in [Bri82]. These distributional assumptions will be relaxed in Section 6.
Proposition 1. Assume that the covariates are multivariate normal with mean 0 and covariance

glm = c ⇥ ols 

i⇤  i.e. xi ⇠ Np(0  ⌃). Then glm can be written as
matrix ⌃ = E⇥xixT
where c 2 R satisﬁes the equation 1 = c E⇥ (2)(hx  olsic )⇤ .
E [yixi] = Ehxi (1)(hxi  i)i .

(4)
Now  denote by (x | ⌃) the multivariate normal density with mean 0 and covariance matrix ⌃. We
recall the well-known property of Gaussian density d(x | ⌃)/dx = ⌃1x(x | ⌃). Using this

Proof of Proposition 1. The optimal point in the optimization problem (3)  has to satisfy the following
normal equations 

3

Algorithm 1 SLS: Scaled Least Squares Estimator

Input: Data (yi  xi)n
Step 1. Compute the least squares estimator: ˆols and ˆy = X ˆols.

i=1

For a sub-sampling based OLS estimator  let S ⇢ [n] be a
random subset and take ˆols = |S|n (XT

S XS)1XT y.

Step 2. Solve the following equation for c 2 R: 1 = c

Use Newton’s root-ﬁnding method:

i=1 (2)(c ˆyi).

nPn

Initialize c = 2/Var (yi);
Repeat until convergence:

c c 
Output: ˆ sls = c ⇥ ˆols.

1

c 1

i=1 (2)(c ˆyi)  1

nPn
i=1 (2)(c ˆyi) + c (3)(c ˆyi) .
nPn

and integration by parts on the right hand side of the above equation  we obtain

Ehxi (1)(hxi  i)i =Z x (1)(hx  i)(x | ⌃) dx = ⌃Eh (2)(hxi  i)i

(this is basically the Stein’s lemma). Combining this with the identity (4)  we conclude the proof.
Proposition 1 and its proof provide the main intuition behind our proposed method. Observe that
in our derivation  we only worked with the right hand side of the normal equations (4) which does
not depend on the response variable yi. The equivalence holds regardless of the joint distribution of
(yi  xi)  whereas in [Bri82]  yi is assumed to follow a single index model. In Section 6  where we
extend the method to non-Gaussian predictors  (5) is generalized via the zero-bias transformations.

(5)

3.1 Regularization
A version of Proposition 1 incorporating regularization — an important tool for datasets where p is
large relative to n or the predictors are highly collinear — is also possible  as outlined brieﬂy in this
section. We focus on `2-regularization (ridge regression) in this section; some connections with lasso
(`1-regularization) are discussed in Section 6 and Corollary 1.
For   0  deﬁne the `2-regularized GLM coefﬁcients 

glm
 = argmax
2Rp

E [yihxi  i  (hxi  i)] 

and the corresponding `2-regularized OLS coefﬁcients ols
glm = glm

0 ). The same argument as above implies that

and ols = ols

0

2


2 kk2
 = E⇥xixT

(6)

i⇤ + I1 E [xiyi] (so

(7)
This suggests that the ordinary ridge regression for the linear model can be used to estimate the
`2-regularized GLM coefﬁcients glm
 . Further pursuing these ideas for problems where regularization
is a critical issue may be an interesting area for future research.

   where  = c .

glm
 = c ⇥ ols

4 SLS: Scaled Least Squares estimator for GLMs
Motivated by the results in the previous section  we design a computationally efﬁcient algorithm for
any GLM task that is as simple as solving the least squares problem; it is described in Algorithm 1.
The algorithm has two basic steps. First  we estimate the OLS coefﬁcients  and then in the second
step we estimate the proportionality constant via a simple root-ﬁnding algorithm.
There are numerous fast optimization methods to solve the least squares problem  and even a
superﬁcial review of these could go beyond the page limits of this paper. We emphasize that this
step (ﬁnding the OLS estimator) does not have to be iterative and it is the main computational
cost of the proposed algorithm. We suggest using a sub-sampling based estimator for ols  where
we only use a subset of the observations to estimate the covariance matrix. Let S ⇢ [n] be a

4

SLS vs MLE : Computation

Method
SLS
MLE

)
c
e
s
(
e
m
T

i

60

40

20

0

4

5

log10(n)

6

1.2

0.9

0.6

0.3

0.0

2
|
β
−
β^

|

SLS vs MLE : Accuracy

Method
SLS
MLE

4

5

log10(n)

6

|S|

XT

S XS1 1

Figure 1: Logistic regression with general Gaussian design. The left plot shows the computational cost (time)
for ﬁnding the MLE and SLS as n grows and p = 200. The right plot depicts the accuracy of the estimators.
In the regime where the MLE is expensive to compute  the SLS is found much more rapidly and has the same
accuracy. R’s built-in functions are used to ﬁnd the MLE.
random sub-sample and denote by XS the sub-matrix formed by the rows of X in S. Then the
sub-sampled OLS estimator is given as ˆols = 1
n XT y. Properties of this estimator
have been well-studied [Ver10  DLFU13  EM15]. For sub-Gaussian covariates  it sufﬁces to use
a sub-sample size of O (p log(p)) [Ver10]. Hence  this step requires a single time computational
cost of O|S|p2 + p3 + np ⇡O p max{p2 log(p)  n}. For other approaches  we refer reader to
[RT08  DLFU13] and the references therein.
The second step of Algorithm 1 involves solving a simple root-ﬁnding problem. As with the ﬁrst
step of the algorithm  there are numerous methods available for completing this task. Newton’s
root-ﬁnding method with quadratic convergence or Halley’s method with cubic convergence may be
appropriate choices. We highlight that this step costs only O (n) per-iteration and that we can attain up
to a cubic rate of convergence. The resulting per-iteration cost is cheaper than other commonly used
batch algorithms by at least a factor of O (p) — indeed  the cost of computing the gradient is O (np).
For simplicity  we use Newton’s root-ﬁnding method initialized at c = 2/Var (yi). Assuming that
the GLM is a good approximation to the true conditional distribution  by the law of total variance and
basic properties of GLMs  we have

It follows that this initialization is reasonable as long as c1

Var (yi) = E [Var (yi | xi)] + Var (E [yi | xi]) ⇡ c1

 + Var (1)(hxi  i).
than Var (1)(hxi  i). Our experiments show that SLS is very robust to initialization.

(8)
 ⇡ E [Var (yi | xi)] is not much smaller

In Figure 1  we compare the performance of our SLS estimator to that of the MLE  when both are used
to analyze synthetic data generated from a logistic regression model under general Gaussian design
with randomly generated covariance matrix. The left plot shows the computational cost of obtaining
both estimators as n increases for ﬁxed p. The right plot shows the accuracy of the estimators. In the
regime n  p  1 — where the MLE is hard to compute — the MLE and the SLS achieve the same
accuracy  yet SLS has signiﬁcantly smaller computation time. We refer the reader to Section 6 for
theoretical results characterizing the ﬁnite sample behavior of the SLS.
5 Experiments
This section contains the results of a variety of numerical studies  which show that the Scaled Least
Squares estimator reaches the minimum achievable test error substantially faster than commonly used
batch algorithms for ﬁnding the MLE. Both logistic and Poisson regression models (two types of
GLMs) are utilized in our analyses  which are based on several synthetic and real datasets.
Below  we brieﬂy describe the optimization algorithms for the MLE that were used in the experiments.
1. Newton-Raphson (NR) achieves locally quadratic convergence by scaling the gradient by
the inverse of the Hessian evaluated at the current iterate. Computing the Hessian has a

per-iteration cost of Onp2  which makes it impractical for large-scale datasets.
2. Newton-Stein (NS) is a recently proposed second-order batch algorithm speciﬁcally de-
signed for GLMs [Erd16]. The algorithm uses Stein’s lemma and sub-sampling to efﬁciently
estimate the Hessian with O (np) per-iteration cost  achieving near quadratic rates.

5

Logis0c	Regression	

Log−Reg / Covariates ~ Σ x {Exp(1)−1}
SLS
NR
NS
BFGS
LBFGS
GD
AGD

Poisson	Regression	

Poi−Reg / Covertype dataset

Log−Reg / Higgs dataset

SLS
NR
NS
BFGS
LBFGS
GD
AGD

2.5

2.0

1.5

1.0

)
r
o
r
r

E

 
t
s
e
T
(
g
o

Poi−Reg / Covariates ~ Σ x Ber( ± 1)
SLS
NR
NS
BFGS
LBFGS
GD
AGD

l

0.5

40

50

0

10

l

30

40

0.5

0.0

2.5

0

10

40

50

0

10

20
30
Time (sec)

(a)	

20
30
Time (sec)

(c)	

Log−Reg / Covariates ~ Σ x {Exp(1)−1}

Log−Reg / Higgs dataset

7.5

10.0

5.0
Time (sec)
(g)	

Poi−Reg / Covertype dataset

2.0

1.5

1.0

)
r
o
r
r

E

 
t
s
e
T
(
g
o

15

10

)
r
o
r
r

E

 
t
s
e
T
(
g
o

Time (sec)

20
(e)	

Poi−Reg / Covariates ~ Σ x Ber( ± 1)
SLS
NR
NS
BFGS
LBFGS
GD
AGD

SLS
NR
NS
BFGS
LBFGS
GD
AGD

2.0

1.5

1.0

)
r
o
r
r

E

 
t
s
e
T
(
g
o

SLS
NR
NS
BFGS
LBFGS
GD
AGD

SLS
NR
NS
BFGS
LBFGS
GD
AGD

	
t
r
a
t
s
	

m
o
d
n
a
R

r
o
r
r

E

 
t
s
e
T

0.5

0.4

0.3

0.2

0.24

	
t
r
a
t
s
	
S
L
O

r
o
r
r

E

 
t
s
e
T

0.22

0.20

r
o
r
r

E

 
t
s
e
T

0.30

0.28

0.26

0.24

0.22

0.25

SLS
NR
NS
BFGS
LBFGS
GD
AGD

0.24

r
o
r
r

E

 
t
s
e
T

0.23

0.18

0

5

10

Time (sec)

15

20

0

10

20

Time (sec)

30

(b)	

(d)	

l

0.5

40

0

l

10

20

Time (sec)

30

40

(f)	

5

0

0.0

2.5

5.0

Time (sec)
(h)	

7.5

10.0

Figure 2: Performance of SLS compared to that of MLE obtained with various optimization algorithms on
several datasets. SLS is represented with red straight line. The details are provided in Table 1.

3. Broyden-Fletcher-Goldfarb-Shanno (BFGS) is the most popular and stable quasi-Newton
method [Nes04]. At each iteration  the gradient is scaled by a matrix that is formed
by accumulating information from previous iterations and gradient computations. The
convergence is locally super-linear with a per-iteration cost of O (np).
4. Limited memory BFGS (LBFGS) is a variant of BFGS  which uses only the recent iterates
and gradients to approximate the Hessian  providing signiﬁcant improvement in terms of
memory usage. LBFGS has many variants; we use the formulation given in [Bis95].

5. Gradient descent (GD) takes a step in the opposite direction of the gradient  evaluated at
the current iterate. Its performance strongly depends on the condition number of the design
matrix. Under certain assumptions  the convergence is linear with O (np) per-iteration cost.
6. Accelerated gradient descent (AGD) is a modiﬁed version of gradient descent with an
additional “momentum” term [Nes83]. Its per iteration cost is O (np) and its performance
strongly depends on the smoothness of the objective function.

For all the algorithms  the step size at each iteration is chosen via the backtracking line search [BV04].
Recall that the proposed Algorithm 1 is composed of two steps; the ﬁrst ﬁnds an estimate of the
OLS coefﬁcients. This up-front computation is not needed for any of the MLE algorithms described
above. On the other hand  each of the MLE algorithms requires some initial value for   but no such
initialization is needed to ﬁnd the OLS estimator in Algorithm 1. This raises the question of how the
MLE algorithms should be initialized  in order to compare them fairly with the proposed method. We
consider two scenarios in our experiments: ﬁrst  we use the OLS estimator computed for Algorithm 1
to initialize the MLE algorithms; second  we use a random initial value.
On each dataset  the main criterion for assessing the performance of the estimators is how rapidly the
minimum test error is achieved. The test error is measured as the mean squared error of the estimated
mean using the current parameters at each iteration on a test dataset  which is a randomly selected
(and set-aside) 10% portion of the entire dataset. As noted previously  the MLE is more accurate
for small n (see Figure 1). However  in the regime considered here (n  p  1)  the MLE and the
SLS perform very similarly in terms of their error rates; for instance  on the Higgs dataset  the SLS
and MLE have test error rates of 22.40% and 22.38%  respectively. For each dataset  the minimum
achievable test error is set to be the maximum of the ﬁnal test errors  where the maximum is taken
over all of the estimation methods. Let ⌃(1) and ⌃(2) be two randomly generated covariance matrices.
The datasets we analyzed were: (i) a synthetic dataset generated from a logistic regression model
with iid {exponential(1)1} predictors scaled by ⌃(1); (ii) the Higgs dataset (logistic regression)
[BSW14]; (iii) a synthetic dataset generated from a Poisson regression model with iid binary(±1)
predictors scaled by ⌃(2); (iv) the Covertype dataset (Poisson regression) [BD99].
In all cases  the SLS outperformed the alternative algorithms for ﬁnding the MLE by a large margin 
in terms of computation. Detailed results may be found in Figure 2 and Table 1. We provide additional
experiments with different datasets in the Supplementary Material.

6

MODEL
DATASET
SIZE
INITIALIZED
PLOT
METHOD
SLS
NR
NS
BFGS
LBFGS
GD
AGD

Table 1: Details of the experiments shown in Figure 2.

LOGISTIC REGRESSION

POISSON REGRESSION

n = 6.0 ⇥ 105  p = 300

⌃⇥{EXP(1)-1}
OLS
RND
(B)
(A)
TIME IN SECONDS / NUMBER OF ITERATIONS (TO REACH MIN TEST ERROR)

n = 1.1⇥107  p = 29
OLS
RND
(D)
(C)

⌃⇥BER(±1)
OLS
(F)

n = 6.0⇥105  p = 300

HIGGS [BSW14]

RND
(E)

RND
(G)

COVERTYPE [BD99]
n = 5.8⇥105  p = 53

OLS
(H)

8.34/4
301.06/6
51.69/8
148.43/31
125.33/39
669/138
218.1/61

2.94/3
82.57/3
7.8/3
24.79/8
24.61/8
134.91/25
35.97/12

13.18/3
37.77/3
27.11/4
660.92/68
6368.1/651
100871/10101 141736/13808
2879.69/277

9.57/3
36.37/3
26.69/4
701.9/68
6946.1/670

2405.5/251

5.42/5
170.28/5
32.71/5
67.24/29
224.6/106
1711/513
103.3/51

3.96/5
130.1/4
36.82/4
72.42/26
357.1/88
1364/374
102.74/40

2.71/6
16.7/8
21.17/10
5.12/7
10.01/14
14.35/25
11.28/15

1.66/20
32.48/18
282.1/216
22.74/59
10.05/17
33.58/87
11.95/25

6 Theoretical results
In this section  we use the zero-bias transformations [GR97] to generalize the equivalence between
OLS and GLMs to settings where the covariates are non-Gaussian.
Deﬁnition 1. Let z be a random variable with mean 0 and variance 2. Then  there exists a
random variable z⇤ that satisﬁes E [zf (z)] = 2E[f (1)(z⇤)]  for all differentiable functions f. The
distribution of z⇤ is said to be the z-zero-bias distribution.

The existence of z⇤ in Deﬁnition 1 is a consequence of Riesz representation theorem [GR97]. The
normal distribution is the unique distribution whose zero-bias transformation is itself (i.e. the normal
distribution is a ﬁxed point of the operation mapping the distribution of z to that of z⇤).
To provide some intuition behind the usefulness of the zero-bias transformation  we refer back to the
proof of Proposition 1. For simplicity  assume that the covariate vector xi has iid entries with mean 0 
and variance 1. Then the zero-bias transformation applied to the j-th normal equation in (4) yields

.

(9)

E [yixij] = Ehxij (1)xijj +⌃ k6=jxikki
|
}

j-th normal equation

{z

= jEh (2)x⇤ijj +⌃ k6=jxikiki
|
}

Zero-bias transformation

{z

The distribution of x⇤ij is the xij-zero-bias distribution and is entirely determined by the distribution
of xij; general properties of x⇤ij can be found  for example  in [CGS10]. If  is well spread  it turns
out that taken together  with j = 1  . . .   p  the far right-hand side in (9) behaves similar to the right
side of (5)  with ⌃ = I; that is  the behavior is similar to the Gaussian case  where the proportionality
relationship given in Proposition 1 holds. This argument leads to an approximate proportionality
relationship for non-Gaussian predictors  which  when carried out rigorously  yields the following.
Theorem 1. Suppose that the covariate vector xi has mean 0 and covariance matrix ⌃ and  fur-
thermore  that the random vector ⌃1/2xi has independent entries and its sub-Gaussian norm is
bounded by . Assume that the function (2) is Lipschitz continuous with constant k. Let kk2 = ⌧
and assume  is r-well-spread in the sense that ⌧/ kk1 = rpp for some r 2 (0  1]. Then  for
c = 1/E⇥ (2)(hxi  glmi)⇤  and ⇢ = ⇢1(⌃1/2) denoting the condition number of ⌃1/2  we have

  where ⌘ = 8k3⇢k⌃1/2k1(⌧ /r)2.

(10)



1

c ⇥ glm  ols1

⌘
p



Theorem 1 is proved in the Supplementary Material. It implies that the population parameters ols
and glm are approximately equivalent up to a scaling factor  with an error bound of O (1/p). The
assumption that glm is well-spread can be relaxed with minor modiﬁcations. For example  if we
have a sparse coefﬁcient vector  where supp(glm) = {j; glm
6= 0} is the support set of glm  then
Theorem 1 holds with p replaced by the size of the support set.
An interesting consequence of Theorem 1 and the remarks following the theorem is that whenever
an entry of glm is zero  the corresponding entry of ols has to be small  and conversely. For   0 
deﬁne the lasso coefﬁcients

j

lasso
 = argmin
2Rp

1

2E⇥(yi  hxi  i)2⇤ + kk1 .

(11)

7

i⇤ = I  we have
Corollary 1. For any   ⌘/|supp(glm)| 
supp(lasso) ⇢ supp(glm). Further  if  and glm also satisfy that 8j 2 supp(glm)  |glm
c  + ⌘/|supp(glm)|  then we have supp(lasso) = supp(glm).

if E [xi] = 0 and E⇥xixT

So far in this section  we have only discussed properties of the population parameters  such as glm.
In the remainder of this section  we turn our attention to results for the estimators that are the main
focus of this paper; these results ultimately build on our earlier results  i.e. Theorem 1.
In order to precisely describe the performance of ˆ sls  we ﬁrst need bounds on the OLS estimator.
The OLS estimator has been studied extensively in the literature; however  for our purposes  we
ﬁnd it convenient to derive a new bound on its accuracy. While we have not seen this exact bound
elsewhere  it is very similar to Theorem 5 of [DLFU13].

i⇤ = ⌃  and that ⌃1/2xi and yi are sub-Gaussian
Proposition 2. Assume that E [xi] = 0  E⇥xixT
with norms  and   respectively. For min denoting the smallest eigenvalue of ⌃  and |S| >⌘p  

| >

j

 

(12)

min r p
 ˆols  ols2  ⌘ 1/2

|S|

(13)

(14)
(15)

with probability at least 1  3ep  where ⌘ depends only on  and .
Proposition 2 is proved in the Supplementary Material. Our main result on the performance of ˆ sls is
given next.
Theorem 2. Let the assumptions of Theorem 1 and Proposition 2 hold with E[k⌃1/2xk2] = ˜µpp.
Further assume that the function f (z) = zE⇥ (2)(hx  olsiz)⇤ satisﬁes f (¯c) > 1 + ¯pp for some ¯c
and ¯ such that the derivative of f in the interval [0  ¯c] does not change sign  i.e.  its absolute value is
lower bounded by > 0. Then  for n and |S| sufﬁciently large  we have

1
p

 
with probability at least 1  5ep  where the constants ⌘1 and ⌘2 are deﬁned by
min kolsk1 max{(b + k/˜µ)  k¯c}⌘  

 ˆ sls  glm1  ⌘1
⌘1 =⌘k¯c3⇢k⌃1/2k1(⌧ /r)2
min ⇣1 + 11/2
⌘2 =⌘¯c1/2

min{n/ log(n) |S|/p}

+ ⌘2r

p

and ⌘> 0 is a constant depending on  and .
Note that the convergence rate of the upper bound in (13) depends on the sum of the two terms  both
of which are functions of the data dimensions n and p. The ﬁrst term on the right in (13) comes from
Theorem 1  which bounds the discrepancy between c ⇥ ols and glm. This term is small when p is
large  and it does not depend on the number of observations n.
The second term in the upper bound (13) comes from estimating ols and c . This term is increasing
in p  which reﬂects the fact that estimating glm is more challenging when p is large. As expected 
this term is decreasing in n and |S|  i.e. larger sample size yields better estimates. When the full OLS
solution is used (|S| = n)  the second term becomes O(pp max{log(n)  p}/n) = O(p/pn)  for p
sufﬁciently large. This suggests that n should be at least of order p2 for good performance.

7 Discussion

In this paper  we showed that the coefﬁcients of GLMs and OLS are approximately proportional
in the general random design setting. Using this relation  we proposed a computationally efﬁcient
algorithm for large-scale problems that achieves the same accuracy as the MLE by ﬁrst estimating the
OLS coefﬁcients and then estimating the proportionality constant through iterations that can attain
quadratic or cubic convergence rate  with only O (n) per-iteration cost.
We brieﬂy mentioned that the proportionality between the coefﬁcients holds even when there is
regularization in Section 3.1. Further pursuing this idea may be interesting for large-scale problems
where regularization is crucial. Another interesting line of research is to ﬁnd similar proportionality
relations between the parameters in other large-scale optimization problems such as support vector
machines. Such relations may reduce the problem complexity signiﬁcantly.

8

References
[BD99]

J. A. Blackard and D. J. Dean  Comparative accuracies of artiﬁcial neural networks and discriminant
analysis in predicting forest cover types from cartographic variables  Comput. Electron. Agr. 24
(1999)  131–151.

[BEM13] M. Bayati  M. A. Erdogdu  and A. Montanari  Estimating lasso risk and noise level  NIPS 26  2013 

[Bis95]
[Bri82]

pp. 944–952.
C. M. Bishop  Neural Networks for Pattern Recognition  Oxford University Press  1995.
D. R Brillinger  A generalized linear model with "Gaussian" regressor variables  A Festschrift For
Erich L. Lehmann  CRC Press  1982  pp. 97–114.

[BSW14] P. Baldi  P. Sadowski  and D. Whiteson  Searching for exotic particles in high-energy physics with

deep learning  Nat. Commun. 5 (2014)  4308–4308.
S. Boyd and L. Vandenberghe  Convex Optimization  Cambridge University Press  2004.

[BV04]
[CGS10] L. H. Y. Chen  L. Goldstein  and Q.-M. Shao  Normal approximation by Stein’s method  Springer 

2010.

[DLFU13] P. Dhillon  Y. Lu  D. P. Foster  and L. Ungar  New subsampling algorithms for fast least squares

regression  NIPS 26 (2013)  360–368.

[EM15] M. A. Erdogdu and A. Montanari  Convergence rates of sub-sampled newton methods  NIPS 28 

2015  pp. 3034–3042.

[Erd15] M. A. Erdogdu  Newton-Stein method: A second order method for GLMs via Stein’s lemma  NIPS

28 (2015)  1216–1224.

[Erd16]

[Fis36]

[Gol07]
[GR97]

[HS52]

[KF09]

[LD89]
[Mar10]
[MN89]
[Nes83]

[Nes04]
[PS75]

[PV15]

[RT08]

  Newton-Stein Method: An optimization method for GLMs via Stein’s Lemma  Journal of

Machine Learning Research (to appear) (2016).
R. A. Fisher  The use of multiple measurements in taxonomic problems  Ann. Eugenic 7 (1936) 
179–188.
L. Goldstein  l1 bounds in normal approximation  Ann. Probab. 35 (2007)  1888–1930.
L. Goldstein and G. Reinert  Stein’s method and the zero bias transformation with application to
simple random sampling  Ann. Appl. Probab. 7 (1997)  935–952.
M. R. Hestenes and E. Stiefel  Methods of conjugate gradients for solving linear systems  J. Res.
Nat. Bur. Stand. 49 (1952)  409–436.
D. Koller and N. Friedman  Probabilistic Graphical Models: Principles and Techniques  MIT press 
2009.
K.-C. Li and N. Duan  Regression analysis under link violation  Ann. Stat. 17 (1989)  1009–1052.
J. Martens  Deep learning via Hessian-free optimization  ICML 27 (2010)  735–742.
P. McCullagh and J. A. Nelder  Generalized Linear Models  2nd ed.  Chapman and Hall  1989.
Y. Nesterov  A method of solving a convex programming problem with convergence rate O(1/k2) 
Soviet Math. Dokl. 27 (1983)  372–376.

  Introductory Lectures on Convex Optimization: A Basic Course  Springer  2004.

C. C. Paige and M. A. Saunders  Solution of sparse indeﬁnite systems of linear equations  SIAM J.
Numer. Anal. 12 (1975)  617–629.
Y. Plan and R. Vershynin  The generalized lasso with non-linear observations  2015  arXiv preprint
arXiv:1502.04071.
V. Rokhlin and M. Tygert  A fast randomized algorithm for overdetermined linear least-squares
regression  P. Natl. Acad. Sci. 105 (2008)  13212–13217.

[TAH15] C. Thrampoulidis  E. Abbasi  and B. Hassibi  Lasso with non-linear measurements is equivalent to

[Ver10]

[WJ08]

one with linear measurements  NIPS 28 (2015)  3402–3410.
R. Vershynin 
arXiv:1011.3027.
M. J. Wainwright and M. I. Jordan  Graphical models  exponential families  and variational inference 
Foundations and Trends in Machine Learning 1 (2008)  1–305.

Introduction to the non-asymptotic analysis of random matrices  2010 

9

,Sheng Chen
Arindam Banerjee
Murat Erdogdu
Lee Dicker
Mohsen Bayati