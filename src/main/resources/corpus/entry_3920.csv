2013,On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations,In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically  we provide  means for drawing either approximate or unbiased samples from Gibbs' distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical high signal - high coupling'' regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. ",On Sampling from the Gibbs Distribution with
Random Maximum A-Posteriori Perturbations

Tamir Hazan

University of Haifa

Subhransu Maji

TTI Chicago

Tommi Jaakkola

CSAIL  MIT

Abstract

In this paper we describe how MAP inference can be used to sample efﬁciently
from Gibbs distributions. Speciﬁcally  we provide means for drawing either ap-
proximate or unbiased samples from Gibbs’ distributions by introducing low di-
mensional perturbations and solving the corresponding MAP assignments. Our
approach also leads to new ways to derive lower bounds on partition functions.
We demonstrate empirically that our method excels in the typical “high signal -
high coupling” regime. The setting results in ragged energy landscapes that are
challenging for alternative approaches to sampling and/or lower bounds.

1

Introduction

Inference in complex models drives much of the research in machine learning applications  from
computer vision  natural language processing  to computational biology. Examples include scene
understanding  parsing  or protein design. The inference problem in such cases involves ﬁnding
likely structures  whether objects  parsers  or molecular arrangements. Each structure corresponds
to an assignment of values to random variables and the likelihood of an assignment is based on
deﬁning potential functions in a Gibbs distribution. Usually  it is feasible to ﬁnd only the most
likely or maximum a-posteriori (MAP) assignment (structure) rather than sampling from the full
Gibbs distribution. Substantial effort has gone into developing algorithms for recovering MAP as-
signments  either based on speciﬁc structural restrictions such as super-modularity [2] or by devising
cutting-planes based methods on linear programming relaxations [19  24]. However  MAP inference
is limited when there are other likely assignments.
Our work seeks to leverage MAP inference so as to sample efﬁciently from the full Gibbs distribu-
tion. Speciﬁcally  we aim to draw either approximate or unbiased samples from Gibbs distributions
by introducing low dimensional perturbations in the potential functions and solving the correspond-
ing MAP assignments. Connections between random MAP perturbations and Gibbs distributions
have been explored before. Recently [17  21] deﬁned probability models that are based on low
dimensional perturbations  and empirically tied them to Gibbs distributions. [5] augmented these
results by providing bounds on the partition function in terms of random MAP perturbations.
In this work we build on these results to construct an efﬁcient sampler for the Gibbs distribution  also
deriving new lower bounds on the partition function. Our approach excels in regimes where there
are several but not exponentially many prominent assignments. In such ragged energy landscapes
classical methods for the Gibbs distribution such as Gibbs sampling and Markov chain Monte Carlo
methods  remain computationally expensive [3  25].

2 Background

Statistical inference problems involve reasoning about the states of discrete variables whose con-
ﬁgurations (assignments of values) specify the discrete structures of interest. We assume that the

1

models are parameterized by real valued potentials θ(x) = θ(x1  ...  xn) < ∞ deﬁned over a dis-
crete product space X = X1 × ··· × Xn. The effective domain is implicitly deﬁned through θ(x)
via exclusions θ(x) = −∞ whenever x (cid:54)∈ dom(θ). The real valued potential functions are mapped
to the probability scale via the Gibbs’ distribution:

p(x1  ...  xn) =

1
Z

exp(θ(x1  ...  xn))  where Z =

exp(θ(x1  ...  xn)).

(1)

(cid:88)

x1 ... xn

The normalization constant Z is called the partition function. The feasibility of using the distribution
for prediction  including sampling from it  is inherently tied to the ability to evaluate the partition
function  i.e.  the ability to sum over the discrete structures being modeled. In general  such counting
problems are often hard  in #P.
A slightly easier problem is that of ﬁnding the most likely assignment of values to variables  also
known as the maximum a-posterior (MAP) prediction.

(MAP)

arg max
x1 ... yn

θ(x1  ...  xn)

(2)

Recent advances in optimization theory have been translated to successful algorithms for solving
such MAP problems in many cases of practical interest. Although the MAP prediction problem is
still NP-hard in general  it is often simpler than sampling from the Gibbs distribution.
Our approach is based on representations of the Gibbs distribution and the partition function using
extreme value statistics of linearly perturbed potential functions. Let {γ(x)}x∈X be a collection of
random variables with zero mean  and consider random potential functions of the form θ(x) + γ(x).
Analytic expressions for the statistics of a randomized MAP predictor  ˆx ∈ argmaxx{θ(x) + γ(x)} 
can be derived for general discrete sets  whenever independent and identically distributed (i.i.d.)
random perturbations are applied for every assignment x ∈ X. Speciﬁcally  when the random
perturbations follow the Gumbel distribution (cf. [12])  we obtain the following result.
Theorem 1. ([4]  see also [17  5]) Let {γ(x)}x∈X be a collection of i.i.d. random variables 
each following the Gumbel distribution with zero mean  whose cumulative distribution function is
F (t) = exp(− exp(−(t + c)))  where c is the Euler constant. Then

(cid:104)
(cid:104)

{θ(x) + γ(x)}(cid:105)

max
x∈X
ˆx ∈ arg max
x∈X

.

{θ(x) + γ(x)}(cid:105)

.

log Z = Eγ

1
Z

exp(θ(ˆx)) = Pγ

The max-stability of the Gumbel distribution provides a straight forward approach to generate un-
biased samples from the Gibbs distribution as well as to approximate the partition function by a
sample mean of random MAP perturbation. Assume we sample j = 1  ...  m independent predic-
tions maxx{θ(x) + γj(x)}  then every maximal argument is an unbiased sample from the Gibbs
distribution. Moreover  the randomized MAP predictions maxx{θ(x) + γj(x)} are independent and
follow the Gumbel distribution  whose variance is π2/6. Therefore Chebyshev’s inequality dictates 
for every t  m

(3)

(cid:104)(cid:12)(cid:12)(cid:12) 1

m

m(cid:88)

j=1

P rγ

{θ(x) + γj(x)} − log Z

max

x

(cid:12)(cid:12)(cid:12) ≥ 

(cid:105) ≤ π

6m2

In general each x = (x1  ...  xn) represents an assignment to n variables. Theorem 1 suggests to
introduce an independent perturbation γ(x) for each such n−dimensional assignment x ∈ X. The
complexity of inference and learning in this setting would be exponential in n. In our work we
propose to investigate low dimensional random perturbations as the main tool to efﬁciently (approx-
imate) sampling from the Gibbs distribution.

3 Probable approximate samples from the Gibbs distribution

Sampling from the Gibbs distribution is inherently tied to estimating the partition function. Markov
properties that simplify the distribution also decompose the computation of the partition function.

2

For example  assume a graphical model with potential functions associated with subsets of variables
α∈A θα(xα). Assume that the subsets are disjoint except for
their common intersection β = ∩α∈A. This separation implies that the partition function can be
computed in lower dimensional pieces

α ⊂ {1  ...  n} so that θ(x) = (cid:80)
(cid:88)

Z =

(cid:89)

(cid:16) (cid:88)

α∈A

xβ

xα\xβ

(cid:17)

exp(θα(xα))

jα=1

α∈A π2
6mα2

As a result  the computation is exponential only in the size of the subsets α ∈ A. Thus 
we can also estimate the partition function with lower dimensional random MAP perturbations 
Eγ[maxxα\xβ{θα(xα) + γα(xα)}]. The random perturbation are now required only for each as-
signment of values to the variables within the subsets α ∈ A rather than the set of all variables.
We approximate such partition functions with low dimensional perturbations and their averages. The
overall computation is cast in a single MAP problem using an extended representation of potential
functions by replicating variables.
Lemma 1. Let A be subsets of variables that are separated by their joint intersection β = ∩α∈Aα.
We create multiple copies of xα  namely ˆxα = (xα jα )jα=1 ... mα  and deﬁne the extended poten-
jα=1 θα(xα jα)/mα. We also deﬁne the extended perturbation model
jα=1 γα jα(xα jα )/mα  where each γα jα (xα jα ) is independent and distributed ac-
cording to the Gumbel distribution with zero mean. Then  for every xβ  with probability at least

tial function ˆθα(ˆxα) = (cid:80)mα
ˆγα(ˆxα) = (cid:80)mα
1 −(cid:80)
(cid:12)(cid:12)(cid:12) max
(cid:8)(cid:88)
(cid:12)(cid:12)(cid:12) 1
(cid:80)m
To compute the sampled average with a single max-operation we introduce the mul-
jα=1 maxxα\xβ{θα(xα) + γα jα (xα)} =
j=1{θα(xα jα ) + γα jα (xα jα )}. By the union bound it holds for every α ∈ A
maxxα jα\xβ
α∈A π2/6mα2. Since xβ is ﬁxed for every α ∈ A
the maximizations are done independently across subsets in ˆx \ xβ  where ˆx is the concatenation of
(cid:110)
(cid:111)

tiple copies ˆxα = (xα jα )jα=1 ... mα thus (cid:80)mα
simultaneously with probability at least 1 −(cid:80)
(cid:111)
all ˆxα  and(cid:88)

log(cid:0) (cid:88)
{θα(xα) + γα jα (xα)} − log(cid:0) (cid:88)

exp(θα(xα))(cid:1)(cid:12)(cid:12)(cid:12) ≤ |A|
exp(θα(xα))(cid:1)(cid:12)(cid:12)(cid:12) ≤ .

Proof: Equation (3) implies that for every xβ with probability at most π2/6mα2 holds

ˆγα(ˆxα)(cid:9) −(cid:88)

(cid:110)(cid:88)

mα(cid:88)

(cid:88)

mα(cid:88)

mα(cid:88)

(cid:88)

ˆθα(ˆxα) +

max
xα\xβ

xα\xβ

xα\xβ

ˆx\xβ

α∈A

α∈A

α∈A

mα

.

jα=1

jα=1

α∈A

α∈A

max
ˆxα\xβ

θα(xα jα ) +

γα jα (xα jα)

θα(xα jα ) + γα jα (xα jα )

= max
ˆx\xβ
α∈A
The proof then follows from the triangle inequality. (cid:3)
Whenever the graphical model has no cycles we can iteratively apply the separation properties with-
out increasing the computational complexity of perturbations. Thus we may randomly perturb the
subsets of potentials in the graph. For notational simplicity we describe our approximate sampling
scheme for pairwise interactions α = (i  j) although it holds for general graphical models without
cycles:

i j∈E θi j(xi  xj) be a graphical model with-
Let ˆθ(x) =
ki kj =1 γi j ki kj (xi ki  xj kj )/mimj
where each perturbation is independent and distributed according to the Gumbel distribution with
zero mean. Then  for every edge (r  s) while mr = ms = 1 (i.e.  they have no multiple copies) there

Theorem 2. Let θ(x) = (cid:80)
i∈V θi(xi) + (cid:80)
(cid:80)mi
ki=1 θ(x1 k1  ...  xn kn )/(cid:81)
i mi  and ˆγi j(xi  xj) = (cid:80)mi mj
holds with probability at least 1 −(cid:80)n
(cid:12)(cid:12)(cid:12) log
(cid:110)ˆθ(x) +
(cid:111)(cid:105)(cid:17) − log

(cid:16) (cid:88)
i=1 π2c/6mi2  where c = maxi |Xi|

out cycles  and let p(x) be the Gibbs distribution deﬁned in Equation (1).

(cid:17)(cid:12)(cid:12)(cid:12) ≤ n

xr  xs ∈ arg max

(cid:88)

ˆγi j(xi  xj)

(cid:16)

p(x)

(cid:104)

Pγ

ˆx

i j∈E

x\xr xs

3

marginal probabilities with a max-operation  if we approximate(cid:80)

Proof: Theorem 1 implies that we sample (xr  xs) approximately from the Gibbs distribution
x\{xr xs} exp(θ(x)). Using graph
separation (or equivalently the Markov property) it sufﬁces to approximate the partial partition func-
tion over the disjoint subtrees Tr  Ts that originate from r  s respectively. Lemma 1 describes this
case for a directed tree with a single parent. We use this by induction on the parents on these directed
trees  noticing that graph separation guarantees: the statistics of Lemma 1 hold uniformly for every
assignment of the parent’s non-descendants as well; the optimal assignments in Lemma 1 are chosen
independently for every child for every assignment of the parent’s non-descendants label. (cid:3)
Our approximated sampling procedure expands the graphical model  creating layers of the original
graph  while connecting edges between vertices in the different layers if an edge exists in the original
graph. We use graph separations (Markov properties) to guarantee that the number of added layers
is polynomial in n  while we approach arbitrarily close to the Gibbs distribution. This construction
preserves the structure of the original graph  in particular  whenever the original graph has no cycles 
the expanded graph does not have cycles as well. In the experiments we show that this probability
model approximates well the Gibbs distribution for graphical models with many cycles.

4 Unbiased sampling using sequential bounds on the partition function

xi+1 ... xn

(cid:16)

(cid:104)

exp

Eγ

ity which is proportional to(cid:80)

In the following we describe how to use random MAP perturbations to generate unbiased samples
from the Gibbs distribution. Sampling from the Gibbs distribution is inherently tied to estimating the
partition function. Assume we could have compute the partition function exactly  then we could have
sample from the Gibbs distribution sequentially: for every dimension we sample xi with probabil-
exp(θ(x)). Unfortunately  approximations to the partition
function  as described in Section 3  cannot provide a sequential procedure that would generate un-
biased samples from the full Gibbs distribution. Instead  we construct a family of self-reducible
upper bounds which imitate the partition function behavior  namely bound the summation over its
exponentiations. These upper bounds extend the one in [5] when restricted to local perturbations.
Lemma 2. Let {γi(xi)} be a collection of i.i.d. random variables  each following the Gumbel
distribution with zero mean. Then for every j = 1  ...  n and every x1  ...  xj−1 holds

γi(xi)}(cid:105)(cid:17) ≤ exp
(cid:16)

γi(xi)}(cid:105)(cid:17)
(cid:88)
maxxj  ... xn{θ(x) + γn(xn)}(cid:105)(cid:17)
In particular  for j = n holds(cid:80)
(cid:2) maxxj+1 ... xn{θ(x) +
i=j γi(xi)(cid:9)(cid:3)(cid:3)  while the right hand side is attained by alternating the maximization with respect
(cid:80)n

Proof: The result is an application of the expectation-optimization interpretation of the partition
function in Theorem 1. The left hand side equals to Eγj
to xj with the expectation of γj+1  ...  γn. The proof then follows by taking the exponent.(cid:3)
We use these upper bounds for every dimension i = 1  ...  n to sample from a probability distribution
that follows a summation over exponential functions  with a discrepancy that is described by the
upper bound. This is formalized below in Algorithm 1

(cid:2) maxxj Eγj+1 ... γn

exp(θ(x)) = exp

{θ(x) +

max

xj+1 ... xn

n(cid:88)

i=j

Eγ

max
xj  ... xn

{θ(x) +

n(cid:88)

Eγn(xn)

i=j+1

xn

(cid:104)
(cid:104)

(cid:16)

xj

.

Algorithm 1 Unbiased sampling from Gibbs distribution using randomized prediction
Iterate over j = 1  ...  n  while keeping ﬁxed x1  ...  xj−1. Set

(cid:2) maxxj+1 ... xn{θ(x)+(cid:80)n
exp(cid:0)Eγ
exp(cid:0)Eγ
(cid:2) maxxj  ... xn{θ(x)+(cid:80)n
2. pj(r) = 1 −(cid:80)

1. pj(xj) =

p(xj)

xj

i=j+1 γi(xi)}(cid:3)(cid:1)
i=j γi(xi)}(cid:3)(cid:1)

.

3. Sample an element according to pj(·). If r is sampled then reject and restart with j = 1.

Otherwise  ﬁx the sampled element xj and continue the iterations.

Output: x1  ...  xn

When we reject the discrepancy  the probability we accept a conﬁguration x is the product of prob-
abilities in all rounds. Since these upper bounds are self-reducible  i.e.  for every dimension i we

4

P

j=1

are using the same quantities that were computed in the previous dimensions 1  ...  i − 1  we are
sampling an accepted conﬁguration proportionally to exp(θ(x))  the full Gibbs distribution.
Theorem 3. Let p(x) be the Gibbs distribution  deﬁned in Equation (1) and let {γi(xi)} be a col-
lection of i.i.d. random variables following the Gumbel distribution with zero mean. Then whenever
Algorithm 1 accepts  it produces a conﬁguration (x1  ...  xn) according to the Gibbs distribution

Proof: The probability of sampling a conﬁguration (x1  ...  xn) without rejecting is

(cid:104)
(cid:105)
Algorithm 1 outputs x(cid:12)(cid:12) Algorithm 1 accepts
{θ(x) +(cid:80)n
{θ(x) +(cid:80)n

exp(cid:0)Eγ

(cid:2) max

(cid:2) max
(cid:2) max

exp(cid:0)Eγ
i=j+1 γi(xi)}(cid:3)(cid:1)
n(cid:89)
i=j γi(xi)}(cid:3)(cid:1) =
exp(cid:0)Eγ
tion  i.e.  P(cid:2)Algorithm 1 accepts(cid:3) = Z(cid:14) exp(cid:0)Eγ

(cid:2) maxx1 ... xn{θ(x) +(cid:80)n

xj+1 ... xn

x1 ... xn

xj  ... xn

exp(θ(x))

= p(x).

The probability of sampling without rejecting is thus the sum of this probability over all conﬁgura-
conditioned on accepting a conﬁguration  it is produced according to the Gibbs distribution. (cid:3).
Acceptance/rejection follows the geometric distribution  therefore the sampling procedure rejects
k times with probability (1 − P [Algorithm 1 accepts])k. The running time of our Gibbs sampler
is determined by the average number of rejections 1/P [Algorithm 1 accepts].
Interestingly  this
average is the quality of the partition upper bound presented in [5]. To augment this result we
investigate in the next section efﬁciently computable lower bounds to the partition function  that are
based on random MAP perturbations. These lower bounds provide a way to efﬁciently determine the
computational complexity for sampling from the Gibbs distribution for a given potential function.

i=1 γi(xi)}(cid:3)(cid:1) .
{θ(x) +(cid:80)n
i=1 γi(xi)}(cid:3)(cid:1). Therefore

5 Lower bounds on the partition function

.

x

(cid:104)

max

maxx

In particular  log Z ≥ Eγ

The realization of the partition function as expectation-optimization pair in Theorem 1 provides
efﬁciently computable lower bounds on the partition function. Intuitively  these bounds correspond
to moving expectations (or summations) inside the maximization operations. In the following we
present two lower bounds that are derived along these lines  the ﬁrst holds in expectation and the
second holds in probability.
Corollary 1. Consider a family of subsets α ∈ A and let xα be a set of variables {xi}i∈α restricted
to the indexes in α. Assume that the random variables γα(xα) are i.i.d. according to the Gumbel
distribution with zero mean  for every α  xα. Then
∀α ∈ A log Z ≥ Eγ

(cid:104)
(cid:8)θ(x) + γα(xα)(cid:9)(cid:105)
α∈A γα(xα)(cid:9)(cid:105)
(cid:8)θ(x) + 1|A|
(cid:80)
Proof: Let ¯α = {1  ...  n} \ α then Z =(cid:80)
(cid:80)
exp(θ(x)) ≥(cid:80)
second result is attained while averaging these lower bounds log Z ≥(cid:80)

maxx ¯α exp(θ(x)). The ﬁrst
result is derived by swapping the maximization with the exponent  and applying Theorem 1. The
α∈A 1|A| Eγ[maxx{θ(x) +
γα(xα)}]  and by moving the summation inside the maximization operation. (cid:3)
The expected lower bound requires to invoke a MAP solver multiple times. Although this expecta-
tion may be estimated with a single MAP execution  the variance of this random MAP prediction
is around
n. We suggest to recursively use Lemma 1 to lower bound the partition function with a
single MAP operation in probability.
Corollary 2. Let θ(x) be a potential
function over x = (x1  ...  xn). We create multi-
ple copies of xi  namely xi ki for ki = 1  ...  mi  and deﬁne the extended potential function

ki=1 θ(x1 k1  ...  xn kn )/(cid:81) mi. We deﬁne the extended perturbation model ˆγi(xi) =

ˆθ(x) = (cid:80)mi
(cid:80)mi
Gumbel distribution with zero mean. Then  with probability at least 1 −(cid:80)n
holds log Z ≥ maxˆx{ˆθ(x) +(cid:80)n

ki=1 γi ki(xi ki)/mi where each perturbation is independent and distributed according to the
i=1 π2|dom(θ)|/6mi2

i=1 ˆγi(xi)} − n

√

x ¯α

xα

xα

.

5

lower bounds

unbiased samplesr complexity

approximate sampler

Figure 1: Left: comparing our expected lower and probable lower bounds with structured mean-ﬁeld and
belief propagation on attractive models with high signal and varying coupling strength. Middle: estimating
our unbiased sampling procedure complexity on spin glass models of varying sizes. Right: Comparing our
approximate sampling procedure on attractive models with high signal.

Proof: We estimate the expectation-optimization value of the log-partition function iteratively for
every dimension  while replacing each expectation with its sampled average  as described in Lemma
1. Our result holds for every potential function  thus the statistics in each recursion hold uniformly
for every x with probability at least 1 − π2|dom(θ)|/6mi2. We then move the averages inside the
maximization operation  thus lower bounding the n−approximation of the partition function. (cid:3)
The probable lower bound that we provide does not assume graph separations thus the statistical
guarantees are worse than the ones presented in the approximation scheme of Theorem 2. Also 
since we are seeking for lower bound  we are able relax our optimization requirements and thus to
use vertex based random perturbations γi(xi). This is an important difference that makes this lower
bound widely applicable and very efﬁcient.

6 Experiments

We evaluated our approach on spin glass models θ(x) = (cid:80)

i∈V θixi +(cid:80)

(i j)∈E θi jxixj. where
xi ∈ {−1  1}. Each spin has a local ﬁeld parameter θi  sampled uniformly from [−1  1]. The
spins interact in a grid shaped graphical model with couplings θi j  sampled uniformly from [0  c].
Whenever the coupling parameters are positive the model is called attractive as adjacent variables
give higher values to positively correlated conﬁgurations. Attractive models are computationally
appealing as their MAP predictions can be computed efﬁciently by the graph-cut algorithm [2].
We begin by evaluating our lower bounds  presented in Section 5  on 10 × 10 spin glass models.
Corollary 1 presents a lower bound that holds in expectation. We evaluated these lower bounds
while perturbing the local potentials with γi(xi). Corollary 2 presents a lower bound that holds
in probability and requires only a single MAP prediction on an expanded model. We evaluate the
probable bound by expanding the model to 1000 × 1000 grids  ignoring the discrepancy . For both
the expected lower bound and the probable lower bound we used graph-cuts to compute the random
MAP perturbations. We compared these bounds to the different forms of structured mean-ﬁeld  tak-
ing the one that performed best: standard structured mean-ﬁeld that we computed over the vertical
chains [8  1]  and the negative tree re-weighted computed on the horizontal and vertical trees [14].
We also compared to the sum-product belief propagation algorithm  which was recently proven to
produce lower bounds for attractive models [20  18]. We computed the error in estimating the loga-
rithm of the partition function  averaged over 10 spin glass models  see Figure 1. One can see that
the probable bound is the tightest when considering the medium and high coupling domain  which
is traditionally hard for all methods. As it holds in probability it might generate a solution which is
not a lower bound. One can also verify that on average this does not happen. The expected lower
bound is signiﬁcantly worse for the low coupling regime  in which many conﬁgurations need to be
taken into account. It is (surprisingly) effective for the high coupling regime  which is characterized
by a few dominant conﬁgurations.
Section 4 describes an algorithm that generates unbiased samples from the full Gibbs distribution.
Focusing on spin glass models with strong local ﬁeld potentials  it is well know that one cannot
produce unbiased samples from the Gibbs distributions in polynomial time [3]. Theorem 3 connects

6

Image + annotation

MAP solution

Average of 20 samples

Error estimates

Figure 2: Example image with the boundary annotation (left) and the error estimates obtained using our
method (right). Thin structures of the object are often lost in a single MAP solution (middle-left)  which are
recovered by averaging the samples (middle-right) leading to better error estimates.

the computational complexity of our unbiased sampling procedure to the gap between the logarithm
of the partition function and its upper bound in [5]. We use our probable lower bound to estimate this
gap on large grids  for which we cannot compute the partition function exactly. Figure 1 suggests
that the running time for this sampling procedure is sub-exponential.
Sampling from the Gibbs distribution in spin glass models with non-zero local ﬁeld potentials is
computationally hard [7  3]. The approximate sampling technique in Theorem 3 suggests a method
to overcome this difﬁculty by efﬁciently sampling from a distribution that approximates the Gibbs
distribution on its marginal probabilities. Although our theory is only stated for graphs without
cycles  it can be readily applied to general graphs  in the same way the (loopy) belief propaga-
tion algorithm is applied. For computational reasons we did not expand the graph. Also  we ex-
periment both with pairwise perturbations  as Theorem 2 suggests  and with local perturbations 
which are guaranteed to preserve the potential function super-modularity. We computed the local
marginal probability errors of our sampling procedure  while comparing to the standard methods
of Gibbs sampling  Metropolis and Swendsen-Wang1. In our experiments we let them run for at
most 1e8 iterations  see Figure 1. Both Gibbs sampling and the Metropolis algorithm perform sim-
ilarly (we omit the Gibbs sampler performance for clarity). Although these algorithms as well as
the Swendsen-Wang algorithm directly sample from the Gibbs distribution  they typically require
exponential running time to succeed on spin glass models. Figure 1 shows that these samplers are
worse than our approximate samplers. Although we omit from the plots for clarity  our approximate
sampling marginal probabilities compare to those of the sum-product belief propagation and the tree
re-weighted belief propagation [22]. Nevertheless  our sampling scheme also provides a probability
notion  which lacks in the belief propagation type algorithms. Surprisingly  the approximate sampler
that uses pairwise perturbations performs (slightly) worse than the approximate sampler that only
use local perturbations. Although this is not explained by our current theory  it is an encouraging
observation  since approximate sampler that uses random MAP predictions with local perturbations
is orders of magnitude faster.
Lastly  we emphasize the importance of probabilistic reasoning over the current variational methods 
such as tree re-weighted belief propagation [22] or max-marginal probabilities [10]  that only gen-
erate probabilities over small subsets of variables. The task we consider is to obtain pixel accurate
boundaries from rough boundaries provided by the user. For example in an image editing application
the user may provide an input in the form of a rough polygon and the goal is to reﬁne the boundaries
using the information from the gradients in the image. A natural notion of error is the average devi-
ation of the marked boundary from the true boundary of the image. Given a user boundary we set
up a graphical model on the pixels using foreground/background models trained from regions well
inside/outside the marked boundary. Exact binary labeling can be obtained using the graph-cuts al-
gorithm. From this we can compute the expected error by sampling multiple solutions using random
MAP predictors and averaging. On a dataset of 10 images which we carefully annotated to obtain
pixel accurate boundaries we ﬁnd that random MAP perturbations produce signiﬁcantly more accu-
rate estimates of boundary error compared to a single MAP solution. On average the error estimates
obtained using random MAP perturbations is off by 1.04 pixels from the true error (obtained from
ground truth) whereas the MAP which is off by 3.51 pixels. Such a measure can be used in an active
annotation framework where the users can iteratively ﬁx parts of the boundary that contain errors.

1We used Talya Meltzer’s inference package.

7

Figure 2 shows an example annotation  the MAP solution  the mean of 20 random MAP solutions 
and boundary error estimates.

7 Related work

The Gibbs distribution plays a key role in many areas of science  including computer science  statis-
tics and physics. To learn more about its roles in machine learning  as well as its standard samplers 
we refer the interested reader to the textbook [11]. Our work is based on max-statistics of collections
of random variables. For comprehensive introduction to extreme value statistics we refer the reader
to [12].
The Gibbs distribution and its partition function can be realized from the statistics of random
MAP perturbations with the Gumbel distribution (see Theorem 1)  [12  17  21  5]. Recently 
[16  9  17  21  6] explore the different aspects of random MAP predictions with low dimensional
perturbation. [16] describe sampling from the Gaussian distribution with random Gaussian pertur-
bations. [17] show that random MAP predictors with low dimensional perturbations share similar
statistics as the Gibbs distribution. [21] describe the Bayesian perspectives of these models and their
efﬁcient sampling procedures. [9  6] consider the generalization properties of such models within
PAC-Bayesian theory. In our work we formally relate random MAP perturbations and the Gibbs
distribution. Speciﬁcally  we describe the case for which the marginal probabilities of random MAP
perturbations  with the proper expansion  approximate those of the Gibbs distribution. We also
show how to use the statistics of random MAP perturbations to generate unbiased samples from
the Gibbs distribution. These probability models generate samples efﬁciently thorough optimiza-
tion: they have statistical advantages over purely variational approaches such as tree re-weighted
belief propagation [22] or max-marginals [10]  and they are faster than standard Gibbs samplers and
Markov chain Monte Carlo approaches when MAP prediction is efﬁcient [3  25]. Other methods
that efﬁciently produce samples include Herding [23] and determinantal processes [13].
Our suggested samplers for the Gibbs distribution are based on low dimensional representation of
the partition function  [5]. We augment their results in a few ways. In Lemma 2 we reﬁne their
upper bound  to a series of sequentially tighter bounds. Corollary 2 shows that the approximation
scheme of [5] is in fact a lower bound that holds in probability. Lower bounds for the partition func-
tion have been extensively developed in the recent years within the context of variational methods.
Structured mean-ﬁeld methods are inner-bound methods where a simpler distribution is optimized
as an approximation to the posterior in a KL-divergence sense [8  1  14]. The difﬁculty comes
from non-convexity of the set of feasible distributions. Surprisingly  [20  18] have shown that the
sum-product belief propagation provides a lower bound to the partition function for super-modular
potential functions. This result is based on the four function theorem which considers nonnegative
functions over distributive lattices.

8 Discussion

This work explores new approaches to sample from the Gibbs distribution. Sampling from the Gibbs
distribution is key problem in machine learning. Traditional approaches  such as Gibbs sampling 
fail in the “high-signal high-coupling” regime that results in ragged energy landscapes. Following
[17  21]  we showed here that one can take advantage of efﬁcient MAP solvers to generate approx-
imate or unbiased samples from the Gibbs distribution  when we randomly perturb the potential
function. Since MAP predictions are not affected by ragged energy landscapes  our approach excels
in the “high-signal high-coupling” regime. As a by-product to our approach we constructed lower
bounds to the partition functions  which are both tighter and faster than the previous approaches in
the ”high-signal high-coupling” regime.
Our approach is based on random MAP perturbations that estimate the partition functions with
expectation. In practice we compute the empirical mean. [15] show that the deviation of the sampled
mean from its expectation decays exponentially.
The computational complexity of our approximate sampling procedure is determined by the pertur-
bations dimension. Currently  our theory do not describe the success of the probability model that is
based on the maximal argument of perturbed MAP program with local perturbations.

8

References
[1] Alexandre Bouchard-Cˆot´e and Michael I Jordan. Optimization of structured mean ﬁeld objec-

tives. In AUAI  pages 67–74  2009.

[2] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts.

PAMI  2001.

[3] L.A. Goldberg and M. Jerrum. The complexity of ferromagnetic ising with local ﬁelds. Com-

binatorics Probability and Computing  16(1):43  2007.

[4] E.J. Gumbel and J. Lieblein. Statistical theory of extreme values and some practical applica-

tions: a series of lectures  volume 33. US Govt. Print. Ofﬁce  1954.

[5] T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori pertur-

bations. In Proceedings of the 29th International Conference on Machine Learning  2012.

[6] T. Hazan  S. Maji  Keshet J.  and T. Jaakkola. Learning efﬁcient random maximum a-posteriori
predictors with non-decomposable loss functions. Advances in Neural Information Processing
Systems  2013.

[7] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the ising model.

SIAM Journal on computing  22(5):1087–1116  1993.

[8] M.I. Jordan  Z. Ghahramani  T.S. Jaakkola  and L.K. Saul. An introduction to variational

methods for graphical models. Machine learning  37(2):183–233  1999.

[9] J. Keshet  D. McAllester  and T. Hazan. Pac-bayesian approach for minimization of phoneme

error rate. In ICASSP  2011.

[10] Pushmeet Kohli and Philip HS Torr. Measuring uncertainty in graph cut solutions–efﬁciently

computing min-marginal energies using dynamic graph cuts. In ECCV  pages 30–43. 2006.

[11] D. Koller and N. Friedman. Probabilistic graphical models. MIT press  2009.
[12] S. Kotz and S. Nadarajah. Extreme value distributions: theory and applications. World Scien-

tiﬁc Publishing Company  2000.

[13] A. Kulesza and B. Taskar. Structured determinantal point processes. In Proc. Neural Informa-

tion Processing Systems  2010.

[14] Qiang Liu and Alexander T Ihler. Negative tree reweighted belief propagation. arXiv preprint

arXiv:1203.3494  2012.

[15] Francesco Orabona  Tamir Hazan  Anand D Sarwate  and Tommi. Jaakkola. On measure con-

centration of random maximum a-posteriori perturbations. arXiv:1310.4227  2013.

[16] G. Papandreou and A. Yuille. Gaussian sampling by local perturbations. In Proc. Int. Conf. on

Neural Information Processing Systems (NIPS)  pages 1858–1866  December 2010.

[17] G. Papandreou and A. Yuille. Perturb-and-map random ﬁelds: Using discrete optimization to

learn and sample from energy models. In ICCV  Barcelona  Spain  November 2011.

[18] Nicholas Ruozzi. The bethe partition function of log-supermodular graphical models. arXiv

preprint arXiv:1202.6035  2012.

[19] D. Sontag  T. Meltzer  A. Globerson  T. Jaakkola  and Y. Weiss. Tightening LP relaxations for

MAP using message passing. In Conf. Uncertainty in Artiﬁcial Intelligence (UAI)  2008.

[20] E.B. Sudderth  M.J. Wainwright  and A.S. Willsky. Loop series and Bethe variational bounds
in attractive graphical models. Advances in neural information processing systems  20  2008.
[21] D. Tarlow  R.P. Adams  and R.S. Zemel. Randomized optimum models for structured predic-

tion. In Proceedings of the 15th Conference on Artiﬁcial Intelligence and Statistics  2012.

[22] M. J. Wainwright  T. S. Jaakkola  and A. S. Willsky. A new class of upper bounds on the log

partition function. Trans. on Information Theory  51(7):2313–2335  2005.

[23] Max Welling. Herding dynamical weights to learn. In Proceedings of the 26th Annual Inter-

national Conference on Machine Learning  pages 1121–1128. ACM  2009.

[24] T. Werner. High-arity interactions  polyhedral relaxations  and cutting plane algorithm for soft

constraint optimisation (map-mrf). In CVPR  pages 1–8  2008.

[25] J. Zhang  H. Liang  and F. Bai. Approximating partition functions of the two-state spin system.

Information Processing Letters  111(14):702–710  2011.

9

,Tamir Hazan
Subhransu Maji
Tommi Jaakkola
Praneeth Netrapalli
Niranjan U N
Sujay Sanghavi
Animashree Anandkumar
Prateek Jain