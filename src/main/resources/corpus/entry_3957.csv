2017,On the Consistency of Quick Shift,Quick Shift is a popular mode-seeking and clustering algorithm. We present finite sample statistical consistency guarantees for Quick Shift on mode and cluster recovery under mild distributional assumptions. We then apply our results to construct a consistent modal regression algorithm.,On the Consistency of Quick Shift

Heinrich Jiang

Google Inc.

1600 Amphitheatre Parkway  Mountain View  CA 94043

heinrich.jiang@gmail.com

Abstract

Quick Shift is a popular mode-seeking and clustering algorithm. We present ﬁnite
sample statistical consistency guarantees for Quick Shift on mode and cluster
recovery under mild distributional assumptions. We then apply our results to
construct a consistent modal regression algorithm.

1

Introduction

Quick Shift [16] is a clustering and mode-seeking procedure that has received much attention in
computer vision and related areas. It is simple and proceeds as follows: it moves each sample to its
closest sample with a higher empirical density if one exists in a τ radius ball  where the empirical
density is taken to be the Kernel Density Estimator (KDE). The output of the procedure can thus be
seen as a graph whose vertices are the sample points and a directed edge from each sample to its next
point if one exists. Furthermore  it can be seen that Quick Shift partitions the samples into trees which
can be taken as the ﬁnal clusters  and the root of each such tree is an estimate of a local maxima.
Quick Shift was designed as an alternative to the better known mean-shift procedure [4  5]. Mean-shift
performs a gradient ascent of the KDE starting at each sample until -convergence. The samples that
correspond to the same points of convergence are in the same cluster and the points of convergence
are taken to be the estimates of the modes. Both procedures aim at clustering the data points by
incrementally hill-climbing to a mode in the underlying density. Some key differences are that Quick
Shift restricts the steps to sample points and has the extra τ parameter. In this paper  we show that
Quick Shift can surprisingly attain strong statistical guarantees without the second-order density
assumptions required to analyze mean-shift.
We prove that Quick Shift recovers the modes of an arbitrary multimodal density at a minimax optimal
rate under mild nonparametric assumptions. This provides an alternative to known procedures with
similar statistical guarantees; however such procedures only recover the modes but fail to inform us
how to assign the sample points to a mode which is critical for clustering. Quick Shift on the other
hand recovers both the modes and the clustering assignments with statistical consistency guarantees.
Moreover  Quick Shift’s ability to do all of this has been extensively validated in practice.
A unique feature of Quick Shift is that it has a segmentation parameter τ which allows practioners
to merge clusters corresponding to certain less salient modes of the distribution. In other words  if
a local mode is not the maximizer of its τ-radius neighborhood  then its corresponding cluster will
become merged to that of another mode. Current consistent mode-seeking procedures [6  12] fail to
allow one to control such segmentation. We give guarantees on how Quick Shift does this given an
arbitrary setting of τ.
We show that Quick Shift can also be used to recover the cluster tree. In cluster tree estimation  the
known procedures with the strongest statistical consistency guarantees include Robust Single Linkage
(RSL) [2] and its variants e.g. [13  7]. We show that Quick Shift attains similar guarantees.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Thus  Quick Shift  a simple and already popular procedure  can simultaneously recover the modes
with segmentation tuning  provide clustering assignments to the appropriate mode  and can estimate
the cluster tree of an unknown density f with the strong consistency guarantees. No other procedure
has been shown to have these properties.
Then we use Quick Shift to solve the modal regression problem [3]  which involves estimating the
modes of the conditional density f (y|X) rather than the mean as in classical regression. Traditional
approaches use a modiﬁed version of mean-shift. We provide an alternative using Quick Shift which
has precise statistical consistency guarantees under much more mild assumptions.

Figure 1: Quick Shift example. Left: τ = ∞. The procedure returns one tree  whose head is the
sample with highest empirical density. Right: τ set to a lower value. The edges with length greater
than τ are no longer present when compared to the left. We are left with three clusters.

2 Assumptions and Supporting Results

Input: Samples X[n] := {x1  ...  xn}  KDE bandwidth h  segmentation parameter τ > 0.
Initialize directed graph G with vertices {x1  ...  xn} and no edges.
for i = 1 to n do

if there exists x ∈ X[n] such that (cid:98)fh(x) > (cid:98)fh(xi) and ||x − xi|| ≤ τ then
Add to G a directed edge from xi to argminxj∈X[n]:(cid:98)fh(xj )>(cid:98)fh(xi)||xi − xj||.

Algorithm 1 Quick Shift

end if
end for
return G.

2.1 Setup
Let X[n] = {x1  ...  xn} be n i.i.d. samples drawn from distribution F with density f over the
uniform measure on Rd.
Assumption 1 (Hölder Density). f is Hölder continuous on compact support X ⊆ Rd. i.e. |f (x) −
f (x(cid:48))| ≤ Cα||x − x(cid:48)||α for all x  x(cid:48) ∈ X and some 0 < α ≤ 1 and Cα > 0.
Deﬁnition 1 (Level Set). The λ level set of f is deﬁned as Lf (λ) := {x ∈ X : f (x) ≥ λ}.
Deﬁnition 2 (Hausdorff Distance). dH (A  A(cid:48)) = max{supx∈A d(x  A(cid:48))  supx∈A(cid:48) d(x  A)}  where
d(x  A) := inf x(cid:48)∈A ||x − x(cid:48)||.
The next assumption says that the level sets are continuous w.r.t. the level in the following sense
where we denote the -interior of A as A(cid:9) := {x ∈ A  inf y∈∂A d(x  y) ≥ } (∂A is the boundary
of A):
Assumption 2 (Uniform Continuity of Level Sets). For each  > 0  there exists δ > 0 such that for
0 < λ ≤ λ(cid:48) ≤ ||f||∞ with |λ − λ(cid:48)| < δ  then Lf (λ)(cid:9) ⊆ Lf (λ(cid:48)).
Remark 1. Procedures that try to incrementally move points to nearby areas of higher density will
have difﬁculties in regions where there is little or no change in density. The above assumption is a
simple and mild formulation which ensures there are no such ﬂat regions.
Remark 2. Note that our assumptions are quite mild when compared to analyses of similar proce-
dures like mean-shift  which require at least second-order smoothness assumptions. Interestingly  we
only require Hölder continuity.

2

2.2 KDE Bounds

numbers such that(cid:82)

Rd K(u)du = 1.

We next give uniform bounds on KDE required to analyze Quick Shift.
Deﬁnition 3. Deﬁne kernel function K : Rd → R≥0 where R≥0 denotes the non-negative real

We make the following mild regularity assumptions on K.
Assumption 3. (Spherically symmetric  non-increasing  and exponential decays) There exists non-
increasing function k : R≥0 → R≥0 such that K(u) = k(|u|) for u ∈ Rd and there exists
ρ  Cρ  t0 > 0 such that for t > t0  k(t) ≤ Cρ · exp(−tρ).
Remark 3. These assumptions allow the popular kernels such as Gaussian  Exponential  Silverman 
uniform  triangular  tricube  Cosine  and Epanechnikov.
Deﬁnition 4 (Kernel Density Estimator). Given a kernel K and bandwidth h > 0 the KDE is deﬁned
by

(cid:98)fh(x) =

1

n · hd

n(cid:88)

i=1

K

(cid:18) x − Xi

(cid:19)

.

h

Here we provide the uniform KDE bound which will be used for our analysis  established in [11].
Lemma 1. [(cid:96)∞ bound for α-Hölder continuous functions. Theorem 2 of [11]] There exists positive
constant C(cid:48) depending on f and K such that the following holds with probability at least 1 − 1/n
uniformly in h > (log n/n)1/d.

|(cid:98)fh(x) − f (x)| < C(cid:48) ·

(cid:32)

(cid:114)

hα +

log n
n · hd

(cid:33)

.

sup
x∈Rd

3 Mode Estimation

In this section  we give guarantees about the local modes returned by Quick Shift. We make the
additional assumption that the modes are local maxima points with negative-deﬁnite Hessian.
Assumption 4. [Modes] A local maxima of f is a connected region M such that the density is
constant on M and decays around its boundaries. Assume that each local maxima of f is a point 
which we call a mode. Let M be the modes of f where M is a ﬁnite set. Then let f be twice
differentiable around a neighborhood of each x ∈ M and let f have a negative-deﬁnite Hessian at
each x ∈ M and those neighborhoods are disjoint.
This assumption leads to the following.
Lemma 2 (Lemma 5 of [6]). Let f satisfy Assumption 4. There exists rM   ˇC  ˆC > 0 such that the
following holds for all x0 ∈ M simultaneously.

ˇC · |x0 − x|2 ≤ f (x0) − f (x) ≤ ˆC · |x0 − x|2 

for all x ∈ Ax0 where Ax0 is a connected component of {x : f (x) ≥ inf x(cid:48)∈B(x0 rM ) f (x)} which
contains x0 and does not intersect with other modes.

The next assumption ensures that the level sets don’t become arbitrarily thin as long as we are
sufﬁciently away from the modes.
Assumption 5. [Level Set Regularity] For each σ  r > 0  there exists η > 0 such that the following
holds for all connected components A of Lf (λ) with λ > 0 and A (cid:54)⊆ ∪x0∈MB(x0  r). If x lies on
the boundary of A  then Vol(B(x  σ) ∩ A) > η where Vol is volume w.r.t. the uniform measure on Rd.
We next give the result about mode recovery for Quick Shift. It says that as long as τ is small enough 
then as the number of samples grows  the roots of the trees returned by Quick Shift will bijectively
correspond to the true modes of f and the estimation errors will match lower bounds established by
Tsybakov [15] up to logarithmic factors. We defer the proof to Theorem 2 which is a generalization
of the following result.
Theorem 1 (Mode Estimation guarantees for Quick Shift). Let τ < rM /2 and Assumptions 1  2  3  4 

and 5 hold. Choose h such that (log n)2/ρ · h → 0 and log n/(nhd) → 0 as n → ∞. Let (cid:99)M be the

3

heads of the trees in G (returned by Algorithm 1). There exists constant C depending on f and K
such that for n sufﬁciently large  with probability at least 1 − 1/n 

(cid:32)

dH (M  (cid:99)M)2 < C

(cid:114)

(cid:33)

.

log n
n · hd

(log n)4/ρh2 +

and |M| = |(cid:99)M|. In particular  taking h ≈ n−1/(4+d) optimizes the above rate to d(M  (cid:99)M) =

˜O(n−1/(4+d)). This matches the minimax optimal rate for mode estimation up to logarithmic factors.
We now give a stronger notion of mode that ﬁts better for analyzing the role of τ. In the last result  it
was assumed that the practitioner wished to recover exactly the modes of the density f by taking τ
sufﬁciently small. Now  we analyze the case where τ is intentionally set to a particular value so that
Quick Shift produces segmentations that group modes together that are in close proximity to higher
density regions.
Deﬁnition 5. A mode x0 ∈ M is an (r  δ)+-mode if f (x0) > f (x) + δ for all x ∈
B(x0  r)\B(x0  rM ). A mode x0 ∈ M is an (r  δ)−-mode if f (x0) < f (x) − δ for some
x ∈ B(x0  r). Let M+
r δ ⊆ M denote the set of (r  δ)+-modes and (r  δ)−-modes
of f  respectively.

r δ ⊆ M and M−

In other words  an (r  δ)+-mode is a mode that is also a maximizer in a larger ball of radius r by at
least δ when outside of the region where there is quadratic decay and smoothness (B(x0  rM )). An
(r  δ)−-mode is a mode that is not the maximizer in its radius r ball by a margin of at least δ.
The next result shows that Algorithm recovers the (τ +  δ)+-modes of f and excludes the (τ −  δ)−-
modes of f. The proof is in the appendix.
Theorem 2. (Generalization of Theorem 1) Let δ   > 0 and suppose Assumptions 1  2  3  4  and 5
hold. Let h ≡ h(n) be chosen such that h → 0 and log n/(nhd) → 0 as n → ∞. Then there exists
C > 0 depending on f and K such that the following holds for n sufﬁciently large with probability
at least 1 − 1/n. For each x0 ∈ M+

τ− δ  there exists unique ˆx ∈ (cid:99)M such that

(cid:32)
τ + δ\M−

||x0 − ˆx||2 < C

(log n)4/ρh2 +

(cid:114)

(cid:33)

.

log n
n · hd

Moreover  |(cid:99)M| ≤ |M| − |M−

τ− δ|.

In particular  taking  → 0 and δ → 0 gives us an exact characterization of the asymptotic behavior
of Quick Shift in terms of mode recovery.

4 Assignment of Points to Modes

In this section  we give guarantees on how the points are assigned to their respective modes. We
ﬁrst give the following deﬁnition which formalizes how two points are separated by a wide and deep
valley.
Deﬁnition 6. x1  x2 ∈ X are (rs  δ)-separated if there exists a set S such that every path from x1
and x2 intersects with S and

sup

x∈S+B(0 rs)

f (x) <

inf

x∈B(x1 rs)∪B(x2 rs)

f (x) − δ.

Lemma 3. Suppose Assumptions 1  2  3  4  and 5 hold. Let τ < rs/2 and choose h such that
(log n)2/ρ · h → 0 and log n/(nhd) → 0 as n → ∞. Let G be the output of Algorithm 1. The
following holds with probability at least 1 − 1/n for n sufﬁciently large depending on f  K  δ  and τ
uniformly in all x1  x2 ∈ X . If x1 and x2 are (rs  δ)-separated  then there cannot exist a directed
path from x1 to x2 in G.

Proof. Suppose that x1 and x2 are (rs  δ)-separated (with respect to set S) and there exists a
directed path from x1 to x2 in G. Given our choice of τ  there exists some point x ∈ G such that
x ∈ S+B(0  rs) and x is on the path from x1 to x2. We have f (x) < f (x1)−δ. Choose n sufﬁciently

large such that by Lemma 1  supx∈X |(cid:98)fh(x) − f (x)| < δ/2. Thus  we have (cid:98)fh(x) < (cid:98)fh(x1)  which

means a directed path in G starting from x1 cannot contain x  a contradiction. The result follows.

4

Figure 2: Illustration of (rs  δ)-separation in 1 dimension. Here A and B are (rs  δ)-separated by S.
This is because the minimum density level of rs-radius balls around A and B (the red dotted line)
exceeds the maximum density level of the rs-radius ball around S by at least δ (golden dotted line).
In other words  there exists a sufﬁciently wide (controlled by rs and S) and deep (controlled by δ)
valley separating A and B. The results in this section will show that in such cases  these pairs of
points will not be assigned to the same cluster.

This leads to the following consequence about how samples are assigned to their respective modes.
Theorem 3. Assume the same conditions as Lemma 3. The following holds with probability at least
1 − 1/n for n sufﬁciently large depending on f  K  δ  and τ uniformly in x ∈ X and x0 ∈ M. For
each x ∈ X and x0 ∈ M  if x and x0 are (rs  δ)-separated  then x will not be assigned to the tree
corresponding to x0 from Theorem 1.
Remark 4. In particular  taking δ → 0 and rs → 0 gives us guarantees for all points which have a
unique mode in which it can be assigned to.

We now give a more general version of (rs  δ)-separation  in which the condition holds if every path
between the two points dips down at some point. The same results as the above extend for this
deﬁnition in a straightforward manner.
Deﬁnition 7. x1  x2 ∈ X are (rs  δ)-weakly-separated if there exists a set S  with x1  x2 (cid:54)∈ S +
B(0  rs)  such that every path P from x1 and x2 satisifes the following. (1) P ∩ S (cid:54)= ∅ and (2)

sup

x∈P∩S+B(0 rs)

f (x) <

x∈B(x(cid:48)

1 rs)∪B(x(cid:48)

inf

2 rs)

f (x) − δ 

1  x(cid:48)

2 are deﬁned as follows. Let P1 be the path obtained by starting at x1 and following
where x(cid:48)
P until it intersects S  and P2 be the path obtained by following P starting from the last time it
intersects S until the end. Then x(cid:48)
2 are points which respectively attain the highest values of f
on P1 and P2.

1 and x(cid:48)

5 Cluster Tree Recovery

The connected components of the level sets as the density level varies forms a hierarchical structure
known as the cluster tree.
Deﬁnition 8 (Cluster Tree). The cluster tree of f is given by

Deﬁnition 9. Let G(λ) be the subgraph of G with vertices x ∈ X[n] such that (cid:98)fh(x) > λ and edges

Cf (λ) := connected components of {x ∈ X : f (x) ≥ λ}.

between pairs of vertices which have corresponding edges in G. Let ˜G(λ) be the sets of vertices
corresponding to the connected components of G(λ).
Deﬁnition 10. Suppose that A is a collection of sets of points in Rd. Then deﬁne Link(A  δ)
to be the result of repeatedly removing pairs A1  A2 ∈ A from A (A1 (cid:54)= A2) that satisfy
inf a1∈A1 inf a2∈A2 ||a1 − a2|| < δ and adding A1 ∪ A2 to A until no such pairs exist.
Parameter settings for Algorithm 2: Suppose that τ ≡ τ (n) is chosen as a function of n such
such that τ → 0 as n → ∞  τ (n) ≥ (log2 n/n)1/d and h ≡ h(n) is chosen such that h → 0 and
log n/(nhd) → 0 as n → ∞.
The following is the main result of this section  the proof is in the appendix.

5

Algorithm 2 Quick Shift Cluster Tree Estimator

Input: Samples X[n] := {X1  ...  Xn}  KDE bandwidth h  segmentation parameter τ > 0.
Let G be the output of Quick Shift (Algorithm 1) with above parameters.

For λ > 0  let (cid:98)Cf (λ) := Link( ˜G(λ)  τ ).
return (cid:98)Cf

Theorem 4 (Consistency). Algorithm 2 converges in probability to the true cluster tree of f under
merge distortion (deﬁned in [7]).
Remark 5. By combining the result of this section with the mode estimation result  we can obtain
the following interpretation. For any level λ  a component in G(λ) estimates a connected component
of the λ-level set of f  and that further  the trees within that component in G(λ) have a one-to-one
correspondence with the modes in the connected component.

Figure 3: Illustration on 1-dimensional density with three modes A  B  and C. When restricting Quick
Shift’s output to samples have empirical density above a certain threshold and connecting nearby
clusters  then this approximates the connected components of the true density level set. Moreover  we
give guarantees that such points will be assigned to clusters which correspond to modes within its
connected component.

6 Modal Regression
Suppose that we have joint density f (X  y) on Rd × R w.r.t. to the Lebesgue measure. In modal
regression  we are interested in estimating the modes of the conditional f (y|X = x) given samples
from the joint distribution.

Algorithm 3 Quick Shift Modal Regression

Input: Samples D := {(x1  y1)  ...  (xn  yn)}  bandwidth h  τ > 0  and x ∈ X .

Let Y = {y1  ...  yn} and (cid:98)fh be the KDE computed w.r.t. D.
if there exists yj ∈ [yi − τ  yi + τ ] ∩ Y such that (cid:98)fh(x  yj) > (cid:98)fh(x  yi) then
Add to G an directed edge from yi to argminyi∈Y :(cid:98)fh(x yj )>(cid:98)fh(x yi)||yi − yj||.

Initialize directed graph G with vertices Y and no edges.
for i = 1 to n do

end if
end for
return The roots of the trees of G as the estimates of the modes of f (y|X = x).

Theorem 5 (Consistency of Quick Shift Modal Regression). Suppose that τ ≡ τ (n) is chosen as a
function of n such such that τ → 0 as n → ∞  τ (n) ≥ (log2 n/n)1/d and h ≡ h(n) is chosen such
that h → 0 and log n/(nhd+1) → 0 as n → ∞. Let Mx be the modes of the conditional density
in x such that f (y|X = x) and K satisﬁes Assumptions 1  2  3  4  and 5 

f (y|X = x) and (cid:99)Mx be the output of Algorithm 3. Then with probability at least 1 − 1/n uniformly

dH (Mx  (cid:99)Mx) → 0 as n → ∞.

6

7 Related Works

Mode Estimation. Perhaps the most popular procedure to estimate the modes is mean-shift; however 
it has proven quite difﬁcult to analyze. Arias-Castro et al. [1] made much progress by utilizing
dynamical systems theory to show that mean-shift’s updates converge to the correct gradient ascent
steps. The recent work of Dasgupta and Kpotufe [6] was the ﬁrst to give a procedure which recovers
the modes of a density with minimax optimal statistical guarantees in a multimodal density. They do
this by using a top-down traversal of the density levels of a proximity graph  borrowing from work
in cluster tree estimation. The procedure was shown to recover exactly the modes of the density at
minimax optimal rates.
In this work  we showed that Quick Shift attains the same guarantees while being a simpler approach
than known procedures that attain these guarantees [6  12]. Moreover unlike these procedures  Quick
Shift also assigns the remaining samples to their appropriate modes. Furthermore  Quick Shift also
has a segmentation tuning parameter τ which allows us to merge the clusters of modes that are not
maximal in its τ-radius neighborhood into the clusters of other modes. This is useful as in practice 
one may not wish to pick up every single local maxima  especially when there are local maxima that
can be grouped together by proximity. We formalized the segmentation of such modes and identify
which modes get returned and which ones become merged into other modes’ clusters by Quick Shift.
Cluster Tree Estimation. Work on cluster tree estimation has a long history. Some early work on
density based clustering by Hartigan [9] modeled the clusters of a density as the regions {x : f (x) ≥
λ} for some λ. This is called the density level-set of f at level λ. The cluster tree of f is the hierarchy
formed by the inﬁnite collection of these clusters over all λ. Chaudhuri and Dasgupta [2] introduced
Robust Single Linkage (RSL) which was the ﬁrst cluster tree estimation procedure with precise
statistical guarantees. Shortly after  Kpotufe and Luxburg [13] provided an estimator that ensured
false clusters were removed using used an extra pruning step. Interestingly  Quick Shift does not
require such a pruning step  since the points near cluster boundaries naturally get assigned to regions
with higher density and thus no spurious clusters are formed near these boundaries. Sriperumbudur
and Steinwart [14]  Jiang [10]  Wang et al. [17] showed that the popular DBSCAN algorithm [8] also
estimates these level sets. Eldridge et al. [7] introduced the merge distortion metric for cluster tree
estimates  which provided a stronger notion of consistency. We use their framework to analyze Quick
Shift and show that this simple estimator is consistent in merge distortion.

Figure 4: Density-based clusters discovered by level-set model {x : f (x) ≥ λ} (e.g. DBSCAN) vs
Quick Shift on a one dimensional density. Left two images: level sets for two density level settings.
Unassigned regions are noise and have no cluster assignment. Right two images: Quick Shift with
two different τ settings. The latter is a hill-climbing based clustering assignment.

Modal Regression. Nonparametric modal regression [3] is an alternative to classical regression 
where we are interested in estimating the modes of the conditional density f (y|X = x) rather
than the mean. Current approaches primarily use a modiﬁcation of mean-shift; however analysis
for mean-shift require higher order smoothness assumptions. Using Quick Shift instead for modal
regression requires less regularity assumptions while having consistency guarantees.

8 Conclusion

We provided consistency guarantees for Quick Shift under mild assumptions. We showed that
Quick Shift recovers the modes of a density from a ﬁnite sample with minimax optimal guarantees.
The approach of this method is considerably different from known procedures that attain similar
guarantees. Moreover  Quick Shift allows tuning of the segmentation and we provided an analysis
of this behavior. We also showed that Quick Shift can be used as an alternative for estimating the

7

cluster tree which contrasts with current approaches which utilize proximity graph sweeps. We
then constructed a procedure for modal regression using Quick Shift which attains strong statistical
guarantees.

Appendix

Mode Estimation Proofs
Lemma 4. Suppose Assumptions 1  2  3  4  and 5 hold. Let ¯r > 0 and h ≡ h(n) be chosen such
that h → 0 and log n/(nhd) → 0 as n → ∞. Then the following holds for n sufﬁciently large with
probability at least 1 − 1/n. Deﬁne

(cid:40)
argmaxx∈B(x0 ¯r)∩X[n] (cid:98)fh(x)  we have

˜r2 := max

32 ˆC
ˇC

(cid:114)

(cid:41)

.

(log n)4/ρh2  17 · C(cid:48)

log n
n · hd

Suppose x0 ∈ M and x0 is the unique maximizer of f on B(x0  ¯r). Then letting ˆx :=

||x0 − ˆx|| < ˜r.

Proof sketch. This follows from modifying the proof of Theorem 3 of [11] by replacing Rd\B(x0  ˜r)
with B(x0  ¯r)\B(x0  ˜r). This leads us to

(cid:98)fh(x) >

inf

x∈B(x0 rn)

sup

x∈B(x0 ¯r)\B(x0 ˜r)

(cid:98)fh(x) 

where rn := minx∈X[n] |x0 − x| and n is chosen sufﬁciently large such that ˜r < τ. Thus  |x0 − ˆx| ≤
τ− δ. Let ˆx := argmaxx∈B(x0 τ )∩X[n] (cid:98)fh(x).
˜r.
We ﬁrst show that ˆx ∈ (cid:99)M.
Proof of Theorem 2. Suppose that x0 ∈ M+
(cid:27)
remains to show that ˆx = argmaxx∈B(ˆx τ )∩X[n] (cid:98)fh(x). We have B(ˆx  τ ) ⊆ B(x0  τ + ˜r). Choose
n sufﬁciently large such that (i) ˜r <   (ii) by Lemma 1  supx∈X |(cid:98)fh(x) − f (x)| < δ/4 and (iii)

(log n)4/ρh2  17 · C(cid:48)(cid:113) log n

By Lemma 4  we have |x0 − ˆx| ≤ ˜r where ˜r2 := max

τ + δ\M−

(cid:26)

32 ˆC
ˇC

n·hd

. It

˜r2 < δ/(4 ˆC). Now  we have

sup

x∈B(x0 τ +˜r)\B(x0 τ )

(cid:98)fh(x) ≤

f (x) + δ/4 ≤ f (x0) − 3δ/4

sup

x∈B(x0 τ +˜r)\B(x0 τ )

≤ f (ˆx) + ˆC ˜r2 − 3δ/4 < f (ˆx) − δ/2 < (cid:98)fh(ˆx).

choosing n sufﬁciently large such that ˜r < τ /2  we obtain ˆx ∈ B(ˆx(cid:48)  τ ). This implies that ˆx = ˆx(cid:48)  as
desired.

Thus  ˆx = argmaxx∈B(ˆx τ )∩X[n] (cid:98)fh(x). Hence  ˆx ∈ (cid:99)M.
Next  we show that it is unique. To do this  suppose that ˆx(cid:48) ∈ (cid:99)M such that ||ˆx(cid:48) − x0|| ≤ τ /2. Then
we have both ˆx = argmaxx∈B(ˆx τ )∩X[n] (cid:98)fh(x) and ˆx(cid:48) = argmaxx∈B(ˆx(cid:48) τ )∩X[n] (cid:98)fh(x). However 
We now show |(cid:99)M| ≤ |M| − |M−
τ− δ|. Suppose that ˆx ∈ (cid:99)M. Let τ0 := min{/3  τ /3  rM /2}. We
show that B(ˆx  τ0) ∩ M (cid:54)= ∅. Suppose otherwise. Let λ = f (ˆx). By Assumptions 2 and 5  we
Lf (λ + σ)) ≥ η. Choose n sufﬁciently large such that (i) by Lemma 1  supx∈X |(cid:98)fh(x) − f (x)| <
have that there exists σ > 0 and η > 0 such that the following holds uniformly: Vol(B(ˆx  τ0) ∩
Chaudhuri and Dasgupta [2]. Then (cid:98)fh(x) > λ+σ/2 > (cid:98)fh(ˆx) but x ∈ B(ˆx  τ0)  a contradiction since
min σ/2  δ/4 and (ii) there exists a sample x ∈ B(ˆx  /3) ∩ Lf (λ + σ) ∩ X[n] by Lemma 7 of
ˆx is the maximizer of the KDE of the samples in its τ-radius neighborhood. Thus  B(ˆx  τ0)∩M (cid:54)= ∅.
Now  suppose that there exists x0 ∈ B(ˆx  τ0) ∩ M−
τ− δ. Then  there exists x(cid:48) ∈ B(x0  τ − 2τ0)

8

such that f (x(cid:48)) ≥ f (x0) + δ. Then  if ¯x is the closest sample point to x(cid:48)  we have for n sufﬁciently
But ¯x ∈ B(ˆx  τ ) ∩ X[n]  contradicting the fact that ˆx is the maximizer of the KDE over samples in
its τ-radius neighborhood. Thus  B(ˆx  τ0) ∩ (M\M−

large  |x(cid:48) − ¯x| ≤ τ0 and f (¯x) ≥ f (x0) + δ/2 and thus (cid:98)fh(¯x) > f (¯x) − δ/4 ≥ f (ˆx) + δ/4 > (cid:98)fh(ˆx).
Finally  suppose that there exists ˆx  ˆx(cid:48) ∈ (cid:99)M such that x0 ∈ M\M−

τ− δ and x0 ∈ B(ˆx  τ0) ∩

B(ˆx(cid:48)  τ0). Then  ˆx  ˆx(cid:48) ∈ B(x0  τ0)  thus |ˆx − ˆx(cid:48)| ≤ τ and thus ˆx = ˆx(cid:48)  as desired.

τ− δ) (cid:54)= ∅.

where the last inequality holds for n sufﬁciently large so that τ is sufﬁciently small. Thus  we have

λ − . Given our choice of τ  it follows by Lemma 7 of [2] that B(x  τ /2) ∩ X[n] is non-empty for n
sufﬁciently large. Let x(cid:48) ∈ B(x  τ /2) ∩ X[n]. Choose n sufﬁciently large such that by Lemma 1  we

Cluster Tree Estimation Proofs
Lemma 5 (Minimality). The following holds with probability at least 1 − 1/n. If A is a connected
for any  > 0 as n → ∞.

component of {x ∈ X : f (x) ≥ λ}  then A ∩ X[n] is contained in the same component in (cid:98)Cf (λ − )
Proof. It sufﬁces to show that for each x ∈ A  there exists x(cid:48) ∈ B(x  τ /2)∩ X[n] such that (cid:98)fh(x(cid:48)) >
have supx∈X |(cid:98)fh(x)− f (x)| < /2. We have f (x(cid:48)) ≥ inf B(x τ /2) f (x) ≥ λ− Cα(τ /2)α > λ− /2 
(cid:98)fh(x(cid:48)) > λ −   as desired.
(cid:98)Cf (µ + ) for any  > 0 as n → ∞.
Proof. It sufﬁces to assume that λ = µ + . Let A(cid:48) and B(cid:48) be the connected components of
{x ∈ X : f (x) ≥ µ + /2} which contain A and B respectively. By the uniform continuity of f 
there exists ˜r > 0 such that A + B(0  3˜r) ⊆ A(cid:48). We have supx∈A(cid:48)\(A+B(0 ˜r)) f (x) = µ +  − (cid:48) for
Choose n sufﬁciently large such that by Lemma 1  we have supx∈X |(cid:98)fh(x) − f (x)| < (cid:48)/2.
some (cid:48) > 0.
Thus  supx∈A(cid:48)\(A+B(0 ˜r)) (cid:98)fh(x) < µ +  − (cid:48)/2. Hence  points in (cid:98)Cf (µ + ) cannot belong to

Lemma 6 (Separation). Suppose that A and B are distinct connected components of {x ∈ X :
f (x) ≥ λ} which merge at {x ∈ X : f (x) ≥ µ}. Then A ∩ X[n] and B ∩ X[n] are separated in

A(cid:48)\ (A + B(0  ˜r)). Since A(cid:48) also contains A + B(0  3˜r)  it means that there cannot be a path from
A to B with points of empirical density at least µ +  with all edges of length less than ˜r. The result
follows by taking n sufﬁciently large so that τ < ˜r  as desired.

Proof of Theorem 4. By the regularity assumptions on f and Theorem 2 of [7]  we have that Al-
gorithm 2 has both uniform minimality and uniform separation (deﬁned in [7])  which implies
convergence in merge distortion.

Modal Regression Proofs

Proof of Theorem 5. There are two directions to show. (1) if ˆy ∈ (cid:99)Mx then ˆy is a consistent estimator
of some mode y0 ∈ Mx. (2) For each mode  y0 ∈ M  there exists a unique ˆy ∈ (cid:99)M which estimates

by Lemma 1  sup(x(cid:48) y(cid:48)) |(cid:98)fh(x(cid:48)  y(cid:48)) − f (x(cid:48)  y(cid:48))| < δ/2. Then (cid:98)fh(x  y) > λ + δ/2 > (cid:98)fh(x  ˆy) but

it.
We ﬁrst show (1). We show that [ˆy − τ  ˆy + τ ] ∩ Mx (cid:54)= ∅. Suppose otherwise. Let λ = f (x  ˆy).
Choose σ < τ /4. Then by Assumptions 2 and 5  there exists η > 0 such that taking  = τ /2  we
have that there exists δ > 0 such that {(x  y(cid:48)) : y(cid:48) ∈ [ˆy − τ  ˆy + τ ]} ∩ Lf (λ + δ) contains connected
set A where Vol(A) > η. Choose n sufﬁciently large such that (i) there exists y ∈ A ∩ Y   and (ii)
y ∈ [ˆy − τ  ˆy + τ ]  a contradiction since ˆy is the maximizer of the KDE in τ radius neighborhood
when restricted to X = x. Thus  there exists y0 ∈ Mx such that y0 ∈ [ˆy − τ  ˆy + τ ]. Moreover
this y0 ∈ Mx must be unique by Lemma 2. As n → 0  we have τ → 0 and thus consistency is
established for ˆy estimating y0.
Now we show (2). Suppose that y0 ∈ Mx. From the above  for n sufﬁciently large  the maximizer of
the KDE in [y0 − 2τ  y0 + 2τ ] ∩ Y is contained in [y0 − τ  y0 + τ ]. Thus  there exists a root of the
tree contained in [y0 − τ  y0 + τ ] and taking τ → 0 gives us the desired result.

9

Acknowledgements

I thank the anonymous reviewers for their valuable feedback.

References
[1] Ery Arias-Castro  David Mason  and Bruno Pelletier. On the estimation of the gradient lines
of a density and the consistency of the mean-shift algorithm. Journal of Machine Learning
Research  2015.

[2] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.

Advances in Neural Information Processing Systems  pages 343–351  2010.

In

[3] Yen-Chi Chen  Christopher R Genovese  Ryan J Tibshirani  Larry Wasserman  et al. Nonpara-

metric modal regression. The Annals of Statistics  44(2):489–514  2016.

[4] Yizong Cheng. Mean shift  mode seeking  and clustering. IEEE transactions on pattern analysis

and machine intelligence  17(8):790–799  1995.

[5] Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis.

IEEE Transactions on pattern analysis and machine intelligence  24(5):603–619  2002.

[6] Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-nn density and mode estimation. In

Advances in Neural Information Processing Systems  pages 2555–2563  2014.

[7] Justin Eldridge  Mikhail Belkin  and Yusu Wang. Beyond hartigan consistency: Merge distortion

metric for hierarchical clustering. In COLT  pages 588–606  2015.

[8] Martin Ester  Hans-Peter Kriegel  Jörg Sander  and Xiaowei Xu. A density-based algorithm for
discovering clusters in large spatial databases with noise. In Kdd  volume 96  pages 226–231 
1996.

[9] John A Hartigan. Consistency of single linkage for high-density clusters. Journal of the

American Statistical Association  76(374):388–394  1981.

[10] Heinrich Jiang. Density level set estimation on manifolds with dbscan.

Conference on Machine Learning  pages 1684–1693  2017.

In International

[11] Heinrich Jiang. Uniform convergence rates for kernel density estimation. In International

Conference on Machine Learning  pages 1694–1703  2017.

[12] Heinrich Jiang and Samory Kpotufe. Modal-set estimation with an application to clustering. In

International Conference on Artiﬁcial Intelligence and Statistics  pages 1197–1206  2017.

[13] Samory Kpotufe and Ulrike V Luxburg. Pruning nearest neighbor cluster trees. In Proceedings
of the 28th International Conference on Machine Learning (ICML-11)  pages 225–232  2011.
[14] Bharath Sriperumbudur and Ingo Steinwart. Consistency and rates for clustering with dbscan.

In Artiﬁcial Intelligence and Statistics  pages 1090–1098  2012.

[15] Aleksandr Borisovich Tsybakov. Recursive estimation of the mode of a multivariate distribution.

Problemy Peredachi Informatsii  26(1):38–45  1990.

[16] Andrea Vedaldi and Stefano Soatto. Quick shift and kernel methods for mode seeking. In

European Conference on Computer Vision  pages 705–718. Springer  2008.

[17] Daren Wang  Xinyang Lu  and Alessandro Rinaldo. Optimal rates for cluster tree estimation

using kernel density estimators. arXiv preprint arXiv:1706.03113  2017.

10

,Heinrich Jiang