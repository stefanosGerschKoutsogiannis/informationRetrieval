2011,Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss,We consider latent structural versions of probit loss and ramp loss. We show that  these surrogate loss functions are consistent in the strong sense that for any feature map  (finite or infinite dimensional) they yield predictors approaching the infimum  task loss achievable by any linear predictor over the given features.  We also give  finite sample generalization bounds (convergence rates) for these loss functions.  These bounds suggest that probit loss converges more rapidly.  However  ramp loss is more easily optimized and may ultimately  be more practical.,Generalization Bounds and Consistency for Latent

Structural Probit and Ramp Loss

David McAllester

TTI-Chicago

mcallester@ttic.edu

Joseph Keshet
TTI-Chicago

jkeshet@ttic.edu

Abstract

We consider latent structural versions of probit loss and ramp loss. We show that
these surrogate loss functions are consistent in the strong sense that for any feature
map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum
task loss achievable by any linear predictor over the given features. We also give
ﬁnite sample generalization bounds (convergence rates) for these loss functions.
These bounds suggest that probit loss converges more rapidly. However  ramp
loss is more easily optimized on a given sample.

1 Introduction

Machine learning has become a central tool in areas such as speech recognition  natural language
translation  machine question answering  and visual object detection. In modern approaches to these
applications systems are evaluated with quantitative performance metrics. In speech recognition one
typically measures performance by the word error rate. In machine translation one typically uses
the BLEU score. Recently the IBM deep question answering system was trained to optimize the
Jeopardy game show score. The PASCAL visual object detection challenge is scored by average
precision in recovering object bounding boxes. No metric is perfect and any metric is controversial 
but quantitative metrics provide a basis for quantitative experimentation and quantitative experimen-
tation has lead to real progress. Here we adopt the convention that a performance metric is given as a
task loss — a measure of a quantity of error or cost such as the word error rate in speech recognition.
We consider general methods for minimizing task loss at evaluation time.
Although the goal is to minimize task loss  most systems are trained by minimizing a surrogate loss
different from task loss. A surrogate loss is necessary when using scale-sensitive regularization in
training a linear classiﬁer. A linear classiﬁer selects the output that maximizes an inner product of a
feature vector and a weight vector. The output of a linear classiﬁer does not change when the weight
vector is scaled down. But for most regularizers of interest  such as a norm of the weight vector 
scaling down the weight vector drives the regularizer to zero. So directly regularizing the task loss
of a linear classiﬁer is meaningless.
For binary classiﬁcation standard surrogate loss functions include log loss  hinge loss  probit loss 
and ramp loss. Unlike binary classiﬁcation  however  the applications mentioned above involve
complex (or structured) outputs. The standard surrogate loss functions for binary classiﬁcation have
generalizations to the structured output setting. Structural log loss is used in conditional random
ﬁelds (CRFs) [7]. Structural hinge loss is used in structural SVMs [13  14]. Structural probit loss is
deﬁned and empirically evaluated in [6]. A version of structural ramp loss is deﬁned and empirically
evaluated in [3] (but see also [12] for a treatment of the fundamental motivation for ramp loss). All
four of these structural surrogate loss functions are deﬁned formally in section 2.1

1The deﬁnition of ramp loss used here is slightly different from that in [3].

1

This paper is concerned with developing a better theoretical understanding of the relationship be-
tween surrogate loss training and task loss testing for structured labels. Structural ramp loss is
justiﬁed in [3] as being a tight upper bound on task loss. But of course the tightest upper bound on
task loss is the task loss itself. Here we focus on generalization bounds and consistency. A ﬁnite
sample generalization bound for probit loss was stated implicitly in [9] and an explicit probit loss
bound is given in [6]. Here we review the ﬁnite sample bounds for probit loss and prove a ﬁnite
sample bound for ramp loss. Using these bounds we show that probit loss and ramp loss are both
consistent in the sense that for any arbitrary feature map (possibly inﬁnite dimensional) optimizing
these surrogate loss functions with appropriately weighted regularization approaches  in the limit of
inﬁnite training data  the minimum loss achievable by a linear predictor over the given features. No
convex surrogate loss function  such as log loss or hinge loss  can be consistent in this sense — for
any nontrivial convex surrogate loss function one can give examples (a single feature sufﬁces) where
the learned weight vector is perturbed by outliers but where the outliers do not actually inﬂuence the
optimal task loss.
Both probit loss and ramp loss can be optimized in practice by stochastic gradient descent. Ramp
loss is simpler and easier to implement. The subgradient update for ramp loss is similar to a per-
ceptron update — the update is a difference between a “good” feature vector and a “bad” feature
vector. Ramp loss updates are closely related to updates derived from n-best lists in training machine
translaiton systems [8  2]. Ramp loss updates regularized by early stopping have been shown to be
effective in phoneme alignment [10]. It is also shown in [10] that in the limit of large weight vectors
the expected ramp loss update converges to the true gradient of task loss. This result suggests consis-
tency for ramp loss  a suggestion conﬁrmed here. A practical stochastic gradient descent algorithm
for structural probit loss is given in [6] where it is also shown that probit loss can be effective for
phoneme recognition. Although the generalization bounds suggest that probit loss converges faster
than ramp loss  ramp loss seems easier to optimize.
We formulate all the notions of loss in the presence of latent structure as well as structured labels.
Latent structure is information that is not given in the labeled data but is constructed by the prediction
algorithm. For example  in natural language translation the alignment between the words in the
source and the words in the target is not explicitly given in a translation pair. Grammatical structures
are also not given in a translation pair but may be constructed as part of the translation process. In
visual object detection the position of object parts is not typically annotated in the labeled data but
part position estimates may be used as part of the recognition algorithm. Although the presence of
latent structure makes log loss and hinge loss non-convex  latent strucure seems essential in many
applications. Latent structural log loss  and the notion of a hidden CRF  is formulated in [11]. Latent
structural hinge loss  and the notion of a latent structural SVM  is formulated in [15].

2 Formal Setting and Review
We consider an arbitrary input space X and a ﬁnite label space Y. We assume a source probability
distribution over labeled data  i.e.  a distribution over pairs (x  y)  where we write Ex y [f(x  y)] for
the expectation of f(x  y). We assume a loss function L such that for any two labels y and ˆy we have
that L(y  ˆy) ∈ [0  1] is the loss (or cost) when the true label is y and we predict ˆy. We will work with
inﬁnite dimensional feature vectors. We let ‘2 be the set of ﬁnite-norm inﬁnite-dimensional vectors
— the set of all square-summable inﬁnite sequences of real numbers. We will be interested in linear
predictors involving latent structure. We assume a ﬁnite set Z of “latent labels”. For example  we
might take Z to be the set of all parse trees of source and target sentences in a machien translation
system. In machine translation the label y is typically a sentence with no parse tree speciﬁed. We can
recover the pure structural case  with no latent information  by taking Z to be a singleton set. It will
be convenient to deﬁne S to be the set of pairs of a label and a latent label. An element s of S will
be called an augmented label and we deﬁne L(y  s) by L(y  (ˆy  z)) = L(y  ˆy). We assume a feature
map φ such that for an input x and augmented label s we have φ(x  s) ∈ ‘2 with ||φ(x  s)|| ≤ 1.2
Given an input x and a weight vector w ∈ ‘2 we deﬁne the prediction ˆsw(x) as follows.

ˆsw(x) = argmax

s

w>φ(x  s)

2We note that this setting covers the ﬁnite dimensional case because the range of the feature map can be

taken to be a ﬁnite dimensional subset of ‘2 — we are not assuming a universal feature map.

2

Our goal is to use the training data to learn a weight vector w so as to minimize the expected loss
on newly drawn labeled data Ex y [L(y  ˆsw(x))]. We will assume an inﬁnite sequence of training
data (x1  y1)  (x2  y2)  (x3  y3)  . . . drawn IID from the source distribution and use the following
notations.

L(w  x  y) = L(y  ˆsw(x))

L(w) = Ex y [L(w  x  y)]

L∗ = inf w∈‘2 L(w)

ˆLn(w) = 1

n

i=1 L(w  xi  yi)

We adopt the convention that in the deﬁnition of L(w  x  y) we break ties in deﬁnition of ˆsw(x) in
favor of augmented labels of larger loss. We will refer to this as pessimistic tie breaking.
Here we deﬁne latent structural log loss  hinge loss  ramp loss and probit loss as follows.

Pn

Llog(w  x  y) = ln

= ln Zw(x) − ln Zw(x  y)

>

exp(w

Φ(x  s)) Zw(x  y) =

exp(w

>

φ(x  (y  z)))

X

z

”

”

” −“
” −“
” − w

>

w

>

w

max

z

max

s
>

φ(x  s) + L(y  s)

Φ(x  (y  z))

max

φ(x  s) + L(y  s)

Φ(x  s)

max

φ(x  s) + L(y  s)

Φ(x  ˆsw(x))

1

X

Pw(y|x)
Zw(x) =

s

“
“
“

>

w

>

>

w

w

max

s

s

s

Lhinge(w  x  y) =

Lramp(w  x  y) =

=

Lprobit(w  x  y) = E [L(y  ˆsw+(x))]

In the deﬁnition of probit loss we take  to be zero-mean unit-variance isotropic Gaussian noise —
for each feature dimension j we have that j is an independent zero-mean unit-variance Gaussian
variable.3 More generally we will write E [f()] for the expectation of f() where  is Gaussian
noise. It is interesting to note that Llog  Lhinge  and Lramp are all naturally differences of convex
functions and hence can be optimized by CCCP.
In the case of binary classiﬁcation we have S = Y = {−1  1}  φ(x  y) = 1
2 yφ(x)  L(y  y0) = 1y6=y0
and we deﬁne the margin m = yw>φ(x). We then have the following where the expression for
Lprobit(w  x  y) assumes ||Φ(x)|| = 1.

Llog(w  x  y) = ln (1 + e−m)

Lhinge(w  x  y) = max(0  1 − m)
Lramp(w  x  y) = min(1  max(0  1 − m)) Lprobit(w  x  y) = P∼N (0 1)[ ≥ m]

Returning to the general case we consider the relationship between hinge and ramp loss. First we
consider the case where Z is a singleton set — the case of no latent structure. In this case hinge
loss is convex in w — the hinge loss becomes a maximum of linear functions. Ramp loss  however 
remains a difference of nonlinear convex functions even for Z singleton. Also  in the case where Z
is singleton one can easily see that hinge loss is unbounded — wrong labels may score arbitrarily
better than the given label. Hinge loss remains unbounded in case of non-singleton Z. Ramp loss 
on the other hand  is bounded by 1 as follows.

Lramp(w  x  y) =

w>Φ(x  s) + L(y  s)

(cid:16)
≤ (cid:16)

max

s

max

s

(cid:17) − w>Φ(x  ˆsw(x))
(cid:17) − w>Φ(x  ˆsw(x)) = 1

w>Φ(x  s) + 1

Next  as is emphasized in [3]  we note that ramp loss is a tighter upper bound on task loss than
is hinge loss. To see this we ﬁrst note that it is immediate that Lhinge(w  x  y) ≥ Lramp(w  x  y).
3In inﬁnite dimension we have that with probability one |||| = ∞ and hence w+ is not in ‘2. The measure
underling E [f ()] is a Gaussian process. However  we still have that for any unit-norm feature vector Φ the
inner product >Φ is distributed as a zero-mean unit-norm scalar Gaussian and Lprobit(w  x  y) is therefore
well deﬁned.

3

Furthermore  the following derivation shows Lramp(w  x  y) ≥ L(w  x  y) where we assume pes-
simistic tie breaking in the deﬁnition of ˆsw(x).

(cid:17) − w>Φ(x  ˆsw(x))

Lramp(w  x  y) =

w>Φ(x  s) + L(y  s)

max

s

(cid:16)

≥ w>Φ(x  ˆsw(x)) + L(y  ˆsw(x)) − w>Φ(x  ˆsw(x)) = L(y  ˆsw(x))

But perhaps the most important property of ramp loss is the following.

lim
α→∞ Lramp(αw  x  y) = L(w  x  y)

(1)

This can be veriﬁed by noting that as α goes to inﬁnity the maximum of the ﬁrst term in ramp loss
must occur at s = ˆsw(x).
Next we note that Optimizing Lramp through subgradient descent (rather than CCCP) yields the
following update rule (here we ignore regularization).

(2)

(3)

∆w ∝ φ(x  ˆsw(x)) − φ(x  ˆs+

w(x  y))
w>φ(x  s) + L(y  s)

ˆs+
w(x  y) = argmax

s

We will refer to (2) as the ramp loss update rule. The following is proved in [10] under mild
conditions on the probability distribution over pairs (x  y).

(cid:2)φ(x  ˆs+

αw(x  y)) − φ(x  ˆsw(x))(cid:3)

∇wL(w) = lim

α→∞ αEx y

Equation (3) expresses a relationship between the expected ramp loss update and the gradient of
generalization loss. Signiﬁcant empirical success has been achieved with the ramp loss update rule
using early stopping regularization [10]. But both (1) and (3) suggests that regularized ramp loss
should be consistent as is conﬁrmed here.
Finally it is worth noting that Lramp and Lprobit are meaningful for an arbitrary prediction space S 
label space Y  and loss function L(y  s) between a label and a prediction. Log loss and hinge loss
can be generalized to arbitrary prediction and label spaces provided that we assume a compatibility
relation between predictions and labels. The framework of independent prediction and label spaces
is explored more fully in [5] where a notion of weak-label SVM is deﬁned subsuming both ramp
and hinge loss as special cases.

3 Consistency of Probit Loss

We start with the consistency of probit loss which is easier to prove. We consider the following
learning rule where the regularization parameter λn is some given function of n.

ˆwn = argmin

w

probit(w) + λn
ˆLn
2n

||w||2

(4)

We now prove the following fairly straightforward consequence of a generalization bound appearing
in [6].
Theorem 1 (Consistency of Probit loss). For ˆwn deﬁned by (4)  if the sequence λn increases without
bound  and λn ln n/n converges to zero  then with probability one over the draw of the inﬁnite
sample we have limn→∞ Lprobit( ˆwn) = L∗.
Unfortunately  and in contrast to simple binary SVMs  for a latent binary SVM (an LSVM) there
exists an inﬁnite sequence w1  w2  w3  . . . such that Lprobit(wn) approaches L∗ but L(wn) remains
bounded away from L∗ (we omit the example here). However  the learning algorithm (4) achieves
consistency in the sense that the stochastic predictor deﬁned by ˆwn +  where  is Gaussian noise
has a loss which converges to L∗.
To prove theorem 1 we start by reviewing the generalization bound of [6]. The departure point
for this generalization bound is the following PAC-Bayesian theorem where P and Q range over
probability measures on a given space of predictors and L(Q) and ˆLn(Q) are deﬁned as expectations
over selecting a predictor from Q.

4

(cid:18)

 

 

Theorem 2 (from [1]  see also [4]). For any ﬁxed prior distribution P and ﬁxed λ > 1/2 we
have that with probability at least 1 − δ over the draw of the training data the following holds
simultaneously for all Q.

(cid:18)

(cid:18) KL(Q  P ) + ln 1

(cid:19)(cid:19)

δ

n

L(Q) ≤

1
1 − 1

2λ

ˆLn(Q) + λ

For the space of linear predictors we take the prior P to be the zero-mean unit-variance Gaussian
distribution and for w ∈ ‘2 we deﬁne the distribution Qw to be the unit-variance Gaussian centered
at w. This gives the following corollary of (5).
Corollary 1 (from [6]). For ﬁxed λn > 1/2 we have that with probability at least 1 − δ over the
(cid:18) 1
draw of the training data the following holds simultaneously for all w ∈ ‘2.
2||w||2 + ln 1

(cid:19)(cid:19)

(cid:18)

δ

Lprobit(w) ≤

1
1 − 1

2λn

ˆLn
probit(w) + λn

n

To prove theorem 1 from (6) we consider an arbitrary unit-norm weight vector w∗ and an arbitrary
scalar α > 0. Setting δ to 1/n2  and noting that ˆwn is the minimizer of the right hand side of (6) 
we have the following with probability at least 1 − 1/n2 over the draw of the sample.

(cid:18) 1

(cid:19)(cid:19)

Lprobit( ˆwn) ≤

1
1 − 1

2λn

probit(αw∗) + λn
ˆLn

2 α2 + 2 ln n

n

A standard Chernoff bound argument yields that for w∗ and α > 0 selected prior to drawing the
sample  we have the following with probability at least 1 − 1/n2 over the choice of the sample.

(5)

(6)

(7)

(8)
Combining (7) and (8) with a union bound yields that with probability at least 1− 2/n2 we have the
following.

probit(αw∗) ≤ Lprobit(αw∗) +
ˆLn

Lprobit( ˆwn) ≤

1
1 − 1

2λn

Lprobit(αw∗) +

n

(cid:18) 1

rln n

n

(cid:19)!

Because the probability that the above inequality is violated goes as 1/n2  with probability one over
the draw of the sample we have the following.

n→∞ Lprobit( ˆwn) ≤ lim
lim
n→∞

1
1 − 1

2λn

Lprobit(αw∗) +

+ λn

2 α2 + 2 ln n

n

Under the conditions on λn given in the statement of theorem 1 we then have

n→∞ Lprobit( ˆwn) ≤ Lprobit(αw∗).
lim

Because this holds with probability one for any α  the following must also hold with probability one.
(9)

n→∞ Lprobit( ˆwn) ≤ lim
lim

α→∞ Lprobit(αw∗)

Now consider
α→∞ Lprobit(αw  x  y) = lim
lim
α→∞

E [L(αw +   x  y)] = lim
σ→0

E [L(w + σ  x  y)] .

We have that limσ→0 E [L(w + σ  x  y)] is determined by the augmented labels s that are tied for
the maximum value of w>Φ(x  s). There is some probability distribution over these tied values that
occurs in the limit of small σ. Under the pessimistic tie breaking in the deﬁnition of L(w  x  y) we
then get limα→∞ Lprobit(αw  x  y) ≤ L(w  x  y). This in turn gives the following.

α→∞ Lprobit(αw) = Ex y
lim

(10)
Combining (9) and (10) yields limn→∞ Lprobit( ˆwn) ≤ L(w∗). Since for any w∗ this holds with
probability one  with probability one we also have limn→∞ Lprobit( ˆwn) ≤ L∗. Finally we note
Lprobit(w) = E [L(w + )] ≥ L∗ which then gives theorem 1.

lim
α→∞ Lprobit(αw  x  y)

i ≤ Ex y [L(w  x  y)] = L(w)

h

5

rln n
(cid:18) 1

n

+ λn

rln n

n

(cid:19)!

2 α2 + 2 ln n

4 Consistency of Ramp Loss

Now we consider the following ramp loss training equation.

ˆwn = argmin

w

ramp(w) + γn
ˆLn
2n

||w||2

(11)

The main result of this paper is the following.
Theorem 3 (Consistency of Ramp Loss). For ˆwn deﬁned by (11)  if the sequence γn/ ln2 n increases
without bound  and the sequence γn/(n ln n) converges to zero  then with probability one over the
draw of the inﬁnite sample we have limn→∞ Lprobit((ln n) ˆwn) = L∗.
As with theorem 1  theorem 3 is derived from a ﬁnite sample generalization bound. The bound
is derived from (6) by upper bounding ˆLn
ramp(w). From section 3 we
have that limσ→0 Lprobit(w/σ  x  y) ≤ L(w  x  y) ≤ Lramp(w  x  y). This can be converted to the
following lemma for ﬁnite σ where we recall that S is the set of augmented labels s = (y  z).
Lemma 1.

probit(w/σ) in terms of ˆLn

Lprobit

  x  y

(cid:16) w

σ

r

8 ln

|S|
σ

(cid:17) ≤ Lramp(w  x  y) + σ + σ
(cid:16) w

(cid:17) ≤ σ + max

  x  y

s: m(s)≤M

σ

L(y  s)

Proof. We ﬁrst prove that for any σ > 0 we have

Lprobit

where

m(s) = w>∆φ(s) ∆φ(s) = φ(x  ˆsw(x)) − φ(x  s) M = σ

(cid:2)1Φ()

(cid:3).

To prove (12) we note that for m(s) > M we have the following where P[Φ()] abbreviates
E

(cid:20)

(cid:21)

(cid:2)−>∆φ(s) ≥ m(s)/σ(cid:3)
(cid:19)

(cid:18)

P[ˆsw+σ(x) = s] ≤ P[(w + σ)>∆φ(s) ≤ 0] = P

≤ P∼N (0 1)

 ≥ M
2σ

≤ exp

− M 2
8σ2

= σ
|S|

E [L(y  ˆsw+σ(x))] ≤ P [∃s : m(s) > M ˆsw+σ(x) = s] + max
s:m(s)≤M

L(y  s)

(12)

r

8 ln

|S|
σ

.

The following calculation shows that (12) implies the lemma.

Lprobit

  x  y

s: m(s)≤M

L(y  s)

(cid:16) w

σ

≤ σ + max

L(y  s)

s:m(s)≤M

(cid:17) ≤ σ + max
(cid:18)
(cid:16)

(cid:19)

+ M

L(y  s) − m(s)

+ M

s: m(s)≤M

≤ σ +
max
≤ σ +
L(y  s) − m(s)
= σ + Lramp(w  x  y) + M

max

s

(cid:17)

Inserting lemma 1 into (6) we get the following.
Theorem 4. For λn > 1/2 we have that with probability at least 1− δ over the draw of the training
data the following holds simultaneously for all w and σ > 0.

r

  ||w||2

!!

|S|
σ

+ λn

2σ2 + ln 1

δ

n

(13)

Lprobit

ˆLn
ramp(w) + σ + σ

8 ln

 

(cid:17) ≤

(cid:16) w

σ

1
1 − 1

2λn

6

To prove theorem 3 we now take σn = 1/ ln n and λn = γn/ ln2 n. We then have that ˆwn is the
minimizer of the right hand side of (13). This observation yields the following for any unit-norm
vector w∗ and scalar α > 0 where we have set δ = 1/n2.

1 +p8 ln(|S| ln n)

ln n

!

+ γnα2
2n

+

2γn
n ln n

(14)

 

Lprobit((ln n) ˆwn) ≤

1

1 − ln2 n

2γn

ˆLramp(αw∗) +

As in section 3  we use a Chernoff bound for the single vector w∗ and scalar α to bound ˆLramp(αw∗)
in terms of Lramp(αw∗) and then take the limit as n → ∞ to get the following with probability one.

n→∞ Lprobit((ln n) ˆwn) ≤ Lramp(αw∗)
lim

The remainder of the proof is the same in section 3 but where we now use limα→∞ Lramp(αw∗) =
L(w∗) whose proof we omit.

5 A Comparison of Convergence Rates

To compare the convergence rates implicit in (6) and (13) we note that in (13) we can optimize σ as a

function of other quantities in the bound.4 An approximately optimal value for σ is(cid:0)λn||w||2/n(cid:1)1/3

which gives the following.

(cid:17) ≤

(cid:16) w

σ

1
1 − 1

2λn

Lprobit

 
(cid:16)(cid:0)|| ˆwn||2/n(cid:1)1/3(cid:17)

ˆLn
ramp(w) +

(cid:19)1/3 

(cid:18) λn||w||2
as opposed to (6) which gives O(cid:0)|| ˆwn||2/n(cid:1). This

λn ln 1
δ

|S|
σ

r

!

!

(15)

8 ln

3
2

+

+

n

n

We have that (15) gives ˜O
suggests that while probit loss and ramp loss are both consistent  ramp loss may converge more
slowly.

6 Discussion and Open Problems

(cid:16)

The contributions of this paper are a consistency theorem for latent structural probit loss and both
a generalization bound and a consistency theorem for latent structural ramp loss. These bounds
suggest that probit loss converges more rapidly. However  we have only proved upper bounds on
generalization loss and it remains possible that these upper bounds  while sufﬁcient to show consis-
tency  are not accurate characterizations of the actual generalization loss. Finding more deﬁnitive
statements  such as matching lower bounds  remains an open problem.
The deﬁnition of ramp loss used here is not the only one possible. In particular we can consider the
following variant.
L0
ramp(w  x  y) =

w>Φ(x  s)
ramp as well as Lramp. Experiments indicate that L0

max
Relations (1) and (3) both hold for L0
ramp per-
forms somewhat better than Lramp under early stopping of subgradient descent. However it seems
that it is not possible to prove a bound of the form of (15) for L0
ramp. A frustrating observation is
that L0
ramp remains an open
problem.
The isotropic Gaussian noise distribution used in the deﬁnition of Lprobit is not optimal. A uni-
formly tighter upper bound on generalization loss is achieved by optimizing the posterior in the
PAC-Bayesian theorem. Finding a practical more optimal use of the PAC-Bayesian theorem also
remains an open problem.

ramp(0  x  y) = 0. Finding a meaningful ﬁnite-sample statement for L0

w>φ(x  s) − L(y  s)

(cid:17) −(cid:16)

max

(cid:17)

s

s

4In the consistency proof it was more convenient to set σ = 1/ln n which is plausibly nearly optimal

anyway.

7

References
[1] Olivier Catoni. PAC-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical
Learning. Institute of Mathematical Statistics LECTURE NOTES MONOGRAPH SERIES 
2007.

[2] D. Chiang  K. Knight  and W. Wang. 11 001 new features for statistical machine translation.

In Proc. NAACL  2009  2009.

[3] Chuong B. Do  Quoc Le  Choon Hui Teo  Olivier Chapelle  and Alex Smola. Tighter bounds

for structured estimation. In nips  2008.

[4] Pascal Germain  Alexandre Lacasse  Francois Laviolette  and Mario Marchand. Pac-bayesian

learning of linear classiﬁers. In ICML  2009.

[5] Ross Girshick  Pedro Felzenszwalb  and David McAllester. Object detection with grammar

models. In NIPS  2011.

[6] Joseph Keshet  David McAllester  and Tamir Hazan. Pac-bayesian approach for minimization
of phoneme error rate. In International Conference on Acoustics  Speech  and Signal Process-
ing (ICASSP)  2011.

[7] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the Eightneenth International
Conference on Machine Learning  pages 282–289  2001.

[8] P. Liang  A. Bouchard-Ct  D. Klein  and B. Taskar. An end-to-end discriminative approach to
machine translation. In International Conference on Computational Linguistics and Associa-
tion for Computational Linguistics (COLING/ACL)  2006.

[9] David McAllester. Generalization bounds and consistency for structured labeling. In G. Bakir
nd T. Hofmann  B. Scholkopf  A. Smola  B. Taskar  and S. V. N. Vishwanathan  editors  Pre-
dicting Structured Data. MIT Press  2007.

[10] David A. McAllester  Tamir Hazan  and Joseph Keshet. Direct loss minimization for structured

prediction. In Advances in Neural Information Processing Systems 24  2010.

[11] A. Quattoni  S. Wang  L.P. Morency  M Collins  and T Darrell. Hidden conditional random

ﬁelds. PAMI  29  2007.

[12] R.Collobert  F.H.Sinz  J.Weston  and L.Bottou. Trading convexity for scalability. In ICML 

2006.

[13] B. Taskar  C. Guestrin  and D. Koller. Max-margin markov networks. In Advances in Neural

Information Processing Systems 17  2003.

[14] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector machine learning for
interdependent and structured output spaces. In Proceedings of the Twenty-First International
Conference on Machine Learning  2004.

[15] Chun-Nam John Yu and T. Joachims. Learning structural svms with latent variables. In Inter-

national Conference on Machine Learning (ICML)  2009.

8

,Yunwen Lei
Urun Dogan
Alexander Binder
Marius Kloft
Wenlin Wang
Chenyang Tao
Zhe Gan
Guoyin Wang
Liqun Chen
Xinyuan Zhang
Ruiyi Zhang
Qian Yang
Ricardo Henao
Lawrence Carin