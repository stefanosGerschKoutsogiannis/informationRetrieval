2019,Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching,We propose a scalable Gromov-Wasserstein learning (S-GWL) method and establish a novel and theoretically-supported paradigm for large-scale graph analysis.
The proposed method is based on the fact that Gromov-Wasserstein discrepancy is a pseudometric on graphs. 
Given two graphs  the optimal transport associated with their Gromov-Wasserstein discrepancy provides the correspondence between their nodes and achieves graph matching. 
When one of the graphs is a predefined graph with isolated but self-connected nodes ($i.e.$  disconnected graph)  the optimal transport indicates the clustering structure of the other graph and achieves graph partitioning. 
Further  we extend our method to multi-graph partitioning and matching by learning a Gromov-Wasserstein barycenter graph for multiple observed graphs. 
Our method combines a recursive $K$-partition mechanism with a warm-start proximal gradient algorithm  whose time complexity is $\mathcal{O}(K(E+V)\log_K V)$ for graphs with $V$ nodes and $E$ edges. 
To our knowledge  our method is the first attempt to make Gromov-Wasserstein discrepancy applicable to large-scale graph analysis and unify graph partitioning and matching into the same framework.
It outperforms state-of-the-art graph partitioning and matching methods  achieving a trade-off between accuracy and efficiency.,Scalable Gromov-Wasserstein Learning for

Graph Partitioning and Matching

Hongteng Xu1 2

Dixin Luo2

Lawrence Carin2

1Inﬁnia ML Inc.

2Duke University

{hongteng.xu  dixin.luo  lcarin}@duke.edu

Abstract

We propose a scalable Gromov-Wasserstein learning (S-GWL) method and estab-
lish a novel and theoretically-supported paradigm for large-scale graph analysis.
The proposed method is based on the fact that Gromov-Wasserstein discrepancy
is a pseudometric on graphs. Given two graphs  the optimal transport associated
with their Gromov-Wasserstein discrepancy provides the correspondence between
their nodes and achieves graph matching. When one of the graphs has isolated
but self-connected nodes (i.e.  a disconnected graph)  the optimal transport indi-
cates the clustering structure of the other graph and achieves graph partitioning.
Using this concept  we extend our method to multi-graph partitioning and match-
ing by learning a Gromov-Wasserstein barycenter graph for multiple observed
graphs; the barycenter graph plays the role of the disconnected graph  and since
it is learned  so is the clustering. Our method combines a recursive K-partition
mechanism with a regularized proximal gradient algorithm  whose time complexity
is O(K(E + V ) logK V ) for graphs with V nodes and E edges. To our knowledge 
our method is the ﬁrst attempt to make Gromov-Wasserstein discrepancy applicable
to large-scale graph analysis and unify graph partitioning and matching into the
same framework. It outperforms state-of-the-art graph partitioning and matching
methods  achieving a trade-off between accuracy and efﬁciency.

Introduction

1
Gromov-Wasserstein distance [42  29] was originally designed for metric-measure spaces  which can
measure distances between distributions in a relational way  deriving an optimal transport between
the samples in distinct spaces. Recently  the work in [11] proved that this distance can be extended to
Gromov-Wasserstein discrepancy (GW discrepancy) [37]  which deﬁnes a pseudometric for graphs.
Accordingly  the optimal transport between two graphs indicates the correspondence between their
nodes. This work theoretically supports the applications of GW discrepancy to structural data analysis 
e.g.  2D/3D object matching [30  28  8]  molecule analysis [43  44]  network alignment [49]  etc.
Unfortunately  although GW discrepancy-based methods are attractive theoretically  they are often
inapplicable to large-scale graphs  because of high computational complexity. Additionally  these
methods are designed for two-graph matching  ignoring the potential of GW discrepancy to other
tasks  like graph partitioning and multi-graph matching. As a result  the partitioning and the matching
of large-scale graphs still typically rely on heuristic methods [16  12  45  27]  whose performance is
often sub-optimal  especially in noisy cases.
Focusing on the issues above  we design a scalable Gromov-Wasserstein learning (S-GWL) method
and establish a new and uniﬁed paradigm for large-scale graph partitioning and matching. As
illustrated in Figure 1(a)  given two graphs  the optimal transport associated with their Gromov-
Wasserstein discrepancy provides the correspondence between their nodes. Similarly  graph partition-
ing corresponds to calculating the Gromov-Wasserstein discrepancy between an observed graph and
a disconnected graph  as shown in Figure 1(b). The optimal transport connects each node of the ob-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) Graph matching

(c) Multi-graph matching

(b) Graph partitioning

(d) Multi-graph partitioning

(e) Comparisons on accuracy and efﬁciency

Figure 1: (a)-(d) Illustrations of graph partitioning and matching in the GWL framework. (c  d) The barycenter
graph in black and its optimal transports to observed graphs are learned jointly. (d) When the barycenter graph
is initialized as a graph with few isolated nodes  the optimal transports indicate aligned partitions of observed
graph. (e) We test various graph matching methods in 10 trials on an Intel i7 CPU. In each trial  the source graph
has 2 000 nodes and the target graph has 100 more noisy nodes and corresponding edges. The graphs yield either
Gaussian partition model [7] or Barabási-Albert model [4]. The GWL-based methods (‘?’) obtains higher node
correctness than other baselines (‘•’)  and our S-GWL (big ‘?’) achieves a trade-off on accuracy and efﬁciency.
served graph with an isolated node of the disconnected graph  yielding a partitioning. In Figures 1(c)
and 1(d)  taking advantage of the Gromov-Wasserstein barycenter in [37]  we achieve multi-graph
matching and partitioning by learning a “barycenter graph”. For arbitrary two or more graphs  the
correspondence (or the clustering structure) among their nodes can be established indirectly through
their optimal transports to the barycenter graph.
The four tasks in Figures 1(a)-1(d) are explicitly uniﬁed in our Gromov-Wasserstein learning (GWL)
framework  which corresponds to the same GW discrepancy-based optimization problem. To improve
its scalability  we introduce a recursive mechanism to the GWL framework  which recursively applies
K-way partitioning to decompose large graphs into a set of aligned sub-graph pairs  and then matches
each pair of sub-graphs. When calculating GW discrepancy  we design a regularized proximal
gradient method  that considers the prior information of nodes and performs updates by solving a
series of convex sub-problems. The sparsity of edges further helps us reduce computations. These
acceleration strategies yield our S-GWL method: for graphs with V nodes and E edges  its time
complexity is O(K(E + V ) logK V ) and memory complexity is O(E + V K). To our knowledge 
our S-GWL is the ﬁrst to make GW discrepancy applicable to large-scale graph analysis. Figure 1(e)
illustrates the effectiveness of S-GWL on graph matching  with more results presented in Section 5.
2 Graph Analysis Based on Gromov-Wasserstein Learning
Denote a measure graph as G(V  C  µ)  where V = {vi}|V|i=1 is the set of nodes  C = [cij] 2 R|V|⇥|V|
is the adjacency matrix  and µ = [µi] 2 ⌃|V| is a Borel probability measure deﬁned on V. The
adjacency matrix is continuous for weighted graph while binary for unweighted graph. In practice 
µ is an empirical distribution of nodes  which can be estimated by a function of node degree. A
K-way graph partitioning aims to decompose a graph G into K sub-graphs by clustering its nodes 
i.e.  {Gk = G(Vk  Ck  µk)}K
k=1  where [kVk = V and Vk \V k0 = ; for k 6= k0. Given two graphs
Gs and Gt  graph matching aims to ﬁnd a correspondence between their nodes  i.e.  ⇡ : Vs 7! Vt.
Many real-world networks are modeled using graph theory  and graph partitioning and matching are
important for community detection [21  16] and network alignment [39  40  54]  respectively. In this
section  we propose a Gromov-Wasserstein learning framework to unify these two problems.
2.1 Gromov-Wasserstein discrepancy between graphs
Our GWL framework is based on a pseudometric on graphs called Gromov-Wasserstein discrepancy:
Deﬁnition 2.1 ([11]). Denote the collection of measure graphs as G. For each p 2 [1 1] and each
Gs  Gt 2G   the Gromov-Wasserstein discrepancy between Gs and Gt is
(1)
ij  ct

dgw(Gs  Gt) := minT2⇧(µs µt)⇣Xi j2Vs Xi0 j02Vt |cs

where ⇧(µs  µt) = {T  0|T 1|Vt| = µs  T >1|Vs| = µt}.
GW discrepancy compares graphs in a relational way  measuring how the edges in a graph compare
to those in the other graph. It is a natural extension of the Gromov-Wasserstein distance deﬁned for
metric-measure spaces [29]. We refer the reader to [29  11  36] for mathematical foundations.

i0j0|pTii0Tjj0⌘ 1

p  

2

i 2V s corresponds to the node vt

Graph matching According to the deﬁnition  GW discrepancy measures the distance between two
graphs  and the optimal transport T = [Tij] 2 ⇧(µs  µt) is a joint distribution of the graphs’ nodes:
Tij indicates the probability that the node vs
j 2V t. As shown in
Figure 1(a)  the optimal transport achieves an assignment of the source nodes to the target ones.
Graph partitioning Besides graph matching  this paradigm is also suitable for graph partitioning.
Recall that most existing graph partitioning methods obey the modularity maximization principle [16 
12]: for each partitioned sub-graph  its internal edges should be dense  while its external edges
connecting with other sub-graphs should be sparse. This principle implies that if we treat each
sub-graph as a “super node” [21  47  34]  an ideal partitioning should correspond to a disconnected
graph with K isolated  but self-connected super nodes. Therefore  we achieve K-way partitioning
by calculating the GW discrepancy between the observed graph G and a disconnected graph  i.e. 
dgw(G  Gdc)  where Gdc = G(Vdc  diag(µdc)  µdc). |Vdc| = K. µdc 2 ⌃K is a node distribution 
whose derivation is in Appendix A.1. diag(µdc) is the adjacency matrix of Gdc. As shown in
Figure 1(b)  the optimal transport is a |V| ⇥ K matrix. The maximum in each row of the matrix
indicates the cluster of a node.
2.2 Gromov-Wasserstein barycenter graph for analysis of multiple graphs
Multi-graph matching Distinct from most graph matching methods [17  13  39  14]  which mainly
focus on two-graph matching  our GWL framework can be readily extended to multi-graph cases 
by introducing the Gromov-Wasserstein barycenter (GWB) proposed in [37]. Given a set of graphs
{Gm}M

m=1  their p-order Gromov-Wasserstein barycenter is a barycenter graph deﬁned as

G( ¯V  ¯C  ¯µ) := arg min ¯G XM

m=1

!mdp

gw(Gm  ¯G) 

(2)

where ! = [!m] 2 ⌃M contains predeﬁned weights  and ¯G = G( ¯V  ¯C 2 R| ¯V|⇥| ¯V|  ¯µ 2 ⌃| ¯V|) is the
barycenter graph with a predeﬁned number of nodes. The barycenter graph minimizes the weighted
average of its GW discrepancy to observed graphs. It is an average of the observed graphs aligned by
their optimal transports. The matrix ¯C is a “soft” adjacency matrix of the barycenter. Its elements
reﬂect the conﬁdence of the edges between the corresponding nodes in ¯V. As shown in Figure 1(c) 
the barycenter graph works as a “reference” connecting with the observed graphs. For each node
in the barycenter graph  we can ﬁnd its matched nodes in different graphs with the help of the
corresponding optimal transport. These matched nodes construct a node set  and two arbitrary nodes
in the set are a correspondence. The collection of all the node sets achieves multi-graph matching.
Multi-graph partitioning We can also use the barycenter graph to achieve multi-graph partitioning 
with the learned barycenter graph playing the role of the aforementioned disconnected graph. Given
two or more graphs  whose nodes may have unobserved correspondences  existing partitioning
methods [21  16  12  6  34] only partition them independently because they are designed for clustering
nodes in a single graph. As a result  the ﬁrst cluster of a graph may correspond to the second cluster
of another graph. Without the correspondence between clusters  we cannot reduce the search space
in matching tasks. Although this correspondence can be estimated by matching two coarse graphs
that treat the clusters as their nodes  this strategy not only introduces additional computations but
also leads to more uncertainty on matching  because different graphs are partitioned independently
without leveraging structural information from each other. By learning a barycenter graph for multiple
graphs  we can partition them and align their clusters simultaneously. As shown in Figure 1(d)  when
applying K-way multi-graph partitioning  we initialize a disconnected graph with K isolated nodes
gw(Gm  ¯G). For each node of the

as the barycenter graph  and then learn it by min ¯GPM

barycenter graph  its matched nodes in each observed graph belong to the same cluster.
3 Scalable Gromov-Wasserstein Learning
Based on Gromov-Wasserstein discrepancy and the barycenter graph  we have established a GWL
framework for graph partitioning and matching. To make this framework scalable to large graphs  we
propose a regularized proximal gradient method to calculate GW discrepancy and integrate multiple
acceleration strategies to greatly reduce the computational complexity of GWL.
3.1 Regularized proximal gradient method
Inspired by the work in [48  49]  we calculate the GW discrepancy in (1) based on a proximal gradient
method  which decomposes a complicated non-convex optimization problem into a series of convex
sub-problems. For simplicity  we set p = 2 in (1  2). Given two graphs Gs = G(Vs  Cs  µs) and

m=1 !mdp

3

(3)

ij  ct

Gt = G(Vt  Ct  µt)  in the n-th iteration  we update the current optimal transport T (n) by calculating
gw(Gs  Gt):
d2
T (n+1) = arg minT2⇧(µs µt) Xi j2Vs Xi0 j02Vt |cs

i0j0|2T (n)
= arg minT2⇧(µs µt)hL(Cs  Ct  T (n))  Ti + KL(TkT (n)).

ii0 Tjj0 + KL(TkT (n))

|Vt|
ij )  Tij + T (n)

Here  L(Cs  Ct  T ) = Csµs1>
+ 1|Vs|µ>t C>t  2CsT C>t   derived based on [37]  and
h· ·i represents the inner product of two matrices. The Kullback-Leibler (KL) divergence  i.e. 
KL(TkT (n)) =Pij Tij log(Tij/T (n)
  is added as the proximal term. We can solve
(3) via the Sinkhorn-Knopp algorithm [41  15] with nearly-linear convergence [1]. As demonstrated
in [49]  the global convergence of this proximal gradient method is guaranteed  so repeating (3) leads
to a stable optimal transport  denoted as bT . Additionally  this method is robust to hyperparameter  
achieving better convergence and numerical stability than the entropy-based method in [37].
Learning the barycenter graph is also based on the proximal gradient method. Given M graphs  we
estimate their barycenter graph via alternating optimization. In the n-th iteration  given the previous
barycenter graph ¯G(n) = G( ¯V  ¯C(n)  ¯µ)  we update M optimal transports via solving (3). Given the
updated optimal transports {T (n+1)
m=1  we update the adjacency matrix of the barycenter graph by
}M
1

m

ij

!m(T (n+1)

m

)>CmT (n+1)

m

.

(4)

¯C(n+1) =

¯µ ¯µ>Xm

˜µ = (n + a)b.

µ = ˜µ/k ˜µk1 

The weights !  the number of the nodes | ¯V| and the node distribution ¯µ are predeﬁned.
Different from the work in [49  37]  we use the following initialization strategies to achieve a
regularized proximal gradient method and estimate optimal transports with few iterations.
Node distributions We estimate the node distribution µ of a graph empirically by a function of node
degree  which reﬂects the local topology of nodes  e.g.  the density of neighbors. In particular  for a
graph with |V| nodes  we ﬁrst calculate a vector of node degree  i.e.  n = [ni] 2 Z|V|  where ni is
the number of neighbors of the i-th node. Then  we estimate the node distribution µ as
(5)
where a  0 and b  0 are the hyperparameters controlling the shape of the distribution. For the
graphs with isolated nodes  whose ni’s are zeros  we set a > 0 to avoid numerical issues when solving
(3). For the graphs whose nodes obey to power-law distributions  i.e.  Barabási-Albert graphs  we
set b 2 [0  1) to balance the probabilities of different nodes. This function generalizes the empirical
settings used in other methods: when a = 0 and b = 1  we derive the distribution based on the
normalized node degree used in [49]; when b = 0  we assume the distribution is uniform as the work
in [37  44] does. We ﬁnd that the node distributions have a huge inﬂuence on the stability and the
performance of our learning algorithms  which will be discussed in the following sections.
Optimal transports For graph analysis  we can leverage prior knowledge to get a better regularization
of optimal transport. Generally  the nodes with similar local topology should be matched with a
high probability. Therefore  given two node distributions µs and µt  we construct a node-based
cost matrix Cnode 2 R|Vs|⇥|Vt|  whose element is cij = |µs
j|  and add a regularization term
hCnode  T (n)i to (3). As a result  in the learning phase  we replace the L(Cs  Ct  T (n)) in (3) with
L(Cs  Ct  T (n)) + ⌧ Cnode  where ⌧ controls the signiﬁcance of Cnode. Introducing the proposed
regularizer helps us measure the similarity between nodes directly  which extends our GW discrepancy
to the fused GW discrepancy in [44  43]. In such a situation  the main difference here is that we
use the proximal gradient method to calculate the discrepancy  rather than the conditional gradient
method in [43].
Barycenter graphs When learning GWB  the work in [37] ﬁxed the node distribution to be uniform
In practice  however  both the node distribution of the barycenter graph and its optimal transports
to observed graphs are unknown. In such a situation  we need to ﬁrst estimate the node distribution
¯µ = [¯µ1  ...  ¯µ| ¯V|]. Without loss of generality  we assume that the node distribution of the barycenter
graph is sorted  i.e.  ¯µ1  ...  ¯µ| ¯V|. We estimate the node distribution via the weighted average of
the sorted and re-sampled node distributions of observed graphs:

i  µt

¯µ = XM

m=1

!minterpolate

| ¯V|(sort(µm)) 

(6)

4

m=1   | ¯V|  !)

T (n+1)

m

= ProxGrad(Gm  ¯G(n)  ).

m for m = 1  ..  M.

j|.
i  µt

Calculate ¯C(n+1) via (4).
n = n + 1.

8: Output: bTm = T (n)

Algorithm 2 GWB({Gm}M
1: Set n = 0.
2: Initialize ¯µ via (6). ¯C(n) = diag( ¯µ).
3: While not converge
For m = 1  ...  M
4:
5:
6:
7:

Algorithm 1 ProxGrad(Gs  Gt  )
1: Set n = 0  a = µs.
2: Calculate Cnode with cij = |µs
3: Initialize T (n) = µsµ>t .
4: While not converge
5: G = e(Cnode+L(Cs Ct T (n)))/  T (n).
6: b = µt/(G>a)  and a = µs/(Gb).
7: T (n+1) = diag(a)Gdiag(b)  then n = n + 1.
8: Output: bT = T (n).
where sort(·) sorts the elements of the input vector in descending order  and interpolate
| ¯V|(·) samples
| ¯V| values from the input vector via bilinear interpolation. Given the node distribution  we initialize
the optimal transports via the method mentioned above.
Algorithms 1 and 2 show the details of our method  where “” and “·/·” represent elementwise
multiplication and division  respectively. The GWL framework for the tasks in Figures 1(a)-1(d) are
implemented based on these two algorithms  with details in Appendix A.1.
3.2 A recursive K-partition mechanism for large-scale graph matching
Assume that the observed graphs have comparable size  whose number of nodes and edges are
denoted as V and E  respectively. When using the proximal gradient method directly to calculate the
GW discrepancy between two graphs  the time complexity  in the worst case  is O(V 3) because the
L(Cs  Ct  T (n)) in (3) involves CsT C>t . Even if we consider the sparsity of edges and implement
sparse matrix multiplications  the time complexity is still as high as O(EV ).
To improve the scalability of our GWL framework  we introduce a recursive K-partition mechanism 
recursively decomposing observed large graphs to a set of aligned small graphs. As shown in
Figure 2(a)  given two graphs  we ﬁrst calculate their barycenter graph (with K nodes) and achieve
their joint K-way partitioning. For each node of the barycenter graph  the corresponding sub-
graphs extracted from the observed two graphs construct an aligned sub-graph pair  shown as the
dotted frames connected with grey circles in Figure 2(a). For each aligned sub-graph pair  we
further calculate its barycenter graph and decompose the pair into more and smaller sub-graph pairs.
Repeating the above step  we ﬁnally calculate the GW discrepancy between the sub-graphs in each
pair  and ﬁnd the correspondence between their nodes. Note that this recursive mechanism is also
applicable to multi-graph matching: for multiple graphs  in the ﬁnal step we calculate the GWB
among the sub-graphs in each set. The details of our S-GWL method are provided in Appendix A.2.
Complexity analysis In Table 1  we compare the time and memory complexity of our S-GWL method
with other matching methods. The Hungarian algorithm [24] has time complexity O(V 3) [17  33  50].
Denoting the largest node degree in a graph as d  the time complexity of GHOST [35] is O(d4).
The methods above take the graph afﬁnity matrix as input  so their memory complexity in the worst
case is O(V 4). MI-GRAAL [23]  HubAlign [19] and NETAL [32] are relatively efﬁcient  with time
complexity O(V E + V 2 log V )  O(V 2 log V ) and O(E2 + EV log V )  respectively. CPD+Emb
ﬁrst learns D-dimensional node embeddings [18]  and then registers the embeddings by the CPD
method [31]  whose time complexity is O(DV 2). The memory complexity of these four methods
is O(V 2). For GW discrepancy-based methods  the GWL+Emb in [49] achieves graph matching
and node embedding jointly. It uses the distance matrix of node embeddings and breaks the sparsity
of edges  so its time complexity is O(V 3) and memory complexity is O(V 2). The time complexity
of GWL is O(V E)  but its memory complexity is still O(V 2) because the L(Cs  Ct  T (n)) in (3)
is a dense matrix. Our S-GWL combines the recursive mechanism with the regularized proximal
gradient method and implements the CsT (n)C>t
in (3) by sparse matrix multiplications. Ideally  we
can apply R = blogK V c recursions. In the r-th recursion we calculate Kr barycenter graphs for Kr
sub-graph pairs. The sub-graphs in each pair have O( V
Proposition 3.1. Suppose that we have M graphs  each of which has V nodes and E edges. With
the help of the recursive K-partition mechanism  the time complexity of our S-GWL method is
O(M K(E + V ) logK V )  and its memory complexity is O(M (E + V K)).
Choosing K = 2 and ignoring the number of graphs  we obtain the complexity shown in Table 1.
Our S-GWL has lower computational time complexity and memory requirements than many existing

Kr ) nodes. As a result  we have

5

GWL

GWL

GWL

GWL
S-GWL (K2 R3)
S-GWL (K4 R2)
S-GWL (K8 R1)

(a) Scheme of our S-GWL method

Figure 2: (a) An illustration of S-GWL. (b) Comparisons on runtime.

(b) Runtime

Table 1: Comparisons for graph matching methods on time and memory complexity.

Hungarian GHOST⇤

MI-GRAAL

HubAlign

NETAL

CPD+Emb GWL+Emb GWL

S-GWL

Time O(·)
d4
Memory O(·)
V 4
* d is the largest node degree in a graph.

V 3
V 4

V E+V 2 log V V 2 log V E2+EV log V DV 2
V 2

V 2

V 2

V 2

V 3
V 2

V E 2(E+V ) log V
V 2

E + 2V

methods. Figure 2(b) visualizes the runtime of GWL and S-GWL on matching synthetic graphs. The
S-GWL methods with different conﬁgurations (i.e.  the number of partitions K and that of recursions
R) are consistently faster than GWL. More detailed analysis is provided in Appendix A.3.
4 Related Work
Gromov-Wasserstein learning GW discrepancy has been applied in many matching problems 
e.g.  registering 3D objects [28  29] and matching vocabulary sets between different languages [2].
Focusing on graphs  a fused Gromov-Wasserstein distance is proposed in [44  43]  combining GW
discrepancy with Wasserstein discrepancy [46]. The work in [49] further takes node embedding
into account  learning the GW discrepancy between two graphs and their node embeddings jointly.
The appropriateness of these methods is supported by [11]  which proves that GW discrepancy is a
pseudometric on measure graphs. Recently  an adversarial learning method based on GW discrepancy
is proposed in [9]  which jointly trains two generative models in incomparable spaces. The work
in [37] further proposes Gromov-Wasserstein barycenters for clustering distributions and interpolating
shapes. Currently  GW discrepancy is mainly calculated based on Sinkhorn iterations [41  15  5  37] 
whose applications to large-scale graphs are challenging because of its high complexity. Our S-GWL
method is the ﬁrst attempt to make GW discrepancy applicable to large-scale graph analysis.
Graph partitioning and graph matching Graph partitioning is important for community detection
in networks. Many graph partitioning methods have been proposed  such as Metis [21]  EdgeBe-
tweenness [16]  FastGreedy [12]  Label Propagation [38]  Louvain [6] and Fluid Community [34].
All of these methods explore the clustering structure of nodes heuristically based on the modularity-
maximization principle [16  12]. Graph matching is important for network alignment [39  40  54]
and 2D/3D object registration [31  51  20  53]. Traditional methods formulate graph matching as a
quadratic assignment problem (QAP) and solve it based on the Hungarian algorithm [17  33  51  50] 
which are only applicable to small graphs. For large graphs like protein networks  many heuristic
methods have been proposed  such as GRAAL [22]  IsoRank [40]  PISwap [10]  MAGNA++ [45] 
NETAL [32]  HubAlign [19]  and GHOST [35]  which mainly focus on two-graph matching and are
sensitive to the noise in graphs. With the help of GW discrepancy  our work establishes a uniﬁed
framework for graph partitioning and matching  that can be readily extended to multi-graph cases.
5 Experiments
The implementation of our S-GWL method can be found at https://github.com/HongtengXu/s-gwl.
We compare it with state-of-the-art methods for graph partitioning and matching. All the methods are
run on an Intel i7 CPU with 4GB memory. Implementation details and a further set of experimental
results are provided in Appendix B.
5.1 Graph partitioning
We ﬁrst verify the performance of the GWL framework on graph partitioning  comparing it with the
following four baselines: Metis [21]  FastGreedy [12]  Louvain [6]  and Fluid Community [34].
We consider synthetic and real-world data. Similar to [52]  we compare these methods in terms of
adjusted mutual information (AMI) and runtime. Each synthetic graph is a Gaussian random partition
graph with N nodes and K clusters. The size of each cluster is drawn from a normal distribution
N (200  10). The nodes are connected within clusters with probability pin and between clusters with
probability pout. The ratio pout
indicates the clearness of the clustering structure  and accordingly
pin

6

Table 2: Comparisons for graph partitioning methods on AMI  time complexity and runtime (second).

Metis

Method

Louvain
Time complexity O(V +E+K log K) O(V E log V ) O(V log V )
AMI Time
(N  pin  pout)
0.747 22.889
(4000  0.2  0.05)
0.574 95.114
(4000  0.2  0.1)
0.005 290.846
(4000  0.2  0.15)

AMI Time
0.247 55.435
0.064 65.441
0.002 80.322

AMI
0.413
0.009
0.002

Time
1.744
2.340
3.592

FastGreedy

Fluid
O(E)

AMI Time
0.776 21.580
0.577 111.043
0.005 203.225

GWL

O((E + V )K)
AMI
Time
0.812
13.033
0.590
12.740
0.012
12.901

Table 3: Comparisons for graph partitioning methods on AMI.

Method
Dataset
EU-Email

Indian-Village

Metis

FastGreedy

Louvain

Raw Noisy Raw Noisy Raw Noisy Raw Noisy Raw Noisy
0.349
0.421
0.664
0.834

0.272 — 0.338
0.633 — 0.401

0.312
0.882

0.459
0.857

0.118
0.275

0.434
0.880

0.246
0.513

Fluid

GWL

“—”: Fluid is inapplicable when the networks have disconnected nodes or sub-graphs.

the difﬁculty of partitioning. We set N = 4000  pin = 0.2  and pout 2{ 0.05  0.1  0.15}. Under
each conﬁguration (N  pin  pout)  we simulate 10 graphs. For each method  its average performance
on these 10 graphs is listed in Table 2. GWL outperforms the alternatives consistently on AMI.
Additionally  as shown in Table 2  GWL has time complexity comparable to other methods  especially
when the graph is sparse  e.g.  E = O(V log V ). According to the runtime in practice  GWL is faster
than most baselines except Metis  likely because Metis is implemented in the C language while GWL
and other methods are based on Python.
Table 3 lists the performance of different methods on two real-world datasets. The ﬁrst dataset is the
email network from a large European research institution [25]. The network contains 1 005 nodes
and 25 571 edges. The edge (vi  vj) in the network mean that person vi sent person vj at least one
email  and each node in the network belongs to exactly one of 42 departments at the research institute.
The second dataset is the interactions among 1 991 villagers in 12 Indian villages [3]. Furthermore 
to verify the robustness of GWL to noise  we not only consider the raw data of these two datasets
but also create their noisy version by adding 10% more noisy edges between different communities
(i.e.  departments and villages). Experimental results show that GWL is at least comparable to its
competitors on raw data  and it is more robust to noise than other methods.

5.2 Graph matching
For two-graph matching  we compare our S-GWL method with the following baselines: PISwap [10] 
GHOST [35]  MI-GRAAL [23]  MAGNA++ [45]  HubAlign [19]  NETAL [32]  CPD+Emb [18 
31]  the GWL framework based on Algorithm 1  and the GWL+Emb in [49]. We test all methods
on both synthetic and real-world data. For each method  given the learned correspondence set S and
the ground-truth correspondence set Sreal  we calculate node correctness as NC = |S \ Sreal|/|S|⇥
100%. The runtime of each method is recorded as well.
In the synthetic dataset  each source graph G(Vs Es) obeys a Gaussian random partition model [7] or
Barabási-Albert model [4]. For each source graph  we generate a target graph by adding |Vs|⇥ q%
noisy nodes and |Es|⇥ q% noisy edges to the source graph. Figure 1(e) compares our S-GWL with the
baselines when |Vs| = 2000 and q = 5. For each method  its average node correctness and runtime
on matching 10 synthetic graph pairs are plotted. Compared with existing heursitic methods  GW
discrepancy-based methods (GWL+Emb  GWL and S-GWL) obtain much higher node correctness.
GWL+Emb achieves the highest node correctness  with runtime comparable to many baselines. Our
GWL framework does not learn node embeddings when matching graphs  so it is slightly worse
than GWL+Emb on node correctness but achieves about 10 times acceleration. Our S-GWL method
further accelerates GWL with the help of the recursive mechanism. It obtains high node correctness
and makes its runtime comparable to the fastest methods (HubAlign and NETAL).
In addition to graph matching on synthetic data  we also consider two real-world matching tasks. The
ﬁrst task is matching the protein-protein interaction (PPI) network of yeast with its noisy version.
The PPI network of yeast contains 1 004 proteins and their 4 920 high-conﬁdence interactions.
Its noisy version contains q% more low-conﬁdence interactions  and q 2{ 5  10  15  20  25}. The
dataset is available on https://www3.nd.edu/~cone/MAGNA++/. The second task is matching user
accounts in different communication networks. The dataset is available on http://vacommunity.org/
VAST+Challenge+2018+MC3  which records the communications among a company’s employees.
Following the work in [49]  we extract 622 employees and their call-network and email-network.

7

Table 4: Comparisons for graph matching methods on node correctness (%) and runtime (second).

Dataset
Method
PISwap
GHOST

Yeast 5% noise Yeast 15% noise Yeast 25% noise MC3 sparse
Time
NC
10.27
0.10
17.86
11.06
MI-GRAAL 18.03
72.89
425.16
48.13
MAGNA++
2.11
50.00
HubAlign
1.23
6.87
NETAL
87.54
3.59
CPD+Emb
GWL+Emb
83.66
608.76
89.43
82.37
81.08
8.39

Time
22.09
35.54
240.03
624.17
3.89
2.09
108.62
1537.93
210.86
74.64

Time
15.80
25.67
189.21
603.29
3.27
1.91
103.22
1340.58
190.97
68.58

Time
18.31
30.22
202.77
630.60
3.50
2.06
110.19
1499.20
212.16
70.06

NC
0.10
0.40
6.87
25.04
35.16
0.90
2.09
66.63
65.34
61.85

NC
0.00
0.30
5.18
13.61
12.85
1.00
2.00
57.97
58.76
56.27

NC
6.32
21.27
35.53
7.88
36.21
36.87
4.35
40.45
34.21
36.92

GWL
S-GWL

MC3 dense
Time
NC
11.81
0.00
22.90
0.03
0.64
197.65
447.86
0.09
2.29
3.86
1.30
1.77
95.68
0.48
4.23
831.80
93.94
3.96
4.03
9.01

Table 5: Comparisons for multi-graph matching methods on yeast networks.
6 graphs

3 graphs

4 graphs

5 graphs

Method

MultiAlign

GWL
S-GWL

NC@1 NC@all NC@1 NC@all NC@1 NC@all NC@1 NC@all
62.97
63.84
60.06

45.19
46.22
43.33

—
68.73
68.53

—
39.14
38.45

—
71.61
73.21

—
31.57
33.27

—
76.49
76.99

—
28.39
29.68

For each communication network  we construct a dense version and a sparse one: the dense version
keeps all the communications (edges) among the employees  while the sparse version only preserves
the communications happening more than 8 times. We test different methods on i) matching yeast’s
PPI network with its 5%  15% and 25% noisy versions; and ii) matching the employee call-network
with their email-network in both sparse and dense cases. Table 4 shows the performance of various
methods in these two tasks. Similar to the experiments on synthetic data  the GW discrepancy-based
methods outperform other methods on node correctness  especially for highly-noisy graphs  and our
S-GWL method achieves a good trade-off between accuracy and efﬁciency.
Given the PPI network of yeast and its 5 noisy versions  we test GWL and S-GWL for multi-graph
matching. We consider several existing multi-graph matching methods and ﬁnd that the methods
in [33  51  50] are not applicable for the graphs with hundreds of nodes because i) their time
complexity is at least O(V 3)  and ii) they suffer from inadequate memory on our machine (with 4GB
memory) because their memory complexity in the worst case is O(V 4). The IsoRankN in [26] can
align multiple PPI networks jointly  but it needs conﬁdence scores of protein pairs as input  which are
not available for our dataset. The only applicable baseline we are aware of is the MultiAlign in [54].
However  it can only achieve three-graph matching. Table 5 lists the performance of various methods.
Given learned correspondence sets  each of which is a set of matched nodes from different graphs 
NC@1 represents the percentage of the set containing at least a pair of correctly-matched nodes  and
NC@all represents the percentage of the set in which arbitrary two nodes are matched correctly. Both
GWL and S-GWL obtain comparable performance to MultiAlign on three-graph matching  and GWL
is the best. When the number of graphs increases  NC@1 increases while NC@all decreases for all
the methods  and S-GWL becomes even better than GWL.

6 Conclusion and Future Work
We have developed a scalable Gromov-Wasserstein learning method  achieving large-scale graph
partitioning and matching in a uniﬁed framework  with theoretical support. Experiments show that our
approach outperforms state-of-the-art methods in many situations. However  it should be noted that
our S-GWL method is sensitive to its hyperparameters. Speciﬁcally  we observed in our experiments
that the  in (3) should be set carefully according to observed graphs. Generally  for large-scale graphs
we have to use a large  and solve (3) with many iterations. The a and b in (5) are also signiﬁcant for
the performance of our method. The settings of these hyperparameters and their inﬂuences are shown
in Appendix B. In the future  we will further study the inﬂuence of hyperparameters on the rate of
convergence and set the hyperparameters adaptively according to observed data. Additionally  our
S-GWL method can decompose a large graph into many independent small graphs  so we plan to
further accelerate it by parallel processing and/or distributed learning.
Acknowledgements This research was supported in part by DARPA  DOE  NIH  ONR and NSF. We
thank Dr. Hongyuan Zha for helpful discussions.

8

References
[1] J. Altschuler  J. Weed  and P. Rigollet. Near-linear time approximation algorithms for optimal
transport via sinkhorn iteration. In Advances in Neural Information Processing Systems  pages
1964–1974  2017.

[2] D. Alvarez-Melis and T. Jaakkola. Gromov-wasserstein alignment of word embedding spaces.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing 
pages 1881–1890  2018.

[3] A. Banerjee  A. G. Chandrasekhar  E. Duﬂo  and M. O. Jackson. The diffusion of microﬁnance.

Science  341(6144):1236498  2013.

[4] A.-L. Barabási et al. Network science. Cambridge university press  2016.
[5] J.-D. Benamou  G. Carlier  M. Cuturi  L. Nenna  and G. Peyré. Iterative Bregman projections
for regularized transportation problems. SIAM Journal on Scientiﬁc Computing  37(2):A1111–
A1138  2015.

[6] V. D. Blondel  J.-L. Guillaume  R. Lambiotte  and E. Lefebvre. Fast unfolding of communities
in large networks. Journal of statistical mechanics: theory and experiment  2008(10):P10008 
2008.

[7] U. Brandes  M. Gaertler  and D. Wagner. Experiments on graph clustering algorithms. In

European Symposium on Algorithms  pages 568–579. Springer  2003.

[8] A. M. Bronstein  M. M. Bronstein  R. Kimmel  M. Mahmoudi  and G. Sapiro. A Gromov-
Hausdorff framework with diffusion geometry for topologically-robust non-rigid shape matching.
International Journal of Computer Vision  89(2-3):266–286  2010.

[9] C. Bunne  D. Alvarez-Melis  A. Krause  and S. Jegelka. Learning generative models across

incomparable spaces. NeurIPS Workshop on Relational Representation Learning  2018.

[10] L. Chindelevitch  C.-Y. Ma  C.-S. Liao  and B. Berger. Optimizing a global alignment of protein

interaction networks. Bioinformatics  29(21):2765–2773  2013.

[11] S. Chowdhury and F. Mémoli. The Gromov-Wasserstein distance between networks and stable

network invariants. arXiv preprint arXiv:1808.04337  2018.

[12] A. Clauset  M. E. Newman  and C. Moore. Finding community structure in very large networks.

Physical review E  70(6):066111  2004.

[13] L. P. Cordella  P. Foggia  C. Sansone  and M. Vento. A (sub) graph isomorphism algorithm
for matching large graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence 
26(10):1367–1372  2004.

[14] T. Cour  P. Srinivasan  and J. Shi. Balanced graph matching. In NIPS  pages 313–320  2007.
[15] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in

neural information processing systems  pages 2292–2300  2013.

[16] M. Girvan and M. E. Newman. Community structure in social and biological networks.

Proceedings of the national academy of sciences  99(12):7821–7826  2002.

[17] S. Gold and A. Rangarajan. A graduated assignment algorithm for graph matching. IEEE

Transactions on Pattern Analysis and Machine Intelligence  18(4):377–388  1996.

[18] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In KDD  pages

855–864  2016.

[19] S. Hashemifar and J. Xu. Hubalign: An accurate and efﬁcient method for global alignment of

protein–protein interaction networks. Bioinformatics  30(17):i438–i444  2014.

[20] S.-H. Jun  S. W. Wong  J. Zidek  and A. Bouchard-Côté. Sequential graph matching with

sequential monte carlo. In AISTATS  pages 1075–1084  2017.

[21] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular

graphs. SIAM Journal on scientiﬁc Computing  20(1):359–392  1998.

[22] O. Kuchaiev  T. Milenkovi´c  V. Memiševi´c  W. Hayes  and N. Pržulj. Topological network
alignment uncovers biological function and phylogeny. Journal of the Royal Society Interface 
page rsif20100063  2010.

[23] O. Kuchaiev and N. Pržulj.

Integrative network alignment reveals large regions of global

network similarity in yeast and human. Bioinformatics  27(10):1390–1396  2011.

[24] H. W. Kuhn. The hungarian method for the assignment problem. Naval research logistics

[25] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http:

quarterly  2(1-2):83–97  1955.

//snap.stanford.edu/data  June 2014.

9

[26] C.-S. Liao  K. Lu  M. Baym  R. Singh  and B. Berger. Isorankn: spectral methods for global

alignment of multiple protein networks. Bioinformatics  25(12):i253–i258  2009.

[27] N. Malod-Dognin and N. Pržulj. L-GRAAL: Lagrangian graphlet-based network aligner.

Bioinformatics  31(13):2182–2189  2015.

[28] F. Mémoli. Spectral Gromov-Wasserstein distances for shape matching. In ICCV Workshops 

pages 256–263  2009.

[29] F. Mémoli. Gromov-Wasserstein distances and the metric approach to object matching. Foun-

dations of computational mathematics  11(4):417–487  2011.

[30] F. Mémoli and G. Sapiro. Comparing point clouds. In Proceedings of the 2004 Eurograph-

ics/ACM SIGGRAPH symposium on Geometry processing  pages 32–40  2004.

[31] A. Myronenko and X. Song. Point set registration: Coherent point drift. IEEE Transactions on

Pattern Analysis and Machine Intelligence  32(12):2262–2275  2010.

[32] B. Neyshabur  A. Khadem  S. Hashemifar  and S. S. Arab. NETAL: A new graph-based method
for global alignment of protein–protein interaction networks. Bioinformatics  29(13):1654–1662 
2013.

[33] D. Pachauri  R. Kondor  and V. Singh. Solving the multi-way matching problem by permutation
synchronization. In Advances in neural information processing systems  pages 1860–1868 
2013.

[34] F. Parés  D. Garcia-Gasulla  A. Vilalta  J. Moreno  E. Ayguadé  J. Labarta  U. Cortés  and
T. Suzumura. Fluid communities: A competitive and highly scalable community detection
algorithm. Complex Networks & Their Applications VI  pages 229–240  2018.

[35] R. Patro and C. Kingsford. Global network alignment using multiscale spectral signatures.

Bioinformatics  28(23):3105–3114  2012.

[36] G. Peyré  M. Cuturi  et al. Computational optimal transport. Foundations and Trends R in
[37] G. Peyré  M. Cuturi  and J. Solomon. Gromov-wasserstein averaging of kernel and distance

Machine Learning  11(5-6):355–607  2019.

matrices. In International Conference on Machine Learning  pages 2664–2672  2016.

[38] U. N. Raghavan  R. Albert  and S. Kumara. Near linear time algorithm to detect community

structures in large-scale networks. Physical review E  76(3):036106  2007.

[39] R. Sharan and T. Ideker. Modeling cellular machinery through biological network comparison.

Nature biotechnology  24(4):427  2006.

[40] R. Singh  J. Xu  and B. Berger. Global alignment of multiple protein interaction networks with
application to functional orthology detection. Proceedings of the National Academy of Sciences 
2008.

[41] R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.

Paciﬁc Journal of Mathematics  21(2):343–348  1967.

[42] K.-T. Sturm et al. On the geometry of metric measure spaces. Acta mathematica  196(1):65–131 

2006.

2008.

[43] T. Vayer  L. Chapel  R. Flamary  R. Tavenard  and N. Courty. Fused Gromov-Wasserstein
distance for structured objects: theoretical foundations and mathematical properties. arXiv
preprint arXiv:1811.02834  2018.

[44] T. Vayer  L. Chapel  R. Flamary  R. Tavenard  and N. Courty. Optimal transport for structured

data. arXiv preprint arXiv:1805.09114  2018.

[45] V. Vijayan  V. Saraph  and T. Milenkovi´c. MAGNA++: Maximizing accuracy in global network

alignment via both node and edge conservation. Bioinformatics  31(14):2409–2411  2015.

[46] C. Villani. Optimal transport: Old and new  volume 338. Springer Science & Business Media 

[47] L. Wang  T. Lou  J. Tang  and J. E. Hopcroft. Detecting community kernels in large social
networks. In 2011 IEEE 11th International Conference on Data Mining  pages 784–793. IEEE 
2011.

[48] Y. Xie  X. Wang  R. Wang  and H. Zha. A fast proximal point method for Wasserstein distance.

arXiv preprint arXiv:1802.04307  2018.

[49] H. Xu  D. Luo  H. Zha  and L. Carin. Gromov-wasserstein learning for graph matching and

node embedding. arXiv preprint arXiv:1901.06003  2019.

[50] J. Yan  J. Wang  H. Zha  X. Yang  and S. Chu. Consistency-driven alternating optimization for
multigraph matching: A uniﬁed approach. IEEE Transactions on Image Processing  24(3):994–
1009  2015.

10

[51] J. Yan  H. Xu  H. Zha  X. Yang  H. Liu  and S. Chu. A matrix decomposition perspective to

multiple graph matching. In ICCV  pages 199–207  2015.

[52] Z. Yang  R. Algesheimer  and C. J. Tessone. A comparative analysis of community detection

algorithms on artiﬁcial networks. Scientiﬁc reports  6:30750  2016.

[53] T. Yu  J. Yan  Y. Wang  W. Liu  et al. Generalizing graph matching beyond quadratic assignment

model. In NIPS  pages 861–871  2018.

[54] J. Zhang and S. Y. Philip. Multiple anonymized social networks alignment. In ICDM  pages

599–608  2015.

11

,Qinqing Zheng
Ryota Tomioka
Hongteng Xu
Dixin Luo
Lawrence Carin