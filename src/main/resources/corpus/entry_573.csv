2012,Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning,We consider the estimation of an i.i.d.\ vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. We present a method  called adaptive generalized approximate message passing (Adaptive GAMP)  that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$. The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations.  This analysis shows that the adaptive GAMP method can yield asymptotically consistent parameter estimates  which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic  general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.,Approximate Message Passing with Consistent

Parameter Estimation and Applications to Sparse

Learning

Ulugbek S. Kamilov

EPFL

ulugbek.kamilov@epfl.ch

Sundeep Rangan

Polytechnic Institute of New York University

srangan@poly.edu

Alyson K. Fletcher

University of California  Santa Cruz

afletcher@soe.ucsc.edu

Michael Unser

EPFL

michael.unser@epfl.ch

Abstract

We consider the estimation of an i.i.d. vector x ∈ Rn from measurements y ∈ Rm
obtained by a general cascade model consisting of a known linear transform fol-
lowed by a probabilistic componentwise (possibly nonlinear) measurement chan-
nel. We present a method  called adaptive generalized approximate message pass-
ing (Adaptive GAMP)  that enables joint learning of the statistics of the prior
and measurement channel along with estimation of the unknown vector x. Our
method can be applied to a large class of learning problems including the learn-
ing of sparse priors in compressed sensing or identiﬁcation of linear-nonlinear
cascade models in dynamical systems and neural spiking processes. We prove
that for large i.i.d. Gaussian transform matrices the asymptotic componentwise
behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar
state evolution equations. This analysis shows that the adaptive GAMP method
can yield asymptotically consistent parameter estimates  which implies that the
algorithm achieves a reconstruction quality equivalent to the oracle algorithm that
knows the correct parameter values. The adaptive GAMP methodology thus pro-
vides a systematic  general and computationally efﬁcient method applicable to a
large range of complex linear-nonlinear models with provable guarantees.

Introduction

1
Consider the estimation of a random vector x ∈ Rn from a measurement vector y ∈ Rm. As
illustrated in Figure 1  the vector x  which is assumed to have i.i.d. components xj ∼ PX  is passed
through a known linear transform that outputs z = Ax ∈ Rm. The components of y ∈ Rm are
generated by a componentwise transfer function PY |Z. This paper addresses the cases where the
distributions PX and PY |Z have some parametric uncertainty that must be learned so as to properly
estimate x.
This joint estimation and learning problem with linear transforms and componentwise nonlinearities
arises in a range of applications  including empirical Bayesian approaches to inverse problems in sig-
nal processing  linear regression and classiﬁcation [1  2]  and  more recently  Bayesian compressed

1

Figure 1: Measurement model considered in this work. The vector x ∈ Rn with an i.i.d. prior
PX (x|λx) passes through the linear transform A ∈ Rm×n followed by a componentwise nonlinear
channel PY |Z(y|z  λz) to result in y ∈ Rm. The prior PX and the nonlinear channel PY |Z depend
on the unknown parameters λx and λz  respectively. We propose adaptive GAMP to jointly estimate
x and (λx  λz) given the measurements y.

sensing for estimation of sparse vectors x from underdetermined measurements [3–5]. Also  since
the parameters in the output transfer function PY |Z can model unknown nonlinearities  this problem
formulation can be applied to the identiﬁcation of linear-nonlinear cascade models of dynamical
systems  in particular for neural spike responses [6–8].
In recent years  there has been considerable interest in so-called approximate message passing
(AMP) methods for this estimation problem. The AMP techniques use Gaussian and quadratic
approximations of loopy belief propagation (LBP) to provide estimation methods that are computa-
tionally efﬁcient  general and analytically tractable. However  the AMP methods generally require
that the distributions PX and PY |Z are known perfectly. When the parameters λx and λz are un-
known  various extensions have been proposed including combining AMP methods with Expecta-
tion Maximization (EM) estimation [9–12] and hybrid graphical models approaches [13]. In this
work  we present a novel method for joint parameter and vector estimation called adaptive gen-
eralized AMP (adaptive GAMP)  that extends the GAMP method of [14]. We present two major
theoretical results related to adaptive GAMP: We ﬁrst show that  similar to the analysis of the stan-
dard GAMP algorithm  the componentwise asymptotic behavior of adaptive GAMP can be exactly
described by a simple scalar state evolution (SE) equations [14–18]. An important consequence of
this result is a theoretical justiﬁcation to the EM-GAMP algorithm in [9–12] which is a special
case of adaptive GAMP with a particular choice of adaptation functions. Our second result demon-
strates the asymptotic consistency of adaptive GAMP when adaptation functions correspond to the
maximum-likelihood (ML) parameter estimation. We show that when the ML estimation is com-
puted exactly  the estimated parameters converge to the true values and the performance of adaptive
GAMP asymptotically coincides with the performance of the oracle GAMP algorithms that knows
correct parameter values. Adaptive GAMP thus provides a computationally-efﬁcient method for
solving a wide variety of joint estimation and learning problems with a simple  exact performance
characterization and provable conditions for asymptotic consistency.
All proofs and some technical details that have been omitted for space appear in the full paper [19]
that also provides more background and simulations.

2 Adaptive GAMP

Approximate message passing (AMP) refers to a class of algorithms based on Gaussian approx-
imations of loopy belief propagation (LBP) for the estimation of the vectors x and z according
to the model described in Section 1. These methods originated from CDMA multiuser detection
problems in [15  20  21]; more recently  they have attracted considerable attention in compressed
sensing [17  18  22]. The Gaussian approximations used in AMP are closely related to standard ex-
pectation propagation techniques [23  24]  but with additional simpliﬁcations that exploit the linear
coupling between the variables x and z. The key beneﬁts of AMP methods are their computa-
tional performance  their large domain of application  and  for certain large random A  their exact
asymptotic performance characterizations with testable conditions for optimality [15–18]. This pa-
per considers an adaptive version of the so-called generalized AMP (GAMP) method of [14] that
extends the algorithm in [22] to arbitrary output distributions PY |Z.
The original GAMP algorithm of [14] requires that the distributions PX and PY |Z are known. We
propose an adaptive GAMP  shown in Algorithm 1  to allow for simultaneous estimation of the
distributions PX and PY |Z along with the estimation of x and z. The algorithm assumes that distri-
butions PX and PY |Z have some parametric forms

PX (x|λx)  PY |Z(y|z  λz) 

(1)

2

ComponentwiseOutput ChannelAvailableMeasurementsUnknowni.i.d. signalMixing MatrixUnknownLinearMeasurementsSignal Priorsequence of estimates(cid:98)xt and(cid:98)zt for x and z along with parameter estimates(cid:98)λt

for parameters λx ∈ Λx and λz ∈ Λz and for parameter sets Λx and Λz. Algorithm 1 produces a
z  The precise
value of these estimates depends on several factors in the algorithm including the termination criteria
and the choice of what we will call estimation functions Gt
s  and adaptation functions
H t

x and(cid:98)λt

z and Gt

x  Gt

x and H t
z.

Algorithm 1 Adaptive GAMP
Require: Matrix A  estimation functions Gt

1: Initialize t ← 0  s−1 ← 0 and some values for(cid:98)x0  τ 0

x  Gt

s and Gt
z and adaptation functions H t
x 

x and H t
z.

F τ t

x/m

p
z(pt  y  τ t
p)
i  yi  τ t
z(pt
s(pt
i  yi  τ t

2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: until Terminated

{Output node update}
pt ← A(cid:98)xt − st−1τ t
p ← (cid:107)A(cid:107)2
τ t
(cid:98)λt
p (cid:98)λt
(cid:98)zt
z ← H t
p (cid:98)λt
i ← Gt
s ← −(1/m)(cid:80)
i ← Gt
st
τ t
{Input node update}
r ← (cid:107)A(cid:107)2
F τ t
1/τ t
(cid:98)λt
s/n
rATst
rt = xt + τ t
(cid:98)xt+1
x ← H t
x(rt  τ t
r)
j ← Gt
x(rt
j  τ t
x ← (τ t
τ t+1

r (cid:98)λt
r/n)(cid:80)

z) for all i = 1  . . .   m
z) for all i = 1  . . .   m
z)/∂pt
s(pt
i

p (cid:98)λt

i  yi  τ t

i ∂Gt

x) for all j = 1  . . .   n

j ∂Gt

x(rt

j  τ t

x)/∂rj

r (cid:98)λt

x  Gt

z  and Gt

The choice of the estimation and adaptation functions allows for considerable ﬂexibility in the algo-
s can be selected such that the GAMP
rithm. For example  it is shown in [14] that Gt
algorithm implements Gaussian approximations of either max-sum LBP or sum-product LBP that
approximate the maximum-a-posteriori (MAP) or minimum-mean-squared-error (MMSE) estimates
of x given y  respectively. The adaptation functions can also be selected for a number of different
parameter-estimation strategies. Because of space limitation  we present only the estimation func-
tions for the sum-product GAMP algorithm from [14] along with an ML-type adaptation. Some of
the analysis below  however  applies more generally.
As described in [14]  the sum-product estimation can be implemented with the functions

Gt

x(r  τr (cid:98)λx)
z(p  y  τp (cid:98)λz)
s(p  y  τp (cid:98)λz)

Gt

Gt

(cid:16)

:= E[X|R = r  τr (cid:98)λx] 
:= E[Z|P = p  Y = y  τp (cid:98)λz] 
(cid:17)
z(p  y  τp (cid:98)λz) − p
Vx ∼ N (0  τr)  X ∼ PX (·|(cid:98)λx) 
Y ∼ PY |Z(·|Z (cid:98)λz).

1
τp

Gt

:=

 

where the expectations are with respect to the scalar random variables

Vz ∼ N (0  τp) 

R = X + Vx 
Z = P + Vz 

(i.e. ((cid:98)λx (cid:98)λz) = (λx  λz))  the outputs(cid:98)xt and(cid:98)zt can be interpreted as sum products estimates of

(3a)
(3b)
The estimation functions (2) correspond to scalar estimates of random variables in additive white
Gaussian noise (AWGN). A key result of [14] is that  when the parameters are set to the true values
the conditional expectations E(x|y) and E(z|y). The algorithm thus reduces the vector-valued
estimation problem to a computationally simple sequence of scalar AWGN estimation problems
along with linear transforms.
The estimation functions H t
x and H t
and λz. In the special case when H t

z in Algorithm 1 produce the estimates for the parameters λx
x and H t

z produce ﬁxed outputs
r) = λ

t
z  H t
p) = λ

x(rt  τ t

t
x 

H t

z(pt  yt  τ t

(2a)
(2b)

(2c)

3

t
t
for pre-computed values of λ
x  the adaptive GAMP algorithm reduces to the standard (non-
z and λ
adaptive) GAMP algorithm of [14]. The non-adaptive GAMP algorithm can be used when the
parameters λx and λz are known.
When the parameters λx and λz are unknown  it has been proposed in [9–12] that they can be
estimated via an EM method that exploits that fact that GAMP provides estimates of the posterior
distributions of x and z given the current parameter estimates. As described in the full paper [19] 
this EM-GAMP method corresponds to a special case of the Adaptive GAMP method for a particular
choice of the adaptation functions H t
However  in this work  we consider an alternate parameter estimation method based on ML adapta-
tion. The ML adaptation uses the following fact that we will rigorously justify below: For certain
large random A  at any iteration t  the components of the vectors rt and the joint vectors (pt  yt)
will be distributed as

x and H t
z.

Vx ∼ N (0  ξr)  X ∼ PX (·|λ∗
x) 

x and λ∗

(Z  P ) ∼ N (0  Kp) 

R = αrX + Vx 
Z = P + Vz 

(4a)
(4b)
where λ∗
z are the “true” parameters and the scalars αr and ξr and the covariance matrix Kp
are some parameters that depend on the estimation and adaptation functions used in the previous
iterations. Remarkably  the distributions of the components of rt and (pt  yt) will follow (4) even
if the estimation functions in the iterations prior to t used the incorrect parameter values. The
adaptive GAMP algorithm can thus attempt to estimate the parameters via a maximum likelihood
(ML) estimation:

Y ∼ PY |Z(·|Z  λ∗
z) 

H t

x(rt  τ t
r)

:= arg max
λx∈Λx

max

(αr ξr)∈Sx(τ t
r )

H t

z(pt  y  τ t
p)

:= arg max
λz∈Λz

max

Kp∈Sz(τ t
p)

 1
n−1(cid:88)
m−1(cid:88)

j=0

n

1
m

i=0

(cid:40)

φx(rt

j  λx  αr  ξr)

(cid:41)

φz(pt

i  yi  Kp)

 

  

where Sx and Sz are sets of possible values for the parameters αr  ξr and Kp  φx and φz are the
log-likelihoods

φx(r  λx  αr  ξr) = log pR(r|λx  αr  ξr) 
φz(p  y  λz  Kp) = log pP Y (p  y|λz  Kp)

and pR and pP Y are the probability density functions corresponding to the distributions in (4).

3 Convergence and Asymptotic Consistency with Gaussian Transforms

3.1 General State Evolution Analysis

Before proving the asymptotic consistency of the adaptive GAMP method with ML adaptation  we
ﬁrst prove a more general convergence result. Among other consequences  the result will justify
the distribution model (4) assumed by the ML adaptation. Similar to the SE analyses in [14  18]
we consider the asymptotic behavior of the adaptive GAMP algorithm with large i.i.d. Gaussian
matrices. The assumptions are summarized as follows. Details can be found in the full paper [19 
Assumption 2].

Assumption 1 Consider the adaptive GAMP algorithm running on a sequence of problems indexed
by the dimension n  satisfying the following:

(a) For each n  the matrix A ∈ Rm×n has i.i.d. components with Aij ∼ N (0  1/m) and the
dimension m = m(n) is a deterministic function of n satisfying n/m → β for some β > 0
as n → ∞.

(b) The input vectors x and initial condition(cid:98)x0 are deterministic sequences whose components

converge empirically with bounded moments of order s = 2k − 2 as

(5a)

(5b)

(6a)
(6b)

(7)

n→∞(x (cid:98)x0)

lim

= (X  (cid:98)X 0) 

PL(s)

4

to some random vector (X  (cid:98)X 0) for k = 2. See [19] for a precise statement of this type of

convergence.

(c) The output vectors z and y ∈ Rm are generated by

(8)
for some scalar function h(z  w) where the disturbance vector w is deterministic  but em-
pirically converges as

z = Ax  y = h(z  w) 

(9)
with s = 2k−2  k = 2 and W is some random variable. We let PY |Z denote the conditional
distribution of the random variable Y = h(Z  W ).

lim
n→∞ w

= W 

PL(s)

(d) Suitable continuity assumptions on the estimation functions Gt

x  Gt

z and Gt

s and adaptation

functions H t

x and H t

z – see [19] for details.

x := {(xj  rt
θt

Now deﬁne the sets of vectors

j (cid:98)xt+1
j )  j = 1  . . .   n} 

z := {(zi (cid:98)zt
adaptive GAMP estimate(cid:98)xt as well as rt. The second vector  θt
“true ” but unknown  output vector z  its GAMP estimate(cid:98)zt  as well as pt and the observed input y.

x  represents the components of the the “true ” but unknown  input vector x  its
z  contains the components of the

i)  i = 1  . . .   m}.

The ﬁrst vector set  θt

i   yi  pt

(10)

θt

x and θt

The sets θt
z are implicitly functions of the dimension n. Our main result  Theorem 1 below 
characterizes the asymptotic joint distribution of the components of these two sets as n → ∞.
Speciﬁcally  we will show that the empirical distribution of the components of θt
z converge
to a random vectors of the form

x and θt

where X is the random variable in the initial condition (7). Rt and (cid:98)X t+1 are given by

θ

θ

(11)

x := (X  Rt  (cid:98)X t+1) 

t

t

z := (Z (cid:98)Z t  Y  P t) 
(cid:98)X t+1 = Gt

Rt = αt

rX + V t 

for some deterministic constants αt
(Z  P t) ∼ N (0  Kt

p)  and

r  ξt

V t ∼ N (0  ξt
r) 
t
r and λ
x that will be deﬁned momentarily. Similarly 

x(Rt  τ t

t
r  λ
x)

r  τ t

(12)

Y ∼ PY |Z(·|Z) 
t
p and λ
where W is the random variable in (9) and Kt
z are also deterministic constants. The determin-
istic constants above can be computed iteratively with the following state evolution (SE) equations
shown in Algorithm 2.

z(P t  Y  τ t

(13)

t
z) 

p  λ

(cid:98)Z t = Gt

Theorem 1 Consider the random vectors θt
sumption 1. Let θ
equations in Algorithm 2. Then  for any ﬁxed t  almost surely  the components of θt
empirically with bounded moments of order k = 2 as

z generated by the outputs of GAMP under As-
t
z be the random vectors in (11) with the parameters determined by the SE
z converge

t
x and θ

x and θt

x and θt

n→∞ θt
lim

x

PL(k)

= θ

t
x 

n→∞ θt
lim

z

PL(k)

= θ

t
z.

where θ

t
x and θ

t
z are given in (11). In addition  for any t  the limits

t
λt
x = λ
x 

lim
n

λt
z = λ

t
z 

lim
n

r = τ t
τ t
r 

lim
n

p = τ t
τ t
p 

lim
n

also hold almost surely.

(17)

(18)

Similar to several other analyses of AMP algorithms such as [14–18]  the theorem provides a scalar
equivalent model for the componentwise behavior of the adaptive GAMP method. That is  asymp-
totically the components of the sets θt
z in (10) are distributed identically to simple scalar
random variables. The parameters in these random variables can be computed via the SE equations

x and θt

5

Algorithm 2 Adaptive GAMP State Evolution
Given the distributions in Assumption 1  compute the sequence of parameters as follows:

• Initialization: Set t = 0 with

x = cov(X  (cid:98)X 0) 

where the expectation is over the random variables (X  (cid:98)X 0) in Assumption 1(b) and τ 0

x = τ 0
τ 0
x  

K0

(14)

x is

the initial value in the GAMP algorithm.

• Output node update: Compute the variables associated with θ

τ t
p = βτ t
x 
r = −E−1
τ t

αt
r = τ t
r

E

(cid:20) ∂
(cid:20) ∂

∂p

∂z

Kt

p = βKt
x 

Gt

s(P t  Y  τ t

s((cid:98)P   h(z  W )  τ t

Gt

t
p  λ
z)

(cid:21)

λ

t
z = H t
t
p  λ
z)

 

t
z:

r)2E(cid:104)

z(P t  τ t

p) 

ξt
r = (τ t

(cid:21)

(cid:12)(cid:12)(cid:12)(cid:12)z=Z

.

where the expectations are over the random variables (P t  Y  W ).

• Input node update: Compute the variables associated with θ

t
x:

(cid:20) ∂

λ

t
x(Rt  τ t
x = H t
E

= τ t
r

τ t+1
x

r) 

(cid:21)

x = cov(X  (cid:98)X t+1) 

where the expectation is over the random variable (X  (cid:98)X t+1).

∂r

Gt

x(Rt  τ t

t
x)
r  λ

  Kt+1

Gt

s(P t  Y  τ t

t
p  λ
z)

(cid:105)

(15a)

  (15b)

(15c)

(16a)

(16b)

(14)  (15) and (16)  which can be evaluated with one or two-dimensional integrals. From this scalar
equivalent model  one can compute a large class of componentwise performance metrics such as
mean-squared error (MSE) or detection error rates. Thus  the SE analysis shows that for  essentially
arbitrary estimation and adaptation functions  and distributions on the true input and disturbance  we
can exactly evaluate the asymptotic behavior of the adaptive GAMP algorithm. In addition  when
the parameter values λx and λz are ﬁxed  the SE equations in Algorithm 2 reduce to SE equations
for the standard (non-adaptive) GAMP algorithm described in [14].

3.2 Asymptotic Consistency with ML Adaptation

The general result  Theorem 1  can be applied to the adaptive GAMP algorithm with arbitrary es-
timation and adaptation function. In particular  the result can be used to rigorously justify the SE
analysis of the EM-GAMP presented in [11  12]. Here  we use the result to prove the asymptotic
parameter consistency of Adaptive GAMP with ML adaptation. The key point is to realize that
the distributions (12) and (13) exactly match the distributions (4) assumed by the ML adaptation
functions (5). Thus  the ML adaptation should work provided that the maximizations in (5) yield
the correct parameter estimates. This condition is essentially an identiﬁability requirement that we
make precise with the following deﬁnitions.
Deﬁnition 1 Consider a family of distributions  {PX (x|λx)  λx ∈ Λx}  a set Sx of parameters
(αr  ξr) of a Gaussian channel and function φx(r  λx  αr  ξr). We say that PX (x|λx) is identiﬁable
with Gaussian outputs with parameter set Sx and function φx if:

(a) The sets Sx and Λx are compact.
(b) For any “true” parameters λ∗

(cid:98)λx = arg max

x ∈ Λx  and (αr  ξr) ∈ Sx  the maximization
r  ξ∗
x  α∗
r ]  
max

rX + V  λx  αr  ξr)|λ∗

E [φx(α∗

(19)

is well-deﬁned  unique and returns the true value (cid:98)λx = λ∗

(αr ξr)∈Sx

λx∈Λx

respect to X ∼ PX (·|λ∗

x) and V ∼ N (0  ξ∗
r ).

x. The expectation in (19) is with

6

(c) Suitable continuity assumptions – see [19] for details.

Deﬁnition 2 Consider a family of conditional distributions  {PY |Z(y|z  λz)  λz ∈ Λz} generated
by the mapping Y = h(Z  W  λz) where W ∼ PW is some random variable and h(z  w  λz) is
a scalar function. Let Sz be a set of covariance matrices Kp and let φz(y  p  λz  Kp) be some
function. We say that conditional distribution family PY |Z(·|·  λz) is identiﬁable with Gaussian
inputs with covariance set Sz and function φz if:

(a) The parameter sets Sz and Λz are compact.
(b) For any “true” parameter λ∗

(cid:98)λz = arg max

z ∈ Λz and true covariance K∗
is well-deﬁned  unique and returns the true value (cid:98)λz = λ∗
z) and (Z  P ) ∼ N (0  K∗
p).

respect to Y |Z ∼ PY |Z(y|z  λ∗

E(cid:2)φz(Y  P  λz  Kp)|λ∗

max
Kp∈Sz

λz∈Λz

p  the maximization

(cid:3)  

z  K∗

p

z  The expectation in (20) is with

(20)

(c) Suitable continuity assumptions – see [19] for details.

Deﬁnitions 1 and 2 essentially require that the parameters λx and λz can be identiﬁed through a
maximization. The functions φx and φz can be the log likelihood functions (6a) and (6b)  although
we permit other functions as well. See [19] for further discussion of the likelihood functions as well
as the choice of the parameter sets Sx and Sz.
Theorem 2 Let PX (·|λx) and PY |Z(·|·  λz) be families of input and output distributions that are
identiﬁable in the sense of Deﬁnitions 1 and 2. Consider the outputs of the adaptive GAMP algo-
rithm using the ML adaptation functions (5) using the functions φx and φz and parameter sets in
Deﬁnitions 1 and 2. In addition  suppose Assumption 1(a) to (c) hold where the distribution of X is
given by PX (·|λ∗
x ∈ Λx and the conditional distribution of Y given
x) for some “true” parameter λ∗
z ∈ Λz. Then  under suitable continuity
Z is given by PY |Z(y|z  λ∗
conditions (see [19] for details)  for any ﬁxed t 

z) for some “true” parameter λ∗

(a) The components of θt

x and θt

z in (10) converge empirically with bounded moments of order

k = 2 as in (17) and the limits (18) hold almost surely.

r) for some t  then limn→∞(cid:98)λt
p) for some t  then limn→∞(cid:98)λt

t
z = λ

r) ∈ Sx(τ t

r  ξt
p ∈ Sz(τ t

(b) If (αt

(c) If Kt

t

x almost surely.

x = λ
z = λ∗

x = λ∗
z almost surely.

The theorem shows  remarkably  that for a very large class of the parameterized distributions  the
adaptive GAMP algorithm with ML adaptation is able to asymptotically estimate the correct param-
eters. Also  once the consistency limits in (b) and (c) hold  the SE equations in Algorithm 2 reduce
to the SE equations for the non-adaptive GAMP method running with the true parameters. Thus 
we conclude there is asymptotically no performance loss between the adaptive GAMP algorithm
and a corresponding oracle GAMP algorithm that knows the correct parameters in the sense that the
empirical distributions of the algorithm outputs are described by the same SE equations.

4 Numerical Example: Estimation of a Gauss-Bernoulli input

Recent results suggest that there is considerable value in learning of priors PX in the context of
compressed sensing [25]  which considers the estimation of sparse vectors x from underdetermined
measurements (m < n) . It is known that estimators such as LASSO offer certain optimal min-max
performance over a large class of sparse distributions [26]. However  for many particular distribu-
tions  there is a potentially large performance gap between LASSO and MMSE estimator with the
correct prior. This gap was the main motivation for [9  10] which showed large gains of the EM-
GAMP method due to its ability to learn the prior. Here  we present a simple simulation to illustrate
the performance gain of adaptive GAMP and its asymptotic consistency. Speciﬁcally  Fig. 2 com-
pares the performance of adaptive GAMP for estimation of a sparse Gauss-Bernoulli signal x ∈ Rn
from m noisy measurements

y = Ax + w 

7

Figure 2: Reconstruction of a Gauss-Bernoulli signal from noisy measurements. The average recon-
struction MSE is plotted against (a) measurement ratio m/n and (b) AWGN variance σ2. The plots
illustrate that adaptive GAMP yields considerable improvement over (cid:96)1-based LASSO estimator.
Moreover  it exactly matches the performance of oracle GAMP that knows the prior parameters.

where the additive noise w is random with i.i.d. entries wi ∼ N (0  σ2). The signal of length
n = 400 has 20% nonzero components drawn from the Gaussian distribution of variance 5. Adap-
tive GAMP uses EM iterations  which are used to approximate ML parameter estimation  to jointly
x = 5). The performance of
recover the unknown signal x and the true parameters λx = (ρ = 0.2  σ2
adaptive GAMP is compared to that of LASSO with MSE optimal regularization parameter  and or-
acle GAMP that knows the parameters of the prior exactly. For generating the graphs  we performed
1000 random trials by forming the measurement matrix A from i.i.d. zero-mean Gaussian random
variables of variance 1/m. In Figure 2(a)  we keep the variance of the noise ﬁxed to σ2 = 0.1 and
plot the average MSE of the reconstruction against the measurement ratio m/n. In Figure 2(b)  we
keep the measurement ratio ﬁxed to m/n = 0.75 and plot the average MSE of the reconstruction
against the noise variance σ2. For completeness  we also provide the asymptotic MSE values com-
puted via SE recursion. The results illustrate that GAMP signiﬁcantly outperforms LASSO over the
whole range of m/n and σ2. Moreover  the results corroborate the consistency of adaptive GAMP
which achieves nearly identical quality of reconstruction with oracle GAMP. The performance re-
sults here and in [19] indicate that adaptive GAMP can be an effective method for estimation when
the parameters of the problem are difﬁcult to characterize and must be estimated from data.

5 Conclusions and Future Work

We have presented an adaptive GAMP method for the estimation of i.i.d. vectors x observed through
a known linear transforms followed by an arbitrary  componentwise random transform. The proce-
dure  which is a generalization of EM-GAMP methodology of [9  10]  estimates both the vector x
as well as parameters in the source and componentwise output transform. In the case of large i.i.d.
Gaussian transforms with ML parameter estimation  it is shown that the adaptive GAMP method is
provably asymptotically consistent in that the parameter estimates converge to the true values. This
convergence result holds over a large class of models with essentially arbitrarily complex parame-
terizations. Moreover  the algorithm is computationally efﬁcient since it reduces the vector-valued
estimation problem to a sequence of scalar estimation problems in Gaussian noise. We believe that
this method is applicable to a large class of linear-nonlinear models with provable guarantees and
that it can have applications in a wide range of problems. We have mentioned the use of the method
for learning sparse priors in compressed sensing. Future work will include possible extensions to
non-Gaussian matrices.

References

[1] M. Tipping  “Sparse Bayesian learning and the relevance vector machine ” J. Machine Learning Research 

vol. 1  pp. 211–244  Sep. 2001.

[2] M. West  “Bayesian factor regressionm models in the “large p  small n” paradigm ” Bayesian Statistics 

vol. 7  2003.

8

(a)(b)Noise variance (    )Measurement ratio (       )MSE (dB)MSE (dB)0.511.52(cid:239)14(cid:239)13(cid:239)12(cid:239)11(cid:239)10(cid:239)9(cid:239)8(cid:239)7Measurement ratio (m/n)MSE (dB)  State EvolutionLASSOOracle GAMPAdaptive GAMP10(cid:239)310(cid:239)210(cid:239)1(cid:239)35(cid:239)30(cid:239)25(cid:239)20(cid:239)15(cid:239)10Noise Variance ((cid:109)2)MSE (dB)  [3] D. Wipf and B. Rao  “Sparse Bayesian learning for basis selection ” IEEE Trans. Signal Process.  vol. 52 

no. 8  pp. 2153–2164  Aug. 2004.

[4] S. Ji  Y. Xue  and L. Carin  “Bayesian compressive sensing ” IEEE Trans. Signal Process.  vol. 56  pp.

2346–2356  Jun. 2008.

[5] V. Cevher  “Learning with compressible priors ” in Proc. NIPS  Vancouver  BC  Dec. 2009.
[6] S. Billings and S. Fakhouri  “Identiﬁcation of systems containing linear dynamic and static nonlinear

elements ” Automatica  vol. 18  no. 1  pp. 15–26  1982.

[7] I. W. Hunter and M. J. Korenberg  “The identiﬁcation of nonlinear biological systems: Wiener and Ham-

merstein cascade models ” Biological Cybernetics  vol. 55  no. 2–3  pp. 135–144  1986.

[8] O. Schwartz  J. W. Pillow  N. C. Rust  and E. P. Simoncelli  “Spike-triggered neural characterization ” J.

Vision  vol. 6  no. 4  pp. 484–507  Jul. 2006.

[9] J. P. Vila and P. Schniter  “Expectation-maximization Bernoulli-Gaussian approximate message passing ”
in Conf. Rec. 45th Asilomar Conf. Signals  Syst. & Comput.  Paciﬁc Grove  CA  Nov. 2011  pp. 799–803.
[10] ——  “Expectation-maximization Gaussian-mixture approximate message passing ” in Proc. Conf. on

Inform. Sci. & Sys.  Princeton  NJ  Mar. 2012.

[11] F. Krzakala  M. M´ezard  F. Sausset  Y. Sun  and L. Zdeborov´a  “Statistical physics-based reconstruction

in compressed sensing ” arXiv:1109.4424  Sep. 2011.

[12] ——  “Probabilistic reconstruction in compressed sensing: Algorithms  phase diagrams  and threshold

achieving matrices ” arXiv:1206.3953  Jun. 2012.

[13] S. Rangan  A. K. Fletcher  V. K. Goyal  and P. Schniter  “Hybrid generalized approximation message
passing with applications to structured sparsity ” in Proc. IEEE Int. Symp. Inform. Theory  Cambridge 
MA  Jul. 2012  pp. 1241–1245.

[14] S. Rangan  “Generalized approximate message passing for estimation with random linear mixing ” in

Proc. IEEE Int. Symp. Inform. Theory  Saint Petersburg  Russia  Jul.–Aug. 2011  pp. 2174–2178.

[15] D. Guo and C.-C. Wang  “Asymptotic mean-square optimality of belief propagation for sparse linear

systems ” in Proc. IEEE Inform. Theory Workshop  Chengdu  China  Oct. 2006  pp. 194–198.

[16] ——  “Random sparse linear systems observed via arbitrary channels: A decoupling principle ” in Proc.

IEEE Int. Symp. Inform. Theory  Nice  France  Jun. 2007  pp. 946–950.

[17] S. Rangan  “Estimation with random linear mixing  belief propagation and compressed sensing ” in Proc.

Conf. on Inform. Sci. & Sys.  Princeton  NJ  Mar. 2010  pp. 1–6.

[18] M. Bayati and A. Montanari  “The dynamics of message passing on dense graphs  with applications to

compressed sensing ” IEEE Trans. Inform. Theory  vol. 57  no. 2  pp. 764–785  Feb. 2011.

[19] U. S. Kamilov  S. Rangan  A. K. Fletcher  and M. Unser  “Approximate message passing with consistent

parameter estimation and applications to sparse learning ” arXiv:1207.3859 [cs.IT]  Jul. 2012.

[20] J. Boutros and G. Caire  “Iterative multiuser joint decoding: Uniﬁed framework and asymptotic analysis ”

IEEE Trans. Inform. Theory  vol. 48  no. 7  pp. 1772–1793  Jul. 2002.

[21] T. Tanaka and M. Okada  “Approximate belief propagation  density evolution  and neurodynamics for

CDMA multiuser detection ” IEEE Trans. Inform. Theory  vol. 51  no. 2  pp. 700–706  Feb. 2005.

[22] D. L. Donoho  A. Maleki  and A. Montanari  “Message-passing algorithms for compressed sensing ”

Proc. Nat. Acad. Sci.  vol. 106  no. 45  pp. 18 914–18 919  Nov. 2009.

[23] T. P. Minka  “A family of algorithms for approximate Bayesian inference ” Ph.D. dissertation  Mas-

sachusetts Institute of Technology  Cambridge  MA  2001.

[24] M. Seeger  “Bayesian inference and optimal design for the sparse linear model ” J. Machine Learning

Research  vol. 9  pp. 759–813  Sep. 2008.

[25] E. J. Cand`es and T. Tao  “Near-optimal signal recovery from random projections: Universal encoding

strategies?” IEEE Trans. Inform. Theory  vol. 52  no. 12  pp. 5406–5425  Dec. 2006.

[26] D. Donoho  I. Johnstone  A. Maleki  and A. Montanari  “Compressed sensing over (cid:96)p-balls: Minimax

mean square error ” in Proc. ISIT  St. Petersburg  Russia  Jun. 2011.

9

,Dan Alistarh
Jennifer Iglesias
Milan Vojnovic
Remi Lam
Karen Willcox