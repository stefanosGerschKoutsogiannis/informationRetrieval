2017,SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud,Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task.  However  this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load  or worse  maliciously modify the inference results sent to the client. We propose SafetyNets  a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically  SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks  i.e.  those that can be represented as arithmetic circuits. Our empirical results on three- and four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability  while achieving state-of-the-art accuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition tasks (75.22%).,SafetyNets: Veriﬁable Execution of Deep Neural

Networks on an Untrusted Cloud

Zahra Ghodsi  Tianyu Gu  Siddharth Garg

New York University

{zg451  tg1553  sg175}@nyu.edu

Abstract

Inference using deep neural networks is often outsourced to the cloud since it is
a computationally demanding task. However  this raises a fundamental issue of
trust. How can a client be sure that the cloud has performed inference correctly?
A lazy cloud provider might use a simpler but less accurate model to reduce its
own computational load  or worse  maliciously modify the inference results sent to
the client. We propose SafetyNets  a framework that enables an untrusted server
(the cloud) to provide a client with a short mathematical proof of the correctness of
inference tasks that they perform on behalf of the client. Speciﬁcally  SafetyNets
develops and implements a specialized interactive proof (IP) protocol for veriﬁable
execution of a class of deep neural networks  i.e.  those that can be represented
as arithmetic circuits. Our empirical results on three- and four-layer deep neural
networks demonstrate the run-time costs of SafetyNets for both the client and server
are low. SafetyNets detects any incorrect computations of the neural network by
the untrusted server with high probability  while achieving state-of-the-art accuracy
on the MNIST digit recognition (99.4%) and TIMIT speech recognition tasks
(75.22%).

1

Introduction

Recent advances in deep learning have shown that multi-layer neural networks can achieve state-of-
the-art performance on a wide range of machine learning tasks. However  training and performing
inference (using a trained neural network for predictions) can be computationally expensive. For this
reason  several commercial vendors have begun offering “machine learning as a service" (MLaaS)
solutions that allow clients to outsource machine learning computations  both training and inference 
to the cloud.
While promising  the MLaaS model (and outsourced computing  in general) raises immediate security
concerns  speciﬁcally relating to the integrity (or correctness) of computations performed by the
cloud and the privacy of the client’s data [16]. This paper focuses on the former  i.e.  the question
of integrity. Speciﬁcally  how can a client perform inference using a deep neural network on an
untrusted cloud  while obtaining strong assurance that the cloud has performed inference correctly?
Indeed  there are compelling reasons for a client to be wary of a third-party cloud’s computations. For
one  the cloud has a ﬁnancial incentive to be “lazy." A lazy cloud might use a simpler but less accurate
model  for instance  a single-layer instead of a multi-layer neural network  to reduce its computational
costs. Further the cloud could be compromised by malware that modiﬁes the results sent back to
the client with malicious intent. For instance  the cloud might always mis-classify a certain digit in
a digit recognition task  or allow unauthorized access to certain users in a face recognition based
authentication system.
The security risks posed by cloud computing have spurred theoretical advances in the area of veriﬁable
computing (VC) [21]. The idea is to enable a client to provably (and cheaply) verify that an untrusted

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

server has performed computations correctly. To do so  the server provides to the client (in addition
to the result of computation) a mathematical proof of the correctness of the result. The client rejects 
with high probability  any incorrectly computed results (or proofs) provided by the server  while
always accepting correct results (and corresponding proofs) 1. VC techniques aim for the following
desirable properties: the size of the proof should be small  the client’s veriﬁcation effort must be
lower than performing the computation locally  and the server’s effort in generating proofs should not
be too high.
The advantage of proof-based VC is that it provides unconditional  mathematical guarantees on the
integrity of computation performed by the server. Alternative solutions for veriﬁable execution require
the client to make trust assumptions that are hard for the client to independently verify. Trusted
platform modules [7]  for instance  require the client to place trust on the hardware manufacturer  and
assume that the hardware is tamper-proof. Audits based on the server’s execution time [15] require
precise knowledge of the server’s hardware conﬁguration and assume  for instance  that the server is
not over-clocked.

The work in this paper leverages pow-
erful VC techniques referred to as in-
teractive proof (IP) systems [5  9  18 
19]. An IP system consists of two en-
tities  a prover (P)  i.e.  the untrusted
server  and a veriﬁer (V)  i.e.  the
client. The framework is illustrated in
Figure 1. The veriﬁer sends the prover
an input x  say a batch of test images 
and asks the prover to compute a func-
Figure 1: High-level overview of the SafetyNets IP protocol.
tion y = f (x). In our setting  f (.) is
In this example  an untrusted server intentionally changes
a trained multi-layer neural network
the classiﬁcation output from 4 to 5.
that is known to both the veriﬁer and
prover  and y is the neural network’s classiﬁcation output for each image in the batch. The prover
performs the computation and sends the veriﬁer a purported result y(cid:48) (which is not equal to y if the
prover cheats). The veriﬁer and prover then engage in n rounds of interaction. In each round  the
veriﬁer sends the prover a randomly picked challenge  and the prover provides a response based on
the IP protocol. The veriﬁer accepts that y(cid:48) is indeed equal to f (x) if it is satisﬁed with the prover’s
response in each round  and rejects otherwise.
A major criticism of IP systems (and  indeed  all existing VC techniques) when used for verifying
general-purpose computations is that the prover’s overheads are large  often orders of magnitude
more than just computing f (x) [21]. Recently  however  Thaler [18] showed that certain types of
computations admit IP protocols with highly efﬁcient veriﬁers and provers  which lays the foundations
for the specialized IP protocols for deep neural networks that we develop in this paper.

Paper Contributions. This paper introduces SafetyNets  a new (and to the best of our knowledge 
the ﬁrst) approach for veriﬁable execution of deep neural networks on untrusted clouds. Speciﬁcally 
SafetyNets composes a new  specialized IP protocol for the neural network’s activation layers with
Thaler’s IP protocol for matrix multiplication to achieve end-to-end veriﬁability  dramatically reducing
the bandwidth costs versus a naive solution that veriﬁes the execution of each layer of the neural
network separately.
SafetyNets applies to a certain class of neural networks that can be represented as arithmetic circuits
that perform computations over ﬁnite ﬁelds (i.e.  integers modulo a large prime p). Our implementa-
tion of SafetyNets addresses several practical challenges in this context  including the choice of the
prime p  its relationship to accuracy of the neural network  and to the veriﬁer and prover run-times.
Empirical evaluations on the MNIST digit recognition and TIMIT speech recognition tasks illustrate
that SafetyNets enables practical  low-cost veriﬁable outsourcing of deep neural network execution
without compromising classiﬁcation accuracy. Speciﬁcally  the client’s execution time is 8×-80×
lower than executing the network locally  the server’s overhead in generating proofs is less than 5% 
and the client/server exchange less than 8 KBytes of data during the IP protocol. SafetyNets’ security

1Note that the SafetyNets is not intended to and cannot catch any inherent mis-classiﬁcations due to the

model itself  only those that result from incorrect computations of the model by the server.

2

Client(verifier)Untrusted Server(prover)digit=4 5challenge 1response 1Random ChallengeVerifyExecute Neural NetworkCompute Responsechallenge nresponse n...Random ChallengeRejectCompute ResponseInput ImageRejectguarantees ensure that a client can detect any incorrect computations performed by a malicious
server with probability vanishingly close to 1. At the same time  SafetyNets achieves state-of-the-art
classiﬁcation accuracies of 99.4% and 75.22% on the MNIST and TIMIT datasets  respectively.

2 Background

In this section  we begin by reviewing necessary background on IP systems  and then describe the
restricted class of neural networks (those that can be represented as arithmetic circuits) that SafetyNets
handles.

2.1

Interactive Proof Systems

Existing IP systems proposed in literature [5  9  18  19] use  at their heart  a protocol referred to as
the sum-check protocol [13] that we describe here in some detail  and then discuss its applicability in
verifying general-purpose computations expressed as arithmetic circuits.

Sum-check Protocol Consider a d-degree n-variate polynomial g(x1  x2  . . .   xn)  where each
variable xi ∈ Fp (Fp is the set of all natural numbers between zero and p − 1  for a given prime p)
and g : Fn

p → Fp. The prover P seeks to prove the following claim:

y =

. . .

g(x1  x2  . . .   xn)

(1)

(cid:88)

(cid:88)

(cid:88)

x1∈{0 1}

x2∈{0 1}

xn∈{0 1}

that is  the sum of g evaluated at 2n points is y. P and V now engage in a sum-check protocol to
verify this claim. In the ﬁrst round of the protocol  P sends the following unidimensional polynomial
(2)

(cid:88)

(cid:88)

(cid:88)

g(x1  x2  . . .   xn)

h(x1) =

. . .

x2∈{0 1}

x3∈{0 1}

xn∈{0 1}

to V in the form of its coefﬁcients. V checks if h(0) + h(1) = y. If yes  it proceeds  otherwise
it rejects P’s claim. Next  V picks a random value q1 ∈ Fp and evaluates h(q1) which  based on
Equation 2  yields a new claim:

h(q1) =

. . .

g(q1  x2  . . .   xn).

(3)

(cid:88)

(cid:88)

(cid:88)

x2∈{0 1}

x3∈{0 1}

xn∈{0 1}

V now recursively calls the sum-check protocol to verify this new claim. By the ﬁnal round of the
sum-check protocol  P returns the value g(q1  q2  . . .   qn) and the V checks if this value is correct by
evaluating the polynomial by itself. If so  V accepts the original claim in Equation 1  otherwise it
rejects the claim.
[2] V rejects an incorrect claim by P with probability greater than (1 − ) where
Lemma 2.1.
 = nd

p is referred to as the soundness error.

IPs for Verifying Arithmetic Circuits
In their seminal work  Goldwasser et al. [9] demonstrated
how sum-check can be used to verify the execution of arithmetic circuits using an IP protocol now
referred to as GKR. An arithmetic circuit is a directed acyclic graph of computation over elements of
a ﬁnite ﬁeld Fp in which each node can perform either addition or multiplication operations (modulo
p). While we refer the reader to [9] for further details of GKR  one important aspect of the protocol
bears mention.
GKR organizes nodes of an arithmetic circuit into layers; starting with the circuit inputs  the outputs
of one layer feed the inputs of the next. The GKR proof protocol operates backwards from the circuit
outputs to its inputs. Speciﬁcally  GKR uses sum-check to reduce the prover’s assertion about the
circuit output into an assertion about the inputs of the output layer. This assertion is then reduced to
an assertion about the inputs of the penultimate layer  and so on. The protocol continues iteratively till
the veriﬁer is left with an assertion about the circuit inputs  which it checks on its own. The layered
nature of GKR’s prover aligns almost perfectly with the structure of a multi-layer neural network and
motivates the use of an IP system based on GKR for SafetyNets.

3

2.2 Neural Networks as Arithmetic Circuits

As mentioned before  SafetyNets applies to neural networks that can be expressed as arithmetic
circuits. This requirement places the following restrictions on the neural network layers.

Quadratic Activations The activation functions in SafetyNets must be polynomials with integer
coefﬁcients (or  more precisely  coefﬁcients in the ﬁeld Fp). The simplest of these is the element-wise
quadratic activation function whose output is simply the square of its input. Other commonly used
activation functions such as ReLU  sigmoid or softmax activations are precluded  except in the ﬁnal
output layer. Prior work has shown that neural networks with quadratic activations have the same
representation power as networks with threshold activations and can be efﬁciently trained [6  12].

Sum Pooling Pooling layers are commonly used to reduce the network size  to prevent overﬁtting
and provide translation invariance. SafetyNets uses sum pooling  wherein the output of the pooling
layer is the sum of activations in each local region. However  techniques such as max pooling [10]
and stochastic pooling [22] are not supported since max and divisions operations are not easily
represented as arithmetic circuits.
Finite Field Computations SafetyNets supports computations over elements of the ﬁeld Fp  that
is  integers in the range {− p−1
2 }. The inputs  weights and all intermediate values
computed in the network must lie in this range. Note that due to the use of quadratic activations
and sum pooling  the values in the network can become quite large. In practice  we will pick large
primes to support these large values. We note that this restriction applies to the inference phase only;
the network can be trained with ﬂoating point inputs and weights. The inputs and weights are then
re-scaled and quantized  as explained in Section 3.3  to ﬁnite ﬁeld elements.
We note that the restrictions above are shared by a recently proposed technique  CryptoNets [8]  that
seeks to perform neural network based inference on encrypted inputs so as to guarantee data privacy.
However  Cryptonets does not guarantee integrity and compared to SafetyNets  incurs high costs
for both the client and server (see Section 4.3 for a comparison). Conversely  SafetyNets is targeted
towards applications where integrity is critical  but does not provide privacy.

2   . . .   0  . . .   p−1

2.3 Mathematical Model

p

p

is:

p

  and biases bi−1 ∈ Fni
p .

An L layer neural network with the constraints discussed above can be modeled  without loss of
generality  as follows. The input to the network is x ∈ Fn0×b
  where n0 is the dimension of each
input and b is the batch size. Layer i ∈ [1  L] has ni output neurons2  and is speciﬁed using a weight
matrix wi−1 ∈ Fni×ni−1
The output of Layer i ∈ [1  L]  yi ∈ Fni×b
yi = σquad(wi−1.yi−1 + bi−11T ) ∀i ∈ [1  L − 1]; yL = σout(wL−1.yL−1 + bL−11T ) 
(4)
where σquad(.) is the quadratic activation function  σout(.) is the activation function of the output
layer  and 1 ∈ Fb
p is the vector of all ones. We will typically use softmax activations in the output
layer. We will also ﬁnd it convenient to introduce the variable zi ∈ Fni+1×b

deﬁned as

p

zi = wi.yi + bi1T ∀i ∈ [0  L − 1].

(5)
The model captures both fully connected and convolutional layers; in the latter case the weight matrix
is sparse. Further  without loss of generality  all successive linear transformations in a layer  for
instance sum pooling followed by convolutions  are represented using a single weight matrix.
With this model in place  the goal of SafetyNets is to enable the client to verify that yL was correctly
computed by the server. We note that as in prior work [19]  SafetyNets amortizes the prover and
veriﬁer costs over batches of inputs. If the server incorrectly computes the output corresponding to
any input in a batch  the veriﬁer rejects the entire batch of computations.

2The 0th layer is deﬁned to be input layer and thus y0 = x.

4

3 SafetyNets

We now describe the design and implementation of our end-to-end IP protocol for verifying execution
of deep networks. The SafetyNets protocol is a specialized form of the IP protocols developed by
Thaler [18] for verifying “regular" arithmetic circuits  that themselves specialize and reﬁne prior
work [5]. The starting point for the protocol is a polynomial representation of the network’s inputs
and parameters  referred to as a multilinear extension.
Multilinear Extensions Consider a matrix w ∈ Fn×n
. Each row and column of w can be
referenced using m = log2(n) bits  and consequently one can represent w as a function W :
{0  1}m × {0  1}m → Fp. That is  given Boolean vectors t  u ∈ {0  1}m  the function W (t  u)
returns the element of w at the row and column speciﬁed by Boolean vectors t and u  respectively.
p → Fp that has the following
A multi-linear extension of W is a polynomial function ˜W : Fm
two properties: (1) given vectors t  u ∈ Fm
p such that ˜W (t  u) = W (t  u) for all points on the unit
hyper-cube  that is  for all t  u ∈ {0  1}m; and (2) ˜W has degree 1 in each of its variables. In the
remainder of this discussion  we will use ˜X  ˜Yi and ˜Zi and ˜Wi to refer to multi-linear extensions of
x  yi  zi  and wi  respectively  for i ∈ [1  L]. We will also assume  for clarity of exposition  that
the biases  bi are zero for all layers. The supplementary draft describes how biases are incorporated.
Consistent with the IP literature  the description of our protocol refers to the client as the veriﬁer and
the server as the prover.

p × Fm

p

Protocol Overview The veriﬁer seeks to check the result yL provided by the prover corresponding
to input x. Note that yL is the output of the ﬁnal activation layer which  as discussed in Section 2.2 
is the only layer that does not use quadratic activations  and is hence not amenable to an IP.
Instead  in SafetyNets  the prover computes and sends zL−1 (the input of the ﬁnal activation layer) as a
result to the veriﬁer. zL−1 has the same dimensions as yL and therefore this reﬁnement has no impact
on the server to client bandwidth. Furthermore  the veriﬁer can easily compute yL = σout(zL−1)
locally.
Now  the veriﬁer needs to check whether the prover computed zL−1 correctly. As noted by Vu
et al. [19]  this check can be replaced by a check on whether the multilinear extension of zL−1 is
correctly computed at a randomly picked point in the ﬁeld  with minimal impact on the soundness
error. That is  the veriﬁer picks two vectors  qL−1 ∈ Flog(nL)
at random 
evaluates ˜ZL−1(qL−1  rL−1)  and checks whether it was correctly computed using a specialized
sum-check protocol for matrix multiplication due to Thaler [18] (described in Section 3.1).
Since zL−1 depends on wL−1 and yL−1  sum-check yields assertions on the values of
˜WL−1(qL−1  sL−1) and ˜YL−1(sL−1  rL−1)  where sL−1 ∈ Flog(nL−1)
is another random vector
picked by the veriﬁer during sum-check.
˜WL−1(qL−1  sL−1) is an assertion about the weight of the ﬁnal layer. This is checked by the veriﬁer
locally since the weights are known to both the prover and veriﬁer. Finally  the veriﬁer uses our
specialized sum-check protocol for activation layers (described in Section 3.2) to reduce the assertion
on ˜YL−1(sL−1  rL−1) to an assertion on ˜ZL−2(qL−2  sL−2). The protocol repeats till it reaches the
input layer and produces an assertion on ˜X(s0  r0)  the multilinear extension of the input x. The
veriﬁer checks this locally. If at any point in the protocol  the veriﬁer’s checks fail  it rejects the
prover’s computation. Next  we describe the sum-check protocols for matrix multiplication and
activation that SafetyNets uses.

and rL−1 ∈ Flog(b)

p

p

p

3.1 Sum-check for Matrix Multiplication

Since zi = wi.yi (recall we assumed zero biases for clarity)  we can check an assertion about the
multilinear extension of zi evaluated at randomly picked points qi and ri by expressing ˜Zi(qi  ri)
as [18]:

˜Zi(qi  ri) =

˜Wi(qi  j). ˜Yi(j  ri)

(6)

(cid:88)

j∈{0 1}log(ni)

5

Note that Equation 6 has the same form as the sum-check problem in Equation 1. Consequently the
sum-check protocol described in Section 2.1 can be used to verify this assertion. At the end of the
sum-check rounds  the veriﬁer will have assertions on ˜Wi which it checks locally  and ˜Yi which is
checked using the sum-check protocol for quadratic activations described in Section 3.2.
The prover run-time for running the sum-check protocol in layer i is O(ni(ni−1 + b))  the veriﬁer’s
run-time is O(nini−1) and the prover/veriﬁer exchange 4 log(ni) ﬁeld elements.

3.2 Sum-check for Quadratic Activation

In this step  we check an assertion about the output of quadratic activation layer i  ˜Yi(si  ri)  by
writing it in terms of the input of the activation layer as follows:

˜Yi(si  ri) =

˜I(si  j) ˜I(ri  k) ˜Z 2

i−1(j  k) 

(7)

j∈{0 1}log(ni) k∈{0 1}log(b)

(cid:88)

where ˜I(.  .) is the multilinear extension of the identity matrix. Equation 7 can also be veriﬁed using
the sum-check protocol  and yields an assertion about ˜Zi−1  i.e.  the inputs to the activation layer.
This assertion is in turn checked using the protocol described in Section 3.1.
The prover run-time for running the sum-check protocol in layer i is O(bni)  the veriﬁer’s run-
time is O(log(bni)) and the prover/veriﬁer exchange 5 log(bni) ﬁeld elements. This completes the
theoertical description of the SafetyNets specialized IP protocol.
Lemma 3.1. The SafetyNets veriﬁer rejects incorrect computations with probability greater than

(1 − ) where  = 3b(cid:80)L
In practice  with p = 261 − 1 the soundness error < 1
sizes.

is the soundness error.

i=0 ni
p

230 for practical network parameters and batch

Implementation

i ∈ Rni−1×ni and b(cid:48)

3.3
The fact that SafetyNets operates only on elements in a ﬁnite ﬁeld Fp during inference imposes a
practical challenge. That is  how do we convert ﬂoating point inputs and weights from training into
ﬁeld elements  and how do we select the size of the ﬁeld p?
i ∈ Rni be the ﬂoating point parameters obtained from training for
Let w(cid:48)
each layer i ∈ [1  L]. We convert the weights to integers by multiplying with a constant β > 1 and
rounding  i.e.  wi = (cid:98)βw(cid:48)
i(cid:101). We do the same for inputs with a scaling factor α  i.e.  x = (cid:98)αx(cid:48)(cid:101). Then 
to ensure that all values in the network scale isotropically  we must set bi = (cid:98)α2i−1
While larger α and β values imply lower quantization errors  they also result in large values in the
network  especially in the layers closer to the output. Similar empirical observations were made
by the CryptoNets work [8]. To ensure accuracy the values in the network must lie in the range
[− p−1
2 ]; this inﬂuences the choice of the prime p. On the other hand  we note that large primes
increase the veriﬁer and prover run-time because of the higher cost of performing modular additions
and multiplications.
As in prior works [5  18  19]  we restrict our choice of p to Mersenne primes since they afford efﬁcient
modular arithmetic implementations  and speciﬁcally to the primes p = 261 − 1 and p = 2127 − 1.
For a given p  we explore and different values of α and β and use the validation dataset to the pick the
ones that maximize accuracy while ensuring that the values in the network lie within [− p−1
2   p−1
2 ].

i(cid:101).
β(2i−1+1)b(cid:48)

2   p−1

4 Empirical Evaluation

In this section  we present empirical evidence to support our claim that SafetyNets enables low-cost
veriﬁable execution of deep neural networks on untrusted clouds without compromising classiﬁcation
accuracy.

6

(a) MNIST

(b) MNIST-Back-Rand

(c) TIMIT

Figure 2: Evolution of training and test error for the MNIST  MNIST-Back-Rand and TIMIT tasks.

4.1 Setup

Datasets We evaluated SafetyNets on three classiﬁcations tasks. (1) Handwritten digit recognition
on the MNIST dataset  using 50 000 training  10 000 validation and 10 000 test images. (2) A
more challenging version of digit recognition  MNIST-Back-Rand  an artiﬁcial dataset generated
by inserting a random background into MNIST image [1]. The dataset has 10 000 training  2 000
validation and 50 000 test images. ZCA whitening is applied to the raw dataset before training and
testing [4]. (3) Speech recognition on the TIMIT dataset  split into a training set with 462 speakers 
a validation set with 144 speakers and a testing set with 24 speakers. The raw audio samples are
pre-processed as described by [3]. Each example includes its preceding and succeeding 7 frames 
resulting in a 1845-dimensional input in total. During testing  all labels are mapped to 39 classes [11]
for evaluation.

Neural Networks For the two MNIST tasks  we used a convolutional neural network same as [23]
with 2 convolutional layers with 5 × 5 ﬁlters  a stride of 1 and a mapcount of 16 and 32 for the
ﬁrst and second layer respectively. Each convolutional layer is followed by quadratic activations
and 2 × 2 sum pooling with a stride of 2. The fully connected layer uses softmax activation. We
refer to this network as CNN-2-Quad. For TIMIT  we use a four layer network described by [3]
with 3 hidden  fully connected layers with 2000 neurons and quadratic activations. The output layer
is fully connected with 183 output neurons and softmax activation. We refer to this network as
FcNN-3-Quad. Since quadratic activations are not commonly used  we compare the performance
of CNN-2-Quad and FcNN-3-Quad with baseline versions in which the quadratic activations are
replaced by ReLUs. The baseline networks are CNN-2-ReLU and FcNN-3-ReLU.
The hyper-parameters for training are selected based on the validation datasets. The Adam Optimizer
is used for CNNs with learning rate 0.001  exponential decay and dropout probability 0.75. The
AdaGrad optimizer is used for FcNNs with a learning rate of 0.01 and dropout probability 0.5. We
found that norm gradient clipping was required for training the CNN-2-Quad and FcNN-3-Quad
networks  since the gradient values for quadratic activations can become large.
Our implementation of SafetyNets uses Thaler’s code for the IP protocol for matrix multiplication
[18] and our own implementation of the IP for quadratic activations. We use an Intel Core i7-4600U
CPU running at 2.10 GHz for benchmarking.

4.2 Classiﬁcation Accuracy of SafetyNets

SafetyNets places certain restrictions on the activation function (quadratic) and requires weights
and inputs to be integers (in ﬁeld Fp). We begin by analyzing how (and if) these restrictions impact
classiﬁcation accuracy/error. Figure 2 compares training and test error of CNN-2-Quad/FcNN-3-Quad
versus CNN-2-ReLU/FcNN-3-ReLU. For all three tasks  the networks with quadratic activations are
competitive with networks that use ReLU activations. Further  we observe that the networks with
quadratic activations appear to converge faster during training  possibly because their gradients are
larger despite gradient clipping.
Next  we used the scaling and rounding strategy proposed in Section 3.3 to convert weights and
inputs to integers. Table 1 shows the impact of scaling factors α and β on the classiﬁcation error and
maximum values observed in the network during inference for MNIST-Back-Rand. The validation

7

 0 0.5 1 1.5 2 2.5 200 400 600 800 1000 1200Error (%)Time (s)CNN-2-ReLU TrainCNN-2-ReLU TestCNN-2-Quad TrainCNN-2-Quad Test 0 2 4 6 8 10 0 200 400 600 800 1000 1200Error (%)Time (s)CNN-2-ReLU TrainCNN-2-ReLU TestCNN-2-Quad TrainCNN-2-Quad Test 10 20 30 40 50 60 70 80 10000 20000 30000 40000Error (%)Time (s)FcNN-3-ReLU TrainFcNN-3-ReLU TestFcNN-3-Quad TrainFcNN-3-Quad TestTable 1: Validation error and maximum value observed in the network for MNIST-Rand-Back and
different values of scaling parameters  α and β. Shown in bold red font are values of α and β that are
infeasible because the maximum value exceeds that allowed by prime p = 261 − 1.
Err
0.04
0.037
0.035
0.036
0.036

α = 32
Max
6.6 × 1014
1.0 × 1016
1.6 × 1017
2.6 × 1018
4.2 × 1019

α = 64
Max
8.8 × 1016
1.3 × 1018
2.1 × 1019
3.5 × 1020
5.6 × 1021

α = 16
Max
5.5 × 1012
8.3 × 1013
1.3 × 1015
2.1 × 1016
3.4 × 1017

Err
0.042
0.039
0.036
0.038
0.038

α = 8
Max
4.0 × 1010
6.9 × 1011
1.1 × 1013
1.7 × 1014
2.8 × 1015

Err
0.073
0.072
0.072
0.073
0.073

Err
0.039
0.038
0.037
0.037
0.037

β
4
8
16
32
64

α = 4
Max
4.0 × 108
6.1 × 109
9.4 × 1010
1.5 × 1012
2.5 × 1013

Err
0.188
0.194
0.188
0.186
0.185

error drops as α and β are increased. On the other hand  for p = 261 − 1  the largest value allowed is
1.35 × 1018; this rules out α and β greater than 64  as shown in the table. For MNIST-Back-Rand 
we pick α = β = 16 based on validation data  and obtain a test error of 4.67%. Following a similar
methodology  we obtain a test error of 0.63% for MNIST (p = 261 − 1) and 25.7% for TIMIT
(p = 2127 − 1). We note that SafetyNets does not support techniques such as Maxout [10] that have
demonstrated lower error on MNIST (0.45%). Ba et al. [3] report an error of 18.5% for TIMIT using
an ensemble of nine deep neural networks  which SafetyNets might be able to support by verifying
each network individually and performing ensemble averaging at the client-side.
4.3 Veriﬁer and Prover Run-times

The relevant performance metrics for SafetyNets are (1)
the client’s (or veriﬁer’s) run-time  (2) the server’s run-
time which includes baseline time to execute the neural
network and overhead of generating proofs  and (3) the
bandwidth required by the IP protocol. Ideally  these quan-
tities should be small  and importantly  the client’s run-
time should be smaller than the case in which it executes
the network by itself. Figure 3 plots run-time data over
input batch sizes ranging from 256 to 2048 for FcNN-
Quad-3.
For FcNN-Quad-3  the client’s time for verifying proofs
is 8× to 82× faster than the baseline in which it executes
Figure 3: Run-time of veriﬁer  prover
FcNN-Quad-3 itself  and decreases with batch size. The
and baseline execution time for the arith-
increase in the server’s execution time due to the over-
metic circuit representation of FcNN-
head of generating proofs is only 5% over the baseline
Quad-3 versus input batch size.
unveriﬁed execution of FcNN-Quad-3. The prover and
veriﬁer exchange less than 8 KBytes of data during the IP protocol for a batch size of 2048  which is
negligible (less than 2%) compared to the bandwidth required to communicate inputs and outputs
back and forth. In all settings  the soundness error   i.e.  the chance that the veriﬁer fails to detect
incorrect computations by the server is less than 1
230   a negligible value. We note SafetyNets has
signiﬁcantly lower bandwidth costs compared to an approach that separately veriﬁes the execution of
each layer using only the IP protocol for matrix multiplication.
A closely related technique  CryptoNets [8]  uses homomorphic encryption to provide privacy  but not
integrity  for neural networks executing in the cloud. Since SafetyNets and CryptoNets target different
security goals a direct comparison is not entirely meaningful. However  from the data presented in
the CryptoNets paper  we note that the client’s run-time for MNIST using a CNN similar to ours and
an input batch size b = 4096 is about 600 seconds  primarily because of the high cost of encryptions.
For the same batch size  the client-side run-time of SafetyNets is less than 10 seconds. Recent work
has also looked at how neural networks can be trained in the cloud without compromising the user’s
training data [14]  but the proposed techniques do not guarantee integrity. We expect that SafetyNets
can be extended to address the veriﬁable neural network training problem as well.
5 Conclusion
In this paper  we have presented SafetyNets  a new framework that allows a client to provably verify
the correctness of deep neural network based inference running on an untrusted clouds. Building
upon the rich literature on interactive proof systems for verifying general-purpose and specialized
computations  we designed and implemented a specialized IP protocol tailored for a certain class

8

 0.1 1 10 100 10002829210211212Running Time (s)Input Batch SizeFcNN-Quad-3 Exe TimeAdditional Prover TimeVerifier Timeof deep neural networks  i.e.  those that can be represented as arithmetic circuits. We showed that
placing these restrictions did not impact the accuracy of the networks on real-world classiﬁcation
tasks like digit and speech recognition  while enabling a client to veriﬁably outsource inference
to the cloud at low-cost. For our future work  we will apply SafetyNets to deeper networks and
extend it to address both integrity and privacy. There are VC techniques [17] that guarantee both  but
typically come at higher costs. Further  building on prior work on the use of IPs to build veriﬁable
hardware [20]  we intend to deploy the SafetyNets protocol in the design of a veriﬁable hardware
accelerator for neural network inference.

References
[1] Variations on the MNIST digits. http://www.iro.umontreal.ca/~lisa/twiki/bin/

view.cgi/Public/MnistVariations.

[2] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University

Press  2009.

[3] J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in Neural Information

Processing Systems  pages 2654–2662  2014.

[4] A. Coates  A. Ng  and H. Lee. An analysis of single-layer networks in unsupervised feature
learning. In International Conference on Artiﬁcial Intelligence and Statistics  pages 215–223 
2011.

[5] G. Cormode  J. Thaler  and K. Yi. Verifying computations with streaming interactive proofs.

Proceedings of the Very Large Database Endowment  pages 25–36  2011.

[6] A. Gautier  Q. N. Nguyen  and M. Hein. Globally optimal training of generalized polynomial
neural networks with nonlinear spectral methods. In Advances in Neural Information Processing
Systems  pages 1687–1695  2016.

[7] R. Gennaro  C. Gentry  and B. Parno. Non-interactive veriﬁable computing: Outsourcing

computation to untrusted workers. Annual Cryptology Conference  pages 465–482  2010.

[8] R. Gilad-Bachrach  N. Dowlin  K. Laine  K. Lauter  M. Naehrig  and J. Wernsing. Cryptonets:
Applying neural networks to encrypted data with high throughput and accuracy. In International
Conference on Machine Learning  pages 201–210  2016.

[9] S. Goldwasser  Y. T. Kalai  and G. N. Rothblum. Delegating computation: interactive proofs for

muggles. Symposium on Theory of Computing  pages 113–122  2008.

[10] I. J. Goodfellow  D. Warde-Farley  M. Mirza  A. Courville  and Y. Bengio. Maxout networks.

arXiv preprint arXiv:1302.4389  2013.

[11] K. Lee and H. Hon. Speaker-independent phone recognition using hidden markov models.

IEEE Transactions on Acoustics  Speech  and Signal Processing  pages 1641–1648  1989.

[12] R. Livni  S. Shalev-Shwartz  and O. Shamir. On the computational efﬁciency of training neural

networks. In Advances in Neural Information Processing Systems  pages 855–863  2014.

[13] C. Lund  L. Fortnow  H. Karloff  and N. Nisan. Algebraic methods for interactive proof systems.

Journal of the ACM  pages 859–868  1992.

[14] P. Mohassel and Y. Zhang. Secureml: A system for scalable privacy-preserving machine

learning. IACR Cryptology ePrint Archive  2017.

[15] F. Monrose  P. Wyckoff  and A. D. Rubin. Distributed execution with remote audit. In Network

and Distributed System Security Symposium  pages 3–5  1999.

[16] N. Papernot  P. McDaniel  A. Sinha  and M. Wellman. Towards the science of security and

privacy in machine learning. arXiv preprint arXiv:1611.03814  2016.

[17] B. Parno  J. Howell  C. Gentry  and M. Raykova. Pinocchio: Nearly practical veriﬁable

computation. In Symposium on Security and Privacy  pages 238–252  2013.

9

[18] J. Thaler. Time-optimal interactive proofs for circuit evaluation. In International Cryptology

Conference  pages 71–89  2013.

[19] V. Vu  S. Setty  A. J. Blumberg  and M. Walﬁsh. A hybrid architecture for interactive veriﬁable

computation. In Symposium on Security and Privacy  pages 223–237  2013.

[20] R. S. Wahby  M. Howald  S. Garg  A. Shelat  and M. Walﬁsh. Veriﬁable asics. In Symposium

on Security and Privacy  pages 759–778  2016.

[21] M. Walﬁsh and A. J. Blumberg. Verifying computations without reexecuting them. Communi-

cations of the ACM  pages 74–84  2015.

[22] M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural

networks. arXiv preprint arXiv:1301.3557  2013.

[23] Y. Zhang  P. Liang  and M. J. Wainwright. Convexiﬁed convolutional neural networks. arXiv

preprint arXiv:1609.01000  2016.

Proof of Lemma 3.1

Lemma 3.1 The SafetyNets veriﬁer rejects incorrect computations with probability greater than

(1 − ) where  = 3b(cid:80)L

i=0 ni
p

is the soundness error.

Proof. Verifying a multi-linear extension of the output sampled at a random point  instead of each
value adds a soundness error of  = bnL
p . Each instance of the sum-check protocol adds to the
soundness error [19]. The IP protocol for matrix multiplication adds a soundness error of  = 2ni−1
in Layer i [18]. Finally  the IP protocol for quadratic activations adds a soundness error of  = 3bni
p
. The

in Layer i [18]. Summing together we get a total soundness error of 2(cid:80)L−1

i=0 ni+3(cid:80)L−1

i=1 bni+bnL

p

p

ﬁnal result is an upper bound on this value.

Handling Bias Variables

We assumed that the bias variables were zero  allowing us to write bmzi = wi.yi while it should be
bmzi = wi.yi + bi1T . Let z(cid:48)
i = wi.yi We seek to convert an assertion on ˜Zi(qi  ri) to an assertion
on ˜Z(cid:48)

i. We can do so by noting that:

˜Zi(qi  ri) =

˜I(j  qi)( ˜Z(cid:48)

i(j  ri) + ˜Bi(j))

(8)

(cid:88)

j∈{0 1}log(ni)

which can be reduced to sum-check and thus yields an assertion on ˜Bi which the veriﬁer checks
locally and ˜Z(cid:48)

i  which is passed to the IP protocol for matrix multiplication.

10

,Zahra Ghodsi
Tianyu Gu
Siddharth Garg
Joshua Lee
Prasanna Sattigeri
Gregory Wornell