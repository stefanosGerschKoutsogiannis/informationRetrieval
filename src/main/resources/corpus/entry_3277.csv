2016,Long-term Causal Effects via Behavioral Game Theory,Planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new policy. % One critical shortcoming of classical experimental methods  however  is that they typically do not take into account the dynamic nature of response to policy changes. For instance  in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue  agents may adapt their bidding in response to the experimental pricing changes. Thus  causal effects of the new pricing policy after such adaptation period  the {\em long-term causal effects}  are not captured by the classical methodology even though they clearly are more indicative of the value of the new policy. %  Here  we formalize a framework to define and estimate long-term causal effects of   policy changes in multiagent economies.  Central to our approach is behavioral game theory  which we leverage   to formulate the ignorability assumptions that are necessary for causal inference.  Under such assumptions we estimate long-term causal effects through a latent space approach  where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time.,Long-term causal effects via behavioral game theory

Panagiotis (Panos) Toulis

David C. Parkes

Econometrics & Statistics  Booth School

Department of Computer Science

University of Chicago
Chicago  IL  60637

Harvard University

Cambridge  MA  02138

panos.toulis@chicagobooth.edu

parkes@eecs.harvard.edu

Abstract

Planned experiments are the gold standard in reliably comparing the causal effect
of switching from a baseline policy to a new policy. One critical shortcoming of
classical experimental methods  however  is that they typically do not take into
account the dynamic nature of response to policy changes. For instance  in an
experiment where we seek to understand the effects of a new ad pricing policy on
auction revenue  agents may adapt their bidding in response to the experimental
pricing changes. Thus  causal effects of the new pricing policy after such adapta-
tion period  the long-term causal effects  are not captured by the classical method-
ology even though they clearly are more indicative of the value of the new policy.
Here  we formalize a framework to deﬁne and estimate long-term causal effects
of policy changes in multiagent economies. Central to our approach is behavioral
game theory  which we leverage to formulate the ignorability assumptions that are
necessary for causal inference. Under such assumptions we estimate long-term
causal effects through a latent space approach  where a behavioral model of how
agents act conditional on their latent behaviors is combined with a temporal model
of how behaviors evolve over time.

1

Introduction

A multiagent economy is comprised of agents interacting under speciﬁc economic rules. A common
problem of interest is to experimentally evaluate changes to such rules  also known as treatments  on
an objective of interest. For example  an online ad auction platform is a multiagent economy  where
one problem is to estimate the effect of raising the reserve price on the platform’s revenue. Assessing
causality of such effects is a challenging problem because there is a conceptual discrepancy between
what needs to be estimated and what is available in the data  as illustrated in Figure 1.
What needs to be estimated is the causal effect of a policy change  which is deﬁned as the difference
between the objective value when the economy is treated  i.e.  when all agents interact under the
new rules  relative to when the same economy is in control  i.e.  when all agents interact under the
baseline rules. Such deﬁnition of causal effects is logically necessitated from the designer’s task 
which is to select either the treatment or the control policy based on their estimated revenues  and
then apply such policy to all agents in the economy. The long-term causal effect is the causal effect
deﬁned after the system has stabilized  and is more representative of the value of policy changes
in dynamical systems. Thus  in Figure 1 the long-term causal effect is the difference between the
objective values at the top and bottom endpoints  marked as the “targets of inference”.
What is available in the experimental data  however  typically comes from designs such as the so-
called A/B test  where we randomly assign some agents to the treated economy (new rules B) and
the others to the control economy (baseline rules A)  and then compare the outcomes. In Figure 1
the experimental data are depicted as the solid time-series in the middle of the plot  marked as the
“observed data”.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: The two inferential tasks for causal inference in multiagent economies. First  infer agent actions
across treatment assignments (y-axis)  particularly  the assignment where all agents are in the treated economy
(top assignment  Z = 1)  and the assignment where all agents are in the control economy (bottom assignment 
Z = 0). Second  infer across time  from t0 (last observation time) to long-term T . What we seek in order to
evaluate the causal effect of the new treatment is the difference between the objectives (e.g.  revenue) at the two
inferential target endpoints.

Therefore the challenge in estimating long-term causal effects is that we generally need to perform
two inferential tasks simultaneously  namely 

(i) infer outcomes across possible experimental policy assignments (y-axis in Figure 1)  and
(ii) infer long-term outcomes from short-term experimental data (x-axis in Figure 1).

The ﬁrst task is commonly known as the “fundamental problem of causal inference” [14  19] be-
cause it underscores the impossibility of observing in the same experiment the outcomes for both
policy assignments that deﬁne the causal effect; i.e.  that we cannot observe in the same experiment
both the outcomes when all agents are treated and the outcomes when all agents are in control  the
assignments of which are denoted by Z = 1 and Z = 0  respectively  in Figure 1. In fact the
role of experimental design  as conceived by R.A. Fisher [8]  is exactly to quantify the uncertainty
about such causal effects that cannot be observed due to the aforementioned fundamental problem 
by using standard errors that can be observed in a carefully designed experiment.
The second task  however  is unique to causal inference in dynamical systems  such as the multiagent
economies that we study in this paper  and has received limited attention so far. Here  we argue that
it is crucial to study long-term causal effects  i.e.  effects measured after the system has stabilized 
because such effects are more representative of the value of policy changes. If our analysis focused
only on the observed data part depicted in Figure 1  then policy evaluation would reﬂect transient
effects that might differ substantially from the long-term effects. For instance  raising the reserve
price in an auction might increase revenue in the short-term but as agents adapt their bids  or switch
to another platform altogether  the long-term effect could be a net decrease in revenue [13].

1.1 Related work and our contributions

There have been several important projects related to causal inference in multiagent economies. For
instance  Ostrovsky and Schwartz [16] evaluated the effects of an increase in the reserve price of
Yahoo! ad auctions on revenue. Auctions were randomly assigned to an increased reserve price
treatment  and the effect was estimated using difference-in-differences (DID)  which is a popular
econometric method [6  7  16]. In relation to Figure 1  DID extrapolates across assignments (y-axis)
and across time (x-axis) by making a strong additivity assumption [1  3  Section 5.2]  speciﬁcally 
by assuming that the dependence of revenue on reserve price and time is additive.
In a structural approach  Athey et.al. [4] studied the effects of auction format (ascending versus
sealed bid) on competition for timber tracts. In relation to Figure 1  their approach extrapolates

2

across assignments by assuming that agent individual valuations for tracts are independent of the
treatment assignment  and extrapolates across time by assuming that the observed agent bids are
already in equilibrium. Similar approaches are followed in econometrics for estimation of general
equilibrium effects [11  12].
In a causal graph approach [17] Bottou et.al. [5] studied effects of changes in the algorithm that
scores Bing ads on the ad platform’s revenue.
In relation to Figure 1  their approach is non-
experimental and extrapolates across assignments and across time by assuming a directed acyclic
graph (DAG) as the correct data model  which is also assumed to be stable with respect to treatment
assignment  and by estimating counterfactuals through the ﬁtted model.
Our work is different from prior work because it takes into account the short-term aspect of experi-
mental data to evaluate long-term causal effects  which is the key conceptual and practical challenge
that arises in empirical applications. In contrast  classical econometric methods  such as DID  as-
sume strong linear trends from short-term to long-term  whereas structural approaches typically
assume that the experimental data are already long-term as they are observed in equilibrium. We
refer the reader to Sections 2 and 3 of the supplement for more detailed comparisons.
In summary  our key contribution is that we develop a formal framework that (i) articulates the
distinction between short-term and long-term causal effects  (ii) leverages behavioral game-theoretic
models for causal analysis of multiagent economies  and (iiii) explicates theory that enables valid
inference of long-term causal effects.

2 Deﬁnitions
Consider a set of agents I and a set of actions A  indexed by i and a  respectively. The experiment
designer wants to run an experiment to evaluate a new policy against the baseline policy relative to
an objective. In the experiment each agent is assigned to one policy  and the experimenter observes
how agents act over time. Formally  let Z = (Zi) be the |I| × 1 assignment vector where Zi = 1
denotes that agent i is assigned to the new policy  and Zi = 0 denotes that i is assigned to the
baseline policy; as a shorthand  Z = 1 denotes that all agents are assigned to the new policy  and
Z = 0 denotes that all agents are assigned to the baseline policy  where 1  0 generally denote an
appropriately-sized vector of ones and zeroes  respectively. In the simplest case  the experiment is

an A/B test  where Z is uniformly random on {0  1}|I| subject to(cid:80)

i Zi = |I|/2.

After the initial assignment Z agents play actions at discrete time points from t = 0 to t = t0. Let
Ai(t; Z) ∈ A be the random variable that denotes the action of agent i at time t under assignment
Z. The population action αj(t; Z) ∈ ∆|A|  where ∆p denotes the p-dimensional simplex  is the fre-
quency of actions at time t under assignment Z of agents that were assigned to game j; for example 
assuming two actions A = {a1  a2}  then α1(0; Z) = [0.2  0.8] denotes that  under assignment Z 
20% of agents assigned to the new policy play action a1 at t = 0  while the rest play a2. We assume
that the objective value for the experimenter depends on the population action  in a similar way that 
say  auction revenue depends on agents’ aggregate bidding. The objective value in policy j at time
t under assignment Z is denoted by R(αj(t; Z))  where R : ∆|A| → R. For instance  suppose in
the previous example that a1 and a2 produce revenue $10 and −$2  respectively  each time they are
played  then R is linear and R([.2  .8]) = 0.2 · $10 − 0.8 · $2 = $0.4.
Deﬁnition 1 The average causal effect on objective R at time t of the new policy relative to the
baseline is denoted by CE(t) and is deﬁned as

CE(t) = E (R(α1(t; 1)) − R(α0(t; 0))) .

(1)

Suppose that (t0  T ] is the time interval required for the economy to adapt to the experimental con-
ditions. The exact deﬁnition of T is important but we defer this discussion for Section 3.1. The
designer concludes that the new policy is better than the baseline if CE(T ) > 0. Thus  CE(T )
is the long-term average causal effect and is a function of two objective values  R(α1(T ; 1)) and
R(α0(T ; 0))  which correspond to the two inferential target endpoints in Figure 1. Neither value is
observed in the experiment because agents are randomly split between policies  and their actions are
observed only for the short-term period [0  t0]. Thus we need to (i) extrapolate across assignments
by pivoting from the observed assignment to the counterfactuals Z = 1 and Z = 0; (ii) extrap-
olate across time from the short-term data [0  t0] to the long-term t = T . We perform these two
extrapolations based on a latent space approach  which is described next.

3

2.1 Behavioral and temporal models

We assume a latent behavioral model of how agents select actions  inspired by models from be-
havioral game theory. The behavioral model is used to predict agent actions conditional on agent
behaviors  and is combined with a temporal model to predict behaviors in the long-term. The two
models are ultimately used to estimate agent actions in the long-term  and thus estimate long-term
causal effects. As the choice of the latent space is not unique  in Section 3.1 we discuss why we
chose to use behavioral models from game theory.
Let Bi(t; Z) denote the behavior that agent i adopts at time t under experimental assignment Z. The
following assumption puts a constraints on the space of possible behaviors that agents can adopt 
which will simplify the subsequent analysis.
Assumption 1 (Finite set of possible behaviors) There is a ﬁxed and ﬁnite set of behaviors B such
that for every time t  assignment Z and agent i  it holds that Bi(t; Z) ∈ B; i.e.  every agent can only
adopt a behavior from B.
Deﬁnition 2 (Behavioral model) The behavioral model for policy j deﬁned by set B of behaviors
is the collection of probabilities

P (Ai(t; Z) = a|Bi(t; Z) = b  Gj) 

(2)
for every action a ∈ A and every behavior b ∈ B  where Gj denotes the characteristics of policy j.
As an example  a non-sophisticated behavior b0 could imply that P (Ai(t; Z) = a|b0  Gj) = 1/|A| 
i.e.  that the agent adopting b0 simply plays actions at random. Conditioning on policy j in Def-
inition 2 allows an agent to choose its actions based on expected payoffs  which depend on the
policy characteristics. For instance  in the application of Section 4 we consider a behavioral model
where an agent picks actions in a two-person game according to expected payoffs calculated from
the game-speciﬁc payoff matrix—in that case Gj is simply the payoff matrix of game j.
The population behavior βj(t; Z) ∈ ∆|B| denotes the frequency at time t under assignment Z of
the adopted behaviors of agents assigned to policy j. Let Ft denote the entire history of population
behaviors in the experiment up to time t. A temporal model of behaviors is deﬁned as follows.

Deﬁnition 3 (Temporal model) For an experimental assignment Z a temporal model for policy j
is a collection of parameters φj(Z)  ψj(Z)  and densities (π  f )  such that for all t 

βj(0; Z) ∼ π(·; φj(Z)) 

βj(t; Z)| Ft−1  Gj ∼ f (·|ψj(Z) Ft−1).

(3)

A temporal model deﬁnes the distribution of population behavior as a time-series with a Markovian
structure. As deﬁned  the temporal model imposes the restriction that the prior π of population
behavior at t = 0 and the density f of behavioral evolution are both independent of treatment
assignment Z.
In other words  regardless of how agents are assigned to games  the population
behavior in the game will evolve according to a ﬁxed model described by f and π. The model
parameters φ  ψ may still depend on the treatment assignment Z.

3 Estimation of long-term causal effects

Here we develop the assumptions that are necessary for inference of long-term causal effects.

Assumption 2 (Stability of initial behaviors) Let ρZ =(cid:80)

i∈I Zi/|I| be the proportion of agents

assigned to the new policy under assignment Z. Then  for every possible assignment Z 

ρZβ1(0; Z) + (1 − ρZ)β0(0; Z) = β(0) 

(4)

where β(0) is a ﬁxed population behavior invariant to Z.

Assumption 3 (Behavioral ignorability) The assignment is independent of population behavior at
time t  conditional on policy and behavioral history up to t; i.e.  for every t > 0 and policy j 

Z|= βj(t; Z) | Ft−1  Gj.

4

Remarks. Assumption 2 implies that the agents do not anticipate the assignment Z as they “have
made up their minds” to adopt a population behavior β(0) before the experiment. Quantities such as
that in Eq. (4) are crucial in causal inference because they can be used as a pivot for extrapolation
across assignments. Assumption 3 states that the treatment assignment does not add information
about the population behavior at time t  if we already know the full behavioral history of up to t 
and the policy which agents are assigned to; hence  the treatment assignment is conditionally ignor-
able. This ignorability assumption precludes  for instance  an agent adopting a different behavior
depending on whether it was assigned with friends or foes in the experiment.
Algorithm 1 is the main methodological contribution of this paper. It is a Bayesian procedure as it
puts priors on parameters φ  ψ of the temporal model  and then marginalizes these parameters out.

Algorithm 1 Estimation of long-term causal effects
Input: Z  T A B  G1  G0 D1 = {a1(t; Z) : t = 0  . . .   t0} D0 = {a0(t; Z) : t = 0  . . .   t0}.
Output: Estimate of long-term causal effect CE(T ) in Eq. (1).
1: By Assumption 3  deﬁne φj ≡ φj(Z)  ψj ≡ ψj(Z).
2: Set µ1 ← 0 and µ0 ← 0  both of size |A|; set ν0 = ν1 = 0.
3: for iter = 1  2  . . . do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end for

Set βj(0; j1) = β(0).
Sample Bj = {βj(t; j1) : t = 0  . . .   T} given ψj and βj(0  j1).
Sample αj(T ; j1) conditional on βj(T ; j1).
Set µj ← µj + P (Dj|Bj  Gj) · R(αj(T ; j1)).
Set νj ← νj + P (Dj|Bj  Gj).

For j = 0  1  sample φj  ψj from prior  and sample βj(0; Z) conditional on φj.
Calculate β(0) = ρZβ1(0; Z) + (1 − ρZ)β0(0; Z).
for j = 0  1 do

14: Return estimate (cid:99)CE(T ) = µ1/ν1 − µ0/ν0.

# behavioral model

end for

# temporal model

Theorem 1 (Estimation of long-term causal effects) Suppose that behaviors evolve according to
a known temporal model  and actions are distributed conditionally on behaviors according to a
known behavioral model. Suppose that Assumptions 1  2 and 3 hold for such models. Then  for
every policy j ∈ {0  1} as the iterations of Algorithm 1 increase  µj/νj → E (R(αj(T ; j1))|Dj) .

The output (cid:99)CE(T ) of Algorithm 1 asymptotically estimates the long-term causal effect  i.e. 
Remarks. Theorem 1 shows that (cid:99)CE(T ) consistently estimates the long-term causal effect in Eq. (1).

E((cid:99)CE(T )) = E (R(α1(T ; 1)) − R(α0(T ; 0))) ≡ CE(T ).

We note that it is also possible to derive the variance of this estimator with respect to the random-
ization distribution of assignment Z. To do so we ﬁrst create a set of assignments Z by repeatedly
sampling Z according to the experimental design. Then we adapt Algorithm 1 so that (i) Step 4 is
removed; (ii) in Step 5  β(0) is sampled from its posterior distribution conditional on observed data 
which can be obtained from the original Algorithm 1. The empirical variance of the outputs over

Z from the adapted algorithm estimates the variance of the output (cid:99)CE(T ) of the original algorithm.

We leave the full characterization of this variance estimation procedure for future work.

3.1 Discussion

Methodologically  our approach is aligned with the idea that for long-term causal effects we need a
model for outcomes that leverages structural information pertaining to how outcomes are generated
and how they evolve. In our application such structural information is the microeconomic infor-
mation that dictates what agent behaviors are successful in a given policy and how these behaviors
evolve over time.
In particular  Step 1 in the algorithm relies on Assumptions 2 and 3 to infer that model parameters 
φj  ψj are stable with respect to treatment assignment. Step 5 of the algorithm is the key estimation
pivot  which uses Assumption 2 to extrapolate from the experimental assignment Z to the coun-
terfactual assignments Z = 1 and Z = 0  as required in our problem. Having pivoted to such

5

counterfactual assignment  it is then possible to use the temporal model parameters ψj  which are
unaffected by the pivot under Assumption 3  to sample population behaviors up to long-term T   and
subsequently sample agent actions at T (Steps 8 and 9).
Thus  a lot of burden is placed on the behavioral game-theoretic model to predict agent actions 
and the accuracy of such models is still not settled [10]. However  it does not seem necessary
that such prediction is completely accurate  but rather that the behavioral models can pull relevant
information from data that would otherwise be inaccessible without game theory  thereby improving
over classical methods. A formal assessment of such improvement  e.g.  using information theory 
is open for future work. An empirical assessment can be supported by the extensive literature in
behavioral game theory [20  15]  which has been successful in predicting human actions in real-
world experiments [22].
Another limitation of our approach is Assumption 1  which posits that there is a ﬁnite set of pre-
deﬁned behaviors. A nonparametric approach where behaviors are estimated on-the-ﬂy might do
better. In addition  the long-term horizon  T   also needs to be deﬁned a priori. We should be careful
how T interferes with the temporal model since such a model implies a time T (cid:48) at which population
behavior reaches stationarity. Thus if T (cid:48) ≤ T we implicitly assume that the long-term causal effect
of interest pertains to a stationary regime (e.g.  Nash equilibrium)  but if T (cid:48) > T we assume that the
effect pertains to a transient regime  and therefore the policy evaluation might be misguided.

4 Application: Long-term causal effects from a behavioral experiment

In this section  we apply our methodology to experimental data from Rapoport and Boebel [18] 
as reported by McKelvey and Palfrey [15]. The experiment consisted of a series of zero-sum two-
agent games  and aimed at examining the hypothesis that human players play according to minimax
solutions of the game  the so-called minimax hypothesis initially suggested by von Neumann and
Morgenstern [21]. Here we repurpose the data in a slightly artiﬁcial way  including how we construct
the designer’s objective. This enables a suitable demonstration of our approach.
Each game in the experiment was a simultaneous-move game with ﬁve discrete actions for the row
player and ﬁve actions for the column player. The structure of the payoff matrix  given in the
supplement in Table 1  is parametrized by two values  namely W and L; the experiment used two
different versions of payoff matrices  corresponding to payments by the row agent to the column
agent when the row agent won (W )  or lost (L): modulo a scaling factor  Rapoport and Boebel [18]
used (W  L) = ($10 −$6) for game 0 and (W  L) = ($15 −$1) for game 1.
Forty agents  I = {1  2  . . .   40}  were randomized to one game design (20 agents per game)  and
each agent played once as row and once as column  matched against two different agents. Every
match-up between a pair of agents lasted for two periods of 60 rounds  with each round consisting
of a selection of an action from each agent and a payment. Thus  each agent played for four periods
and 240 rounds in total. If Z is the entire assignment vector of length 40  Zi = 1 means that agent
i was assigned to game 1 with payoff matrix (W  L) = ($15 −$1) and Zi = 0 means that i was
assigned to game 0 with payoff matrix (W  L) = ($10 −$6).
In adapting the data  we take advantage of the randomization in the experiment  and ask a question
in regard to long-term causal effects. In particular  assuming that agents pay a fee for each action
taken  which accounts for the revenue of the game  we ask the following question:
”What is the long-term causal effect on revenue if we switch from payoffs (W  L) = ($10 −$6) of
game 0 to payoffs (W  L) = ($15 −$1) of game 1?”.
The games induced by the two aforementioned payoff matrices represent the two different policies
we wish to compare. To evaluate our method  we consider the last period as long-term  and hold out
data from this period. We deﬁne the causal estimand in Eq. (1) as
(α1(T ; 1) − α0(T ; 0)) 

(cid:124)
CE = c

(5)

where T = 3 and c is a vector of coefﬁcients. The interpretation is that  given an element ca of c  the
agent playing action a is assumed to pay a constant fee ca. To check the robustness of our method
we test Algorithm 1 over multiple values of c.

6

4.1

Implementation of Algorithm 1 and results

Here we demonstrate how Algorithm 1 can be applied to estimate the long-term causal effect in
Eq. (5) on the Rapoport & Boebel dataset. To this end we clarify Algorithm 1 step by step  and give
more details in the supplement.
Step 1: Model parameters. For simplicity we assume that the models in the two games share
common parameters  and thus (φ1  ψ1  λ1) = (φ0  ψ0  λ0) ≡ (φ  ψ  λ)  where λ are the parame-
ters of the behavioral model to be described in Step 8. Having common parameters also acts as
regularization and thus helps estimation.
Step 4: Sampling parameters and initial behaviors As explained later we assume that there are
3 different behaviors and thus φ  ψ  λ are vectors with 3 components. Let x ∼ U (m  M ) denote
that every component of x is uniform on (m  M )  independently. We choose diffuse priors for our
parameters  speciﬁcally  φ ∼ U(0  10)  ψ ∼ U(−5  5)  and λ ∼ U(−10  10). Given φ we sample
the initial behaviors as Dirichlet  i.e.  β1(0; Z) ∼ Dir(φ) and β0(0; Z) ∼ Dir(φ)  independently.
Steps 5 & 7: Pivot to counterfactuals. Since we have a completely randomized experiment (A/B
test) it holds that ρZ = 0.5 and therefore β(0) = 0.5(β1(0; Z) + β0(0; Z)). Now we can pivot to the
counterfactual population behaviors under Z = 1 and Z = 0 by setting β1(0; 1) = β0(0; 0) = β(0).
Step 8: Sample counterfactual behavioral history. As the temporal model  we adopt the lag-one
vector autoregressive model  also known as VAR(1). We transform1 the population behavior into
a new variable wt = logit(β1(t; 1)) ∈ R2 (also do so for β0(t; 0)). Such transformation with a
unique inverse is necessary because population behaviors are constrained on the simplex  and thus
form so-called compositional data [2  9]. The VAR(1) model implies that

wt = ψ[1]1 + ψ[2]wt−1 + ψ[3]t 

(6)
where ψ[k] is the kth component of ψ and t ∼ N (0  I) is i.i.d. standard bivariate normal. Eq. (6)
is used to sample the behavioral history  Bj  in Step 8 of Algorithm 1.
Step 9: Behavioral model. For the behavioral model  we adopt the quantal p-response (QLp)
model [20]  which has been successful in predicting human actions in real-world experiments [22].
We choose p = 3 behaviors  namely B = {b0  b1  b2} of increased sophistication parametrized by
λ = (λ[1]  λ[2]  λ[3]) ∈ R3. Let Gj denote the 5 × 5 payoff matrix of game j and let the term
strategy denote a distribution over all actions. An agent with behavior b0 plays the uniform strategy 

P (Ai(t; Z) = a|Bi(t; Z) = b0  Gj) = 1/5.

An agent of level-1 (row player) assumes to be playing only against level-0 agents and thus expects
per-action proﬁt u1 = (1/5)Gj1 (for column player we use the transpose of Gj). The level-1 agent
will then play a strategy proportional to eλ[1]u1  where ex for vector x denotes the element-wise
exponentiation  ex = (ex[k]). The precision parameter λ[1] determines how much an agent insists
on maximizing expected utility; for example  if λ[1] = ∞  the agent plays the action with maximum
expected payoff (best response); if λ[1] = 0  the agent acts as a level-0 agent. An agent of level-
2 (row player) assumes to be playing only against level-1 agents with precision λ[2] and therefore
expects to face strategy proportional to eλ[2]u1. Thus its expected per-action proﬁt is u2 ∝ Gjeλ[2]u1 
and plays strategy ∝ eλ[3]u2.
Given Gj and λ we calculate a 5 × 3 matrix Qj where the kth column is the strategy played by an
agent with behavior bk−1. The expected population action is therefore ¯αj(t; Z) = Qjβj(t; Z). The
population action αj(t; Z) is distributed as a normalized multinomial random variable with expecta-
tion ¯αj(t; Z)  and so P (αj(t; 1)|βj(t; 1)  Gj) = Multi(|I| · αj(t; 1); ¯αj(t; 1))  where Multi(n; p)
is the multinomial density of observations n = (n1  . . .   nK) with probabilities p = (p1  . . .   pK).
Hence  the full likelihood for observed actions in game j in Steps 10 and 11 of Algorithm 1 is given
by the product

T−1(cid:89)

P (Dj|Bj  Gj) =

Multi(|I| · αj(t; j1); ¯αj(t; j1)).

t=0

Running Algorithm 1 on the Rapoport and Boebel dataset yields the estimates shown in Figure 2 
for 25 different fee vectors c  where each component ca is sampled uniformly at random from (0  1).
1y = logit(x) is deﬁned as the function ∆m → Rm−1  y[i] = log(x[i + 1]/x[1])  where x[1] (cid:54)= 0 wlog.

7

Figure 2: Estimates of long-term effects of different methods corresponding to 25 random objective
coefﬁcients c in Eq. (5). For estimates of our method we ran Algorithm 1 for 100 iterations.

We also test difference-in-differences (DID)  which estimates the causal effect through
ˆτ did = [R(α1(2; Z)) − R(α1(0; Z))] − [R(α0(2; Z)) − R(α0(0; Z))] 

and a naive method (“naive” in the plot)  which ignores the dynamical aspect and estimates the long-
term causal effect as ˆτ nai = [R(α1(2; Z)) − R(α0(2; Z))]. Our estimates (“LACE” in the plot) are
closer to the truth (mse = 0.045) than the estimates from the naive method (mse = 0.185) and from
DID (mse = 0.361). This illustrates that our method can pull game-theoretic information from the
data for long-term causal inference  whereas the other methods cannot.

5 Conclusion

One critical shortcoming of statistical methods of causal inference is that they typically do not assess
the long-term effect of policy changes. Here we combined causal inference and game theory to
build a framework for estimation of such long-term effects in multiagent economies. Central to
our approach is behavioral game theory  which provides a natural latent space model of how agents
act and how their actions evolve over time. Such models enable to predict how agents would act
under various policy assignments and at various time points  which is key for valid causal inference.
Working on a real-world dataset [18] we showed how our framework can be applied to estimate the
long-term effect of changing the payoff structure of a normal-form game.
Our framework could be extended in future work by incorporating learning (e.g.  ﬁctitious play 
bandits  no-regret learning) to better model the dynamic response of multiagent systems to policy
changes. Another interesting extension would be to use our framework for optimal design of exper-
iments in such systems  which needs to account for heterogeneity in agent learning capabilities and
for intrinsic dynamical properties of the systems’ responses to experimental treatments.

Acknowledgements

The authors wish to thank Leon Bottou  the organizers and participants of CODE@MIT’15 
GAMES’16  the Workshop on Algorithmic Game Theory and Data Science (EC’15)  and the anony-
mous NIPS reviewers for their valuable feedback. Panos Toulis has been supported in part by the
2012 Google US/Canada Fellowship in Statistics. David C. Parkes was supported in part by NSF
grant CCF-1301976 and the SEAS TomKat fund.

8

References
[1] Alberto Abadie. Semiparametric difference-in-differences estimators. The Review of Economic

Studies  72(1):1–19  2005.

[2] John Aitchison. The statistical analysis of compositional data. Springer  1986.
[3] Joshua D Angrist and J¨orn-Steffen Pischke. Mostly harmless econometrics: An empiricist’s

companion. Princeton university press  2008.

[4] Susan Athey  Jonathan Levin  and Enrique Seira. Comparing open and sealed bid auctions:
Evidence from timber auctions. The Quarterly Journal of Economics  126(1):207–257  2011.
[5] L´eon Bottou  Jonas Peters  Joaquin Qui˜nonero-Candela  Denis X Charles  D Max Chickering 
Elon Portugualy  Dipankar Ray  Patrice Simard  and Ed Snelson. Couterfactual reasoning and
learning systems. J. Machine Learning Research  14:3207–3260  2013.

[6] David Card and Alan B Krueger. Minimum wages and employment: A case study of the fast
food industry in New Jersey and Pennsylvania. American Economic Review  84(4):772–793 
1994.

[7] Stephen G Donald and Kevin Lang. Inference with difference-in-differences and other panel

data. The review of Economics and Statistics  89(2):221–233  2007.

[8] Ronald Aylmer Fisher. The design of experiments. Oliver & Boyd  1935.
[9] Gary K Grunwald  Adrian E Raftery  and Peter Guttorp. Time series of continuous proportions.

Journal of the Royal Statistical Society. Series B (Methodological)  pages 103–116  1993.

[10] P Richard Hahn  Indranil Goswami  and Carl F Mela. A bayesian hierarchical model for
inferring player strategy types in a number guessing game. The Annals of Applied Statistics 
9(3):1459–1483  2015.

[11] James J Heckman  Lance Lochner  and Christopher Taber. General equilibrium treatment

effects: A study of tuition policy. American Economic Review  88(2):3810386  1998.

[12] James J Heckman and Edward Vytlacil. Structural equations  treatment effects  and economet-

ric policy evaluation1. Econometrica  73(3):669–738  2005.

[13] John H Holland and John H Miller. Artiﬁcial adaptive agents in economic theory. The Ameri-

can Economic Review  pages 365–370  1991.

[14] Paul W Holland. Statistics and causal inference. Journal of the American statistical Associa-

tion  81(396):945–960  1986.

[15] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form

games. Games and economic behavior  10(1):6–38  1995.

[16] Michael Ostrovsky and Michael Schwarz. Reserve prices in internet advertising auctions: A
ﬁeld experiment. In Proceedings of the 12th ACM conference on Electronic commerce  pages
59–60. ACM  2011.

[17] Judea Pearl. Causality: models  reasoning and inference. Cambridge University Press  2000.
[18] Amnon Rapoport and Richard B Boebel. Mixed strategies in strictly competitive games: A
further test of the minimax hypothesis. Games and Economic Behavior  4(2):261–283  1992.
[19] Donald B Rubin. Causal inference using potential outcomes. Journal of the American Statis-

tical Association  2011.

[20] Dale O Stahl and Paul W Wilson. Experimental evidence on players’ models of other players.

Journal of Economic Behavior & Organization  25(3):309–327  1994.

[21] J Von Neumann and O Morgenstern. Theory of games and economic behavior. Princeton

University Press  1944.

[22] James R Wright and Kevin Leyton-Brown. Beyond equilibrium: Predicting human behavior

in normal-form games. In Proc. 24th AAAI Conf. on Artiﬁcial Intelligence  2010.

9

,Changyou Chen
Nan Ding
Lawrence Carin
Panagiotis Toulis
David Parkes