2019,Fast  Provably convergent IRLS Algorithm for p-norm Linear Regression,Linear regression in L_p-norm is a canonical optimization problem that arises in several applications  including sparse recovery  semi-supervised learning  and signal processing. Generic convex optimization algorithms for solving L_p-regression are slow in practice. Iteratively Reweighted Least Squares (IRLS) is an easy to implement family of algorithms for solving these problems that has been studied for over 50 years. However  these algorithms often diverge for p > 3  and since the work of Osborne (1985)  it has been an open problem whether there is an IRLS algorithm that converges for p > 3. We propose p-IRLS  the first IRLS algorithm that provably converges geometrically for any p \in [2 \infty). Our algorithm is simple to implement and is guaranteed to find a high accuracy solution in a sub-linear number of iterations. Our experiments demonstrate that it performs even better than our theoretical bounds  beats the standard Matlab/CVX implementation for solving these problems by 10–50x  and is the fastest among available implementations in the high-accuracy regime.,Fast  Provably convergent IRLS Algorithm for

p-norm Linear Regression ⇤

Deeksha Adil

Department of Computer Science

University of Toronto

deeksha@cs.toronto.edu

Richard Peng

School of Computer Science

Georgia Institute of Technology

rpeng@cc.gatech.edu

Sushant Sachdeva

Department of Computer Science

University of Toronto

sachdeva@cs.toronto.edu

Abstract

Linear regression in `p-norm is a canonical optimization problem that arises in
several applications  including sparse recovery  semi-supervised learning  and sig-
nal processing. Generic convex optimization algorithms for solving `p-regression
are slow in practice. Iteratively Reweighted Least Squares (IRLS) is an easy to
implement family of algorithms for solving these problems that has been studied
for over 50 years. However  these algorithms often diverge for p > 3  and since the
work of Osborne (1985)  it has been an open problem whether there is an IRLS
algorithm that is guaranteed to converge rapidly for p > 3. We propose p-IRLS 
the ﬁrst IRLS algorithm that provably converges geometrically for any p 2 [2 1).
Our algorithm is simple to implement and is guaranteed to ﬁnd a high accuracy
solution in a sub-linear number of iterations. Our experiments demonstrate that it
performs even better than our theoretical bounds  beats the standard Matlab/CVX
implementation for solving these problems by 10–50x  and is the fastest among
available implementations in the high-accuracy regime.

1

Introduction

We consider the problem of `p-norm linear regression (henceforth referred to as `p-regression) 

arg min

x2Rn kAx  bkp  

(1)

where A 2 Rm⇥n  b 2 Rm are given and kvkp =Pi |v i|p1/p denotes the `p-norm. This problem
generalizes linear regression and appears in several applications including sparse recovery [CT05] 
low rank matrix approximation [CGK+17]  and graph based semi-supervised learning [AL11].
An important application of `p-regression with p  2 is graph based semi-supervised learning (SSL).
Regularization using the standard graph Laplacian (also called a 2-Laplacian) was introduced in
the seminal paper of Zhu  Gharamani  and Lafferty [ZGL03]  and is a popular approach for graph
based SSL  see e.g. [ZBL+04  BMN04  CSZ09  Zhu05]. The 2-Laplacian regularization suffers from
degeneracy in the limit of small amounts of labeled data [NSZ09]. Several works have since suggested
using the p-Laplacian instead [AL11  BZ13  ZB11] with large p  and have established its consistency
and effectiveness for graph based SSL with small amounts of data [ACR+16  Cal17  RCL19  ST17 

⇤Code for this work is available at https://github.com/utoronto-theory/pIRLS.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

KRSS15]. Recently  p-Laplacians have also been used for data clustering and learning problems
[ETT15  EDT17  HFE18]. Minimizing the p-Laplacian can be easily seen as an `p-regression
problem.
Though `p-regression is a convex programming problem  it is very challenging to solve in practice.
General convex programming methods such as conic programming using interior-point methods (like
those implemented in CVX) are very slow in practice. First order methods do not perform well for
these problems with p > 2 since the gradient vanishes rapidly close to the optimum.
For applications such graph based SSL with p-Laplacians  it is important that we are able to compute
a solution x that approximates the optimal solution x ? coordinate-wise rather than just achieving an
approximately optimal objective value  since these coordinates determine the labels for the vertices.
For such applications  we seek a (1 + ")-approximate solution  an x such that its objective value 
kAx  bkp
p   for some very small " (108
or so) in order achieve a reasonable coordinate-wise approximation. Hence  it is very important
for the dependence on " be log 1/" rather than poly(1/"). A log 1/" guarantee implies a coordinate-
wise convergence guarantee with essentially no loss in the asymptotic running time. Please see
the supplementary material for the derivation and experimental evaluation of the coordinate-wise
convergence guarantees.

p is at most (1 + ") times the optimal value kAx ?  bkp

IRLS Algorithms. A family of algorithms for solving the `p-regression problem are the IRLS
(Iterated Reweighted Least Squares) algorithms. IRLS algorithms have been discovered multiple times
independently and have been studied extensively for over 50 years e.g. [Law61  Ric64  Osb85  GR97]
(see [Bur12] for a detailed survey). The main step in an IRLS algorithm is to solve a weighted least
squares (`2-regression) problem to compute the next iterate 

x (t+1) = arg min

x

(Ax  b)>R(t)(Ax  b) 

(2)

starting from any initial solution x (0) (usually the least squares solution corresponding to R = I ).
Each iteration can be implemented by solving a linear system x (t+1) (A>R(t)A)1A>R(t)b.
Picking R(t) = diag⇣|Ax (t)  b|p2⌘   gives us an IRLS algorithm where the only ﬁxed point is
the minimizer of the regression problem (1) (which is unique for p 2 (1 1)).
The basic version of the above IRLS algorithm converges reliably in practice for p 2 (1.5  3)  and
diverges often even for moderate p (say p  3.5 [RCL19  pg 12]). Osborne [Osb85] proved that the
above IRLS algorithm converges in the limit for p 2 [1  3). Karlovitz [Kar70] proved a similar result
for an IRLS algorithm with a line search for even p > 2. However  both these results only prove
convergence in the limit without any quantitative bounds  and assume that you start close enough
to the solution. The question of whether a suitable IRLS algorithm converges geometrically to the
optimal solution for (1) in a few iterations has been open for over three decades.

" rather than poly( 1

" ).

p2
2(p1) log m

" )  Op(pm log m

Our Contributions. We present p-IRLS  the ﬁrst IRLS algorithm that provably converges geo-
metrically to the optimal solution for `p-regression for all p 2 [2 1). Our algorithm is very similar
to the standard IRLS algorithm for `p regression  and given an "> 0  returns a feasible solution x
for (1) in Op(m
" ) iterations (Theorem 3.1). Here m is the number of
rows in A. We emphasize that the dependence on " is log 1
Our algorithm p-IRLS is very simple to implement  and our experiments demonstrate that it is much
faster than the available implementations for p 2 (2 1) in the high accuracy regime. We study its
performance on random dense instances of `p-regression  and low dimensional nearest neighbour
graphs for p-Laplacian SSL. Our Matlab implementation on a standard desktop machine runs in
at most 2–2.5s (60–80 iterations) on matrices for size 1000 ⇥ 850  or graphs with 1000 nodes and
around 5000 edges  even with p = 50 and " = 108. Our algorithm is at least 10–50x faster than the
standard Matlab/CVX solver based on Interior point methods [GB14  GB08]  while ﬁnding a better
solution. We also converge much faster than IRLS-homotopy based algorithms [RCL19] that are not
even guaranteed to converge to a good solution. For larger p  say p > 20  this difference is even more
dramatic  with p-IRLS obtaining solutions with at least 4 orders of magnitude smaller error with the
same number of iterations. Our experiments also indicate that p-IRLS scales much better than as
indicated by our theoretical bounds  with the iteration count almost unchanged with problem size 
and growing very slowly (at most linearly) with p.

2

1.1 Related Works and Comparison
IRLS algorithms have been used widely for various problems due to their exceptional simplicity and
ease of implementation  including compressive sensing [CW08]  sparse signal reconstruction [GR97] 
and Chebyshev approximation in FIR ﬁlter design [BB94]. There have been various attempts at
analyzing variants of IRLS algorithm for `p-norm minimization. We point the reader to the survey by
Burrus [Bur12] for numerous pointers and a thorough history.
The works of Osborne [Osb85] and Karlovitz [Kar70] mentioned above only prove convergence in
the limit without quantitative bounds and under assumptions on p and that we start close enough.
Several works show that it is similar to Newton’s method (e.g. [Kah72  BBS94])  or that adaptive
step sizes help (e.g. [VB99  VB12]) but do not prove any guarantees.
A few notable works prove convergence guarantees for IRLS algorithms for sparse recovery (even
p < 1 in some cases) [DDFG08  DDFG10  BL18]  and for low-rank matrix recovery [FRW11].
Quantitative convergence bounds for IRLS algorithms for `1 are given by Straszak and Vish-
noi [SV16b  SV16c  SV16a]  inspired by slime-mold dynamics. Ene and Vladu give IRLS algorithms
for `1 and `1 [EV19]. However  both these works have poly(1/") dependence in the number of
iterations  with the best result by [EV19] having a total iteration count roughly m1/3"2/3.
The most relevant theoretical results for `p-norm minimization are Interior point methods [NN94] 
the homotopy method of Bubeck et al [BCLL18]  and the iterative-reﬁnement method of Adil et
al. [AKPS19]. The convergence bounds we prove on the number of iterations required by p-IRLS
p2
2p2 ) has a better dependence on m than Interior Point methods (roughly m1/2)  but
(roughly m
p2
2p ) and
marginally worse than the dependence in the work of Bubeck et al. [BCLL18] (roughly m
Adil et al. [AKPS19] (roughly m
2p+(p2) ). Note that we are comparing the dominant polynomial
terms  and ignoring the smaller poly(p log m/") factors. A follow-up work in this line by a subset of the
authors [AS19] focuses on the large p case  achieving a similar running time to [AKPS19]  but with
linear dependence on p. Also related  but not directly comparable are the works of Bullins [Bul18]
(restricted to p = 4) and the work of Maddison et al. [MPT+18] (ﬁrst order method with a dependence
on the condition number  which could be large).
More importantly  in contrast with comparable second order methods [BCLL18  AKPS19]  our
algorithm is far simpler to implement  and has a locally greedy structure that allows for greedily
optimizing the objective using a line search  resulting in much better performance in practice than that
guaranteed by our theoretical bounds. Unfortunately  there are also no available implementations for
any of the above discussed methods (other than interior point methods) in order to make a comparison.
Another line of heuristic algorithms combines IRLS algorithms with a homotopy based approach
(e.g. [Kah72]. See [Bur12]). These methods start from a solution for p = 2  and slowly increase p
multiplicatively  using an IRLS algorithm for each phase and the previous solution as a starting point.
These algorithms perform better in practice than usual IRLS algorithms. However  to the best of our
knowledge  they are not guaranteed to converge  and no bounds on their performance are known.
Rios [Rio19] provides an efﬁcient implementation of such a method based on the work of Rios et
al. [RCL19]  along with detailed experiments. Our experiments show that our algorithm converges
much faster than the implementation from Rios (see Section 4).

p2

2 Preliminaries

We ﬁrst deﬁne some terms that we will use in the formal analysis of our algorithm. For our analysis
we use a more general form of the `p-regression problem 

arg min

x :C x =d kAx  bkp .

(3)

Setting C and d to be empty recovers the standard `p-regression problem.
Deﬁnition 2.1 (Residual Problem). The residual problem of (3) at x is deﬁned as 

max

:C =0

g>A  2p2>A>RA  ppkAkp
p.

Here R = diag|Ax  b|p2 and g = pR(Ax  b) is the gradient of the objective at x . Deﬁne

() to denote the objective of the residual problem evaluated at .

3

Deﬁnition 2.2 (Approximation to the Residual Problem). Let   1 and ? be the optimum of the
residual problem. A -approximate solution to the residual problem is e such that Ce= 0   and
(e)  1

 (?).

3 Algorithm and Analysis

"

p/16p

p < i do

16p(1+") kAx  bkp

Algorithm 1 p-IRLS Algorithm
1: procedure p-IRLS(A  b "  C   d)
x arg minC x =d kAx  bk2
2:
2 .
i kAx  bkp
3:
while
4:
5:
6:
7:
8:
9:
10:
11:
12:

2 i(p2)/pm(p2)/p

R | Ax  b|p2
g = pR(Ax  b)
s 1
e arg ming>A=i/2 C =0 >A>(R + sI )A
↵ LINESEARCH(A  b  x (t) e)
x (t+1) x (t)  ↵e
if INSUFFICIENTPROGRESSCHECK(A  R + sI  e  i) then i i/2

return x

. ↵ = arg min↵ kA(x  ↵e)  bkp

p

Algorithm 2 Check Progress
1: procedure INSUFFICIENTPROGRESSCHECK(A  R    i)
2:
3:

p

2p2>A>RA

 16p
k ppkAkp
↵0 minn 1
16  
if (↵0 ·e) < ↵0

else return false

4:

5:
6:

1

(16k)1/(p1)o

4 i or >A>(R + sI )A > i/p2 then return true

Our algorithm p-IRLS  described in Algorithm (1)  is the standard IRLS algorithm (equation 2) with
few key modiﬁcations. The ﬁrst difference is that at each iteration t  we add a small systematic
padding s(t)I to the weights R(t). The second difference is that the next iterate x (t+1) is calculated
by performing a line search along the line joining the current iterate x (t) and the standard IRLS iterate

ex(t+1) at iteration t+1 (with the modiﬁed weights) 2. Both these modiﬁcations have been tried in prac-

tice  but primarily from practical justiﬁcations: padding the weights avoids ill-conditioned matrices 
and line-search can only help us converge faster and improves stability [Kar70  VB99  VB99]. Our
key contribution is to show that these modiﬁcations together allow us to provably make ⌦p(m p2
2(p1) )
progress towards the optimum  resulting in a ﬁnal iteration count of Op(m
" ). Finally  at
every iteration we check if the objective value decreases sufﬁciently  and this allows us to adjust
s(t) appropriately. We emphasize here that our algorithm always converges. We prove the following
theorem:
Theorem 3.1. Given any A 2 Rm⇥n  b 2 Rm " > 0  p  2 and x ? = arg minx :C x =d kAx  bkp
p.
Algorithm 1 returns x such that kAx  bkp
p and C x = d  in at most
O⇣p3.5m

p  (1 + ")kAx ?  bkp

The approximation guarantee on the objective value can be translated to a guarantee on coordinate
wise convergence. For details on this refer to the supplementary material.

"⌘ iterations.

2(p1) log m

p2
2(p1) log m

p2

2Note that p-IRLS has been written in a slightly different but equivalent formulation  where it solves for

e= x (t) ex(t+1).

4

3.1 Convergence Analysis

The analysis  at a high level  is based on iterative reﬁnement techniques for `p-norms developed in
the work of Adil et al [AKPS19] and Kyng et al [KPSW19]. These techniques allow us to use a
crude -approximate solver for the residual problem (Deﬁnition 2.1) Op( log m
" ) number of times
to obtain a (1 + ") approximate solution for the `p-regression problem (Lemma 3.2).
In our algorithm  if we had solved the standard weighted `2 problem instead   would be unbounded.
The padding added to the weights allow us to prove that the solution to weighted `2 problem gives a
bounded approximation to the residual problem provided we have the correct padding  or in other
words correct value of i (Lemma 3.3). We will show that the number of iterations where we are
adjusting the value of i are small. Finally  Lemma 3.5 shows that when the algorithm terminates  we
have an "-approximate solution to our main problem. The remaining lemma of this section  Lemma
3.4 gives the loop invariant which is used at several places in the proof of Theorem 3.1. Due to space
constraints  we only state the main lemmas here and defer the proofs to the supplementary material.
We begin with the lemma that talks about our overall iterative reﬁnement scheme. The iterative
reﬁnement scheme in [AKPS19] and [KPSW19] has an exponential dependence on p. We improve
this dependence to a small polynomial in p.
Lemma 3.2.
Starting from x (0) =
arg minC x =d kAx  bk2
2  and iterating as  x (t+1) = x (t)    where  is a -approximate
solution to the residual problem (Deﬁnition 2.1)  we get an "-approximate solution to (3) in at most

Let p  2  and   1.

(Iterative Reﬁnement).

O⇣p2 log m

"⌘ calls to a -approximate solver for the residual problem.

The next lemma talks about bounding the approximation factor   when we have the right value of i.
Lemma 3.3. (Approximation). Let R  g  s ↵ be as deﬁned in lines (5)  (6)  (7) and (9) of Algorithm

1. Let ↵0 be as deﬁned in line (4) of Algorithm 2 and e be the solution of the following program 
2(p1)⌘- approximate
Ife>A>(R+sI )Ae  i/p2 and (↵0·e)  ↵0i

4   then ↵·e is an O⇣p1.5m

>A>(R + sI )A s.t. g>A= i/2  C = 0 .

solution to the residual problem.

arg min


(4)

p2

We next present the loop invariant followed by the conditions for the termination.
Lemma 3.4.
(kAx (t)bkp

(Invariant) At every iteration of

pkAx ?bkp
p)
16p

16p(1+")kAx (0)  bkp
Lemma 3.5. (Termination). Let i be such that (kAx (t)  bkp

 i and i 

p kAx ?  bkp

pm(p2)/2.

"

the while loop  we have C x (t) = d  

p)/16p 2 (i/2  i]. Then 

i 

and 

"

16p(1+")kAx (t)  bkp

p ) kAx (t)  bkp

p  (1 + ")OPT.

kAx (t)  bkp

p  (1 + ")OPT ) i  2

"

16p(1+")kAx (t)  bkp
p.

We next see how Lemmas 3.2 3.3  3.4  and 3.5 together imply our main result  Theorem 3.1.

3.2 Proof of Theorem 3.1

Proof. We ﬁrst show that at termination  the algorithm returns an "-approximate solution. We begin
by noting that the quantity i can only decrease with every iteration. At iteration t  let i0 denote the
smallest number such that (kAx (t)  bkp
p)/16p 2 (i0/2  i0]. Note that i must be
at least i0 (Lemma 3.4). Let us ﬁrst consider the termination condition of the while loop. When
we terminate  "kAx (t)bkp
p  (1 + ")OP T .
Lemma 3.4 also shows that at each iteration our solution satisﬁes C x (t) = d  therefore the solution
returned at termination also satisﬁes the subspace constraints.

16p(1+")  i  i0. Lemma 3.5 now implies thatAx (t)  b

p  kAx ?  bkp

p

p

5

80

60

40

20

s
n
o
i
t
a
r
e
t
I

0

2

1

)
s
(
e
m
T

i

p = 4
p = 8
p = 16
p = 32

0

0

2

4

6

8
-log10(error)

10

12

(a) Size of A ﬁxed to 1000⇥ 850.

s
n
o

i
t

a
r
e

t
I

60

50

40

30

20

10

0

)
s
(
e
m
T

i

2

1

0

0

p = 4
p = 8
p = 16
p = 32

200

400

Size

600

800

1000

(b) Sizes of A: (50 + 100(k 
1)) ⇥ 100k. Error " = 108.

60

50

40

30

20

10

s
n
o

i
t

a
r
e

t
I

0

2

1

)
s
(
e
m
T

i

0

0

10

20
30
Parameter p

40

50

(c) Size of A is ﬁxed to 1000 ⇥
850. Error " = 108.

Figure 2: Random Matrix instances. Comparing the number of iterations and time taken by our algorithm with
the parameters. Averaged over 100 random samples for A and b. Linear solver used : backslash.

We next prove the running time bound. Note that the objective is non increasing with every iteration.

p2

p2

p  which could also be zero.

algorithm does not reduce i. It sufﬁces to prove that in this iteration  the algorithm obtains an

This is because the LINESEARCH returns a factor that minimizes the objective given a direction e 
i.e.  ↵ = arg min kA(x  e)  bkp
We now show that at every iteration the algorithm either reduces i or ﬁnds e that gives a
O⇣p1.5m
2(p1)⌘-approximate solution to the residual problem. Consider an iteration where the
O⇣p1.5m
2(p1)⌘-approximate solution to the residual problem. Since the algorithm does not reduce
i  we must have (↵0e)  ↵0i/4  and e>A>(R + sI )Ae  i/p2. It follows from Lemma 3.3 
we know that e gives the required approximation to the residual problem.
Thus  the algorithm either reduces i or returns an O⇣p1.5m
2(p1)⌘-approximate solution to the
" (Lemma 3.4 gives the value of imin). By Lemma 3.2  the number of steps where the
p log m
most O⇣p3.5m
"⌘. Thus  the total number of iterations required by our algorithm is
O⇣p3.5m
2(p1) log m

"⌘  completing the proof of the theorem.

residual problem. The number of steps in which we reduce i is at most log(iinitial/imin) =

algorithm ﬁnds an approximate solution before it has found a (1 + ")-approximate solution is at

p2

2(p1) log m

p2

p2

4 Experiments

In this section  we detail our results from experiments studying
the performance of our algorithm  p-IRLS. We implemented our
algorithm in Matlab on a standard desktop machine  and evaluated
its performance on two types of instances  random instances for
`p-regression  and graphs for p-Laplacian minimization. We
study the scaling behavior of our algorithm as we change p  " 
and the size of the problem. We compare our performance to the
Matlab/CVX solver that is guaranteed to ﬁnd a good solution 
and to the IRLS/homotopy based implementation from [RCL19]
that is not guaranteed to converge  but runs quite well in practice.
We now describe our instances  parameters and experiments in
detail.

Error : IRLS vs Newton with Homotopy

IRLS
Newton

r
o
r
r

E

10-2

10-4

10-6

10-8

0

10

30

20
Parameter p

40

50

Figure 1: Averaged over 100 ran-
dom samples. Graph: 1000 nodes
(5000-6000 edges). Solver: PCG
with Cholesky preconditioner.

6

p = 4
p = 8
p = 16
p = 32

120

100

s
n
o
i
t
a
r
e
t
I

)
s
(
e
m
T

i

80

60

40

20

0

2

1

s
n
o

i
t

a
r
e

t
I

80

60

40

20

0

)
s
(
e
m
1T

i

p = 4
p = 8
p = 16
p = 32

80

60

40

20

s
n
o
i
t
a
r
e
t
I

0

2

1

)
s
(
e
m
T

i

0

0

2

4

6

8
-log10(error)

10

12

0

0

200

400

Size

600

800

1000

0

0

10

20
30
Parameter p

40

50

(a) Size of graph ﬁxed to 1000
nodes (around 5000-6000 edges).

(b) Number of nodes: 100k. Error
" = 108.

(c) Size of graph ﬁxed to 1000
nodes (around 5000-6000 edges).
Error " = 108.

Figure 3: Graph Instances. Comparing the number of iterations and time taken by our algorithm with the
parameters. Averaged over 100 graph samples. Linear solver used : backslash.

IRLS vs CVX : Random Matrices

IRLS
CVX

80

60

40

20

)
s
(
e
m
T

i

IRLS vs CVX : Random Matrices

IRLS
CVX

30

20

10

)
s
(
e
m
T

i

IRLS vs CVX : Graphs
IRLS
CVX

15

10

5

)
s
(
e
m
T

i

2.2

0

200

400

600

Size

800

1000

0.3

0

2

6

4
Parameter p

8

10

0.5

0

200

400

600

800

1000

Size

)
s
(
e
m
T

i

25
20
15
10
5
0.4

IRLS vs CVX : Graphs
IRLS
CVX

0

2

6

4
Parameter p

8

10

(a) Fixed p = 8. Size of
matrices: 100k ⇥ (50 +
100(k  1)).
Figure 4: Averaged over 100 samples. Precision set to " = 108.CVX solver used : SDPT3 for Matrices and
Sedumi for Graphs.

(c) Fixed p = 8.
The number of nodes :
50k  k = 1  2  ...  10.

(d) Size of graphs ﬁxed to
400 nodes ( around 2000
edges).

(b) Size of matrices ﬁxed
to 500 ⇥ 450.

Instances and Parameters. We consider two types of in-
stances  random matrices and graphs.
1. Random Matrices: We want to solve the problem minx kAx  bkp. In these instances we
use random matrices A and b  where every entry of the matrix is chosen uniformly at random
between 0 and 1.

2. Graphs: We use the graphs described in [RCL19]. The set of vertices is generated by choosing
vectors in [0  1]10 uniformly at random and the edges are created by connecting the 10 nearest
neighbours. Weights of each edge is speciﬁed by a gaussian type function (Eq 3.1 [RCL19]).
Very few vertices (around 10) have labels which are again chosen uniformly at random between
0 and 1. The problem studied on these instances is to determine the minimizer of the `p laplacian.
We formulate this problem into the form minx kAx  bkp
p  details of this formulation can be
found in the Appendix that is in the supplementary material.

Note that we have 3 different parameters for each problem  the size of the instance i.e.  the number
of rows of matrix A  the norm we solve for  p  and the accuracy to which we want to solve each
problem  ". We will consider each of these parameters independently and see how our algorithm
scales with them for both instances.

Benchmark Comparisons. We compare the performance of our program with the following:

1. Standard MATLAB optimization package  CVX [GB14  GB08].

7

2. The most efﬁcient algorithm for `p-semi supervised learning given in [RCL19] was newton’s
method with homotopy. We take their hardest problem  and compare the performance of their
code with ours by running our algorithm for the same number of iterations as them and showing
that we get closer to the optimum  or in other words a smaller error "  thus showing we converge
much faster.

Implementation Details. We normalize the instances by running our algorithm once and dividing
the vector b by the norm of the ﬁnal objective  so that our norms at the end are around 1. We do
this for every instance before we measure the runtime or the iteration count for uniformity and to
avoid numerical precision issues. All experiments were performed on MATLAB 2018b on a Desktop
ubuntu machine with an Intel Core i5-4570 CPU @ 3.20GHz ⇥ 4 processor and 4GB RAM. For
the graph instances  we ﬁx the dimension of the space from which we choose vertices to 10 and the
number of labelled vertices to be 10. The graph instances are generated using the code [Rio19] by
[RCL19]. Other details speciﬁc to the experiment are given in the captions.

4.1 Experimental Results

Dependence on Parameters. Figure 2 shows the dependence of the number of iterations and
runtime on our parameters for random matrices. Similarly for graph instances  Figure 3 shows the
dependence of iteration count and runtime with the parameters. As expected from the theoretical

guarantees  the number of iterations and runtimes increase linearly with log 1

size and p are clearly much better in practice (nearly constant and at most linear respectively) than
the theoretical bounds (m1/2 and p3.5 respectively) for both kinds of instances.

". The dependence on

Comparisons with Benchmarks.

• Figure 4 shows the runtime comparison between our IRLS algorithm p-IRLS and CVX. For all
instances  we ensured that our ﬁnal objective was smaller than the objective of the CVX solver.
As it is clear for both kinds of instances  our algorithm takes a lot lesser time and also increases
more slowly with size and p as compared to CVX. Note that that CVX does a lot better when
p = 2k  but it is still at least 30-50 times slower for random matrices and 10-30 times slower for
graphs.

• Figure 1 shows the performance of our algorithm when compared to the IRLS/Homotopy method
of [RCL19]. We use the same linear solvers for both programs  preconditioned conjugate
gradient with an incomplete cholesky preconditioner and run both programs to the same number
of iterations. The plots indicate the value " as described previously. For our IRLS algorithm we
indicate our upper bound on " and for their procedure we indicate a lower bound on " which
is the relative difference in the objectives achieved by the two algorithms. It is clear that our
algorithm achieves an error that is orders of magnitudes smaller than the error achieved by their
algorithm. This shows that our algorithm has a much faster rate of convergence. Note that there
is no guarantee on the convergence of the method used by [RCL19]  whereas we prove that our
algorithm converges in a small number of iterations.

5 Discussion

To conclude  we present p-IRLS  the ﬁrst IRLS algorithm that provably converges to a high accuracy
solution in a small number of iterations. This settles a problem that has been open for over three
decades. Our algorithm is very easy to implement and we demonstrate that it works very well in
practice  beating the standard optimization packages by large margins. The theoretical bound on the
numbers of iterations has a sub-linear dependence on size and a small polynomial dependence on p 
however in practice  we see an almost constant dependence on size and at most linear dependence on
p in random instances and graphs. In order to achieve the best theoretical bounds we would require
some form of acceleration. For `1 and `1 regression  it has been shown that it is possible to achieve
acceleration  however without geometric convergence. It remains an open problem to give a practical
IRLS algorithm which simultaneously has the best possible theoretical convergence bounds.

8

Acknowledgements

DA is supported by SS’s NSERC Discovery grant and an Ontario Graduate Scholarship. SS is sup-
ported by the Natural Sciences and Engineering Research Council of Canada (NSERC)  a Connaught
New Researcher award  and a Google Faculty Research award. RP is partially supported by the NSF
under Grants No. 1637566 and No. 1718533.

References
[ACR+16] A. E. Alaoui  X. Cheng  A. Ramdas  M. J. Wainwright  and M. I. Jordan. Asymptotic
behavior of `p-based Laplacian regularization in semi-supervised learning. In Vitaly
Feldman  Alexander Rakhlin  and Ohad Shamir  editors  29th Annual Conference on
Learning Theory  volume 49 of Proceedings of Machine Learning Research  pages
879–906  Columbia University  New York  New York  USA  23–26 Jun 2016. PMLR.

[AKPS19] D. Adil  R. Kyng  R. Peng  and S. Sachdeva. Iterative reﬁnement for `p-norm regression.
In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms 
SODA 2019  San Diego  California  USA  January 6-9  2019  pages 1405–1424  2019.

[AL11] M. Alamgir and U. V. Luxburg. Phase transition in the family of p-resistances. In
J. Shawe-Taylor  R. S. Zemel  P. L. Bartlett  F. Pereira  and K. Q. Weinberger  editors  Ad-
vances in Neural Information Processing Systems 24  pages 379–387. Curran Associates 
Inc.  2011.

[AS19] D. Adil and S. Sachdeva. Faster p-norm minimizing ﬂows  via smoothed q-norm
problems. arXiv e-prints  page arXiv:1910.10571  Oct 2019. To appear at ACM-SIAM
Symposium on Discrete Algorithms (SODA 2020).

[BB94] J. A. Barreto and C. S. Burrus. lp complex approximation using iterative reweighted
least squares for ﬁr digital ﬁlters. In Proceedings of ICASSP’94. IEEE International
Conference on Acoustics  Speech and Signal Processing  volume 3  pages III–545. IEEE 
1994.

[BBS94] C.S. Burrus  J.A. Barreto  and I.W. Selesnick. Iterative reweighted least-squares design

of ﬁr ﬁlters. Trans. Sig. Proc.  42(11):2926–2936  November 1994.

[BCLL18] S. Bubeck  M. B. Cohen  Y. T. Lee  and Y. Li. An homotopy method for lp regression
provably beyond self-concordance and in input-sparsity time. In Proceedings of the
50th Annual ACM SIGACT Symposium on Theory of Computing  STOC 2018  pages
1130–1137  New York  NY  USA  2018. ACM.

[BL18] N. Bi and K. Liang. Iteratively reweighted algorithm for signals recovery with coherent
tight frame. Mathematical Methods in the Applied Sciences  41(14):5481–5492  2018.

[BMN04] M. Belkin  I. Matveeva  and P. Niyogi. Regularization and semi-supervised learning on
large graphs. In John Shawe-Taylor and Yoram Singer  editors  Learning Theory  pages
624–638  Berlin  Heidelberg  2004. Springer Berlin Heidelberg.

[Bul18] B. Bullins.

Fast minimization of structured convex quartics.

arXiv:1812.10349  2018.

arXiv preprint

[Bur12] C. S. Burrus.

Iterative reweighted least squares. OpenStax CNX. Available online:

http://cnx. org/contents/92b90377-2b34-49e4-b26f-7fe572db78a1  12  2012.

[BZ13] N. Bridle and X. Zhu. p-voltages: Laplacian regularization for semi-supervised learning
on high-dimensional data. In Eleventh Workshop on Mining and Learning with Graphs
(MLG2013)  2013.

[Cal17] J. Calder. Consistency of lipschitz learning with inﬁnite unlabeled data and ﬁnite labeled

data. CoRR  abs/1710.10364  2017.

9

[CGK+17] F. Chierichetti  S. Gollapudi  R. Kumar  S. Lattanzi  R. Panigrahy  and D. P. Woodruff.
Algorithms for `p low-rank approximation. In D. Precup and Y. W. Teh  editors  Pro-
ceedings of the 34th International Conference on Machine Learning  volume 70 of
Proceedings of Machine Learning Research  pages 806–814  International Convention
Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[CSZ09] O. Chapelle  B. Scholkopf  and A. Zien  Eds. Semi-supervised learning (chapelle  o. et
al.  eds.; 2006) [book reviews]. IEEE Transactions on Neural Networks  20(3):542–542 
March 2009.

[CT05] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on

Information Theory  51(12):4203–4215  Dec 2005.

[CW08] R. Chartrand and Wotao Yin. Iteratively reweighted algorithms for compressive sensing.
In 2008 IEEE International Conference on Acoustics  Speech and Signal Processing 
pages 3869–3872  March 2008.

[DDFG08] I. Daubechies  R. DeVore  M. Fornasier  and S. Gunturk. Iteratively re-weighted least
squares minimization: Proof of faster than linear rate for sparse recovery. In 2008 42nd
Annual Conference on Information Sciences and Systems  pages 26–29  March 2008.

[DDFG10] I. Daubechies  R. DeVore  M. Fornasier  and C. S. Gunturk. Iteratively reweighted
least squares minimization for sparse recovery. Communications on Pure and Applied
Mathematics  63(1):1–38  2010.

[EDT17] A Elmoataz  X Desquesnes  and M Toutain. On the game p-laplacian on weighted graphs
with applications in image processing and data clustering. European Journal of Applied
Mathematics  28(6):922–948  2017.

[ETT15] A. Elmoataz  M. Toutain  and D. Tenbrinck. On the p-laplacian and 1-laplacian on
graphs with applications in image and data processing. SIAM Journal on Imaging
Sciences  8(4):2412–2451  2015.

[EV19] A. Ene and A. Vladu. Improved convergence for `1 and `1 regression via iteratively
reweighted least squares. In Kamalika Chaudhuri and Ruslan Salakhutdinov  editors 
Proceedings of the 36th International Conference on Machine Learning  volume 97 of
Proceedings of Machine Learning Research  pages 1794–1801  Long Beach  California 
USA  09–15 Jun 2019. PMLR.

[FRW11] M. Fornasier  H. Rauhut  and R. Ward. Low-rank matrix recovery via iteratively
reweighted least squares minimization. SIAM Journal on Optimization  21(4):1614–
1640  2011.

[GB08] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In
V. Blondel  S. Boyd  and H. Kimura  editors  Recent Advances in Learning and Control 
Lecture Notes in Control and Information Sciences  pages 95–110. Springer-Verlag
Limited  2008. http://stanford.edu/~boyd/graph_dcp.html.

[GB14] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming 

version 2.1. http://cvxr.com/cvx  March 2014.

[GR97] I. F. Gorodnitsky and B. D. Rao. Sparse signal reconstruction from limited data using fo-
cuss: a re-weighted minimum norm algorithm. IEEE Transactions on Signal Processing 
45(3):600–616  March 1997.

[HFE18] Y. Haﬁene  J. Fadili  and A. Elmoataz. Nonlocal p-laplacian variational problems on

graphs. arXiv preprint arXiv:1810.12817  2018.

[Kah72] S. W. Kahng. Best lp-approximation. Math. Comput.  26(118):505–508  1972.
[Kar70] L.A Karlovitz. Construction of nearest points in the lp  p even  and l1 norms. i. Journal

of Approximation Theory  3(2):123 – 127  1970.

10

[KPSW19] R. Kyng  R. Peng  S. Sachdeva  and D. Wang. Flows in almost linear time via adaptive
preconditioning. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory
of Computing  STOC 2019  pages 902–913  New York  NY  USA  2019. ACM.

[KRSS15] R. Kyng  A. Rao  S. Sachdeva  and D. A. Spielman. Algorithms for lipschitz learning on
graphs. In Proceedings of The 28th Conference on Learning Theory  COLT 2015  Paris 
France  July 3-6  2015  pages 1190–1223  2015.

[Law61] C. L. Lawson. Contribution to the theory of linear least maximum approximation. Ph.D.

dissertation  Univ. Calif.  1961.

[MPT+18] C. J Maddison  D. Paulin  Y. W. Teh  B. O’Donoghue  and A. Doucet. Hamiltonian

descent methods. arXiv preprint arXiv:1809.05042  2018.

[NN94] Y. Nesterov and A. Nemirovskii.

Interior-Point Polynomial Algorithms in Convex

Programming. Society for Industrial and Applied Mathematics  1994.

[NSZ09] Boaz N.  Nathan S.  and Xueyuan Z. Statistical analysis of semi-supervised learning:

The limit of inﬁnite unlabelled data. 2009.

[Osb85] M. R. Osborne. Finite Algorithms in Optimization and Data Analysis. John Wiley &

Sons  Inc.  New York  NY  USA  1985.

[RCL19] M. F. Rios  J. Calder  and G. Lerman. Algorithms for `p-based semi-supervised learning

on graphs. CoRR  abs/1901.05031  2019.

[Ric64] J.R. Rice. The Approximation of Functions  By John R. Rice. Addison-Wesley Series in

Computer Science and Information Processing. 1964.

[Rio19] M. F. Rios. Laplacian_lp_graph_ssl. https://github.com/mauriciofloresML/

Laplacian_Lp_Graph_SSL  2019.

[ST17] D. Slepcev and M. Thorpe. Analysis of p-laplacian regularization in semi-supervised

learning. CoRR  abs/1707.06213  2017.

[SV16a] D. Straszak and N. K. Vishnoi. IRLS and slime mold: Equivalence and convergence.

CoRR  abs/1601.02712  2016.

[SV16b] D. Straszak and N. K. Vishnoi. Natural algorithms for ﬂow problems. In Proceedings of
the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms  SODA 2016 
Arlington  VA  USA  January 10-12  2016  pages 1868–1883  2016.

[SV16c] D. Straszak and N. K. Vishnoi. On a natural dynamics for linear programming. In
Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer
Science  Cambridge  MA  USA  January 14-16  2016  page 291  2016.

[VB99] R. A. Vargas and C. S. Burrus. Adaptive iterative reweighted least squares design of
lp ﬁr ﬁlters. In 1999 IEEE International Conference on Acoustics  Speech  and Signal
Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)  volume 3  pages 1129–1132
vol.3  March 1999.

[VB12] R. A. Vargas and C. S. Burrus. Iterative design of lp digital ﬁlters. CoRR  abs/1207.4526 

2012.

[ZB11] X. Zhou and M. Belkin. Semi-supervised learning by higher order regularization. In
Geoffrey Gordon  David Dunson  and Miroslav Dudík  editors  Proceedings of the
Fourteenth International Conference on Artiﬁcial Intelligence and Statistics  volume 15
of Proceedings of Machine Learning Research  pages 892–900  Fort Lauderdale  FL 
USA  11–13 Apr 2011. PMLR.

[ZBL+04] D. Zhou  O. Bousquet  TN. Lal  J. Weston  and B. Schölkopf. Learning with local and
global consistency. In Advances in Neural Information Processing Systems 16  pages
321–328  Cambridge  MA  USA  June 2004. Max-Planck-Gesellschaft  MIT Press.

11

[ZGL03] X. Zhu  Z. Ghahramani  and J. D. Lafferty. Semi-supervised learning using gaussian
ﬁelds and harmonic functions. In Proceedings of the 20th International conference on
Machine learning (ICML-03)  pages 912–919  2003.

[Zhu05] X. J. Zhu. Semi-supervised learning literature survey. Technical report  University of

Wisconsin-Madison Department of Computer Sciences  2005.

12

,Deeksha Adil
Richard Peng
Sushant Sachdeva