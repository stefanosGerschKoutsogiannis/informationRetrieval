2018,Differentially Private Uniformly Most Powerful Tests for Binomial Data,We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP)  optimizing finite sample performance. We show that in general  DP hypothesis tests can be written in terms of linear constraints  and for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure  we prove a ‘Neyman-Pearson lemma’ for binomial data under DP  where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable  whose distribution we coin “Truncated-Uniform-Laplace” (Tulap)  a generalization of the Staircase and discrete Laplace distributions. Furthermore  we obtain exact p-values  which are easily computed in terms of the Tulap random variable. We show that our results also apply to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that our tests have exact type I error  and are more powerful than current techniques.,Differentially Private Uniformly Most Powerful Tests

for Binomial Data

Jordan Awan

Department of Statistics
Penn State University

University Park  PA 16802

awan@psu.edu

Aleksandra Slavkovi´c
Department of Statistics
Penn State University

University Park  PA 16802

sesa@psu.edu

Abstract

We derive uniformly most powerful (UMP) tests for simple and one-sided hypothe-
ses for a population proportion within the framework of Differential Privacy (DP) 
optimizing ﬁnite sample performance. We show that in general  DP hypothesis tests
can be written in terms of linear constraints  and for exchangeable data can always
be expressed as a function of the empirical distribution. Using this structure  we
prove a ‘Neyman-Pearson lemma’ for binomial data under DP  where the DP-UMP
only depends on the sample sum. Our tests can also be stated as a post-processing
of a random variable  whose distribution we coin “Truncated-Uniform-Laplace”
(Tulap)  a generalization of the Staircase and discrete Laplace distributions. Fur-
thermore  we obtain exact p-values  which are easily computed in terms of the
Tulap random variable. We show that our results also apply to distribution-free
hypothesis tests for continuous data. Our simulation results demonstrate that our
tests have exact type I error  and are more powerful than current techniques.

1

Introduction

Differential Privacy (DP)  introduced by DMNS06  offers a rigorous measure of disclosure risk. To
satisfy DP  a procedure cannot be a deterministic function of the sensitive data  but must incorporate
additional randomness  beyond sampling. Subject to the DP constraint  it is natural to search for a
procedure which maximizes the utility of the output. Many works address the goal of minimizing the
distance between the output of the randomized DP procedure and standard non-private algorithms 
but few attempt to infer properties about the underlying population (for some notable exceptions 
see related work)  which is typically the goal in statistics and scientiﬁc research. In this paper  we
study the setting where each individual contributes a sensitive binary value  and we wish to infer the
population proportion via hypothesis tests  subject to DP. In particular  we derive uniformly most
powerful (UMP) tests for simple and one-sided hypotheses  optimizing ﬁnite sample performance.
UMP tests are fundamental to classical statistics  being closely linked to sufﬁciency  likelihood
inference  and conﬁdence sets. However  ﬁnding UMP tests can be hard and in many cases they do
not even exist (see Sch96  Section 4.4). Our results are the ﬁrst to achieve UMP tests under (  δ)−DP 
and are among the ﬁrst steps towards a general theory of optimal inference under DP.
Related work Vu and Slavkovi´c [VS09] are among the ﬁrst to perform hypothesis tests under
DP. They develop private tests for population proportions as well as for independence in 2 × 2
contingency tables. In both settings  they ﬁx the noise adding distribution  and use approximate
sampling distributions to perform these DP tests. A similar approach is used by Sol14 to develop tests
for normally distributed data. The work of VS09 is extended by WLK15 and GLRV16  developing
additional tests for multinomial data. To implement their tests  WLK15 develop asymptotic sampling
distributions  verifying via simulations that the type I errors are reliable. On the other hand  GLRV16

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

use simulations to compute an empirical type I error. Uhler et al. [USF13] develop DP chi-squared
tests and p-values for GWAS data  and derive the exact sampling distribution of their noisy statistic.
Working under “Local Differential Privacy ” a stronger notion of privacy than DP  GR18 develop
multinomial tests based on asymptotic distributions. Given a DP output  She17 and BRC17 develop
signiﬁcance tests for regression coefﬁcients.
Outside the hypothesis testing setting  there is some work on optimal population inference under
DP. Duchi et al. [DJW18] give general techniques to derive minimax rates under local DP  and in
particular give minimax optimal point estimates for the mean  median  generalized linear models  and
nonparametric density estimation. Karwa and Vadhan [KV17] develop nearly optimal conﬁdence
intervals for normally distributed data with ﬁnite sample guarantees  which could potentially be
inverted to give UMP-unbiased tests.
Related work on developing optimal DP mechanisms for general loss functions such as GV16a and
GRS09  give mechanisms that optimize symmetric convex loss functions  centered at a real statistic.
Similarly  AS18 derive optimal mechanisms among the class of K-Norm Mechanisms.
Our contributions The previous literature on DP hypothesis testing has a few characteristics in
common: 1) nearly all of these proposed methods ﬁrst add noise to the data  and perform their test
as a post-processing procedure  2) all of the hypothesis tests use either asymptotic distributions or
simulations to derive approximate decision rules  and 3) while each procedure is derived intuitively
based on classical theory  none show that they are optimal among all possible DP algorithms.
In contrast  in this paper we search over all DP hypothesis tests at level α  deriving the uniformly
most powerful (UMP) test for a population proportion. In Section 3  we show that arbitrary DP
hypothesis tests  which report ‘Reject’ or ‘Fail to Reject’  can be written in terms of linear inequalities.
In Theorem 3.2  we show that for exchangeable data  DP tests need only depend on the empirical
distribution. We use this structure to ﬁnd closed-form DP-UMP tests for simple hypotheses in
Theorems 4.5 and 5.2  and extend these results to obtain one-sided DP-UMP tests in Corollary 5.3.
These tests are closely tied to our proposed “Truncated-Uniform-Laplace” (Tulap) distribution  which
extends both the discrete Laplace distribution (studied in GRS09)  and the Staircase distribution of
GV16a to the setting of (  δ)-DP. We prove that the Tulap distribution satisﬁes (  δ)-DP in Theorem
6.1. While the tests developed in the previous sections only resulted in the output ‘Reject’ or ‘Fail to
Reject’  in Section 6  we show that our DP-UMP tests can be stated as a post-processing of a Tulap
random variable. From this formulation  we obtain exact p-values via Theorem 6.2 and Algorithm 1
which agree with our DP-UMP tests. In Section 7  we show that our results apply to distribution-free
hypothesis tests of continuous data. In Section 8  we verify through simulations that our UMP tests
have exact type I error  and are more powerful than current techniques.

2 Background and notation

We use capital letters to denote random variables and lowercase letters for particular values. For
a random variable X  we denote FX as its cumulative distribution function (cdf)  fX as either its
probability density function (pdf) or probability mass function (pmf)  depending on the context.
For any set X   the n-fold cartesian product of X is X n = {(x1  x2  . . .   xn) | xi ∈ X }. We
denote elements of X n with an underscore to emphasize that they are vectors. The Hamming
distance metric on X n is H : X n × X n → Z≥0  deﬁned by H(x  x(cid:48)) = #{i | xi (cid:54)= x(cid:48)
i}.
Differential Privacy  introduced by DMNS06  provides a formal measure of disclosure risk. The
notion of DP that we give in Deﬁnition 2.1 more closely resembles the formulation in WZ10  which
uses the language of distributions rather than random mechanisms. It is important to emphasize that
the notion of Differential Privacy in Deﬁnition 2.1 does not involve any distribution model on X n.
Deﬁnition 2.1 (Differential Privacy: DMNS06  WZ10). Let  > 0  δ ≥ 0  and n ∈ {1  2  . . .} be
given. Let X be any set  and (Y   F ) be a measurable space. Let P = {Px | x ∈ X n} be a set of
probability measures on (Y   F ). We say that P satisﬁes (  δ)-Differential Privacy ((  δ) - DP) if
for all B ∈ F and all x  x(cid:48) ∈ X n such that H(x  x(cid:48)) = 1  we have Px(B) ≤ ePx(cid:48)(B) + δ.
In Deﬁnition 2.1  we interpret x ∈ X n as the database we collect  where X is the set of possible
values that one individual can contribute  and Y ∼ Px as the statistical result we report to the public.
With this interpretation  if a set of distributions satisﬁes (  δ)-DP for small values of  and δ  then if
one person’s data is changed in the database  the distribution of Y does not change much. Ideally  is

2

n allows us to disregard events which have small probability. We

a small value less than 1  and δ (cid:28) 1
refer to (  0)-DP as pure DP  and (  δ)-DP as approximate DP.
The focus of this paper is to ﬁnd uniformly most powerful (UMP) hypothesis tests  subject to DP. As
the output of a DP method is necessarily a random variable  we work with randomized hypothesis
tests  which we review in Deﬁnition 2.2. Our notation follows that of Sch96  Chapter 4.
iid∼ fθ  where θ ∈ Θ.
Deﬁnition 2.2 (Hypothesis Test). Let (X1  . . .   Xn) ∈ X n be distributed Xi
Let Θ0  Θ1 be a partition of Θ. A (randomized) test of H0 : θ ∈ Θ0 versus H1 : θ ∈ Θ1 is a
Efθ φ ≤ α.The power
measurable function φ : X n → [0  1]. We say a test φ is at level α if supθ∈Θ0
of φ at θ is denoted βφ(θ) = Efθ φ.
Let Φ be a set of tests. We say that φ∗ ∈ Φ is the uniformly most powerful level α (UMP-α) test
among Φ for H0 : θ ∈ Θ0 versus H1 : θ ∈ Θ1 if 1) supθ∈Θ0 βφ∗ (θ) ≤ α and 2) for any φ ∈ Φ such
that supθ∈Θ0 βφ(θ) ≤ α we have βφ∗ (θ) ≥ βφ(θ)  for all θ ∈ Θ1.
In Deﬁnition 2.2  φ(x) is the probability of rejecting the null hypothesis  given that we observe
x ∈ X n. That is  the output of a test is either ‘Reject’  or ‘Fail to Reject’ with respective probabilities
φ(x)  and 1 − φ(x). While the condition of (  δ)-DP does not involve the randomness of X  for
hypothesis testing  the level  and power of a test depend on the model for X. In Section 3  we study
the set of hypothesis tests which satisfy (  δ)-DP.

3 Problem setup and exchangeability condition
We begin this section by considering arbitrary hypothesis testing problems under DP. Let φ : X n →
[0  1] be any test. Since the only possible outputs of the mechanism are ‘Reject’ or ‘Fail to Reject’
with probabilities φ(x) and 1 − φ(x)  the test φ satisﬁes (  δ)-DP if and only if for all x  x(cid:48) ∈ X n
such that H(x  x(cid:48)) = 1 

φ(x) ≤ eφ(x(cid:48)) + δ

and

(1 − φ(x)) ≤ e(1 − φ(x(cid:48))) + δ.

(1)
Remark 3.1. For any simple hypothesis test  where Φ0 and Φ1 are both singleton sets  the DP-UMP
test φ∗ is the solution to a linear program. If X is ﬁnite  this observation allows one to explore the
structure of DP-UMP tests through numerical linear program solvers.
Given the random vector X ∈ X n  initially it may seem that we need to consider all φ  which
are arbitrary functions of X. However  assuming that X is exchangeable  Theorem 3.2 below says
that for any DP hypothesis tests  we need only consider tests which are functions of the empirical
distribution of X. In other words  φ need not consider the order of the entries in X. This result is
reminiscent of De Finetti’s Theorem (see Sch96  Theorem 1.48) in classical statistics.
Theorem 3.2. Let Θ be a set and {µθ}θ∈Θ be a set of exchangeable distributions on X n. Let
φ : X n → [0  1] be a test satisfying (1). Then there exists φ(cid:48) : X n → [0  1] satisfying (1) which

only depends on the empirical distribution of X  such that(cid:82) φ(cid:48)(x) dµθ =(cid:82) φ(x) dµθ  for all θ ∈ Θ.
For any π ∈ σ(n)  φ(π(x)) satisﬁes (  δ)-DP. By exchangeability (cid:82) φ(π(x)) dµθ =(cid:82) φ(x) dµθ.

π∈σ(n) φ(π(x))  where σ(n) is the symmetric group on n letters.

Proof. Deﬁne φ(cid:48) by φ(cid:48)(x) = 1

(cid:80)

n!

Since condition 1 is closed under convex combinations  and integrals are linear  the result follows.

unknown. Then the statistic X =(cid:80)n

We now state the particular problem which is the focus for the remainder of the paper  where each
individual contributes a sensitive binary value to the database. Let X ∈ {0  1}n be a random
iid∼ Bern(θ)  where θ is
vector  where Xi is the sensitive data of individual i. We model X as Xi
i=1 Xi ∼ Binom(n  θ) encodes the empirical distribution of
X. By Theorem 3.2  we can restrict our attention to tests which are functions of X. Such tests
φ : {0  1  . . .   n} → [0  1] satisfy (  δ) -DP if and only if for all x ∈ {1  2  . . .   n} 

φ(x) ≤ eφ(x − 1) + δ

φ(x − 1) ≤ eφ(x) + δ

3

(2)
(3)

(1 − φ(x)) ≤ e(1 − φ(x − 1)) + δ

(1 − φ(x − 1)) ≤ e(1 − φ(x)) + δ.

 δ =(cid:8)φ : φ satisﬁes (2)-(5)(cid:9).

(4)
(5)

We denote the set of all tests which satisfy (2)-(5) as D n
Remark 3.3. For arbitrary DP hypothesis testing problems  the number of constraints generated by
(1) could be very large  even inﬁnite  but for our problem we only have 4n constraints.

4 Simple DP-UMP tests when δ = 0

In this section  we derive the DP-UMP test when δ = 0 for simple hypotheses. In particular  given
n   > 0  α > 0  θ0 < θ1  and X ∼ Binom(n  θ)  we ﬁnd the UMP test at level α among D n
 0 for
testing H0 : θ = θ0 versus H1 : θ = θ1.
Before developing these tests  we introduce the Truncated-Uniform-Laplace (Tulap) distribution 
deﬁned in Deﬁnition 4.1  which is central to all of our main results. To motivate this distribution 
recall that GV16a show for general loss functions  adding discrete Laplace noise L ∼ DLap(e−) to
X is optimal under (  0)-DP. For this reason  it is natural to consider a test which post-processes
X + L. However  we know by classical UMP theory that since X + L is discrete  a randomized test
is required. Instead of using a randomized test  by adding uniform noise U ∼ Unif(−1/2  1/2) to
X + L  we obtain a continuous sampling distribution  from which a deterministic test is available.
We call the distribution of (X + L + U ) | X as Tulap(X  b  0). The distribution Tulap(X  b  q) is
obtained by truncating within the central (1 − q)th-quantiles of Tulap(X  b  0).
In Deﬁnition 4.1  we use the nearest integer function [·] : R → Z. For any real number t ∈ R  [t] is
deﬁned to be the integer nearest to t. If there are two distinct integers which are nearest to t  we take
[t] to be the even one. Note that  [−t] = −[t] for all t ∈ R.
Deﬁnition 4.1 (Truncated-Uniform-Laplace (Tulap)). Let N and N0 be real-valued random variables.
Let m ∈ R  b ∈ (0  1) and q ∈ [0  1). We say that N0 ∼ Tulap(m  b  0) and N ∼ Tulap(m  b  q) if
N0 and N have the following cdfs:

if x ≤ [m]
if x > [m] 

FN0 (x) =

(cid:40) b−[x−m]

1+b

1 − b[x−m]

1+b

(cid:0)b + (x − m − [x − m] + 1
2 )(1 − b)(cid:1)
(cid:0)b + ([x − m] − (x − m) + 1
2 )(1 − b)(cid:1)
0

if FN0 < q/2
2 ≤ FN0(x) ≤ 1 − q
if q
if FN0 > 1 − q
2 .

FN0 (x)− q

1−q

1

2

FN (x) =

2

Note that a Tulap random variable Tulap(m  b  q) is continuous and symmetric about m.
Remark 4.2. The Tulap distribution extends the staircase and discrete Laplace distributions as
follows: Tulap(0  b  0) d= Staircase(b  1/2) and [Tulap(0  b  0)] d= DLap(b)  where Staircase(b  γ)
is the distribution in GV16a. GV16a show that for a real valued statistic T and convex symmetric
loss functions centered at T   the optimal noise distribution for -DP is Staircase(b  γ) for b = e−
and some γ ∈ (0  1). If the statistic is a count  then GRS09 show that DLap(b) is optimal. Our
results agree with these works when δ = 0  and extend them to the case of arbitrary δ.

Now that we have deﬁned the Tulap distribution  we are ready to develop the UMP test among D n
 0
for the simple hypotheses H0 : θ = θ0 versus H1 : θ = θ1. In classical statistics  the UMP for this
test is given by the Neyman-Pearson lemma  however in the DP framework  our test must satisfy
(2)-(5). Within these constraints  we follow the logic behind the Neyman-Pearson lemma as follows.
Let φ ∈ D n
 0. Thinking of φ(x) deﬁned recursively  equations (2)-(5) give upper and lower bounds
for φ(x) in terms of φ(x − 1). Since θ1 > θ0  and binomial distributions have a monotone likelihood
ratio (MLR) in X  larger values of X give more evidence for θ1 over θ0. Thus  φ(x) should be
increasing in x as much as possible  subject to (2)-(5). Lemma 4.3 shows that taking φ(x) to be such
a function is equivalent to having φ(x) be the cdf of a Tulap random variable.
Lemma 4.3. Let  > 0 be given. Let φ : {0  1  2  . . .   n} → (0  1). The following are equivalent:
1) There exists m ∈ (0  1) such that φ(0) = m and φ(x) = min{eφ(x− 1)  1− e−(1− φ(x− 1))}

for x = 1  . . .   n.

4

2) There exists m ∈ (0  1) such that φ(0) = m and for x = 1  . . .   n 

(cid:40)

φ(x) =

eφ(x − 1)
1 − e−(1 − φ(x − 1))

if φ(x − 1) ≤ 1
if φ(x − 1) > 1

1+e
1+e .

3) There exists m ∈ R such that φ(x) = FN0 (x − m) for x = 0  1  2  . . .   n  where N0 ∼

Tulap(0  b = e−  0).

Proof Sketch. First show that 1) and 2) are equivalent by checking which constraint is active. Then
verify that FN0 (x − m) satisﬁes the recurrence of 2). This can be done using the properties of the
Tulap cdf  stated in Lemma 10.2  found in the Supplementary Material.

While the form of 1) in Lemma 4.3 is intuitive  the connection to the Tulap cdf in 3) allows for a usable
closed-form of the test. This connection with the Tulap distribution is crucial for the development
in Section 6  which shows that the test in Lemma 4.3 can be achieved by post-processing X + N 
where N is distributed as Tulap.
It remains to show that the tests in Lemma 4.3 are in fact UMP among D n
prove this is Lemma 4.4  which is a standard result in the classical hypothesis testing theory.
Lemma 4.4. Let (X   F   µ) be a measure space and let f and g be two densities on X with respect

to µ. Suppose that φ1  φ2 : X → [0  1] are such that(cid:82) φ1f dµ ≥(cid:82) φ2f dµ  and there exists k ≥ 0
such that φ1 ≥ φ2 when g ≥ kf and φ1 ≤ φ2 when g < kf. Then(cid:82) φ1g dµ ≥(cid:82) φ2g dµ.
(cid:82) (φ1 − φ2)(g − kf ) dµ ≥ 0. Hence (cid:82) φ1g dµ −(cid:82) φ2g dµ ≥ k(cid:0)(cid:82) φ1f dµ −(cid:82) φ2f dµ(cid:1) ≥ 0.

Proof. Note that (φ1 − φ2)(g − kf ) ≥ 0 for almost all x ∈ X (with respect to µ). This implies that

 0. The main tool used to

Next we present our key result  Theorem 4.5  which can be viewed as a ‘Neyman-Pearson lemma’ for
binomial data under (  0)-DP. We extend this result in Theorem 5.2 for (  δ)-DP.
Theorem 4.5. Let  > 0  α ∈ (0  1)  0 ≤ θ0 < θ1 ≤ 1  and n ≥ 1 be given. Observe X ∼
Binom(n  θ)  where θ is unknown. Set the decision rule φ∗ : Z → [0  1] by φ∗(x) = FN0(x − m) 
where N0 ∼ Tulap(0  b = e−  0) and m is chosen such that Eθ0φ∗(x) = α. Then φ∗ is UMP-α
test of H0 : θ = θ0 versus H1 : θ = θ1 among D n
 0.
Proof Sketch. Let φ be any other test which satisﬁes (2)-(5) at level α. Then  since φ∗ can be
written in the form of 1) in Lemma 4.3  there exists y ∈ Z such that φ∗(x) ≥ φ(x) when x ≥ y
and φ∗(x) ≤ φ(x) when x < y. By MLR of the binomial distribution and Lemma 4.4  we have
βφ∗ (θ1) ≥ βφ(θ1).

While the classical Neyman-Pearson lemma results in an acceptance and rejection region  the DP-
UMP always has some probability of rejecting the null  due to the constraints (2)-(5). As  ↑ ∞  the
DP-UMP converges to the non-private UMP.
5 Simple and one-sided DP-UMP tests when δ ≥ 0
In this section  we extend the results of Section 4 to allow for δ ≥ 0. We begin by proposing the
form of the DP-UMP test for simple hypotheses. As in Section 4  the DP-UMP test is increasing in
x as much as (2)-(5) allow. Lemma 5.1 states that such a test can be written as the cdf of a Tulap
random variable  where the parameter q depends on  and δ. We omit the proof of Theorem 5.2 
which mimics the proof of Theorem 4.5.
Lemma 5.1. Let  > 0 and δ ≥ 0 be given and set b = e− and q =
φ : {0  1  2  . . .   n} → [0  1]. The following are equivalent:
1) There exists y ∈ {0  1  2  . . .   n} and m ∈ (0  1) such that

1−b+2δb . Let

2δb

0

φ(x) =

m
min{eφ(x − 1) + δ 

1 − e−(1 − φ(x − 1)) + e−δ 

1}

if x < y
if x = y
if x > y.

5

2) There exists y ∈ {0  1  2  . . .   n} and m ∈ (0  1) such that
if x < y
if x = y
if x > y and φ(x − 1) ≤ 1−δ
if x > y and 1−δ
if x > y and φ(x − 1) > 1 − δ.

0
m
eφ(x − 1) + δ
1 − e−(1 − φ(x − 1)) + e−δ
1

φ(x) =

1+e

1+e ≤ φ(x − 1) ≤ 1 − δ



3) There exists m ∈ R such that φ(x) = FN (x − m) where N ∼ Tulap(0  b  q).

Proof Sketch. The equivalence of 1) and 2) only requires determining which constraints are active.
To show the equivalence of 2 and 3  we verify that FN (x− m) satisﬁes the recurrence of 2)  using the
expression of FN (x) in terms of FN0(x) given in Deﬁnition 4.1  and the results of Lemma 4.3.
Theorem 5.2. Let  > 0  δ ≥ 0  α ∈ (0  1)  0 ≤ θ0 < θ1 ≤ 1  and n ≥ 1 be given. Observe
X ∼ Binom(n  θ)  where θ is unknown. Set b = e− and q = 2δb
1−b+2δb . Deﬁne φ∗ : Z → [0  1] by
φ∗(x) = FN (x − m) where N ∼ Tulap(0  b  q) and m is chosen such that Eθ0φ∗(x) = α. Then φ∗
is UMP-α test of H0 : θ = θ0 versus H1 : θ = θ1 among D n
 δ.

So far we have focused on simple hypothesis tests  but since our test only depends on θ0  and not on
θ1  our test is in fact the DP-UMP for one-sided tests  as stated in Corollary 5.3. Corollary 5.3 also
shows that we can use our tests to build DP-UMP tests for H0 : θ ≥ θ0 versus H1 : θ < θ0 as well.
Hence  Corollary 5.3 is our most general result so far  containing Theorems 4.5 and 5.2 as special
cases.
Corollary 5.3. Let X ∼ Binom(n  θ). Set φ∗(x) = FN (x − m1) and ψ∗(x) = 1 − FN (x − m2) 
where N ∼ Tulap
and m1  m2 are chosen such that Eθ0φ∗(x) = α and
 δ for testing H0 : θ ≤ θ0 versus H1 : θ > θ0  and
Eθ0 ψ∗(x) = α. Then φ∗(x) is UMP-α among D n
ψ∗(x) is UMP-α among D n

 δ for testing H0 : θ ≥ θ0 versus H1 : θ < θ0.

0  b = e−  q = 2δb

1−b+2δb

(cid:16)

(cid:17)

6 Optimal one-sided private p-values

For the DP-UMP tests developed in Sections 4 and 5  the output is simply to ‘Reject’ or ‘Fail to
Reject’ H0. In scientiﬁc research  however  p-values are often used to weigh the evidence in favor of
the alternative hypothesis over the null. Informally  a p-value is the smallest level α  for which a test
outputs ‘Reject’. A more formal deﬁnition is given in Deﬁnition 10.4  in the Supplementary Material.
In this section  we show that our proposed DP-UMP tests can be achieved by post-processing a Tulap
random variable. Using this  we develop a differentially private algorithm for releasing a private
p-value which agrees with the DP-UMP tests in Sections 4 and 5. While we state our p-values for
one-sided tests  they also apply to simple tests as a special case.
Since our DP-UMP test from Theorem 5.2 rejects with probability φ∗(x) = FN (x − m)  given
N ∼ FN   φ∗(x) rejects the null if and only if X + N ≥ m. So  our DP-UMP tests can be stated as
a post-processing of X + N. Theorem 6.1 states that releasing X + N satisﬁes (  δ)-DP. By the
post-processing property of DP (see DR14  Proposition 2.1)  once we release X + N  any function
of X + N also satisﬁes (  δ)-DP. Thus  we can compute our private UMP-α tests as a function of
X + N for any α. The smallest α for which we reject the null is the p-value for that test. In fact
Algorithm 1 and Theorem 6.2 give a more elegant method of computing this p-value.
Theorem 6.1. Let X be any set  and T : X n → Z  with ∆(T ) = sup|T (x) − T (x(cid:48))| = 1  where
the supremum is over the set {(x  x(cid:48)) ∈ X n × X n | H(x  x(cid:48)) = 1}. Then the set of distributions
Tulap

T (x)  b = e− 

satisﬁes (  δ)-DP.

(cid:17)(cid:12)(cid:12)(cid:12)x ∈ X n(cid:111)

1−b+2δb

(cid:110)

(cid:16)

2δb

Proof Sketch. Since Tulap random variables are continuous and have MLR in T (x)  by Lemma 10.3
in the Supplementary Material  it sufﬁces to show that for all t ∈ R  the cdf of a Tulap random
variable FN (t− T (x)) satisﬁes (1)  with φ(x) replaced with FN (t− T (x)). This already established
in Lemma 5.1  by the equivalence of 1) and 3).

6

Theorem 6.2. Let  > 0  δ ≥ 0  X ∼ Binom(n  θ) where θ is unknown  and Z|X ∼ Tulap(X  b =
e−  q = 2δb
1) p(θ0  Z) := P (X + N ≥ Z | Z) is a p-value for H0 : θ ≤ θ0 versus H1 : θ > θ0  where the

1−b+2δb ). Then

probability is over X ∼ Binom(n  θ0) and N ∼ Tulap(0  b  q).

2) Let 0 < α < 1 be given. The test φ∗(x) = PZ∼Tulap(x b q)(p(θ0  Z) ≤ α | X) is UMP-α for

H0 : θ ≤ θ0 versus H1 : θ > θ0 among D n
 δ.
3) The output of Algorithm 1 is equal to p(θ0  Z).

It follows from Theorem 6.2 that p(θ0  Z) is the stochastically smallest possible p-value for the
hypothesis test H0 : θ ≤ θ0 versus H1 : θ > θ0 under (  δ)-DP. Note that 1 − p(θ0  Z) =
P (X + N ≤ Z | Z) is the p-value for H0 : θ ≥ θ0 versus H1 : θ < θ0  which agrees with the
UMP-α test in Corollary 5.3.

Algorithm 1 UMP one-sided p-value for binomial data under (  δ)-DP
INPUT: n ∈ N  θ0 ∈ (0  1)   > 0  δ ≥ 0  Z ∼ Tulap
1: Set FN as the cdf of N ∼ Tulap(0  b  q)
2: Set F = (FN (0 − Z)  FN (1 − Z)  . . .   FN (n − Z))(cid:62)

X  b = e−  q = 2δb

(cid:1)θ0
3: Set B = ((cid:0)n
0 (1 − θ0)n−0 (cid:0)n

0 (1 − θ0)n−1  . . .  (cid:0)n
(cid:1)θ1
(cid:1)θn

0 (1 − θ0)n−n)(cid:62)

1−b+2δb

n

1

OUTPUT: F (cid:62)B

0

(cid:16)

(cid:17)

 

To implement Algorithm 1  we must be able to sample a Tulap random variable  which Algorithm
2 provides. The algorithm is based on the expression of Tulap(m  b  0) in terms of geometric
and uniform variables  and uses rejection sampling when q > 0 (see Bis06  Chapter 11 for an
introduction to rejection sampling). A detailed proof that the output of this algorithm follows the
correct distribution can be found in Lemma 10.1 in the Supplementary Material.
Algorithm 2 Sample from Tulap distribution: N ∼ Tulap(m  b  q)
INPUT: m ∈ R  b ∈ (0  1)  q ∈ [0  1).
1: Draw G1  G2
2: Set N = G1 − G2 + U + m
3: If FN0 (N ) < q/2 or FN0 (N ) > 1 − q/2  where N0 ∼ Tulap(m  b  0)  go to 1:
OUTPUT: N

iid∼ Geom(1 − b) and U ∼ Unif(−1/2  1/2)

Remark 6.3. Since we know that releasing X + N  where N is a Tulap random variable  satisﬁes
(  δ)-DP  we can compute more than just p-values by post-processing X + N. We can also compute
point estimates for θ  derive posterior distribution of θ given a prior  and compute conﬁdence intervals
for θ as post-processing of X + N. In the full version of this paper  we will study each of these
objectives  and connect conﬁdence intervals with the DP-UMP tests derived here.
Remark 6.4. One may wonder about the asymptotic properties of the DP-UMP test. It is not hard
to show that for any ﬁxed  > 0  δ  and θ0 ∈ (0  1)  our proposed DP-UMP test has asymptotic
relative efﬁciency (ARE) of 1  relative to the non-private UMP test (see vdV00  Section 14.3 for
an introduction to ARE). Let X ∼ Binom(n  θ0). Deﬁne the two test statistics as T1 = X and
T2 = X + N  where N ∼ Tulap(0  b  q). The ARE of the DP-UMP relative to the non-private UMP
test is (C2/C1)2  where Ci = lim
n→∞
EθTi = nθ  Varθ0(T1) = nθ0(1 − θ0)  and Varθ0 (T2) = nθ0(1 − θ0) + Var(N ). Since Var(N ) is
a constant  we have that C1 = C2 = (θ0(1 − θ0))−1/2.

(cid:19)(cid:46)(cid:112)n Varθ0(Ti)  for i = 1  2. We compute

(cid:12)(cid:12)(cid:12)θ=θ0

(cid:18) d

EθTi

dθ

7 Application to distribution-free inference

In this section  we show how our DP-UMP tests for count data can be used to test certain hypotheses
for continuous data. In particular  we give a DP version of the sign and median test allowing one to
test the median of either paired or independent samples. For an introduction to the sign and median
tests  see Sections 5.4 and 6.4 of GC14. Let  > 0 and δ ∈ [0  1) be given  and let N ∼ Tulap(0  b  q)
for b = e− and q = 2δb

1−b−2δb.

7

d= X and
Sign test: We observe n iid pairs (Xi  Yi) for i = 1  . . .   n. Then for all i = 1  . . .   n  Xi
d= Y for some random variables X and Y . We assume that for any pair (Xi  Yi) we can determine
Yi
if Xi > Yi or not. For simplicity  we also assume that there are no pairs with Xi = Yi. Denote the
unknown probability θ = P (X > Y ). We want to test a hypothesis such as H0 : θ ≤ θ0 versus
H1 : θ > θ0. The sign test uses the test statistic T = #{Xi > Yi}. Since the sensitivity of T is 1  by
Theorem 6.1  T + N satisﬁes (  δ)-DP. Note that the test statistic is distributed as T ∼ Binom(n  θ).
Using Algorithm 1  we obtain a private p-value for the sign test as a post-processing of T + N.
Median test: We observe two independent sets of iid data {Xi}n
i=1  where all Xi
and Yi are distinct values  and we have a total ordering on these values. Then there exists random
d= Y for all i. We want to test H0 : median(X) ≤
variables X and Y such that Xi
median(Y ) versus H1 : median(X) > median(Y ). The median test uses the test statistic T =
#{i | rank(Xi) > n}  where rank(Xi) = #{Xj ≤ Xi} + #{Yj ≤ Xi}. Since the sensitivity
of T is 1  by Theorem 6.1  T + N satisﬁes (  δ)-DP. When median(X) = median(Y )  T ∼
HyperGeom(n = n  m = n  k = n). Using Algorithm 1  with B replaced with the pmf of
HyperGeom(n = n  m = n  k = n)  we obtain a private p-value for the median test as a post-
processing of T + N.

i=1 and {Yi}n

d= X and Yi

8 Simulations

In this section  we study both the empirical power and the empirical type I error of our DP-UMP
test against the normal approximation proposed by VS09. We deﬁne the empirical power to be the
proportion of times a test ‘Rejects’ when the alternative is true  and the empirical type I error as the
proportion of times a test ‘Rejects’ when the null is true. For our simulations  we focus on small
samples as the noise introduced by DP methods is most impactful in this setting.
In Figure 1  we plot the empirical power of our UMP test  the Normal Approximation from VS09 
and the non-private UMP. For each n  we generate 10 000 samples from Binom(n  .95). We privatize
each X by adding N ∼ Tulap(0  e−  0) for the DP-UMP and L ∼ Lap(1/) for the Normal
Approximation. We compute the UMP p-value via Algorithm 1 and the approximate p-value for X+L 

using the cdf of N(cid:0)X  n/4 + 2/2(cid:1). The empirical power is given by (10000)−1#{p-value< .05}.

The DP-UMP test indeed gives higher power compared to the Normal Approximation  but the
approximation does not lose too much power. Next we see that type I error is another issue.
In Figure 2 we plot the empirical type I error of the DP-UMP and the Normal Approximation tests.
We ﬁx  = 1 and δ = 0  and vary θ0. For each θ0  we generate 100 000 samples from Binom(30  θ0).
For each sample  we compute the DP-UMP and Normal Approximation tests at type I error α = .05.
We plot the proportion of times we reject the null as well as moving average curves. The DP-UMP 
which is provably at type I error α = .05 achieves type I error very close to .05  but the Normal
Approximation has a higher type I error for small values of θ0  and a lower type I error for large
values of θ0.

9 Discussion and future directions

In this paper  we derived uniformly most powerful simple and one-sided tests for binary data among
all DP α-level tests. Previously  while various hypothesis tests under DP have been proposed  none
have satisﬁed such an optimality criterion. While our initial DP-UMP tests only output ‘Reject’ or
‘Fail to Reject’  we showed that they can be achieved by post-processing a noisy sufﬁcient statistic.
This allows us to produce private p-values which agree with the DP-UMP tests. Our results can also
be applied to obtain p-values for distribution-free tests  to test some hypotheses about continuous
data under DP.
A simple  yet fundamental observation that underlies our results is that DP tests can be written in
terms of linear constraints. This idea alone allows for a new perspective on DP hypothesis testing 
which is particularly applicable to other discrete problems  such as multinomial models or difference
of population proportions. Stating the problem in this form allows for the consideration of all possible
DP tests  and allows the exploration of UMP tests through numerical linear program solvers.
While the focus of this work is on hypothesis testing  these results can also be applied to obtain
optimal length conﬁdence intervals for binomial data. In fact  classical statistical theory establishes a

8

Figure 1: Empirical power for UMP and Normal
Approximation tests for H0 : θ ≤ .9 versus
H1 : θ ≥ .9. The true value is θ = .95.  = 1
and δ = 0. n varies along the x-axis.

Figure 2: Empirical type I error α for UMP and
Normal Approximation tests for H0 : θ ≤ θ0
versus H1 : θ ≥ θ0. θ0 varies along the x-axis.
n = 30   = 1  and δ = 0. Target is α = .05.

connection between UMP tests and Uniformly Most Accurate (UMA) conﬁdence intervals. Besides
conﬁdence intervals  the p-value function for the test H0 : θ ≥ θ0 versus H1 : θ < θ0 is a cdf
which generates a conﬁdence distribution; see XS13 for a review. Since this p-value corresponds
to the DP-UMP test  this conﬁdence distribution is stochastically more concentrated about the true
θ  than any other private conﬁdence distribution. In the full paper  we plan to explore conﬁdence
intervals and conﬁdence distributions in detail  establishing connections between our approach here
and optimal inference in these settings.
We showed that for exchangeable data  DP tests need only depend on the empirical distribution. For
binary data  the empirical distribution is equivalent to the sample sum  which is a complete sufﬁcient
statistic for the binomial model. However  in general it is not clear whether optimal DP tests are
always a function of complete sufﬁcient statistics as is the case for classical UMP tests. It would be
worth investigating whether there is a notion of sufﬁciency which applies for DP tests.
When δ = 0  our optimal noise adding mechanism  the proposed Tulap distribution  is related to
the discrete Laplace distribution  which GRS09 and GV16a also found is optimal for a general class
of loss functions. For δ > 0  a truncated discrete Laplace distribution is optimal for our problem.
Little previous work has looked into optimal noise adding mechanisms for approximate DP. GV16b
studied this problem to some extent  but did not explore truncated Laplace distributions. Steinke
[Ste18] proposes that truncated Laplace can be viewed as the canonical distribution for approximate
DP in a way that Laplace is canonical for pure DP. Further exploration in the use of truncated Laplace
distributions in the approximate DP setting may be of interest.

Acknowledgements

We would like to thank Vishesh Karwa and Matthew Reimherr for helpful discussions and feedback
on previous drafts. We also thank the reviewers for their helpful comments and suggestions  which
have contributed to many improvements in the presentation of this work. This work is supported in
part by NSF Award No. SES-1534433 to The Pennsylvania State University.

References

[AS18] Jordan Awan and Aleksandra Slavkovi´c. Structure and sensitivity in differential privacy:

Comparing k-norm mechanisms. ArXiv e-prints  January 2018. Under Review.

9

0.20.40.60.81.0nempirical power163264128256512lllllllDP UMPDP Normal ApproximationNon−private UMP0.0400.0450.0500.0550.060thetaempirical type I error0.050.150.250.350.450.550.650.750.850.95llllllllllllllllllllUMPNormal Approximation[Bis06] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science

and Statistics). Springer-Verlag New York  Inc.  Secaucus  NJ  USA  2006.

[BRC17] A. Barrientos  A. Reiter  J.and Machanavajjhala  and Y. Chen. Differentially private

signiﬁcance tests for regression coefﬁcients. ArXiv e-prints  May 2017.

[CB02] G. Casella and R.L. Berger. Statistical Inference. Duxbury advanced series in statistics

and decision sciences. Thomson Learning  2002.

[DJW18] John C. Duchi  Michael I. Jordan  and Martin J. Wainwright. Minimax optimal proce-
dures for locally private estimation. Journal of the American Statistical Association 
113(521):182–201  2018.

[DMNS06] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating Noise to
Sensitivity in Private Data Analysis  pages 265–284. Springer Berlin Heidelberg  Berlin 
Heidelberg  2006.

[DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy.

Foundations and Trends in Theoretical Computer Science  9:211–407  August 2014.

[GC14] J.D. Gibbons and S. Chakraborti. Nonparametric Statistical Inference  Fourth Edition:

Revised and Expanded. Taylor & Francis  2014.

[GLRV16] Marco Gaboardi  Hyun Lim  Ryan Rogers  and Salil Vadhan. Differentially private
chi-squared hypothesis testing: Goodness of ﬁt and independence testing. In Maria Flo-
rina Balcan and Kilian Q. Weinberger  editors  Proceedings of The 33rd International
Conference on Machine Learning  volume 48 of Proceedings of Machine Learning
Research  pages 2111–2120  New York  New York  USA  20–22 Jun 2016. PMLR.

[GR18] Marco Gaboardi and Ryan Rogers. Local private hypothesis testing: Chi-square tests.
In Jennifer Dy and Andreas Krause  editors  Proceedings of the 35th International
Conference on Machine Learning  volume 80 of Proceedings of Machine Learning
Research  pages 1626–1635  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018.
PMLR.

[GRS09] Arpita Ghosh  Tim Roughgarden  and Mukund Sundararajan. Universally utility-
maximizing privacy mechanisms. In Proceedings of the Forty-ﬁrst Annual ACM Sympo-
sium on Theory of Computing  STOC ’09  pages 351–360  New York  NY  USA  2009.
ACM.

[GV16a] Quan Geng and Pramod Viswanath. The optimal noise-adding mechanism in differential

privacy. IEEE Transactions on Information Theory  62(2):925–951  2 2016.

[GV16b] Quan Geng and Pramod Viswanath. Optimal noise adding mechanisms for approximate

differential privacy. IEEE Trans. Information Theory  62(2):952–969  2016.

[IK06] Seidu Inusah and Tomasz J. Kozubowski. A discrete analogue of the laplace distribution.

Journal of Statistical Planning and Inference  136(3):1090 – 1102  2006.

[KV17] Vishesh Karwa and Salil P. Vadhan. Finite sample differentially private conﬁdence

intervals. CoRR  abs/1711.03908  2017.

[Sch96] M.J. Schervish. Theory of Statistics. Springer Series in Statistics. Springer New York 

1996.

[She17] Or Sheffet. Differentially private ordinary least squares. In Doina Precup and Yee Whye
Teh  editors  Proceedings of the 34th International Conference on Machine Learning  vol-
ume 70 of Proceedings of Machine Learning Research  pages 3105–3114  International
Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[Sol14] Eftychia Solea. Differentially private hypothesis testing for normal random variables.

Master’s thesis  The Pennsylvania State University  May 2014.

[Ste18] Thomas Steinke. Private correspondence  2018.

10

[USF13] Caroline Uhler  Aleksandra Slavkovi´c  and Stephen Fienberg. Privacy-preserving data
sharing for genome-wide association studies". Journal of Privacy and Conﬁdentiality  5 
2013.

[vdV00] A.W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Proba-

bilistic Mathematics. Cambridge University Press  2000.

[VS09] Duy Vu and Aleksandra Slavkovi´c. Differential privacy for clinical trial data: Preliminary
evaluations. In Proceedings of the 2009 IEEE International Conference on Data Mining
Workshops  ICDMW ’09  pages 138–143  Washington  DC  USA  2009. IEEE Computer
Society.

[WLK15] Y. Wang  J. Lee  and D. Kifer. Revisiting Differentially Private Hypothesis Tests for

Categorical Data. ArXiv e-prints  November 2015.

[WZ10] Larry Wasserman and Shuheng Zhou. A statistical framework for differential privacy.

JASA  105:489:375–389  2010.

[XS13] Min-ge Xie and Kesar Singh. Conﬁdence distribution  the frequentist distribution
estimator of a parameter: A review. International Statistical Review  81(1):3–39  2013.

11

,Hao Wu
Frank Noe
Jordan Awan
Aleksandra Slavković