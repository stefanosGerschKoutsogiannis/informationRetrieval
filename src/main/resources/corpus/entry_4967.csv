2012,Semi-supervised Eigenvectors for Locally-biased Learning,In many applications  one has information  e.g.  labels that are  provided in a semi-supervised manner  about a specific target region of a  large data set  and one wants to perform machine learning and data analysis  tasks nearby that pre-specified target region.   Locally-biased problems of this sort are particularly challenging for  popular eigenvector-based machine learning and data analysis tools. At root  the reason is that eigenvectors are inherently global quantities. In this paper  we address this issue by providing a methodology to construct  semi-supervised eigenvectors of a graph Laplacian  and we illustrate  how these locally-biased eigenvectors can be used to perform  locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized  directions of maximum variance  conditioned on being well-correlated with an  input seed set of nodes that is assumed to be provided in a semi-supervised  manner. We also provide several empirical examples demonstrating how these  semi-supervised eigenvectors can be used to perform locally-biased learning.,Semi-supervised Eigenvectors
for Locally-biased Learning

Toke Jansen Hansen

Section for Cognitive Systems

DTU Informatics

Technical University of Denmark

tjha@imm.dtu.dk

Michael W. Mahoney

Department of Mathematics

Stanford University
Stanford  CA 94305

mmahoney@cs.stanford.edu

Abstract

In many applications  one has side information  e.g.  labels that are provided
in a semi-supervised manner  about a speciﬁc target region of a large data set 
and one wants to perform machine learning and data analysis tasks “nearby”
that pre-speciﬁed target region. Locally-biased problems of this sort are partic-
ularly challenging for popular eigenvector-based machine learning and data anal-
ysis tools. At root  the reason is that eigenvectors are inherently global quanti-
ties. In this paper  we address this issue by providing a methodology to construct
semi-supervised eigenvectors of a graph Laplacian  and we illustrate how these
locally-biased eigenvectors can be used to perform locally-biased machine learn-
ing. These semi-supervised eigenvectors capture successively-orthogonalized di-
rections of maximum variance  conditioned on being well-correlated with an input
seed set of nodes that is assumed to be provided in a semi-supervised manner. We
also provide several empirical examples demonstrating how these semi-supervised
eigenvectors can be used to perform locally-biased learning.

1

Introduction

We consider the problem of ﬁnding a set of locally-biased vectors that inherit many of the “nice”
properties that the leading nontrivial global eigenvectors of a graph Laplacian have—for example 
that capture “slowly varying” modes in the data  that are fairly-efﬁciently computable  that can be
used for common machine learning and data analysis tasks such as kernel-based and semi-supervised
learning  etc.—so that we can perform what we will call locally-biased machine learning in a prin-
cipled manner.
By locally-biased machine learning  we mean that we have a very large data set  e.g.  represented as
a graph  and that we have information  e.g.  given in a semi-supervised manner  that certain “regions”
of the data graph are of particular interest. In this case  we may want to focus predominantly on those
regions and perform data analysis and machine learning  e.g.  classiﬁcation  clustering  ranking  etc. 
that is “biased toward” those pre-speciﬁed regions. Examples of this include the following.

• Locally-biased community identiﬁcation. In social and information network analysis  one
might have a small “seed set” of nodes that belong to a cluster or community of interest [2 
13]; in this case  one might want to perform link or edge prediction  or one might want to
“reﬁne” the seed set in order to ﬁnd other nearby members.

• Locally-biased image segmentation. In computer vision  one might have a large corpus
of images along with a “ground truth” set of pixels as provided by a face detection algo-
rithm [7  14  15]; in this case  one might want to segment entire heads from the background
for all the images in the corpus in an automated manner.

1

• Locally-biased neural connectivity analysis. In functional magnetic resonance imaging ap-
plications  one might have small sets of neurons that “ﬁre” in response to some external
experimental stimulus [16]; in this case  one might want to analyze the subsequent tem-
poral dynamics of stimulation of neurons that are “nearby ” either in terms of connectivity
topology or functional response.

These examples present considerable challenges for spectral techniques and traditional eigenvector-
based methods. At root  the reason is that eigenvectors are inherently global quantities  thus limiting
their applicability in situations where one is interested in very local properties of the data.
In this paper  we provide a methodology to construct what we will call semi-supervised eigenvectors
of a graph Laplacian; and we illustrate how these locally-biased eigenvectors inherit many of the
properties that make the leading nontrivial global eigenvectors of the graph Laplacian so useful in
applications. To achieve this  we will formulate an optimization ansatz that is a variant of the usual
global spectral graph partitioning optimization problem that includes a natural locality constraint as
well as an orthogonality constraint  and we will iteratively solve this problem.
In more detail  assume that we are given as input a (possibly weighted) data graph G = (V  E)  an
indicator vector s of a small “seed set” of nodes  a correlation parameter κ ∈ [0  1]  and a positive
integer k. Then  informally  we would like to construct k vectors that satisfy the following bicriteria:
ﬁrst  each of these k vectors is well-correlated with the input seed set; and second  those k vectors
describe successively-orthogonalized directions of maximum variance  in a manner analogous to the
leading k nontrivial global eigenvectors of the graph Laplacian. (We emphasize that the seed set s
of nodes  the integer k  and the correlation parameter κ are part of the input; and thus they should
be thought of as being available in a semi-supervised manner.) Somewhat more formally  our main
algorithm  Algorithm 1 in Section 3  returns as output k semi-supervised eigenvectors; each of these
is the solution to an optimization problem of the form of GENERALIZED LOCALSPECTRAL in Fig-
ure 1  and thus each “captures” (say) κ/k of the correlation with the seed set. Our main theoretical
result states that these vectors deﬁne successively-orthogonalized directions of maximum variance 
conditioned on being κ/k-well-correlated with an input seed set s; and that each of these k semi-
supervised eigenvectors can be computed quickly as the solution to a system of linear equations.
From a technical perspective  the work most closely related to ours is that of Mahoney et al. [14].
The original algorithm of Mahoney et al. [14] introduced a methodology to construct a locally-biased
version of the leading nontrivial eigenvector of a graph Laplacian and showed (theoretically and em-
pirically in a social network analysis application) that the resulting vector could be used to partition
a graph in a locally-biased manner. From this perspective  our extension incorporates a natural or-
thogonality constraint that successive vectors need to be orthogonal to previous vectors. Subsequent
to the work of [14]  [15] applied the algorithm of [14] to the problem of ﬁnding locally-biased cuts
in a computer vision application. Similar ideas have also been applied somewhat differently. For
example  [2] use locally-biased random walks  e.g.  short random walks starting from a small seed
set of nodes  to ﬁnd clusters and communities in graphs arising in Internet advertising applications;
[13] used locally-biased random walks to characterize the local and global clustering structure of
a wide range of social and information networks; [11] developed the Spectral Graph Transducer
(SGT)  that performs transductive learning via spectral graph partitioning. The objectives in both
[11] and [14] are considered constrained eigenvalue problems  that can be solved by ﬁnding the
smallest eigenvalue of an asymmetric generalized eigenvalue problem  but in practice this procedure
can be highly unstable [8]. The SGT reduces the instabilities by performing all calculations in a sub-
space spanned by the d smallest eigenvectors of the graph Laplacian  whereas [14] perform a binary
search  exploiting the monotonic relationship between a control parameter and the corresponding
Lagrange multiplier.
In parallel  [3] and a large body of subsequent work including [6] used eigenvectors of the graph
Laplacian to perform dimensionality reduction and data representation  in unsupervised and semi-
supervised settings. Many of these methods have a natural interpretation in terms of kernel-based
learning [18]. Many of these diffusion-based spectral methods also have a natural interpretation
in terms of spectral ranking [21]. “Topic sensitive” and “personalized” versions of these spectral
ranking methods have also been studied [9  10]; and these were the motivation for diffusion-based
methods to ﬁnd locally-biased clusters in large graphs [19  1  14]. Our optimization ansatz is a
generalization of the linear equation formulation of the PageRank procedure [17  14  21]  and the
solution involves Laplacian-based linear equation solving  which has been suggested as a primitive

2

of more general interest in large-scale data analysis [20]. Finally  the form of our optimization
problem has similarities to other work in computer vision applications: e.g.  [23] and [7] ﬁnd good
conductance clusters subject to a set of linear constraints.

2 Background and Notation
Let G = (V  E  w) be a connected undirected graph with n = |V | vertices and m = |E| edges 
in which edge {i  j} has non-negative weight wij. In the following  AG ∈ RV ×V will denote the
adjacency matrix of G  while DG ∈ RV ×V will denote the diagonal degree matrix of G  i.e. 

DG(i  i) = di = (cid:80){i j}∈E wij  the weighted degree of vertex i. Moreover  for a set of vertices

i∈S di. The Laplacian of G is deﬁned as
= DG − AG. (This is also called the combinatorial Laplacian  in which case the normalized

S ⊆ V in a graph  the volume of S is vol(S)
LG
Laplacian of G is LG

= (cid:80)

def
= D

def

def

.)

−1/2
G

−1/2
G LGD

The Laplacian is the symmetric matrix having quadratic form xT LGx = (cid:80)

ij∈E wij(xi − xj)2 
for x ∈ RV . This implies that LG is positive semideﬁnite and that the all-one vector 1 ∈ RV is
the eigenvector corresponding to the smallest eigenvalue 0. The generalized eigenvalues of LGx =
λiDGx are 0 = λ1 < λ2 ≤ ··· ≤ λN . We will use v2 to denote smallest non-trivial eigenvector 
i.e.  the eigenvector corresponding to λ2; v3 to denote the next eigenvector; and so on. Finally  for
a matrix A  let A+ denote its (uniquely deﬁned) Moore-Penrose pseudoinverse. For two vectors
x  y ∈ Rn  and the degree matrix DG for a graph G  we deﬁne the degree-weighted inner product
as xT DGy
i=1 xiyidi. In particular  if a vector x has unit norm  then xT DGx = 1. Given a
subset of vertices S ⊆ V   we denote by 1S the indicator vector of S in RV and by 1 the vector in
RV having all entries set equal to 1.

= (cid:80)n

def

3 Optimization Approach to Semi-supervised Eigenvectors

3.1 Motivation for the Program

Recall the optimization perspective on how one computes the leading nontrivial global eigenvectors
of the normalized Laplacian LG. The ﬁrst nontrivial eigenvector v2 is the solution to the problem
GLOBALSPECTRAL that is presented on the left of Figure 1. Equivalently  although GLOBALSPEC-
TRAL is a non-convex optimization problem  strong duality holds for it and it’s solution may be
computed as v2  the leading nontrivial generalized eigenvector of LG. The next eigenvector v3 is
the solution to GLOBALSPECTRAL  augmented with the constraint that xT DGv2 = 0; and in gen-
eral the tth generalized eigenvector of LG is the solution to GLOBALSPECTRAL  augmented with
the constraints that xT DGvi = 0  for i ∈ {2  . . .   t − 1}. Clearly  this set of constraints and the
constraint xT DG1 = 0 can be written as xT DGQ = 0  where 0 is a (t − 1)-dimensional all-zeros
vector  and where Q is an n× (t− 1) orthogonal matrix whose ith column equals vi (where v1 = 1 
the all-ones vector  is the ﬁrst column of Q).
Also presented in Figure 1 is LOCALSPECTRAL  which includes a constraint requiring the solution
to be well-correlated with an input seed set. This LOCALSPECTRAL optimization problem was in-
troduced in [14]  where it was shown that the solution to LOCALSPECTRAL may be interpreted as
a locally-biased version of the second eigenvector of the Laplacian. In particular  although LOCAL-
SPECTRAL is not convex  it’s solution can be computed efﬁciently as the solution to a set of linear
equations that generalize the popular Personalized PageRank procedure; in addition  by performing
a sweep cut and appealing to a variant of Cheeger’s inequality  this locally-biased eigenvector can
be used to perform locally-biased spectral graph partitioning [14].

3.2 Our Main Algorithm

We will formulate the problem of computing semi-supervised vectors in terms of a primitive op-
timization problem of independent interest. Consider the GENERALIZED LOCALSPECTRAL opti-
mization problem  as shown in Figure 1. For this problem  we are given a graph G = (V  E)  with
associated Laplacian matrix LG and diagonal degree matrix DG; an indicator vector s of a small

3

GLOBALSPECTRAL

LOCALSPECTRAL

GENERALIZED LOCALSPECTRAL

minimize xT LGx

minimize xT LGx

minimize xT LGx

s.t xT DGx = 1
xT DG1 = 0

s.t xT DGx = 1
xT DG1 = 0

xT DGs ≥ √

κ

s.t xT DGx = 1
xT DGQ = 0

xT DGs ≥ √

κ

Figure 1: Left: The usual GLOBALSPECTRAL partitioning optimization problem; the vector achiev-
ing the optimal solution is v2  the leading nontrivial generalized eigenvector of LG with respect
to DG. Middle: The LOCALSPECTRAL optimization problem  which was originally introduced
in [14]; for κ = 0  this coincides with the usual global spectral objective  while for κ > 0  this
produces solutions that are biased toward the seed vector s. Right: The GENERALIZED LOCAL-
SPECTRAL optimization problem we introduce that includes both the locality constraint and a more
general orthogonality constraint. Our main algorithm for computing semi-supervised eigenvectors
will iteratively compute the solution to GENERALIZED LOCALSPECTRAL for a sequence of Q ma-
trices. In all three cases  the optimization variable is x ∈ Rn.

√

“seed set” of nodes; a correlation parameter κ ∈ [0  1]; and an n×ν constraint matrix Q that may be
assumed to be an orthogonal matrix. We will assume (without loss of generality) that s is properly
normalized and orthogonalized so that sT DGs = 1 and sT DG1 = 0. While s can be a general unit
vector orthogonal to 1  it may be helpful to think of s as the indicator vector of one or more vertices
in V   corresponding to the target region of the graph.
In words  the problem GENERALIZED LOCALSPECTRAL asks us to ﬁnd a vector x ∈ Rn that min-
imizes the variance xT LGx subject to several constraints: that x is unit length; that x is orthogonal
to the span of Q; and that x is
κ-well-correlated with the input seed set vector s. In our applica-
tion of GENERALIZED LOCALSPECTRAL to the computation of semi-supervised eigenvectors  we
will iteratively compute the solution to GENERALIZED LOCALSPECTRAL  updating Q to contain
the already-computed semi-supervised eigenvectors. That is  to compute the ﬁrst semi-supervised
eigenvector  we let Q = 1  i.e.  the n-dimensional all-ones vector  which is the trivial eigenvector of
LG  in which case Q is an n× 1 matrix; and to compute each subsequent semi-supervised eigenvec-
tor  we let the columns of Q consist of 1 and the other semi-supervised eigenvectors found in each
of the previous iterations.
To show that GENERALIZED LOCALSPECTRAL is efﬁciently-solvable  note that it is a quadratic
program with only one quadratic constraint and one linear equality constraint. In order to remove the
equality constraint  which will simplify the problem  let’s change variables by deﬁning the n×(n−ν)
matrix F as {x : QT DGx = 0} = {x : x = F y}. That is  F is a span for the null space of QT ;
and we will take F to be an orthogonal matrix. Then  with respect to the y variable  GENERALIZED
LOCALSPECTRAL becomes

minimize

yT F T LGF y

y

(1)

(2)

In terms of the variable x  the solution to this optimization problem is of the form

subject to yT F T DGF y = 1 
κ.

yT F T DGs ≥ √
x∗ = cF(cid:0)F T (LG − γDG) F(cid:1)+
= c(cid:0)F F T (LG − γDG) F F T(cid:1)+

F T DGs

DGs 
√

for a normalization constant c ∈ (0 ∞) and for some γ that depends on
κ. The second line follows
from the ﬁrst since F is an n× (n− ν) orthogonal matrix. This so-called “S-procedure” is described
in greater detail in Chapter 5 and Appendix B of [4]. The signiﬁcance of this is that  although it is
a non-convex optimization problem  the GENERALIZED LOCALSPECTRAL problem can be solved
by solving a linear equation  in the form given in Eqn. (2).
Returning to our problem of computing semi-supervised eigenvectors  recall that  in addition to the
input for the GENERALIZED LOCALSPECTRAL problem  we need to specify a positive integer k
that indicates the number of vectors to be computed. In the simplest case  we would assume that

4

require that each vector is(cid:112)κ/k-well-correlated with the input seed set vector s; but this assumption

we would like the correlation to be “evenly distributed” across all k vectors  in which case we will

can easily be relaxed  and thus Algorithm 1 is formulated more generally as taking a k-dimensional
vector κ = [κ1  . . .   κk]T of correlation coefﬁcients as input.
To compute the ﬁrst semi-supervised eigenvector  we will let Q = 1  the all-ones vector  in which
case the ﬁrst nontrivial semi-supervised eigenvector is

1 = c (LG − γ1DG)+ DGs 
x∗

(3)

where γ1 is chosen to saturate the part of the correlation constraint along the ﬁrst direction. (Note
that the projections F F T from Eqn. (2) are not present in Eqn. (3) since by design sT DG1 = 0.)
That is  to ﬁnd the correct setting of γ1  it sufﬁces to perform a binary search over the possible
values of γ1 in the interval (−vol(G)  λ2(G)) until the correlation constraint is satisﬁed  that is 
until (sT DGx)2 is sufﬁciently close to κ2
To compute subsequent semi-supervised eigenvectors  i.e.  at steps t = 2  . . .   k if one ultimately
wants a total of k semi-supervised eigenvectors  then one lets Q be the n × (t − 1) matrix with ﬁrst
column equal to 1 and with jth column  for i = 2  . . .   t − 1  equal to x∗
j−1 (where we emphasize
that x∗
t−1] 
where x∗
i are successive semi-supervised eigenvectors  and the projection matrix F F T is of the
form F F T = I − DGQ(QT DGDGQ)−1QT DG  due to the degree-weighted inner norm. Then  by
Eqn. (2)  the tth semi-supervised eigenvector takes the form

j−1 is a vector not an element of a vector). That is  Q is of the form Q = [1  x∗

1  see [8  14].

1  . . .   x∗

t = c(cid:0)F F T (LG − γtDG)F F T(cid:1)+

x∗

DGs.

(4)

F F T ← I − DGQ(QT DGDGQ)−1QT DG

Algorithm 1 Semi-supervised eigenvectors
Input: LG  DG  s  κ = [κ1  . . .   κk]T   
Require: sT DG1 = 0  sT DGs = 1  κT 1 ≤ 1
1: Q = [1]
2: for t = 1 to k do
3:
4: (cid:62) ← λ2 where F F T LGF F T v2 = λ2F F T DGF F T v2
5: ⊥ ← −vol(G)
6:
7:
8:
9:
10:
11:
12:
13: end for

γt ← (⊥ + (cid:62))/2 (Binary search over γt)
xt ← (F F T (LG − γtDG)F F T )+F F T DGs
Normalize xt such that xT
if (xT

t DGs)2 > κt then ⊥ ← γt else (cid:62) ← γt end if
t DGs)2 − κt(cid:107) ≤  or (cid:107)(⊥ + (cid:62))/2 − γt(cid:107) ≤ 

until (cid:107)(xT
Augment Q with x∗

t by letting Q = [Q  x∗
t ].

t DGxt = 1

repeat

In more detail  Algorithm 1 presents pseudo-code for our main algorithm for computing semi-
supervised eigenvectors. Several things should be noted about our implementation. First  note
that we implicitly compute the projection matrix F F T . Second  a na¨ıve approach to Eqn. (2) does
not immediately lead to an efﬁcient solution  since DGs will not be in the span of (F F T (LG −
γDG)F F T )  thus leading to a large residual. By changing variables so that x = F F T y  the solu-
tion becomes x∗ ∝ F F T (F F T (LG − γDG)F F T )+F F T DGs. Since F F T is a projection matrix 
this expression is equivalent to x∗ ∝ (F F T (LG − γDG)F F T )+F F T DGs. Third  we exploit that
F F T (LG − γiDG)F F T is an SPSD matrix  and we apply the conjugate gradient method  rather
than computing the explicit pseudoinverse. That is  in the implementation we never represent the
dense matrix F F T   but instead we treat it as an operator and we simply evaluate the result of ap-
plying a vector to it on either side. Fourth  we use that λ2 can never decrease (here we refer to
λ2 as the smallest non-zero eigenvalue of the modiﬁed matrix)  so we only recalculate the upper
bound for the binary search when an iteration saturates without satisfying (cid:107)(xT
t DGs)2 − κt(cid:107) ≤ .
In case of saturation one can for instance recalculate λ2 iteratively by using the inverse iteration
method  vk+1
2   and normalizing such
that (vk+1

∝ (F F T LGF F T − λest
2 = 1.

2 F F T DGF F T )+F F T DGF F T vk

2
)T vk+1

2

5

4

Illustrative Empirical Results

In this section  we will provide a detailed empirical evaluation of our method of semi-supervised
eigenvectors and how they can be used for locally-biased machine learning. Our goal will be two-
fold: ﬁrst  to illustrate how the “knobs” of our method work; and second  to illustrate the usefulness
of the method in a real application. To do so  we will consider:

• Toy data. In Section 4.1  we will consider one-dimensional examples of the popular “small
world” model [22]. This is a parameterized family of models that interpolates between
low-dimensional grids and random graphs; and  as such  it will allow us to illustrate the
behavior of our method and it’s various parameters in a controlled setting.

• Handwritten image data. In Section 4.2  we will consider the data from the MNIST digit
data set [12]. These data have been widely-studied in machine learning and related areas
and they have substantial “local heterogeneity”; and thus these data will allow us to illus-
trate how our method may be used to perform locally-biased versions of common machine
learning tasks such as smoothing  clustering  and kernel construction.

4.1 Small-world Data

To illustrate how the “knobs” of our method work  and in particular how κ and γ interplay  we con-
sider data constructed from the so-called small-world model. To demonstrate how semi-supervised
eigenvectors can focus on speciﬁc target regions of a data graph to capture slowest modes of local
variation  we plot semi-supervised eigenvectors around illustrations of (non-rewired and rewired)
realizations of the small-world graph; see Figure 2.

p = 0 

λ2 = 0.000011 
λ3 = 0.000011 
λ4 = 0.000046 
λ5 = 0.000046.

p = 0.01 

λ2 = 0.000149 
λ3 = 0.000274 
λ4 = 0.000315 
λ5 = 0.000489.

p = 0.01  κ = 0.005 

γ1 = 0.000047 
γ2 = 0.000052 
γ3 = −0.000000 
γ4 = −0.000000.

p = 0.01  κ = 0.05 
γ1 = −0.004367 
γ2 = −0.001778 
γ3 = −0.001665 
γ4 = −0.000822.

(a) Global eigenvectors

(b) Global eigenvectors

(c) Semi-supervised eigenvectors

(d) Semi-supervised eigenvectors

Figure 2: In each case  (a-d) the data consist of 3600 nodes  each connected to it’s 8 nearest-
neighbors. In the center of each subﬁgure  we show the nodes (blue) and edges (black and light
gray are the local edges  and blue are the randomly-rewired edges). In each subﬁgure  we wrap a
plot (black x-axis and gray background) visualizing the 4 smallest semi-supervised eigenvectors 
allowing us to see the effect of random edges (different values of rewiring probability p) and degree
of localization (different values of κ). Eigenvectors are color coded as blue  red  yellow  and green 
starting with the one having the smallest eigenvalue. See the main text for more details.
In Figure 2.a  we show a graph with no randomly-rewired edges (p = 0) and a locality parameter
κ such that the global eigenvectors are obtained. This yields a symmetric graph with eigenvectors
corresponding to orthogonal sinusoids  i.e.  for all eigenvectors  except the all-ones with eigenvalue
0  the algebraic multiplicity is 2  i.e.  the ﬁrst two capture the slowest mode of variation and cor-
respond to a sine and cosine with equal random phase-shift (rotational ambiguity). In Figure 2.b 
random edges have been added with probability p = 0.01 and the locality parameter κ is still cho-
sen such that the global eigenvectors of the rewired graph are obtained. In particular  note small
kinks in the eigenvectors at the location of the randomly added edges. Since the graph is no longer
symmetric  all of the visualized eigenvectors have algebraic multiplicity 1. Moreover  note that the
slow mode of variation in the interval on the top left; a normalized-cut based on the leading global
eigenvector would extract this region since the remainder of the ring is more well-connected due
to the degree of rewiring. In Figure 2.c  we see the same graph realization as in Figure 2.b  except
that the semi-supervised eigenvectors have a seed node at the top of the circle and the correlation

6

parameter κt = 0.005. Note that  like the global eigenvectors  the local approach produces modes
of increasing variation. In addition  note that the neighborhood around “11 o-clock” contains more
mass  when compared with Figure 2.b; the reason for this is that this region is well-connected with
the seed via a randomly added edge. Above the visualization we also show the γt that saturates κt 
i.e.  γt is the Lagrange multiplier that deﬁnes the effective correlation κt. Not shown is that if we
kept reducing κ  then γt would tend towards λt+1  and the respective semi-supervised eigenvector
would tend towards the global eigenvector. Finally  in Figure 2.d  the desired correlation is increased
to κ = 0.05 (thus decreasing the value of γt)  making the different modes of variation more local-
ized in the neighborhood of the seed. It should be clear that  in addition to being determined by the
locality parameter  we can think of γ as a regularizer biasing the global eigenvectors towards the
region near the seed set.

4.2 MNIST Digit Data

σ2
i

(cid:107)xi − xj(cid:107)2)  where σ2

We now demonstrate the semi-supervised eigenvectors as a feature extraction preprocessing step in
a machine learning setting. We consider the well-studied MNIST dataset containing 60000 training
digits and 10000 test digits ranging from 0 to 9. We construct the complete 70000 × 70000 k-NN
graph with k = 10 and with edge weights given by wij = exp(− 4
i being
the Euclidean distance to it’s nearest neighbor  and we deﬁne the graph Laplacian in the usual way.
We evaluate the semi-supervised eigenvectors in a transductive learning setting by disregarding the
majority of labels in the entire training data. We then use a few samples from each class to seed
our semi-supervised eigenvectors  and a few others to train a downstream classiﬁcation algorithm.
Here we choose to apply the SGT of [11] for two main reasons. First  the transductive classiﬁer is
inherently designed to work on a subset of global eigenvectors of the graph Laplacian  making it
ideal for validating that our localized basis constructed by the semi-supervised eigenvectors can be
more informative when we are solely interested in the “local heterogeneity” near a seed set. Second 
using the SGT based on global eigenvectors is a good point of comparison  because we are only
interested in the effect of our subspace representation. (If we used one type of classiﬁer in the local
setting  and another in the global  the classiﬁcation accuracy that we measure would obviously be
biased.) As in [11]  we normalize the spectrum of both global and semi-supervised eigenvectors
by replacing the eigenvalues with some monotonically increasing function. We use λi = i2
k2   i.e. 
focusing on ranking among smallest cuts; see [5]. Furthermore  we ﬁx the regularization parameter
of the SGT to c = 3200  and for simplicity we ﬁx γ = 0 for all semi-supervised eigenvectors 
implicitly deﬁning the effective κ = [κ1  . . .   κk]T . Clearly  other correlation distributions and
values of γ may yield subspaces with even better discriminative properties1.

Labeled points

1 : 1
1 : 10
5 : 50

10 : 100
50 : 500

1

0.39
0.30
0.12
0.09
0.03

2

0.39
0.31
0.15
0.10
0.03

4

0.38
0.25
0.09
0.07
0.03

6

0.38
0.23
0.08
0.06
0.03

#Semi-supervised eigenvectors for SGT

8

0.38
0.19
0.07
0.05
0.03

10
0.36
0.15
0.06
0.05
0.03

1

0.50
0.49
0.49
0.49
0.49

#Global eigenvectors for SGT
20
5
0.27
0.06
0.05
0.04
0.04

10
0.36
0.09
0.08
0.07
0.07

15
0.27
0.08
0.07
0.06
0.06

0.48
0.36
0.09
0.08
0.10

25
0.19
0.06
0.04
0.04
0.04

Table 1: Classiﬁcation error for the SGT based on respectively semi-supervised and global eigenvec-
tors. The ﬁrst column from the left encodes the conﬁguration  e.g.  1:10 interprets as 1 seed and 10
training samples from each class (total of 22 samples - for the global approach these are all used for
training). When the seed is well determined and the number of training samples moderate (50:500)
a single semi-supervised eigenvector is sufﬁcient  where for less data we beneﬁt from using multiple
semi-supervised eigenvectors. All experiments have been repeated 10 times.
Here  we consider the task of discriminating between fours and nines  as these two classes tend to
overlap more than other combinations. (A closed four usually resembles nine more than an “open”
four.) Hence  we expect localization on low order global eigenvectors  meaning that class separation
will not be evident in the leading global eigenvector  but instead will be “buried” further down the
spectrum. Thus  this will illustrate how semi-supervised eigenvectors can represent relevant hetero-
geneities in a local subspace of low dimensionality. Table 1 summarizes our classiﬁcation results
based on respectively semi-supervised and global eigenvectors. Finally  Figure 3 and 4 illustrates
two realizations for the 1:10 conﬁguration  where the training samples are ﬁxed  but where we vary

1A thorough analysis regarding the importance of this parameter will appear in the journal version.

7

the seed nodes  to demonstrate the inﬂuence of the seed. See the caption in these ﬁgures for further
details.

s+ = { }
}

l− = {

l+ = {

s− = { }
}

−→
−
−
−
−
−
−
−
−−

a
t
a
d
t
s
e
T

−−
−
−
−
−
−
−
−
←−

−→
−
−
−
−
−
−
−
−−

a
t
a
d

t
s
e
T

−−
−
−
−
−
−
−
−
←−

Figure 3: Left: Shows a subset of the classiﬁcation results for the SGT based on 5 semi-supervised
eigenvectors seeded in s+ and s−  and trained using samples l+ and l−. Misclassiﬁcations are
marked with black frames. Right: Visualizes all test data spanned by the ﬁrst 5 semi-supervised
eigenvectors  by plotting each component as a function of the others. Red (blue) points correspond
to 4 (9)  whereas green points correspond to remaining digits. As the seed nodes are good repre-
sentatives  we note that the eigenvectors provide a good class separation. We also plot the error as
a function of local dimensionality  as well as the unexplained correlation  i.e.  initial components
explain the majority of the correlation with the seed (effect of γ = 0). The particular realization
based on the leading 5 semi-supervised eigenvectors yields an error of ≈ 0.03 (dashed circle).

l+ = {

s+ = { }
}

l− = {

s− = { }
}

Figure 4: See the general description in Figure 3. Here we illustrate an instance where the s+ shares
many similarities with s−  i.e.  s+ is on the boundary of the two classes. This particular realization
achieves a classiﬁcation error of ≈ 0.30 (dashed circle).
In this constellation we ﬁrst discover
localization on low order semi-supervised eigenvectors (≈ 12 eigenvectors)  which is comparable
to the error based on global eigenvectors (see Table 1)  i.e.  further down the spectrum we recover
from the bad seed and pickup the relevant mode of variation.

In summary: We introduced the concept of semi-supervised eigenvectors that are biased towards
local regions of interest in a large data graph. We demonstrated the feasibility on a well-studied
dataset and found that our approach leads to more compact subspace representations by extracting
desired local heterogeneities. Moreover  the algorithm is scalable as the eigenvectors are computed
by the solution to a sparse system of linear equations  preserving the low O(m) space complexity.
Finally  we foresee that the approach will prove useful in a wide range of data analysis ﬁelds  due to
the algorithm’s speed  simplicity  and stability.

8

1234567891011121314150.080.070.060.050.030.020.030.030.030.030.030.030.030.030.03Classiﬁcation errorUnexplained correlation1 vs. 21 vs. 31 vs. 41 vs. 52 vs. 32 vs. 42 vs. 53 vs. 43 vs. 54 vs. 5#Semi-supervised eigenvectors0.60.50.40.30.20.101234567891011121314150.480.310.300.300.300.290.270.240.200.150.100.040.040.040.04Classiﬁcation errorUnexplained correlation1 vs. 21 vs. 31 vs. 41 vs. 52 vs. 32 vs. 42 vs. 53 vs. 43 vs. 54 vs. 5#Semi-supervised eigenvectors0.60.50.40.30.20.10References
[1] R. Andersen  F.R.K. Chung  and K. Lang. Local graph partitioning using PageRank vectors.
In FOCS ’06: Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer
Science  pages 475–486  2006.

[2] R. Andersen and K. Lang. Communities from seed sets. In WWW ’06: Proceedings of the 15th

International Conference on World Wide Web  pages 223–232  2006.

[3] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data repre-

sentation. Neural Computation  15(6):1373–1396  2003.

[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  Cambridge 

UK  2004.

[5] O. Chapelle  J. Weston  and B. Sch¨olkopf. Cluster Kernels for Semi-Supervised Learning. In

Becker  editor  NIPS 2002  volume 15  pages 585–592  Cambridge  MA  USA  2003.

[6] R.R. Coifman  S. Lafon  A.B. Lee  M. Maggioni  B. Nadler  F. Warner  and S.W. Zucker.
Geometric diffusions as a tool for harmonic analysis and structure deﬁnition in data: Diffusion
maps. Proc. Natl. Acad. Sci. USA  102(21):7426–7431  2005.

[7] A. P. Eriksson  C. Olsson  and F. Kahl. Normalized cuts revisited: A reformulation for seg-
mentation with linear grouping constraints. In Proceedings of th 11th International Conference
on Computer Vision  pages 1–8  2007.

[8] W. Gander  G. H. Golub  and U. von Matt. A constrained eigenvalue problem. Linear Algebra

and its Applications  114/115:815–839  1989.

[9] T.H. Haveliwala. Topic-sensitive PageRank: A context-sensitive ranking algorithm for web

search. IEEE Transactions on Knowledge and Data Engineering  15(4):784–796  2003.

[10] G. Jeh and J. Widom. Scaling personalized web search. In WWW ’03: Proceedings of the 12th

In Proceedings of the

International Conference on World Wide Web  pages 271–279  2003.
[11] T. Joachims. Transductive learning via spectral graph partitioning.

Twentieth International Conference on Machine Learning (ICML-2003)  2003.

[12] Y. Lecun and C. Cortes. The MNIST database of handwritten digits.
[13] J. Leskovec  K.J. Lang  A. Dasgupta  and M.W. Mahoney. Statistical properties of community
In WWW ’08: Proceedings of the 17th

structure in large social and information networks.
International Conference on World Wide Web  pages 695–704  2008.

[14] M. W. Mahoney  L. Orecchia  and N. K. Vishnoi. A local spectral method for graphs: with
applications to improving graph partitions and exploring data graphs locally. Technical report 
2009. Preprint: arXiv:0912.0681.

[15] S. Maji  N. K. Vishnoi  and J. Malik. Biased normalized cuts. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition  pages 2057–2064  2011.

[16] K.A. Norman  S.M. Polyn  G.J. Detre  and J.V. Haxby. Beyond mind-reading: multi-voxel

pattern analysis of fmri data. Trends in Cognitive Sciences  10(9):424–30  2006.

[17] L. Page  S. Brin  R. Motwani  and T. Winograd. The PageRank citation ranking: Bringing

order to the web. Technical report  Stanford InfoLab  1999.

[18] B. Scholkopf and A.J. Smola. Learning with Kernels: Support Vector Machines  Regulariza-

tion  Optimization  and Beyond. MIT Press  Cambridge  MA  USA  2001.

[19] D.A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning  graph
sparsiﬁcation  and solving linear systems. In STOC ’04: Proceedings of the 36th annual ACM
Symposium on Theory of Computing  pages 81–90  2004.

[20] S.-H. Teng. The Laplacian paradigm: Emerging algorithms for massive graphs. In Proceedings
of the 7th Annual Conference on Theory and Applications of Models of Computation  pages
2–14  2010.

[21] S. Vigna. Spectral ranking. Technical report. Preprint: arXiv:0912.0238 (2009).
[22] D.J. Watts and S.H. Strogatz. Collective dynamics of small-world networks. Nature  393:440–

442  1998.

[23] S. X. Yu and J. Shi. Grouping with bias. In Annual Advances in Neural Information Processing

Systems 14: Proceedings of the 2001 Conference  pages 1327–1334  2002.

9

,Alireza Makhzani
Brendan Frey