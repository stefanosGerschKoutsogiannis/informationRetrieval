2013,Relevance Topic Model for Unstructured Social Group Activity Recognition,Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem  we propose a relevance topic model" for jointly learning meaningful mid-level representations upon bag-of-words (BoW) video representations and a classifier with sparse weights. In our approach  sparse Bayesian learning is incorporated into an undirected topic model (i.e.  Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectified linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efficient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classification accuracy  particularly in the case of a very small number of labeled training videos.",Relevance Topic Model for Unstructured Social

Group Activity Recognition

Fang Zhao

Yongzhen Huang

Center for Research on Intelligent Perception and Computing

Institute of Automation  Chinese Academy of Sciences

{fang.zhao yzhuang wangliang tnt}@nlpr.ia.ac.cn

Liang Wang

Tieniu Tan

Abstract

Unstructured social group activity recognition in web videos is a challenging task
due to 1) the semantic gap between class labels and low-level visual features and 2)
the lack of labeled training data. To tackle this problem  we propose a “relevance
topic model” for jointly learning meaningful mid-level representations upon bag-
of-words (BoW) video representations and a classiﬁer with sparse weights. In
our approach  sparse Bayesian learning is incorporated into an undirected topic
model (i.e.  Replicated Softmax) to discover topics which are relevant to video
classes and suitable for prediction. Rectiﬁed linear units are utilized to increase the
expressive power of topics so as to explain better video data containing complex
contents and make variational inference tractable for the proposed model. An
efﬁcient variational EM algorithm is presented for model parameter estimation
and inference. Experimental results on the Unstructured Social Activity Attribute
dataset show that our model achieves state of the art performance and outperforms
other supervised topic model in terms of classiﬁcation accuracy  particularly in the
case of a very small number of labeled training videos.

1

Introduction

The explosive growth of web videos makes automatic video classiﬁcation important for online video
search and indexing. Classifying short video clips which contain simple motions and actions has
been solved well in standard datasets (such as KTH [1]  UCF-Sports [2] and UCF50 [3]). However 
detecting complex activities  specially social group activities [4]  in web videos is a more difﬁcult
task because of unstructured activity context and complex multi-object interaction.
In this paper  we focus on the task of automatic classiﬁcation of unstructured social group activities
(e.g.  wedding dance  birthday party and graduation ceremony in Figure 1)  where the low-level
features have innate limitations in semantic description of the underlying video data and only a
few labeled training videos are available. Thus  a common method is to learn human-deﬁned (or
semi-human-deﬁned) semantic concepts as mid-level representations to help video classiﬁcation
[4]. However  those human deﬁned concepts are hardly generalized to a larger or new dataset. To
discover more powerful representations for classiﬁcation  we propose a novel supervised topic model
called “relevance topic model” to automatically extract latent “relevance” topics from bag-of-words
(BoW) video representations and simultaneously learn a classiﬁer with sparse weights.
Our model is built on Replicated Softmax [5]  an undirected topic model which can be viewed as
a family of different-sized Restricted Boltzmann Machines that share parameters. Sparse Bayesian
learning [6] is incorporated to guide the topic model towards learning more predictive topics which
are associated with sparse classiﬁer weights. We refer to those topics corresponding to non-zero
weights as “relevance” topics. Meanwhile  binary stochastic units in Replicated Softmax are re-
placed by rectiﬁed linear units [7]  which allows each unit to express more information for better

1

(a) Wedding Dance

(b) Birthday Party

(c) Graduation Ceremony

Figure 1: Example videos of the “Wedding Dance”  “Birthday Party” and “Graduation Ceremony”
classes taken from the USAA dataset [4].

explaining video data containing complex content and also makes variational inference tractable for
the proposed model. Furthermore  by using a simple quadratic bound on the log-sum-exp function
[8]  an efﬁcient variational EM algorithm is developed for parameter estimation and inference. Our
model is able to be naturally extended to deal with multi-modal data without changing the learning
and inference procedures  which is beneﬁcial for video classiﬁcation tasks.

2 Related work

The problems of activity analysis and recognition have been widely studied. However  most of the
existing works [9  10] were done on constrained videos with limited contents (e.g.  clean background
and little camera motions). Complex activity recognition in web videos  such as social group activity 
is not much explored. Most relevant to our work is a recent work that learns video attributes to
analyze social group activity [4]. In [4]  a semi-latent attribute space is introduced  which consists
of user-deﬁned attributes  class-conditional and background latent attributes  and an extended Latent
Dirichlet Allocation (LDA) [11] is used to model those attributes as topics. Different from that  our
work discovers a set of discriminative latent topics without extra human annotations on videos.
From the view of graphical models  most similar to our model are the maximum entropy discrimina-
tion LDA (MedLDA) [12] and the generative Classiﬁcation Restricted Boltzmann Machines (gClass-
RBM) [13]  both of which have been successfully applied to document semantic analysis. MedLDA
integrates the max-margin learning and hierarchical directed topic models by optimizing a single
objective function with a set of expected margin constraints. MedLDA tries to estimate parame-
ters and ﬁnd latent topics in a max-margin sense  which is different from our model relying on the
principle of automatic relevance determination [14]. The gClassRBM used to model word count
data is actually a supervised Replicated Softmax. Different from the gClassRBM  instead of point
estimation of classiﬁer parameters  our proposed model learns a sparse posterior distribution over
parameters within a Bayesian paradigm.

3 Models and algorithms

We start with the description of Replicated Softmax  and then by integrating it with sparse Bayesian
learning  propose the relevance topic model for videos. Finally  we develop an efﬁcient variational
algorithm for inference and parameter estimation.

3.1 Replicated Softmax

The Replicated Softmax model is a two-layer undirected graphical model  which can be used to
model sparse word count data and extract latent semantic topics from document collections. Repli-
cated Softmax allows for very efﬁcient inference and learning  and outperforms LDA in terms of
both the generalization performance and the retrieval accuracy on text datasets.
As shown in Figure 2 (left)  this model is a generalization of the restricted Boltzmann machine
(RBM). The bottom layer represents a multinomial visible unit sampled K times (K is the total
number of words in a document) and the top layer represents binary stochastic hidden units.

2

Figure 2: Left: Replicated Softmax model: an undirected graphical model. Right: Relevance topic
model: a mixed graphical model. The undirected part models the marginal distribution of video
BoW vectors v and the directed part models the conditional distribution of video classes y given
latent topics tr by using a hierarchical prior on weights ηηη.

Let a word count vector v ∈ NN be the visible unit (N is the size of the vocabulary)  and a binary
topic vector h ∈ {0  1}F be the hidden units. Then the energy function of the state {v  h} is deﬁned
as follows:

E(v  h; θ) = − N(cid:88)

F(cid:88)

Wijvihj − N(cid:88)

aivi − K

(1)
where θ = {W  a  b}  Wij is the weight connected with vi and hj  ai and bj are the bias terms
of visible and hidden units respectively. The joint distribution over the visible and hidden units is
deﬁned by:

bjhj 

j=1

i=1

j=1

i=1

F(cid:88)

P (v  h; θ) =

1
Z(θ)

exp(−E(v  h; θ))  Z(θ) =

exp(−E(v  h; θ)) 

(2)

where Z(θ) is the partition function. Since exact maximum likelihood learning is intractable  the
contrastive divergence [15] approximation is often used to estimate model parameters in practice.

(cid:88)

(cid:88)

v

h

3.2 Relevance topic model

The relevance topic model (RTM) is an integration of sparse Bayesian learning and Replicated Soft-
max  the main idea of which is to jointly learn discriminative topics as mid-level video representa-
tions and sparse discriminant function as a video classiﬁer.
We represent the video dataset with class labels y ∈ {1  ...  C} as D = {(vm  ym)}M
m=1  where
each video is represented as a BoW vector v ∈ NN . Consider modeling video BoW vectors using
F ] denotes a F-dimensional topic vector of one video.
the Replicated Softmax. Let tr = [tr
According to Equation 2  the marginal distribution over the BoW vector v is given by:

1  ...  tr

P (v; θ) =

1
Z(θ)

exp(−E(v  tr; θ)) 

(3)

(cid:88)

tr

N(cid:88)

Since videos contain more complex and diverse contents than documents  binary topics are far from
ideal to explain video data. We replace binary hidden units in the original Replicated Softmax with
rectiﬁed linear units which are given by:

j = max(0  tj)  P (tj|v; θ) = N (tj|Kbj +
tr

Wijvi  1) 

(4)

i=1

where N (·|µ  τ ) denotes a Gaussian distribution with mean µ and variance τ. The rectiﬁed linear
units taking nonnegative real values can preserve information about relative importance of topics.
Meanwhile  the rectiﬁed Gaussian distribution is semi-conjugate to the Gaussian likelihood. This fa-
cilitates the development of variational algorithms for posterior inference and parameter estimation 
which we describe in Section 3.3.
Let ηηη = {ηηηy}C
as a linear combination of topics: F (y  tr  ηηη) = ηηηT

y=1 denote a set of class-speciﬁc weight vectors. We deﬁne the discriminant function
y tr. The conditional distribution of classes is

3

W1W2vhytrvηcαcCW. . .deﬁned as follows:

and the classiﬁer is given by:

P (y|tr  ηηη) =

(cid:80)C

exp(F (y  tr  ηηη))
y(cid:48)=1 exp(F (y(cid:48)  tr  ηηη))

 

ˆy = arg max

y∈C

E[F (y  tr  ηηη)|v].

The weights ηηη are given a zero-mean Gaussian prior:
P (ηyj|αyj) =

P (ηηη|ααα) =

C(cid:89)

F(cid:89)

C(cid:89)

F(cid:89)

N (ηyj|0  α−1
yj ) 

(5)

(6)

(7)

where ααα = {αααy}C
independently to each weight ηyj. The hyperpriors over ααα are given by Gamma distributions:

y=1 is a set of hyperparameter vectors  and each hyperparameter αyj is assigned

y=1

j=1

y=1

j=1

C(cid:89)

F(cid:89)

C(cid:89)

F(cid:89)

P (ααα) =

P (αyj) =

y=1

j=1

y=1

j=1

Γ(c)

−1dcαc−1

yj e−dα 

(8)

where Γ(c) is the Gamma function. To obtain broad hyperpriors  we set c and d to small values 
e.g.  c = d = 10−4. This hierarchical prior  which is a type of automatic relevance determination
prior [14]  enables the posterior probability of the weights ηηη to be concentrated at zero and thus
effectively switch off the corresponding topics that are considered to be irrelevant to classiﬁcation.
Finally  given the parameters θ  RTM deﬁnes the joint distribution:

P (v  y  tr  ηηη  ααα; θ) = P (v; θ)P (y|tr  ηηη)

P (tj|v; θ)

P (ηyj|αyj)P (αyj)

.

(9)

j=1

y=1

j=1

Figure 2 (right) illustrates RTM as a mixed graphical model with undirected and directed edges.
The undirected part models the marginal distribution of video data and the directed part models the
conditional distribution of classes given latent topics. We can naturally extend RTM to Multimodal
RTM by using the undirected part to model the multimodal data v = {vmodl}L
l=1. Accordingly 
l=1 P (vmodl; θmodl). In Section 3.3  we can see that it will

P (v; θ) in Equation 9 is replaced with(cid:81)L

not change learning and inference rules.

(cid:18) F(cid:89)

(cid:19)(cid:18) C(cid:89)

F(cid:89)

(cid:19)

(cid:90)

3.3 Parameter estimation and inference
For RTM  we wish to ﬁnd parameters θ = {W  a  b} that maximize the log likelihood on D:

log P (D; θ) = log

(10)
and learn the posterior distribution P (ηηη  ααα|D; θ) = P (ηηη  ααα D; θ)/P (D; θ). Since exactly comput-
ing P (D; θ) is intractable  we employ variational methods to optimize a lower bound L on the log
likelihood by introducing a variational distribution to approximate P ({tm}M

m=1  ηηη  ααα|D; θ):

m=1dηηηdααα 

m}M

m=1  ηηη  ααα; θ)d{tm}M

P ({vm  ym  tr

Q({tm}M

m=1  ηηη  ααα) =

q(tmj)

q(ηηη)q(ααα).

(11)

(cid:18) M(cid:89)

F(cid:89)

m=1

j=1

(cid:19)

Using Jensens inequality  we have:

(cid:90)
(cid:16)(cid:81)M
m=1  ηηη  ααα)
m=1 P (vm; θ)P (ym|tr

Q({tm}M

log P (D; θ) (cid:62)

log

(cid:17)
m  ηηη)P (tm|vm; θ)

Q({tm}M

m=1  ηηη  ααα)

P (ηηη|ααα)P (ααα)

d{tm}M

m=1dηηηdααα.

(12)

Note that P (ym|tr
m  ηηη) is not conjugate to the Gaussian prior  which makes it intractable to compute
the variational factors q(ηηη) and q(tmj). Here we use a quadratic bound on the log-sum-exp (LSE)
function [8] to derive a further bound. We rewrite P (ym|tr

m  ηηη) as follows:

P (ym|tr

m  ηηη) = exp(yT

mTr

mηηη − lse(Tr

mηηη)) 

(13)

4

where Tr

mηηη = [(tr

m)T ηηη1  ...  (tr

label ym and lse(x) (cid:44) log(1 +(cid:80)C−1

m)T ηηηC−1]  ym = I(ym = c) is the one-of-C encoding of class
y(cid:48)=1 exp(xy(cid:48))) (we set ηηηC = 0 to ensure identiﬁability). In [8] 
the LSE function is expanded as a second order Taylor series around a point ϕϕϕ  and an upper bound
is found by replacing the Hessian matrix H(ϕϕϕ) with a ﬁxed matrix A = 1
C∗+1 1C∗ 1T
C∗ ]
such that A (cid:31) H(ϕϕϕ)  where C∗ = C − 1  IC∗ is the identity matrix of size M × M and 1C∗ is a
M-vector of ones. Thus  similar to [16]  we have:

2 [IC∗ − 1

log P (ym|tr

m  ηηη) (cid:62) J(ym  tr

mηηη − 1
2

(Tr

mTr

m  ηηη  ϕϕϕm) = yT
sm = Aϕϕϕm − exp(ϕϕϕm − lse(ϕϕϕm)) 
mAϕϕϕm − ϕϕϕT
ϕϕϕT

m exp(ϕϕϕm − lse(ϕϕϕm)) + lse(ϕϕϕm) 

mηηη)T ATr

mηηη + sT

mTr

mηηη − κi 

(14)

(15)

(16)

where ϕϕϕm ∈ RC∗
11  we can obtain a further lower bound:

is a vector of variational parameters. Substituting J(ym  tr

m  ηηη  ϕϕϕm) into Equation

κi =

1
2

M(cid:88)

M(cid:88)

(cid:20) M(cid:88)

(cid:21)

log P (D; θ) (cid:62) L(θ  ϕϕϕ) =

log P (vm; θ) + EQ

J(ym  tr

m  ηηη  ϕϕϕm)

m=1

m=1

+

log P (tm|vm; θ) + log P (ηηη|ααα) + log P (ααα) − Q({tm}M

m=1  ηηη  ααα)

.

(17)

m=1

Now we convert the problem of model training into maximizing the lower bound L(θ  ϕϕϕ) with
respect to the variational posteriors q(ηηη)  q(ααα) and q(t) = {q(tmj)} as well as the parameters
θ and ϕϕϕ = {ϕϕϕm}. We can give some insights into the objective function L(θ  ϕϕϕ): the ﬁrst term
is exactly the marginal log likelihood of video data and the second term is a variational bound of
the conditional log likelihood of classes  thus maximizing L(θ  ϕϕϕ) is equivalent to ﬁnding a set of
model parameters and latent topics which could ﬁt video data well and simultaneously make good
predictions for video classes.
Due to the conjugacy properties of the chosen distributions  we can directly calculate free-form
variational posteriors q(ηηη)  q(ααα) and parameters ϕϕϕ:

q(ηηη) = N (ηηη|Eηηη  Vηηη) 

C(cid:89)

F(cid:89)

q(ααα) =

y=1

j=1

ϕϕϕ = (cid:104)Tr

Gamma(αyj|ˆc  ˆdyj) 
m(cid:105)q(t)Eηηη 
(cid:19)−1

where (cid:104)·(cid:105)q denotes an exception with respect to the distribution q and
M(cid:88)

(cid:11)
q(t) + diag(cid:104)αyj(cid:105)q(ααα)

(cid:18) M(cid:88)

(cid:10)(Tr

m)T ATr
m

  Eηηη = Vηηη

Vηηη =

m=1

ˆc = c +

1
2

  ˆdyj = d +

1
2

(cid:10)η2

yj

(cid:11)

m=1

.

q(ηηη)

(18)

(19)

(20)

(cid:10)(Tr
m)T(cid:11)

q(t)(ym + sm) 

(21)

(22)

For q(t)  the calculation is not immediate because of the rectiﬁcation. Inspired by [17]  we have the
following free-form solution:

q(tmj) =

ωpos

Z

N (tmj|µpos  σ2

pos)u(tmj) +

N (tmj|µneg  σ2

neg)u(−tmj) 

(23)

ωneg

Z

where u(·) is the unit step function. See Appendix A for parameters of q(tmj).
Given θ  through repeating the updates of Equations 18-20 and 23 to maximize L(θ  ϕϕϕ)  we can
obtain the variational posteriors q(ηηη)  q(ααα) and q(t). Then given q(ηηη)  q(ααα) and q(t)  we estimate
θ by using stochastic gradient descent to maximize L(θ  ϕϕϕ)  and the derivatives of L(θ  ϕϕϕ) with

5

j

data

(cid:11)

∂L(θ  ϕϕϕ)
∂Wij

respect to θ are given by:

=(cid:10)vitr
(cid:11)
=(cid:10)tr
where the derivatives of(cid:80)M

∂L(θ  ϕϕϕ)

∂bj

(cid:11)

−(cid:10)vitr
−(cid:10)tr

data

j

M(cid:88)

vmi

1
M
= (cid:104)vi(cid:105)data − (cid:104)vi(cid:105)model 

m=1

(cid:18)
(cid:104)tmj(cid:105)q(t) − N(cid:88)
(cid:18)
(cid:104)tmj(cid:105)q(t) − N(cid:88)

i=1

M(cid:88)

m=1

i=1

j

model

+
∂L(θ  ϕϕϕ)

∂ai

(cid:11)

j

model

+

K
M

m=1 log P (vm; θ) are the same as those in [5].

(cid:19)

 

(24)

Wijvmi − Kbj

(cid:19)

 

(25)

(26)

Wijvmi − Kbj

This leads to the following variational EM algorithm:

E-step: Calculate variational posteriors q(ηηη)  q(ααα) and q(t).
M-step: Estimate parameters θ = {W  a  b} through maximizing L(θ  ϕϕϕ).

These two steps are repeated until L(θ  ϕϕϕ) converges. For the Multimodal RTM learning  we just
additionally calculate the gradients of θmodl for each modality l in the M-step while the updating
rules are not changed.
After the learning is completed  according to Equation 6 the prediction for new videos can be easily
obtained:

(cid:11)

(cid:10)ηηηT

y

(cid:104)tr(cid:105)p(t|v;θ).

q(ηηη)

(27)

ˆy = arg max

y∈C

4 Experiments

We test our models on the Unstructured Social Activity Attribute (USAA) dataset 1 for social group
activity recognition. Firstly  we present quantitative evaluations of RTM in the case of different
modalities and comparisons with other supervised topic models (namely MedLDA and gClass-
RBM). Secondly  we compare Multimodal RTM with some baselines in the case of plentiful and
sparse training data respectively. In all experiments  the contrastive divergence is used to efﬁciently
approximate the derivatives of the marginal log likelihood and the unsupervised training on Repli-
cated Softmax is used to initialize θ.

4.1 Dataset and video representation

The USAA dataset consists of 8 semantic classes of social activity videos collected from the Internet.
The eight classes are: birthday party  graduation party  music performance  non-music performance 
parade  wedding ceremony  wedding dance and wedding reception. The dataset contains a total
of 1466 videos and approximate 100 videos per-class for training and testing respectively. These
videos range from 20 seconds to 8 minutes averaging 3 minutes and contain very complex and
diverse contents  which brings signiﬁcant challenges for content analysis.
Each video is represented using three modalities  i.e.  static appearance  motion  and auditory.
Speciﬁcally  three visual and audio local keypoint features are extracted for each video: scale-
invariant feature transform (SIFT) [18]  spatial-temporal interest points (STIP) [19] and mel-
frequency cepstral coefﬁcients (MFCC) [20]. Then the three features are collected into a BoW
vector (5000 dimensions for SIFT and STIP  and 4000 dimensions for MFCC) using a soft-weighting
clustering algorithm  respectively.

4.2 Model comparisons

To evaluate the discriminative power of video topics learned by RTM  we present quantitative clas-
siﬁcation results compared with other supervised topic models (MedLDA and gClassRBM) in the
case of different modalities. We have tried our best to tune these compared models and report the
best results.

1Available at http://www.eecs.qmul.ac.uk/˜yf300/USAA/download/.

6

Table 1: Classiﬁcation accuracy of different supervised topic models for single-modal features.

Feature

Model

Accuracy

(%)

20 topics
30 topics
40 topics
50 topics
60 topics

SIFT
gClass
RBM
45.40
46.11
47.08
46.81
49.72

Med
LDA
44.72
44.17
43.07
42.80
40.74

RTM
51.99
53.09
55.83
54.17
54.03

Med
LDA
37.28
38.93
40.85
39.75
41.54

STIP
gClass
RBM
42.39
42.25
42.39
41.70
43.35

RTM
48.29
49.11
50.62
51.71
51.17

Med
LDA
34.71
38.55
41.15
41.98
38.27

MFCC
gClass
RBM
41.70
43.62
45.00
44.31
43.48

RTM
45.35
46.67
48.15
47.46
47.33

Table 2: Classiﬁcation accuracy of different methods for multimodal features.

Method

Accuracy

(%)

100 Inst

10 Inst

Multimodal RTM
60 topics
60.22
62.69
90 topics
63.79
120 topics
64.06
150 topics
64.72
180 topics
38.68
60 topics
41.29
90 topics
43.48
120 topics
43.72
150 topics
44.99
180 topics

RS+SVM Direct

SVM-UD+LR

SLAS+LR

54.60
56.10
57.34
59.26
60.63
23.73
28.53
30.59
33.47
35.94

66.0

65.0

65.0

29.0

37.0

40.0

Table 1 shows the classiﬁcation accuracy of different models for three single-modal features: SIFT 
STIP and MFCC. We can see that RTM achieves higher classiﬁcation accuracy than MedLDA and
gClassRBM in all cases  which demonstrates that through leveraging sparse Bayesian learning to
incorporate class label information into topic modeling  RTM can ﬁnd more discriminative topical
representations for complex video data.

4.3 Baseline comparisons

We compare Multimodal RTM with the baselines in [4] which are the best results on the USAA
dataset:
Direct Direct SVM or KNN classiﬁcation on raw video BoW vectors (14000 dimensions)  where
SVM is used for experiments with more than 10 instances and KNN otherwise.
SVM-UD+LR SVM attribute classiﬁers learn 69 user-deﬁned attributes  and then a logistic regres-
sion (LR) classiﬁer is performed according to the attribute classiﬁer outputs.
SLAS+LR Semi-latent attribute space is learned  and then a LR classiﬁer is performed based on the
69 user-deﬁned  8 class-conditional and 8 latent topics.
Besides  we also perform a comparison with another baseline where different modal topics extracted
by Replicated Softmax are connected together as video representations  and then a multi-class SVM
classiﬁer [21] is learned from the representations. This baseline is denoted by RS+SVM.
The results are illustrated in Table 2. Here the number of topics of each modality is assumed to be
the same. When the labeled training data is plentiful (100 instances per class)  the classiﬁcation per-
formance of Multimodal RTM is similar to the baselines in [4]. However  We argue that our model
learns a lower dimensional latent semantic space which provides efﬁcient video representations and
is able to be better generalized to a larger or new dataset because extra human deﬁned concepts are
not required in our model. When considering the classiﬁcation scenario where only a very small
number of training data are available (10 instances per class)  Multimodal RTM can achieve better
performance with an appropriate number (e.g.  (cid:62) 90) of topics because the sparsity of relevance top-
ics learned by RTM can effectively prevent overﬁtting to speciﬁc training instances. In addition  our
model outperforms RS+SVM in both cases  which demonstrates the advantage of jointly learning
topics and classiﬁer weights through sparse Bayesian learning.
It is also interesting to examine the sparsity of relevance topics. Figure 3 illustrates the degree of
correlation between topics and two different classes. We can see that the learned relevance topics are
very sparse  which leads to good generalisation for new instances and robustness for small datasets.

7

Figure 3: Relevance topics discovered by RTM for two different classes. Vertical axis indicates the
degree of positive and negative correlation.

5 Conclusion

This paper has proposed a supervised topic model  the relevance topic model (RTM)  to jointly learn
discriminative latent topical representations and a sparse classifer for recognizing unstructured so-
cial group activity. In RTM  sparse Bayisian learning is integrated with an undirected topic model
to discover sparse relevance topics. Rectiﬁed linear units are employed to better ﬁt complex video
data and facilitate the learning of the model. Efﬁcient variational methods are developed for pa-
rameter estimation and inference. To further improve video classiﬁcation performance  RTM is also
extended to deal with multimodal data. Experimental results demonstrate that RTM can ﬁnd more
predictive video topics than other supervised topic models and achieve state of the art classiﬁcation
performance  particularly in the scenario of lacking labeled training videos.

Appendix A. Parameters of free-form variational posterior q(tmj)

The expressions of parameters in q(tmj) (Equation 23) are listed as follows:

ωpos = N (α|β  γ + 1)  σ2

pos = (γ−1 + 1)−1  µpos = σ2

pos(

1
2
where erfc(·) is the complementary error function and

Z =

pos

+

ωnegerfc

neg = 1  µneg = β 

ωneg = N (α|0  γ)  σ2
1
2

ωposerfc

2σ2

(cid:18) −µpos(cid:113)
(cid:19)
ym + sm −(cid:80)
(cid:69)−1

(cid:42)ηηη·j
(cid:68)

(cid:16)

ηηη·jAηηηT·j

α =

j(cid:48)(cid:54)=j ηηη·j(cid:48)Atr

mj(cid:48)

N(cid:88)

(cid:18) µneg(cid:113)
(cid:17)
(cid:43)

2σ2

neg

α
γ

(cid:19)

 

 

+ β) 

(28)

(29)

(30)

(31)

q(ηηη)q(t)

(32)
We can see that q(tmj) depends on expectations over ηηη and {tmj(cid:48)}j(cid:48)(cid:54)=j  which is consistent with the
graphical model representation of RTM in Figure 2.

Wijvmi + Kbj.

  β =

γ =

q(ηηη)

i=1

ηηη·jAηηηT·j

Acknowledgments

This work was supported by the National Basic Research Program of China (2012CB316300)  Hun-
dred Talents Program of CAS  National Natural Science Foundation of China (61175003  61135002 
61203252)  and Tsinghua National Laboratory for Information Science and Technology Cross-
discipline Foundation.

References

[1] C. Schuldt  I. Laptev  and B. Caputo. Recognizing human actions: a local svm approach. In

ICPR  2004.

8

[2] M. Rodriguez  J. Ahmed  and M. Shah. Action mach a spatio-temporal maximum average

correlation height ﬁlter for action recognition. In CVPR  2008.

[3] Ucf50 action dataset. “http://vision.eecs.ucf.edu/data/ucf50.rar”.
[4] Y.W. Fu  T.M. Hospedales  T. Xiang  and S.G. Gong. Attribute learning for understanding

unstructured social activity. In ECCV  2012.

[5] R. Salakhutdinov and G.E. Hinton. Replicated softmax: an undirected topic model. In NIPS 

2009.

[6] M.E. Tipping. Sparse bayesian learning and the relevance vector machine. JMLR  2001.
[7] V. Nair and G.E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In

ICML  2010.

[8] D. Bohning. Multinomial logistic regression algorithm. AISM  1992.
[9] P. Turaga  R. Chellappa  V.S. Subrahmanian  and O. Udrea. Machine recognition of human

activities: a survey. TCSVT  2008.

[10] J. Varadarajan  R. Emonet  and J.-M. Odobez. A sequential topic model for mining recurrent

activities from long term video logs. IJCV  2013.

[11] D. Blei  A.Y. Ng  and M.I. Jordan. Latent dirichlet allocation. JMLR  2003.
[12] J. Zhu  A. Ahmed  and E.P. Xing. Medlda: Maximum margin supervised topic models. JMLR 

2012.

[13] H. Larochelle  M. Mandel  R. Pascanu  and Y. Bengio. Learning algorithms for the classiﬁca-

tion restricted boltzmann machine. JMLR  2012.

[14] R.M. Neal. Bayesian learning for neural networks. PhD thesis  University of Toronto  1995.
[15] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

Computation  2002.

[16] K.P. Murphy. Machine learning: a probabilistic perspective. MIT Press  2012.
[17] M. Harva and A. Kaban. Variational learning for rectiﬁed factor analysis. Signal Processing 

2007.

[18] D.G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV  2004.
[19] I. Laptev. On space-time interest points. IJCV  2005.
[20] B. Logan. Mel frequency cepstral coefﬁcients for music modeling. ISMIR  2000.
[21] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector learning for interde-

pendent and structured output spaces. In ICML  2004.

9

,Fang Zhao
Yongzhen Huang
Liang Wang
Tieniu Tan
Chaoyue Liu
Mikhail Belkin