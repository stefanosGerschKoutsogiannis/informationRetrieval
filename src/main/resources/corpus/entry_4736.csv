2018,Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling,Ridge leverage scores provide a balance between low-rank approximation and regularization  and are ubiquitous in randomized linear algebra and machine learning.  Deterministic algorithms are also of interest in the moderately big data regime  because deterministic algorithms provide interpretability to the practitioner by having no failure probability and always returning the same results. We provide provable guarantees for deterministic column sampling using ridge leverage scores.   The matrix sketch returned by our algorithm is a column subset of the original matrix  yielding additional interpretability.  Like the randomized counterparts  the deterministic algorithm provides $(1+\epsilon)$  error column subset selection  $(1+\epsilon)$ error projection-cost preservation  and an additive-multiplicative spectral bound.  We also show that under the assumption of power-law decay of ridge leverage scores  this deterministic algorithm is provably as accurate as randomized algorithms. Lastly  ridge regression is frequently used to regularize ill-posed linear least-squares problems.  While ridge regression provides shrinkage for the regression coefficients  many of the coefficients remain small but non-zero. Performing ridge regression with the matrix sketch returned by our algorithm and a particular regularization parameter forces coefficients to zero and has a provable $(1+\epsilon)$ bound on the statistical risk.  As such  it is an interesting alternative to elastic net regularization.,Ridge Regression and Provable Deterministic Ridge

Leverage Score Sampling

Shannon R. McCurdy

California Institute for Quantitative Biosciences

UC Berkeley

Berkeley  CA 94702

smccurdy@berkeley.edu

Abstract

Ridge leverage scores provide a balance between low-rank approximation and reg-
ularization  and are ubiquitous in randomized linear algebra and machine learning.
Deterministic algorithms are also of interest in the moderately big data regime 
because deterministic algorithms provide interpretability to the practitioner by
having no failure probability and always returning the same results. We provide
provable guarantees for deterministic column sampling using ridge leverage scores.
The matrix sketch returned by our algorithm is a column subset of the original
matrix  yielding additional interpretability. Like the randomized counterparts  the
deterministic algorithm provides (1 + ) error column subset selection  (1 + ) error
projection-cost preservation  and an additive-multiplicative spectral bound. We also
show that under the assumption of power-law decay of ridge leverage scores  this
deterministic algorithm is provably as accurate as randomized algorithms. Lastly 
ridge regression is frequently used to regularize ill-posed linear least-squares prob-
lems. While ridge regression provides shrinkage for the regression coefﬁcients 
many of the coefﬁcients remain small but non-zero. Performing ridge regression
with the matrix sketch returned by our algorithm and a particular regularization
parameter forces coefﬁcients to zero and has a provable (1 + ) bound on the
statistical risk. As such  it is an interesting alternative to elastic net regularization.

1

Introduction

Classical leverage scores quantify the importance of each column i for the range space of the sample-
by-feature data matrix A ∈ Rn×d. Classical leverage scores have been used in regression diagnostics 
outlier detection  and randomized matrix algorithms (Velleman and Welsch  1981; Chatterjee and
Hadi  1986; Drineas et al.  2008). Historically  leverage scores were used to select informative
samples (rows  in our matrix orientation). More recently  as datasets with d > n have become more
common  leverage scores have been used to select informative features (columns  in our matrix
orientation). There are many different ﬂavors of leverage scores  and we will focus on a variation
called ridge leverage scores. However  to appreciate the advantages of ridge leverage scores  we also
brieﬂy review classical and rank-k subspace leverage scores.
Ridge leverage scores were introduced by Alaoui and Mahoney (2015) to give statistical bounds
for the Nyström approximation for kernel ridge regression. Alaoui and Mahoney (2015) argue that
ridge leverage scores provide the relevant notion of leverage in the context of kernel ridge regression.
Ridge leverage scores have been successfully used in kernel ridge regression to approximate the
symmetric kernel matrix (∈ Rn×n) by selecting informative samples (Alaoui and Mahoney  2015;
Rudi et al.  2015). Cohen et al. (2017) provide a deﬁnition for ridge leverage scores for selecting
informative features from the non-symmetric sample-by-feature data matrix A ∈ Rn×d. The ridge

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

leverage score ¯τi(A) for the ith column of A is 

(cid:0)AAT + λ2I(cid:1)+

¯τi(A) = aT
i

(1)
where the ith column of A is an (n × 1)-vector denoted by ai  M+ denotes the Moore-Penrose
k||A −
pseudoinverse of M  and λ2 is the regularization parameter. We will always choose λ2 = 1
Ak||2
F   where Ak is the rank-k SVD approximation to A  deﬁned in Sec. 1.2  because this choice of
regularization parameter gives the stated guarantees. In contrast to ridge leverage scores  the rank-k
subspace leverage score τi(Ak) is 

ai 

k )+ai.

i (AkAT

τi(Ak) = aT

(2)
The classical leverage score is the ridge leverage score (and also the rank-k subspace leverage score)
evaluated at k = rank(A) = r ≤ n.
Ridge leverage scores and rank-k subspace leverage scores take two different approaches to mitigating
the small singular values components of AAT in classical leverage scores. Ridge leverage scores
diminish the importance of small principle components through regularization  as opposed to rank-k
subspace leverage scores  which omit the small principle components entirely. Cohen et al. (2017)
argue that regularization is a more natural and stable alternative to omission. For randomized
algorithms with ridge leverage score sampling  Cohen et al. (2017) prove bounds for the spectrum 
column subset selection  and projection-cost preservation (counterparts to our Theorems 1  2  and 3
for deterministic ridge leverage scores  respectively). The ﬁrst and the last bounds hold for a weighted
column subset of the full data matrix. These bounds require O(k log(k/δ)/2) columns  where δ is
the failure probability and  is the error.
In the "big data" era  much attention has been paid to randomized algorithms due to improved
algorithm performance and ease of generalization to the streaming setting. However  for moderately
big data (i.e. the feature set is too large for inspection by humans  but the algorithm performance
is not a limitation)  deterministic algorithms provide more interpretability to the practitioner than
randomized algorithms  since they always provide the same results and have no failure probability.
The usefulness of deterministic algorithms has already been recognized. Papailiopoulos et al. (2014)
introduce a deterministic algorithm for sampling columns from rank-k subspace leverage scores
and provide a columns subset selection bound (the counterpart to our Theorem 2 for deterministic
ridge leverage scores). McCurdy et al. (2017) prove a (1 + ) spectral bound for Papailiopoulos
et al. (2014)’s deterministic algorithm and for random sampling with rank-k subspace leverage scores
(the counterpart to our Theorem 1 for deterministic ridge leverage scores). One major drawback
of using the rank-k subspace leverage scores is that their relative spectral bound is limited to the
rank-k subspace projection of the column subset matrix C and the full data matrix A  so to get a
relative spectral bound on the complete subspace requires k = n. A consequence of this is that
projection-cost preservation also requires k = n (the counterpart to our Theorem 3). One advantage
of using deterministic rather than randomized rank-k subspace leverage score algorithms is that under
the condition of power-law decay in the sorted rank-k subspace leverage scores  the deterministic
algorithm chooses fewer columns than random sampling with the same error for the column subset
a−1 − 1  k) < Ck log(k/δ)/2  where a is
selection bound when max((2k/)
the decay power and C is an absolute constant (Papailiopoulos et al.  2014) (this is the counterpart to
our Theorem 5). In addition  Papailiopoulos et al. (2014) show that many real datasets display power-
law decay in the sorted rank-k subspace leverage scores  illustrating the deterministic algorithm’s
real-world utility.
Ridge regression (Hoerl and Kennard  1970) is a commonly used method to regularize ill-posed linear
least-squares problems. The ridge regression minimization problem is  for outcome y ∈ Rn  features
A ∈ Rn×d  and coefﬁcients x ∈ Rd 

a − 1  (2k/((a − 1)))

1

1

(cid:0)||y − Ax||2
= (cid:0)AT A + λ2I(cid:1)−1

x

ˆxA = argmin

2 + λ2||x||2
AT y.

2

(cid:1)

(3)
where the regularization parameter λ2 penalizes the size of the coefﬁcients in the minimization
problem. We will always choose λ2 = 1
In ridge regression  the underlying statistical model for data generation is 

F for ridge regression with matrix A.

k||A − Ak||2

(4)

y = y∗ + σ2ξ 

2

where y∗ = Ax∗ is a deterministic linear function of the ﬁxed design features A and ξ ∼ N (0  I) is
the random error. The mean squared error is a measure of statistical risk R(ˆy) for the squared error
loss function and estimator ˆy and is 

(cid:2)||ˆy − y∗||2

(cid:3) .

2

R(ˆy) = 1

n Eξ

(5)

Ridge regression is often chosen over regression subset selection procedures for regularization
because  as a continuous shrinkage method  it exhibits lower variability (Breiman  1996). However
many ridge regression coefﬁcients can be small but non-zero  leading to a lack of interpretability for
moderately big data (d > n). The lasso method (Tibshirani  1994) provides continuous shrinkage
and automatic feature selection using an L1 penalty function instead of the L2 penalty function in
ridge regression  but for d > n case  lasso saturates at n features. The elastic net algorithm combines
lasso (L1 penalty function) and ridge regression (L2 penalty function) for continuous shrinkage and
automatic feature selection (Zou and Hastie  2005).

1.1 Contributions

We explore deterministic ridge leverage score (DRLS) sampling for matrix approximation and for
feature selection in concert with ridge regression. This work has two main motivations: (1) the
advantages of ridge leverage scores over rank-k subspace leverage scores  and (2) the advantages
of deterministic algorithms in some practical settings. This work complements Papailiopoulos et al.
(2014)  who considered deterministic rank-k subspace leverage sampling and experiments on real
data  but did not consider DRLS sampling or uses beyond matrix approximation. This work also
complements Cohen et al. (2017)  who considered randomized RLS sampling but did not consider
DRLS sampling  the uses of RLS sampling beyond matrix approximation (e.g. ridge regression)  or
experiments on real data.
We introduce a deterministic algorithm (Algorithm 1) for ridge leverage score sampling inspired by the
deterministic algorithm for rank-k subspace leverage score sampling (Papailiopoulos et al.  2014). By
using ridge leverage scores instead of rank-k subspace scores in the deterministic algorithm  we prove
signiﬁcantly better bounds for the column subset matrix C (see Table 1 for a comparison). We prove
that the same additive-multiplicative spectral bounds (Theorem 1)  (1 + ) columns subset selection
(Theorem 2)  and (1+) projection-cost preservation (Theorem 3) hold for DRLS column sampling as
for random sampling as in Cohen et al. (2017). We show that under the condition of power-law decay
in the ridge leverage scores  the deterministic algorithm chooses fewer columns than random sampling
a−1 − 1  k) < Ck log(k/δ)/2  where
with the same error when max((4k/)
a is the decay power and C is an absolute constant (Theorem 5).
We combine deterministic ridge leverage score column subset selection with ridge regression for a
particular value of the regularization parameter  providing automatic feature selection and continuous
shrinkage. This procedure has a provable (1 + ) bound on the statistical risk (Theorem 4). The
proof techniques are such that a (1 + ) bound on the statistical risk also holds for randomized ridge
leverage score sampling. Our ridge regression theorem is novel to both deterministic and randomized
sampling with ridge leverage scores (as far as we know  this has never been considered for any
leverage score)  another demonstrable advance of the state of the art  and one of our main results.
We also provide a proof-of-concept illustration on real biological data  with ﬁgures included in the
Supplementary Materials. Our real-data illustration makes a strong case for the empirical usefulness
of the DRLS algorithm and bounds. The real data exhibits striking power law decay of the ridge
leverage scores (Figure 7)  justifying the assumptions underlying the use of DRLS sampling (Theorem
5).
Our work is triply beneﬁcial from the interpretability standpoint; it is deterministic  it chooses a
subset of representative columns  and it comes with four desirable error guarantees for all rank-k 
three of which stem from naturalness of the low-rank ridge regularization.

a − 1  (4k/((a − 1)))

1

1

1.2 Notation
The singular value decomposition (SVD) of any complex matrix A is A = UΣV†  where U and
V are square unitary matrices (U†U = UU† = I  V†V = VV† = I)  Σ is a rectangular diagonal
matrix with real non-negative non-increasingly ordered entries. U† is the complex conjugate and

3

Table 1: Comparison of deterministic ridge and rank-k subspace leverage score theorems.

Deterministic Sampling Algorithm

Rank-k Subspace

Papailiopoulos et al. (2014)

Rank-k Ridge
Algorithm 1

Spectral Bound for CCT

Column Subset Selection

Rank-k Projection
Cost Preservation
Approximate Ridge

Regression Risk

Leverage Power-law Decay

Multiplicative  k = n
McCurdy et al. (2017)

all k

Papailiopoulos et al. (2014)

k = n

N/A

all k

Papailiopoulos et al. (2014)

Additive-Multiplicative  all k

Theorem 1

all k

Theorem 2

Theorem 3

Theorem 4

all k

all k

all k

Theorem 5

transpose of U  and I is the identity matrix. The diagonal elements of Σ are called the singular
values  and they are the positive square roots of the eigenvalues of both AA† and A†A  which have
eigenvectors U and V  respectively. U and V are the left and right singular vectors of A.
Deﬁning Uk as the ﬁrst k columns of U and analogously for V  and Σk the square diagonal
†
k is the rank-k SVD approximation to
matrix with the ﬁrst k entries of Σ  then Ak = UkΣkV
A.Furthermore  we refer to matrix with only the last n − k columns of U  V and last n − k entries in
Σ as U\k  V\k  and Σ\k.
The Moore-Penrose pseudo inverse of a rank k matrix A is given by A+ = VkΣ−1
The Frobenius norm ||A||F of a matrix A is given by ||A||2
of a matrix A is given by the largest singular value of A.

F = tr (AA†). The spectral norm ||A||2

k U

†
k.

2 Deterministic Ridge Leverage Score (DRLS) Column Sampling

2.1 The DRLS Algorithm

Algorithm 1. The DRLS algorithm selects for the submatrix C all columns i with ridge leverage
score ¯τi(A) above a threshold θ  determined by the error tolerance . This algorithm is deeply
indebted to the deterministic algorithm of Papailiopoulos et al. (2014). It substitutes ridge leverage
scores for rank-k subspace scores  and has a different stopping parameter. The algorithm is as
follows.

1. Choose the error tolerance  .
2. For every column i  calculate the ridge leverage scores ¯τi(A) (Eqn. 1).
3. Sort the columns by ¯τi(A)  from largest to smallest. The sorted column indices are πi.
4. Deﬁne an empty set Θ = {}. Starting with the largest sorted column index π0  add the

corresponding column index i to the set Θ  in decreasing order  until 

(cid:88)

¯τi(A) > ¯t −  

and then stop. Note that ¯t =(cid:80)d
deﬁne ˜ =(cid:80)

i∈Θ
i=1 ¯τi(A) ≤ 2k (see Sec.1.2 for proof). It will be useful to

(6)

i(cid:54)∈Θ ¯τi(A). Eqn. 6 can equivalently be written as  > ˜.

5. If the set size |Θ| < k  continue adding columns in decreasing order until |Θ| = k.
6. The leverage score ¯τi(A) of the last column i included in Θ deﬁnes the leverage score
7. Introduce a rectangular selection matrix S of size d × |Θ|. If the column indexed by (i  πi)

threshold θ.

is in Θ  then Si πi = 1. Si πi = 0 otherwise. The DRLS submatrix is C = AS.

4

Note that when the ridge leverage scores on either side of the threshold are not equal  the algorithm
returns a unique solution. Otherwise  there are as many solutions as there are columns with equal
ridge leverage scores at the threshold.

Algorithm 1 requires O(min(d  n)nd) arithmetic operations.

3 Approximation Guarantees

3.1 Bounds for DRLS

We derive a new additive-multiplicative spectral approximation bound (Eqn. 7) for the square of the
submatrix C selected with DRLS.
Theorem 1. Additive-Multiplicative Spectral Bound: Let A ∈ Rn×d be a matrix of at least rank k
and ¯τi(A) be deﬁned as in Eqn. 1. Construct C following the DRLS algorithm described in Sec. 2.1.
Then C satisﬁes 

(7)
The symbol (cid:22) denotes the Loewner partial ordering which is reviewed in Sec 1.1 (see Horn and
Johnson (2013) for a thorough discussion).

||A\k||2

F I (cid:22) CCT (cid:22) AAT .

(1 − )AAT − 
k

Conceptually  the Loewner ordering in Eqn. 7 is the generalization of the ordering of real numbers
(e.g. 1 < 1.5) to Hermitian matrices. Statements of Loewner ordering are quite powerful; important
consequences include inequalities for the eigenvalues. We will use Eqn. 7 to prove Theorems 2  3 
and 4. Note that our additive-multiplicative bound holds for an un-weighted column subset of A.
Theorem 2. Column Subset Selection: Let A ∈ Rn×d be a matrix of at least rank k and ¯τi(A) be
deﬁned as in Eqn. 1. Construct C following the DRLS algorithm described in Sec. 2.1. Then C
satisﬁes 

||A − CC+A||2

C k(A)||2

F ≤ ||A − ΠF
C k(A) = (CC+A)k is the best rank-k approximation to A in the

F ≤ (1 + 4)||A\k||2
F  

(8)

0 <  < 1

with
column space of C with the respect to the Frobenius norm.

4 and where ΠF

Column subset selection algorithms are widely used for feature selection for high-dimensional data 
since the aim of the column subset selection problem is to ﬁnd a small number of columns of A that
approximate the column space nearly as well as the top k singular vectors.
Theorem 3. Rank-k Projection-Cost Preservation: Let A ∈ Rn×d be a matrix of at least rank k and
¯τi(A) be deﬁned as in Eqn. 1. Construct C following the DRLS algorithm described in Sec. 2.1.
Then C satisﬁes  for any rank k orthogonal projection X ∈ Rn×n 

(1 − )||A − XA||2

(9)
To simplify the bookkeeping  we prove the lower bound of Theorem 3 with (1 − α) error (α =
2(2 +

2))  and assume 0 <  < 1
2 .

√

F ≤ ||C − XC||2

F ≤ ||A − XA||2
F .

Projection-cost preservation bounds were formalized recently in Feldman et al. (2013); Cohen et al.
(2015). Bounds of this type are important because it means that low-rank projection problems can be
solved with C instead of A while maintaining the projection cost. Furthermore  the projection-cost
preservation bound has implications for k-means clustering  because the k-means objective function
can be written in terms of the orthogonal rank-k cluster indicator matrix (Boutsidis et al.  2009).1
Note that our rank-k projection-cost preservation bound holds for an un-weighted column subset of
A.
A useful lemma on an approximate ridge leverage score kernel comes from combining Theorem 1
and 3.
Lemma 1. Approximate ridge leverage score kernel: Let A ∈ Rn×d be a matrix of at least rank
k and ¯τi(A) be deﬁned as in Eqn. 1. Construct C following the DRLS algorithm described in

1Thanks to Michael Mahoney for this point.

5

||y − Ax||2

 .

d(cid:88)

j=1

K(M) =(cid:0)MMT + 1

Sec. 2.1. Let α be the coefﬁcient in the lower bound of Theorem 3 and assume 0 <  < 1

F I(cid:1)+ for matrix M ∈ Rn×l. Then K(C) and K(A) satisfy 

k||M\k||2

2 . Let

K(A) (cid:22) K(C) (cid:22)

1

1 − (α + 1)

K(A).

(10)

Theorem 4. Approximate Ridge Regression with DRLS: Let A ∈ Rn×d be a matrix of at least rank
k and ¯τi(A) be deﬁned as in Eqn. 1. Construct C following the DRLS algorithm described in Sec.
2.1  let α be the coefﬁcient in the lower bound of Theorem 3  and assume 0 <  < 1
2 . Choose
for ridge regression with a matrix M (Eqn. 3). Under
the regularization parameter λ2 =
these conditions  the statistical risk R(ˆyC) of the ridge regression estimator ˆyC is bounded by the
statistical risk R(ˆyA) of the ridge regression estimator ˆyA:

2α < 1

||M\k||2

k

F

R(ˆyC) ≤ (1 + β)R(ˆyA) 

(11)

where β = 2α(−1+2α+3α2)

(1−α)2

.

Theorem 4 means that there are bounds on the statistical risk for substituting the DRLS selected
column subset matrix for the complete matrix when performing ridge regression with the appropriate
regularization parameter. Performing ridge regression with the column subset C effectively forces
coefﬁcients to be zero and adds the beneﬁts of automatic feature selection to the L2 regularization
problem. We also note that the proof of Theorem 4 relies only on Theorem 1 and Theorem 3 and
facts from linear algebra  so a randomized selection of weighted column subsets that obey similar
bounds to Theorem 1 and Theorem 3 (e.g. Cohen et al. (2017)) will also have bounded statistical risk 
albeit with a different coefﬁcient β. As a point of comparison  consider the elastic net minimization
with our ridge regression regularization parameter:

ˆxE = argmin

x

2 +

||A\k||2

F||x||2

2 + λ1

1
k

|xj|

(12)

The risk of elastic net R(ˆyE) has the following bound in terms of the risk of ridge regression R(ˆyA):

R(ˆyE = AˆxE) = R(ˆyA) + λ2

1

4d||A||2
k2||A\k||4

2

1

F

(13)

2

.

F

1 ≈ β

R(ˆyA)
4d||A||2

k2||A\k||4

This comes from a slight re-working of Theorem 3.1 of Zou and Zhang (2009). The bounds for the
elastic net risk and R(ˆyC) are comparable when λ2
Ridge regression is a special case of kernel ridge regression with a linear kernel. While previous
work in kernel ridge regression has considered the use of ridge leverage scores to approximate the
symmetric kernel matrix by selecting a subset of n informative samples (Alaoui and Mahoney  2015;
Rudi et al.  2015)  to our knowledge  no previous work has used ridge leverage scores to approximate
the symmetric kernel matrix using ridge leverage scores to select a subset of the f informative features
(after the feature mapping of the d-dimensional data points). The latter case would be the natural
generalization of Theorem 4 to non-linear kernels  and remains an interesting open question. Lastly 
we note that placing statistical assumptions on A in the spirit of (Rudi et al.  2015) may lead to an
improved bound for random designs for A.
Theorem 5. Ridge Leverage Power-law Decay: Let A ∈ Rn×d be a matrix of at least rank k and
¯τi(A) be deﬁned as in Eqn. 1. Furthermore  let the ridge leverage scores exhibit power-law decay in
the sorted column index πi 

¯τπi(A) = π−a

i ¯τπ0 (A)

a > 1.

(14)

Construct C following the DRLS algorithm described in Sec. 2.1. The number of sample columns
selected by DRLS is 

(cid:18)(cid:0) 4k

(cid:1) 1



(cid:16) 4k

(a−1)

(cid:17) 1

(cid:19)

|Θ| ≤ max

a − 1 

a−1 − 1  k

.

(15)

6

Theorem 3 of Papailiopoulos et al. (2014) introduces the concept of power-law decay behavior for
leverage scores for rank-k subspace leverage scores. Our Theorem 5 is an adaptation of Papailiopoulos
et al. (2014)’s Theorem 3 for ridge leverage scores.
An obvious extension of Eqn. 7 is the following bound 

(1 − )AAT − 
k

||A\k||2

F I (cid:22) CCT (cid:22) (1 + )AAT +

||A\k||2

F I 


k

(16)

2 ln(cid:0) k

(cid:1))

which also holds for C selected by ridge leverage random sampling methods with O( k
weighted columns and failure probability δ Cohen et al. (2017). Thus  DRLS selects fewer columns
with the same accuracy  in Eqn. 16 for power-law decay in the ridge leverage scores when 

δ

max

(17)
where C is an absolute constant. In particular  when a ≥ 2  the number of columns deterministically
sampled is O(k).2

(a−1)

< C k



a − 1 

a−1 − 1  k

(cid:18)(cid:0) 4k

(cid:1) 1

(cid:16) 4k

(cid:17) 1

(cid:19)

2 ln(cid:0) k

δ

(cid:1) 

4 Biological Data Illustration

We provide a biological data illustration of ridge leverage scores and ridge regression with multi-omic
data from lower-grade glioma (LGG) tumor samples collected by the TCGA Research Network
(http://cancergenome.nih.gov/). Diffuse lower-grade gliomas are inﬁltrative brain tumors that
occur most frequently in the cerebral hemisphere of adults.
The data is publicly available and hosted by the Broad Institute’s GDAC Firehose (Broad Institute
of MIT and Harvard  2016). We download the data using the R tool T CGA2ST AT (Wan et al. 
2016). T CGA2ST AT imports the latest available version-stamped standardized Level 3 dataset on
Firehose. The data collection and data platforms are discussed in detail in the original paper (The
Cancer Genome Atlas Research Network  2015).
We use the following multi-omic data types: mutations (d = 4845)  DNA copy number (alteration
(d = 22618) and variation (d = 22618))  messenger RNA (mRNA) expression (d = 20501)  and
microRNA expression (d = 1046). Methylation data is also available  but we omit it due to memory
constraints. The mRNA and microRNA data is normalized. DNA copy number (variation and
alteration) has an additional pre-processing step; the segmentation data reported by TCGA is turned
into copy number using the R tool CN tools (Zhang  2015) that is imbedded in T CGA2ST AT . The
mutation data is ﬁltered based on status and variant classiﬁcation and then aggregated at the gene
level (Wan et al.  2016).
There are 280 tumor samples and d = 71628 multi-omic features in the downloaded dataset. We
are interested in performing ridge regression with the biologically meaningful outcome variables
relating to mutations of the "IDH1" and "IDH2" gene and deletions of the "1p/19q" chromosome
arms ("codel"). These variables were shown to be predictive of favorable clinical outcomes and can
be found in the supplemental tables (The Cancer Genome Atlas Research Network  2015). We restrict
to samples with these outcome variables (275 tumor samples)  and we drop an additional sample
("TCGA-CS-4944") because it is an outlier with respect to the k = 3 SVD projection of the samples.
This leaves a total of 274 tumor samples with outcome variables "IDH" (a mutation in either "IDH1"
or "IDH2") and "codel" for the analysis.
Lastly  we drop all multi-omic features that have zero columns and greater than 10% missing data
on the 274 tumor samples. We the replace missing values with the mean of the column. This
leaves a ﬁnal multi-omic feature set of d = 68522 for the 274 tumor samples. Our ﬁnal matrix
A ∈ R274×68522 is column mean-centered. Figure 1 shows a pie chart of the breakdown of the ﬁnal
matrix A’s multi-omic feature types.

4.1 Ridge leverage score sampling

Figure 2 shows the spectrum of eigenvalues of AAT for LGG. The eigenvalues range of multiple
orders of magnitude. We choose k = 3 for the DRLS algorithm because these components are

2Thanks to Ahmed El Alaoui for this point.

7

of columns kept  |Θ|  and ˜ =(cid:80)

meaningful for the "IDH" and "codel" outcome variables (see Figures 3  4   and 5). The top
three components capture 79% of the Frobenius norm |A|2
F . Applying the DRLS algorithm with
k = 3   = 0.1 leads to |Θ| = 1512  selecting approximately 0.02% of the total multi-omic features
for the column subset matrix C. The majority of the features selected are mRNA (1473 features) 
and the remainder are microRNA (39 features). Figure 6 shows the relationship between the number
i(cid:54)∈Θ ¯τi(A) for the k = 3 ridge leverage scores. Only a small error
penalty is incurred by a dramatic reduction in the number of columns kept according to Algorithm 1.
Figure 7 shows the power-law decay of the LGG k = 3 ridge leverage scores with sorted column
index. This LGG multi-omic data example shows that ridge leverage score power-law decay occurs
in the wild. Figure 8 shows a histogram of the ratio of ||C − XC||2
F for 1000 random
rank-k = 3 orthogonal projections X. The projections are chosen as the ﬁrst 3 directions from an
orthogonal basis randomly selected with respect to the Haar measure for the orthogonal group O(n)
(Mezzadri  2006). This conﬁrms that the projection cost empirically has very small error. Lastly 
Figure 9 illustrates the k = 3 ridge leverage score regularization of the classical leverage score for the
LGG multi-omic features. As expected  many of the columns’ ridge leverage scores exhibit shrinkage
when compared to the classical leverage scores. Table 2 includes ratios derived from the full data
matrix A and the column subset matrix C selected by the DRLS algorithm with k = 3   = 0.1.

F /||A − XA||2

4.2 Ridge regression with ridge leverage score sampling

We perform ridge regression with the appropriate regularization parameter for two biologically
meaningful outcome variables; the ﬁrst is whether either the "IDH1" or the "IDH2" gene is mutated
and the second whether the "1p/19q" chromosome arms have deletions ("codel"). We encode the
status of each event as ±1. Figures 3  4   and 5 show the top three SVD projections for the tumor
samples  colored by the combined status for "IDH" and "codel". No tumor samples have the "1p/19q"
codeletion and no "IDH" mutation. Visual inspection of the SVD plot conﬁrms that this is a reasonable
regression problem for "IDH" and a difﬁcult regression problem for "codel"; also  logisitic regression
would be more natural for binary outcomes. We proceed anyway  since our objective is to compare
ridge regression with all of the features (A) to ridge regression with the DRLS subset (C) on realistic
biological data. Figures 10 and 11 conﬁrm that the ridge regression ﬁts are close (ˆyA − ˆyC) for
all the tumor samples. Figures 12 and 13 conﬁrm that the ridge regression coefﬁcients are close
(ˆxA − ˆxC) for all the tumor samples. Figure 14 and 15 illustrate the overall performance of ridge
regression for these two outcome variables.
Lastly  we simulate 274 samples y according to the linear model (Eqn. 4)  where y∗ = Ax∗ 
the coefﬁcients x∗ ∼ N (0  I)  and A is the LGG multi-omic feature matrix. We choose σ2 =
{10−3  1  103}. We perform ridge regression with A and then again with C in accordance with
Theorem 4. We calculate the risks R(ˆyA) and R(ˆyC) and ﬁnd that Theorem 4 is not violated. Table
2 shows the risk ratios R(ˆyC)/R(ˆyA) along with other relevant ratios for the ridge leverage scores.

Table 2: Ridge leverage score ratios for k = 3   = 0.1 for LGG tumor multi-omic data. The ratios
are near one  as expected. Ridge regression risk ratio R(ˆyC)/R(ˆyA) for data simulated from the
LGG multi-omic matrix A and Eqn. 4.
||C\k||2

F ave( ¯Σ2/ ¯Σ2
C)

C/Σ2)

F /||A\k||2
0.97

1.03

ave(Σ2

0.85

Algorithm 1
k = 3   = 0.1

σ2 R(ˆyC)/R(ˆyA)
10−3
100
103

0.99
0.99
0.99

Acknowledgements

Research reported in this publication was supported by the National Human Genome Research
Institute of the National Institutes of Health under Award Number F32HG008713. The content is
solely the responsibility of the authors and does not necessarily represent the ofﬁcial views of the
National Institutes of Health. SRM thanks Michael Mahoney  Ahmed El Alaoui  Elaine Angelino 
and Kai Rothauge for thoughtful comments and the Barcellos and Pachter Labs.

8

Supporting Information

Software in the form of python and R code is available at https://github.com/srmcc/
deterministic-ridge-leverage-sampling. Code for downloading the data and reproducing
all of the ﬁgures is included. Proofs and ﬁgures are included in the Supplementary Material.

References
Ahmed El Alaoui and Michael W. Mahoney. 2015. Fast Randomized Kernel Ridge Regression with
Statistical Guarantees. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1 (NIPS’15). MIT Press  Cambridge  MA  USA  775–783. http:
//dl.acm.org/citation.cfm?id=2969239.2969326 http://arxiv.org/abs/1411.0306.

Christos Boutsidis  Petros Drineas  and Michael W Mahoney. 2009. Unsupervised Feature
In Advances in Neural Information Pro-
Selection for the k-means Clustering Problem.
cessing Systems 22  Y. Bengio  D. Schuurmans  J. D. Lafferty  C. K. I. Williams  and
A. Culotta (Eds.). Curran Associates  Inc.  153–161.
http://papers.nips.cc/paper/
3724-unsupervised-feature-selection-for-the-k-means-clustering-problem.
pdf

Leo Breiman. 1996. Heuristics of instability and stabilization in model selection. The Annals of

Statistics 24  6 (Dec. 1996)  2350–2383. https://doi.org/10.1214/aos/1032181158

Broad Institute of MIT and Harvard. 2016. Broad Institute TCGA Genome Data Analysis Center
(2016): Analysis-ready standardized TCGA data from Broad GDAC Firehose 2016_01_28 run.
(Jan. 2016). https://doi.org/10.7908/C11G0KM9 Dataset.

Samprit Chatterjee and Ali S. Hadi. 1986. Inﬂuential Observations  High Leverage Points  and
Outliers in Linear Regression. Statist. Sci. 1  3 (Aug. 1986)  379–393. https://doi.org/10.
1214/ss/1177013622

Michael B. Cohen  Sam Elder  Cameron Musco  Christopher Musco  and Madalina Persu. 2015.
Dimensionality Reduction for k-Means Clustering and Low Rank Approximation. In Proceedings
of the Forty-seventh Annual ACM Symposium on Theory of Computing (STOC ’15). ACM  New
York  NY  USA  163–172. https://doi.org/10.1145/2746539.2746569

Michael B. Cohen  Cameron Musco  and Christopher Musco. 2017. Input Sparsity Time Low-rank
Approximation via Ridge Leverage Score Sampling. In Proceedings of the Twenty-Eighth Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA ’17). Society for Industrial and Applied
Mathematics  Philadelphia  PA  USA  1758–1777. http://dl.acm.org/citation.cfm?id=
3039686.3039801

Petros Drineas  Michael W. Mahoney  and S. Muthukrishnan. 2008. Relative-Error $CUR$ Matrix
Decompositions. SIAM J. Matrix Anal. Appl. 30  2 (Sept. 2008)  844–881. https://doi.org/
10.1137/07070471X

D. Feldman  M. Schmidt  and C. Sohler. 2013. Turning Big data into tiny data: Constant-size
coresets for k-means  PCA and projective clustering. In Proceedings of the Twenty-Fourth Annual
ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics 
1434–1453. https://doi.org/10.1137/1.9781611973105.103

Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Regression: Biased Estimation for Nonorthogo-
nal Problems. Technometrics 12  1 (Feb. 1970)  55–67. https://doi.org/10.1080/00401706.
1970.10488634

Roger A. Horn and Charles R. Johnson. 2013. Matrix analysis (2nd ed ed.). Cambridge University

Press  New York.

Shannon McCurdy  Vasilis Ntranos  and Lior Pachter. 2017. Column subset selection for single-cell

RNA-Seq clustering. bioRxiv (July 2017)  159079. https://doi.org/10.1101/159079

Francesco Mezzadri. 2006. How to generate random matrices from the classical compact groups.

Notices of the American Mathematical Society 54 (Oct. 2006).

9

Dimitris Papailiopoulos  Anastasios Kyrillidis  and Christos Boutsidis. 2014. Provable Deterministic
Leverage Score Sampling. In Proceedings of the 20th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD ’14). ACM  New York  NY  USA  997–1006.
https://doi.org/10.1145/2623330.2623698

Alessandro Rudi  Raffaello Camoriano  and Lorenzo Rosasco. 2015. Less is More: Nystr\"om
Computational Regularization. arXiv:1507.04717 [cs  stat] (July 2015). http://arxiv.org/
abs/1507.04717 arXiv: 1507.04717.

The Cancer Genome Atlas Research Network. 2015. Comprehensive  Integrative Genomic Analysis
of Diffuse Lower-Grade Gliomas. The New England journal of medicine 372  26 (June 2015) 
2481–2498. https://doi.org/10.1056/NEJMoa1402121

Robert Tibshirani. 1994. Regression Shrinkage and Selection Via the Lasso. Journal of the Royal

Statistical Society  Series B 58 (1994)  267–288.

Paul F. Velleman and Roy E. Welsch. 1981. Efﬁcient Computing of Regression Diagnostics. The

American Statistician 35  4 (1981)  234–242. https://doi.org/10.2307/2683296

Ying-Wooi Wan  Genevera I. Allen  and Zhandong Liu. 2016. TCGA2STAT: simple TCGA data
access for integrated statistical analysis in R. Bioinformatics 32  6 (March 2016)  952–954.
https://doi.org/10.1093/bioinformatics/btv677

Jianhua Zhang. 2015. CNTools: Convert segment data into a region by sample matrix to allow for other
high level computational analyses. (2015). http://bioconductor.org/packages/CNTools/
R package version 1.26.0.

Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the Elastic Net. Journal

of the Royal Statistical Society  Series B 67 (2005)  301–320.

Hui Zou and Hao Helen Zhang. 2009. On the adaptive elastic-net with a diverging number of
parameters. The Annals of Statistics 37  4 (Aug. 2009)  1733–1751. https://doi.org/10.
1214/08-AOS625

10

,Peter Sadowski
Daniel Whiteson
Pierre Baldi
Akshay Krishnamurthy
Alekh Agarwal
Miro Dudik
Shannon McCurdy