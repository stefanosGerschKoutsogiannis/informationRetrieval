2018,Almost Optimal Algorithms for Linear Stochastic Bandits with Heavy-Tailed Payoffs,In linear stochastic bandits  it is commonly assumed that payoffs are with sub-Gaussian noises. In this paper  under a weaker assumption on noises  we study the problem of \underline{lin}ear stochastic {\underline b}andits with h{\underline e}avy-{\underline t}ailed payoffs (LinBET)  where the distributions have finite moments of order $1+\epsilon$  for some $\epsilon\in (0 1]$. We rigorously analyze the regret lower bound of LinBET as $\Omega(T^{\frac{1}{1+\epsilon}})$  implying that finite moments of order 2 (i.e.  finite variances) yield the bound of $\Omega(\sqrt{T})$  with $T$ being the total number of rounds to play bandits. The provided lower bound also indicates that the state-of-the-art algorithms for LinBET are far from optimal. By adopting median of means with a well-designed allocation of decisions and truncation based on historical information  we develop two novel bandit algorithms  where the regret upper bounds match the lower bound up to polylogarithmic factors. To the best of our knowledge  we are the first to solve LinBET optimally in the sense of the polynomial order on $T$.  Our proposed algorithms are evaluated based on synthetic datasets  and outperform the state-of-the-art results.,Almost Optimal Algorithms for Linear Stochastic

Bandits with Heavy-Tailed Payoffs

Han Shao∗

Xiaotian Yu∗

Irwin King

Michael R. Lyu

Department of Computer Science and Engineering

The Chinese University of Hong Kong
{hshao xtyu king lyu}@cse.cuhk.edu.hk

Abstract

In linear stochastic bandits  it is commonly assumed that payoffs are with sub-
Gaussian noises. In this paper  under a weaker assumption on noises  we study the
problem of linear stochastic bandits with heavy-tailed payoffs (LinBET)  where
the distributions have ﬁnite moments of order 1 + ϵ  for some ϵ ∈ (0  1]. We
1
rigorously analyze the regret lower bound of LinBET as Ω(T
1+ϵ )  implying that
ﬁnite moments of order 2 (i.e.  ﬁnite variances) yield the bound of Ω(√T )  with T
being the total number of rounds to play bandits. The provided lower bound also
indicates that the state-of-the-art algorithms for LinBET are far from optimal. By
adopting median of means with a well-designed allocation of decisions and trun-
cation based on historical information  we develop two novel bandit algorithms 
where the regret upper bounds match the lower bound up to polylogarithmic fac-
tors. To the best of our knowledge  we are the ﬁrst to solve LinBET optimally in
the sense of the polynomial order on T . Our proposed algorithms are evaluated
based on synthetic datasets  and outperform the state-of-the-art results.

1 Introduction

The decision-making model named Multi-Armed Bandits (MAB)  where at each time step an algo-
rithm chooses an arm among a given set of arms and then receives a stochastic payoff with respect
to the chosen arm  elegantly characterizes the tradeoff between exploration and exploitation in se-
quential learning. The algorithm usually aims at maximizing cumulative payoffs over a sequence of
rounds. A natural and important variant of MAB is linear stochastic bandits with the expected payoff
of each arm satisfying a linear mapping from the arm information to a real number. The model of
linear stochastic bandits enjoys some good theoretical properties  e.g.  there exists a closed-form so-
lution of the linear mapping at each time step in light of ridge regression. Many practical applications
take advantage of MAB and its variants to control decision performance  e.g.  online personalized
recommendations (Li et al.  2010) and resource allocations (Lattimore et al.  2014).

In most previous studies of MAB and linear stochastic bandits  a common assumption is that noises
in observed payoffs are sub-Gaussian conditional on historical information (Abbasi-Yadkori et al. 
2011; Bubeck et al.  2012)  which encompasses cases of all bounded payoffs and many unbounded
payoffs  e.g.  payoffs of an arm following a Gaussian distribution. However  there do exist practical
scenarios of non-sub-Gaussian noises in observed payoffs for sequential decisions  such as high-
probability extreme returns in investments for ﬁnancial markets (Cont and Bouchaud  2000) and
ﬂuctuations of neural oscillations (Roberts et al.  2015)  which are called heavy-tailed noises. Thus 
it is signiﬁcant to completely study theoretical behaviours of sequential decisions in the case of
heavy-tailed noises.

∗The ﬁrst two authors contributed equally.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Many practical distributions  e.g.  Pareto distributions and Weibull distributions  are heavy-tailed 
which perform high tail probabilities compared with exponential family distributions. We consider
a general characterization of heavy-tailed payoffs in bandits  where the distributions have ﬁnite
moments of order 1 + ϵ  where ϵ ∈ (0  1]. When ϵ = 1  stochastic payoffs are generated from distri-
butions with ﬁnite variances. When 0 <ϵ< 1  stochastic payoffs are generated from distributions
with inﬁnite variances (Shao and Nikias  1993). Note that  different from sub-Gaussian noises in the
traditional bandit setting  noises from heavy-tailed distributions do not enjoy exponentially decaying
tails  and thus make it more difﬁcult to learn a parameter of an arm.

The regret of MAB with heavy-tailed payoffs has been well addressed by Bubeck et al. (2013) 
where stochastic payoffs have bounds on raw or central moments of order 1 + ϵ. For MAB with
ﬁnite variances (i.e.  ϵ = 1)  the regret of truncation algorithms or median of means recovers the op-
timal regret for MAB under the sub-Gaussian assumption. Recently  Medina and Yang (2016) inves-
tigated theoretical guarantees for the problem of linear stochastic bandits with heavy-tailed payoffs
(LinBET). It is surprising to ﬁnd that  for ϵ = 1  the regret of bandit algorithms by Medina and Yang
4 ) 2  which is far away from the regret of the state-of-the-art al-

(2016) to solve LinBET is !O(T 3
gorithms (i.e.  !O(√T )) in linear stochastic bandits under the sub-Gaussian assumption (Dani et al. 

2008a; Abbasi-Yadkori et al.  2011). Thus  the most interesting and non-trivial question is

Is it possible to recover the regret of !O(√T ) when ϵ = 1 for LinBET?

In this paper  we answer this question afﬁrmatively. Speciﬁcally  we investigate the problem of
LinBET characterized by ﬁnite (1 + ϵ)-th moments  where ϵ ∈ (0  1]. The problem of LinBET
raises several interesting challenges. The ﬁrst challenge is the lower bound of the problem  which
remains unknown. The technical issues come from the construction of an elegant setting for LinBET 
and the derivation of a lower bound with respect to ϵ. The second challenge is how to develop a
robust estimator for the parameter in LinBET  because heavy-tailed noises greatly affect errors of
the conventional least-squares estimator. It is worth mentioning that Medina and Yang (2016) has
tried to tackle this challenge  but their estimators do not make full use of the contextual information
of chosen arms to eliminate the effect from heavy-tailed noises  which eventually leads to large
regrets. The third challenge is how to successfully adopt median of means and truncation to solve
LinBET with regret upper bounds matching the lower bound as closely as possible.

two algorithms  which are !O(T

Our Results. First of all  we rigorously analyze the lower bound on the problem of LinBET 
1
which enjoys a polynomial order on T as Ω(T
1+ϵ ). The lower bound provides two essential hints:
one is that ﬁnite variances in LinBET yield a bound of Ω(√T )  and the other is that algorithms
by Medina and Yang (2016) are sub-optimal. Then  we develop two novel bandit algorithms to
solve LinBET based on the basic techniques of median of means and truncation. Both the algo-
rithms adopt the optimism in the face of uncertainty principle  which is common in bandit prob-
lems (Abbasi-Yadkori et al.  2011; Munos et al.  2014). The regret upper bounds of the proposed
1
1+ϵ )  match the lower bound up to polylogarithmic factors. To the
best of our knowledge  we are the ﬁrst to solve LinBET almost optimally. We conduct experiments
based on synthetic datasets  which are generated by Student’s t-distribution and Pareto distribution 
to demonstrate the effectiveness of our algorithms. Experimental results show that our algorithms
outperform the state-of-the-art results. The contributions of this paper are summarized as follows:
• We provide the lower bound for the problem of LinBET characterized by ﬁnite (1 + ϵ)-th
moments  where ϵ ∈ (0  1]. In the analysis  we construct an elegant setting of LinBET 
which results in a regret bound of Ω(T
• We develop two novel bandit algorithms  which are named as MENU and TOFU (with
details shown in Section 4). The MENU algorithm adopts median of means with a well-
designed allocation of decisions and the TOFU algorithm adopts truncation via historical

1+ϵ ) in expectation for any bandit algorithm.

1

information. Both algorithms achieve the regret !O(T

• We conduct experiments based on synthetic datasets to demonstrate the effectiveness of
our proposed algorithms. By comparing our algorithms with the state-of-the-art results 
we show improvements on cumulative payoffs for MENU and TOFU  which are strictly
consistent with theoretical guarantees in this paper.

1

1+ϵ ) with high probability.

2We omit polylogarithmic factors of T for !O(·).

2

2 Preliminaries and Related Work

In this section  we ﬁrst present preliminaries  i.e.  notations and learning setting of LinBET. Then 
we give a detailed discussion on the line of research for bandits with heavy-tailed payoffs.

d) 1

2.1 Notations
For a positive integer K  [K] ! {1  2 ···   K}. Let the ℓ-norm of a vector x ∈ Rd be ∥x∥ℓ !
(xℓ
1 + ··· + xℓ
ℓ   where ℓ ≥ 1 and xi is the i-th element of x with i ∈ [d]. For r ∈ R  its absolute
value is |r|  its ceiling integer is ⌈r⌉  and its ﬂoor integer is ⌊r⌋. The inner product of two vectors x  y
is denoted by x⊤y = ⟨x  y⟩. Given a positive deﬁnite matrix A ∈ Rd×d  the weighted Euclidean
norm of a vector x ∈ Rd is ∥x∥A = √x⊤Ax. B(x  r) denotes a Euclidean ball centered at x with
radius r ∈ R+  where R+ is the set of positive numbers. Let e be Euler’s number  and Id ∈ Rd×d
an identity matrix. Let 1{·} be an indicator function  and E[X] the expectation of X.

2.2 Learning Setting

For a bandit algorithm A  we consider sequential decisions with the goal to maximize cumulative
payoffs  where the total number of rounds for playing bandits is T . For each round t = 1 ···   T  
the bandit algorithm A is given a decision set Dt ⊆ Rd such that ∥x∥2 ≤ D for any x ∈ Dt. A
has to choose an arm xt ∈ Dt and then observes a stochastic payoff yt(xt). For notation simplicity 
we also write yt = yt(xt). The expectation of the observed payoff for the chosen arm satisﬁes a
linear mapping from the arm to a real number as yt(xt) ! ⟨xt θ ∗⟩ + ηt  where θ∗ is an underly-
ing parameter with ∥θ∗∥2 ≤ S and ηt is a random noise. Without loss of generality  we assume
E [ηt|Ft−1] = 0  where Ft−1 ! {x1 ···   xt}∪{ η1 ···  η t−1} is a σ-ﬁltration and F0 = ∅.
Clearly  we have E[yt(xt)|Ft−1] = ⟨xt θ ∗⟩. For an algorithm A  to maximize cumulative payoffs
is equivalent to minimizing the regret as

R(A  T ) !" T#t=1

t  θ ∗⟩$ −" T#t=1

⟨x∗

⟨xt θ ∗⟩$ =

T#t=1

⟨x∗
t − xt θ ∗⟩ 

(1)

t denotes the optimal decision at time t for θ∗  i.e.  x∗

where x∗
t ∈ arg maxx∈Dt⟨x  θ∗⟩. In this paper 
we will provide high-probability upper bound of R(A  T ) with respect to A  and provide the lower
bound for LinBET in expectation for any algorithm. The problem of LinBET is deﬁned as below.
Deﬁnition 1 (LinBET). Given a decision set Dt for time step t = 1 ···   T   an algorithm A  of
which the goal is to maximize cumulative payoffs over T rounds  chooses an arm xt ∈ Dt. With

Ft−1  the observed stochastic payoff yt(xt) is conditionally heavy-tailed  i.e.  E%|yt|1+ϵ|Ft−1& ≤ b
or E%|yt − ⟨xt θ ∗⟩|1+ϵ|Ft−1& ≤ c  where ϵ ∈ (0  1]  and b  c ∈ (0  +∞).

2.3 Related Work

The model of MAB dates back to 1952 with the original work by Robbins et al. (1952)  and its
inherent characteristic is the trade-off between exploration and exploitation. The asymptotic lower
bound of MAB was developed by Lai and Robbins (1985)  which is logarithmic with respect to the
total number of rounds. An important technique called upper conﬁdence bound was developed to
achieve the lower bound (Lai and Robbins  1985; Agrawal  1995). Other related techniques to solve
the problem of sequential decisions include Thompson sampling (Thompson  1933; Chapelle and Li 
2011; Agrawal and Goyal  2012) and Gittins index (Gittins et al.  2011).

The problem of MAB with heavy-tailed payoffs characterized by ﬁnite (1 + ϵ)-th moments has
been well investigated (Bubeck et al.  2013; Vakili et al.  2013; Yu et al.  2018). Bubeck et al. (2013)
pointed out that ﬁnite variances in MAB are sufﬁcient to achieve regret bounds of the same order
as the optimal regret for MAB under the sub-Gaussian assumption  and the order of T in regret
bounds increases when ϵ decreases. The lower bound of MAB with heavy-tailed payoffs has been
analyzed (Bubeck et al.  2013)  and robust algorithms by Bubeck et al. (2013) are optimal. The-
oretical guarantees by Bubeck et al. (2013); Vakili et al. (2013) are for the setting of ﬁnite arms.
In Vakili et al. (2013)  primary theoretical results were presented for the case of ϵ> 1. We notice
that the case of ϵ> 1 is not interesting  because it reduces to the case of ﬁnite variances in MAB.

3

For the problem of linear stochastic bandits  which is also named linear reinforcement learning
by Auer (2002)  the lower bound is Ω(d√T ) when contextual information of arms is from a d-
dimensional space (Dani et al.  2008b). Bandit algorithms matching the lower bound up to poly-
logarithmic factors have been well developed (Auer  2002; Dani et al.  2008a; Abbasi-Yadkori et al. 
2011; Chu et al.  2011). Notice that all these studies assume that stochastic payoffs contain sub-
Gaussian noises. More variants of MAB can be discussed by Bubeck et al. (2012).

It is surprising to ﬁnd that the lower bound of LinBET remains unknown. In Medina and Yang
(2016)  bandit algorithms based on truncation and median of means were presented. When ϵ is ﬁnite

is the regret of the state-of-the-art algorithms in linear stochastic bandits under the sub-Gaussian

for LinBET  the algorithms by Medina and Yang (2016) cannot recover the bound of !O(√T ) which
assumption. Medina and Yang (2016) conjectured that it is possible to recover !O(√T ) with ϵ being

a ﬁnite number for LinBET. Thus  it is urgent to conduct a thorough analysis of the conjecture in
consideration of the importance of heavy-tailed noises in real scenarios. Solving the conjecture
generalizes the practical applications of bandit models. Practical motivating examples for bandits
with heavy-tailed payoffs include delays in end-to-end network routing (Liebeherr et al.  2012) and
sequential investments in ﬁnancial markets (Cont and Bouchaud  2000).

Recently  the assumption in stochastic payoffs of MAB was relaxed from sub-Gaussian noises to
bounded kurtosis (Lattimore  2017)  which can be viewed as an extension of Bubeck et al. (2013).
The interesting point of Lattimore (2017) is the scale free algorithm  which might be practical in
applications. Besides  Carpentier and Valko (2014) investigated extreme bandits  where stochastic
payoffs of MAB follow Fréchet distributions. The setting of extreme bandits ﬁts for the real scenario
of anomaly detection without contextual information. The order of regret in extreme bandits is
characterized by distributional parameters  which is similar to the results by Bubeck et al. (2013).

It is worth mentioning that  for linear regression with heavy-tailed noises  several interesting studies
have been conducted. Hsu and Sabato (2016) proposed a generalized method in light of median of
means for loss minimization with heavy-tailed noises. Heavy-tailed noises in Hsu and Sabato (2016)
might come from contextual information  which is more complicated than the setting of stochastic
payoffs in this paper. Therefore  linear regression with heavy-tailed noises usually requires a ﬁnite
fourth moment. In Audibert et al. (2011)  the basic technique of truncation was adopted to solve
robust linear regression in the absence of exponential moment condition. The related studies in this
line of research are not directly applicable for the problem of LinBET.

3 Lower Bound

In this section  we provide the lower bound for LinBET. We consider heavy-tailed payoffs with ﬁnite
(1 + ϵ)-th raw moments in the analysis. In particular  we construct the following setting. Assume
d ≥ 2 is even (when d is odd  similar results can be easily derived by considering the ﬁrst d − 1
dimensions). For Dt ⊆ Rd with t ∈ [T ]  we ﬁx the decision set as D1 = ··· = DT = D(d). Then 
the ﬁxed decision set is constructed as D(d) ! {(x1 ···   xd) ∈ Rd
+ : x1 + x2 = ··· = xd−1 + xd =
1}  which is a subset of intersection of the cube [0  1]d and the hyperplane x1 + ··· + xd = d/2.
We deﬁne a set Sd ! {(θ1 ···  θ d) : ∀i ∈ [d/2]   (θ2i−1 θ 2i) ∈{ (2∆  ∆)  (∆  2∆)}} with ∆ ∈
(0  1/d]. The payoff functions take values in {0  (1/∆) 1
ϵ } such that  for every x ∈ D(d)  the expected
payoff is θ⊤

∗ x. To be more speciﬁc  we have the payoff function of x as

y(x) =’( 1
∆) 1

0

ϵ with a probability of ∆ 1

ϵ θ⊤
∗ x 
with a probability of 1 − ∆ 1

ϵ θ⊤

∗ x.

(2)

We have the theorem for the lower bound of LinBET as below.

Theorem 1 (Lower Bound of LinBET). If θ∗ is chosen uniformly at random from Sd  and the
payoff for each x ∈ D(d) is in {0  (1/∆) 1
∗ x  then for any algorithm A and every
T ≥ (d/12)

ϵ } with mean θ⊤

ϵ
1+ϵ   we have

E [R(A  T )] ≥

d
192

1

1+ϵ .

T

4

(3)

ˆθn k

take median of means of {ˆθn j}k
j=1
ˆθn 1

···
calculate k LSEs with payoffs on {xi}n
···
...
···
N = ⌊ T
k ⌋

k = ⌈24 log( eT
δ )⌉

...
x3

...
x1

...
x2

x1

x2

x3

i=1

xn

...
xn

ˆθn k∗

˜l1

x1

x1
...
x1

xN

...
xN

···
...
···

k = T

1+ϵ
1+3ϵ

i=1

˜ln

ˆθn
calculate LSE with {˜li}n
···

···
take median of means
of payoffs on {xi}k  i ∈ [n]
···
···
...
···

···
···
...
···

···
···
...
···
N = T

xN
...
xN

xn
...
xn

2ϵ

1+3ϵ

xN

xn

(a) Framework of MENU

(b) Framework of MoM

Figure 1: Framework comparison between our MENU and MoM by Medina and Yang (2016).

In the proof of Theorem 1  we ﬁrst prove the lower bound when d = 2  and then generalize the
argument to any d > 2. We notice that the parameter in the original d-dimensional space is rear-
ranged to d/2 tuples  each of which is a 2-dimensional vector as (θ2i−1 θ 2i) ∈{ (2∆  ∆)  (∆  2∆)}
with i ∈ [d/2]. If the i-th tuple of the parameter is selected as (2∆  ∆)  then the i-th tuple of the
optimal arm is (x∗ 2i−1  x∗ 2i) = (1  0). In this case  if we deﬁne the i-th tuple of the chosen arm as
(xt 2i−1  xt 2i)  the instantaneous regret is ∆(1− xt 2i−1). Then  the regret can be represented as an
integration of ∆(1 − xt 2i−1) over D(d). Finally  with common inequalities in information theory 
we obtain the regret lower bound by setting ∆= T − ϵ
We notice that martingale differences to prove the lower bound for linear stochastic bandits
in (Dani et al.  2008a) are not directly feasible for the proof of lower bound in LinBET  because
under our construction of heavy-tailed payoffs (i.e.  Eq. (4))  the information of ϵ is excluded. Be-
sides  our proof is partially inspired by Bubeck (2010). We show the detailed proof of Theorem 1 in
Appendix A.

1+ϵ /12.

Remark 1. The above lower bound provides two essential hints: one is that ﬁnite variances in
LinBET yield a bound of Ω(√T )  and the other is that algorithms proposed by Medina and Yang
(2016) are far from optimal. The result in Theorem 1 strongly indicates that it is possible to design

bandit algorithms recovering !O(√T ) with ﬁnite variances.

4 Algorithms and Upper Bounds

In this section  we develop two novel bandit algorithms to solve LinBET  which turns out to be al-
most optimal. We rigorously prove regret upper bounds for the proposed algorithms. In particular 
our core idea is based on the optimism in the face of uncertainty principle (OFU). The ﬁrst algo-
rithm is median of means under OFU (MENU) shown in Algorithm 1  and the second algorithm is
truncation under OFU (TOFU) shown in Algorithm 2. For comparisons  we directly name the ban-
dit algorithm based on median of means in Medina and Yang (2016) as MoM  and name the bandit
algorithm based on conﬁdence region with truncation in Medina and Yang (2016) as CRT.
Both algorithms in this paper adopt the tool of ridge regression. At time step t  let ˆθt be the ℓ2-
regularized least-squares estimate (LSE) of θ∗ as ˆθt = V −1
t Yt  where Xt ∈ Rt×d is a matrix
of which rows are x⊤
t Xt + λId  Yt ! (y1 ···   yt) is a vector of the historical
observed payoffs until time t and λ> 0 is a regularization parameter.

1  ···   x⊤

t   Vt ! X ⊤

t X ⊤

4.1 MENU and Regret

Description of MENU. To conduct median of means in LinBET  it is common to allocate T
pulls of bandits among N ≤ T epochs  and for each epoch the same arm is played multiple
times to obtain an estimate of θ∗. We ﬁnd that there exist different ways to construct the epochs.
We design the framework of MENU in Figure 1(a)  and show the framework of MoM designed

5

n=1

k ⌋  V0 = λId  C0 = B(0  S)

Algorithm 1 Median of means under OFU (MENU)
1: input d  c  ϵ  δ  λ  S  T   {Dn}N
2: initialization: k = ⌈24 log" eT
3: for n = 1  2  ···   N do
4:
5:
6:
7:
8:
9:

δ #⌉  N = ⌊ T
(xn  ˜θn) = arg max(x θ)∈Dn×Cn−1⟨x  θ⟩
Play xn with k times and observe payoffs yn 1  yn 2 ···   yn k
Vn = Vn−1 + xnx⊤
n
n $n
For j ∈ [k]  ˆθn j = V −1
For j ∈ [k]  let rj be the median of {∥ˆθn j − ˆθn s∥Vn : s ∈ [k]\j}
k∗ = arg minj∈[k] rj
2 S&
βn = 3%(9dc)
1−ϵ
1
2(1+ϵ) + λ
1+ϵ n
Cn = {θ : ∥θ − ˆθn k∗∥Vn ≤ βn}

i=1 yi jxi

10:

1

11:
12: end for

◃ to select an arm

◃ to calculate LSE for the j-th group

◃ to take median of means of estimates

◃ to update the conﬁdence region

by Medina and Yang (2016) in Figure 1(b). For MENU and MoM  we have the following three dif-
ferences. First  for each epoch n = 1 ···   N   MENU plays the same arm xn by O(log(T )) times 
1+3ϵ ) times. Second  at epoch n with historical payoffs 
while MoM plays the same arm by O(T
MENU conducts LSEs by O(log(T )) times  each of which is based on {xi}n
i=1  while MoM con-
ducts LSE by one time based on intermediate payoffs calculated via median of means of observed
payoffs. Third  MENU adopts median of means of LSEs  while MoM adopts median of means of
the observed payoffs. Intuitively  the execution of multiple LSEs will lead to the improved regret of
MENU. With a better trade-off between k and N in Figure 1(a)  we derive an improved upper bound
of regret in Theorem 2.

1+ϵ

In light of Figure 1(a)  we develop algorithmic procedures in Algorithm 1 for MENU. We notice that 
in order to guarantee the median of means of LSEs not far away from the true underlying parameter
with high probability  we construct the conﬁdence interval in Line 10 of Algorithm 1. Now we have
the following theorem for the regret upper bound of MENU.
Theorem 2 (Regret Analysis for the MENU Algorithm). Assume that for all t and xt ∈ Dt with
∥xt∥2 ≤ D  ∥θ∗∥2 ≤ S  |x⊤
t θ∗|≤ L and E[|ηt|1+ϵ|Ft−1] ≤ c. Then  with probability at least 1− δ 
for every T ≥ 256 + 24 log (e/δ)  the regret of the MENU algorithm satisﬁes
λd (.
R(MENU  T ) ≤ 6%(9dc)

1+ϵ)2d log’1 +

1+ϵ’24 log’ eT

δ ( + 1( ϵ

2 S + L& T

1
1+ϵ + λ

T D2

1

1

The technical challenges in MENU (i.e.  Algorithm 1) and its proofs are discussed as follows. Based
on the common techniques in linear stochastic bandits (Abbasi-Yadkori et al.  2011)  to guarantee

the instantaneous regret in LinBET  we need to guarantee∥θ∗− ˆθn k∗∥Vn ≤ βn with high probability.
We attack this issue by guaranteeing ∥θ∗ − ˆθn j∥Vn ≤ βn/3 with a probability of 3/4  which could
reduce to a problem of bounding a weighted sum of historical noises. Interestingly  by conducting
singular value decomposition on Xn (of which rows are x⊤
n )  we ﬁnd that 2-norm of the
weights is no greater than 1. Then the weighted sum can be bounded by a term as O*n
2(1+ϵ)+.

With a standard analysis in linear stochastic bandits from the instantaneous regret to the regret  we
achieve the above results for MENU. We show the detailed proof of Theorem 2 in Appendix B.

1  ···   x⊤

1−ϵ

Remark 2. For MENU  we adopt the assumption of heavy-tailed payoffs on central moments 
which is required in the basic technique of median of means (Bubeck et al.  2013). Besides  there
exists an implicit mild assumption in Algorithm 1 that  at each epoch n  the decision set must contain
the selected arm xn at least k times  which is practical in applications  e.g.  online personalized
recommendations (Li et al.  2010). The condition of T ≥ 256 + 24 log (e/δ) is required for T ≥ k.
1+ϵ )  which implies that ﬁnite variances in LinBET are

1

The regret upper bound of MENU is !O(T
sufﬁcient to achieve !O(√T ).

4.2 TOFU and Regret

Description of TOFU. We demonstrate the algorithmic procedures of TOFU in Algorithm 2. We
point out two subtle differences between our TOFU and the algorithm of CRT as follows. In TOFU 

6

Algorithm 2 Truncation under OFU (TOFU)
1: input d  b  ϵ  δ  λ  T   {Dt}T
2: initialization: V0 = λId  C0 = B(0  S)
3: for t = 1  2 · ··   T do
t
4:

2(1+ϵ)

t=1

1−ϵ

ϵ

log( 2T

δ )( 1

t and X ⊤
t = [x1 ···   xt]
X ⊤
t

bt =’ b
(xt  ˜θt) = arg max(x θ)∈Dt×Ct−1⟨x  θ⟩
Play xt and observe a payoff yt
Vt = Vt−1 + xtx⊤
[u1 ·· ·   ud]⊤ = V −1/2
for i = 1 · ··   d do
end for
t = V −1/2
θ†
1  · ··   u⊤
βt = 4√db
δ ## ϵ
1+ϵ"log" 2dT
Update Ct = {θ : ∥θ − θ†

Y †
i = (y11ui 1y1≤bt  ···   yt1ui tyt≤bt )

t∥Vt ≤ βt}

d Y †
d )
1+ϵ t

1 Y †

(u⊤

1−ϵ
2(1+ϵ) + λ

t

t

1

5:
6:
7:
8:
9:
10:
11:
12:

13:
14:
15: end for

◃ to select an arm

◃ to truncate the payoffs

1
2 S

◃ to update the conﬁdence region

Table 1: Statistics of synthetic datasets in experiments. For Student’s t-distribution  ν denotes the
degree of freedom  lp denotes the location  sp denotes the scale. For Pareto distribution  α denotes
the shape and sm denotes the scale. NA denotes not available.

dataset

Dt

distribution {parameters}

{#arms #dimensions}

{ϵ  b  c}

mean of the
optimal arm

S1

S2

S3

S4

{20 10}

{100 20}

{20 10}

{100 20}

Student’s t-distribution
{ν = 3  lp = 0  sp = 1}

Student’s t-distribution
{ν = 3  lp = 0  sp = 1}

{α = 2  sm = x⊤

Pareto distribution
t θ∗
2 }
Pareto distribution
t θ∗
2 }

{α = 2  sm = x⊤

{1.00  NA  3.00}

{1.00  NA  3.00}

{0.50  7.72  NA}

4.00

7.40

3.10

{0.50  54.37  NA}

11.39

to obtain the accurate estimate of θ∗  we need to trim all historical payoffs for each dimension
individually. Besides  the truncating operations depend on the historical information of arms. By
contrast  in CRT  the historical payoffs are trimmed once  which is controlled only by the number
of rounds for playing bandits. Compared to CRT  our TOFU achieves a tighter conﬁdence interval 
which can be found from the setting of βt. Now we have the following theorem for the regret upper
bound of TOFU.
Theorem 3 (Regret Analysis for the TOFU Algorithm). Assume that for all t and xt ∈ Dt with
t θ∗|≤ L and E[|yt|1+ϵ|Ft−1] ≤ b. Then  with probability at least 1− δ 
∥xt∥2 ≤ D  ∥θ∗∥2 ≤ S  |x⊤
for every T ≥ 1  the regret of the TOFU algorithm satisﬁes
1+ϵ"4√db
λd -.

2 S + L$.2d log 1 +

1+ϵ log  2dT

R(TOFU  T ) ≤ 2T

δ -- ϵ

T D2

+ λ

1+ϵ

1

1

1

Similarly to the proof in Theorem 2  we can achieve the above results for TOFU. Due to space
limitation  we show the detailed proof of Theorem 3 in Appendix C.

is worth pointing out

Remark 3. For TOFU  we adopt the assumption of heavy-tailed payoffs on raw moments.
that  when ϵ = 1  we have regret upper bound for TOFU as
It

!O(d√T )  which implies that we recover the same order of d as that under sub-Gaussian assump-

tion (Abbasi-Yadkori et al.  2011). A weakness in TOFU is high time complexity  because for each
round TOFU needs to truncate all historical payoffs. The time complexity might be reasonably
reduced by dividing T into multiple epochs  each of which contains only one truncation.

7

5 Experiments

In this section  we conduct experiments based on synthetic datasets to evaluate the performance
of our proposed bandit algorithms: MENU and TOFU. For comparisons  we adopt two baselines:
MoM and CRT proposed by Medina and Yang (2016). We run multiple independent repetitions for
each dataset in a personal computer under Windows 7 with Intel CPU@3.70GHz and 16GB memory.

5.1 Datasets and Setting

To show effectiveness of bandit algorithms  we will demonstrate cumulative payoffs with respect
to number of rounds for playing bandits over a ﬁxed ﬁnite-arm decision set. For veriﬁcations  we
adopt four synthetic datasets (named as S1–S4) in the experiments  of which statistics are shown
in Table 1. The experiments on heavy tails require ϵ  b or ϵ  c to be known  which corresponds to
the assumptions of Theorem 2 or Theorem 3. According to the required information  we can apply
MENU or TOFU into practical applications. We adopt Student’s t and Pareto distributions because
they are common in practice. For Student’s t-distributions  we easily estimate c  while for Pareto
distributions  we easily estimate b. Besides  we can choose different parameters (e.g.  larger values)
in the distributions  and recalculate the parameters of b and c.

For S1 and S2  which contain different numbers of arms and different dimensions for the contextual
information  we adopt standard Student’s t-distribution to generate heavy-tailed noises. For the cho-
sen arm xt ∈ Dt  the expected payoff is x⊤
t θ∗  and the observed payoff is added a noise generated
from a standard Student’s t-distribution. We generate each dimension of contextual information for
an arm  as well as the underlying parameter  from a uniform distribution over [0  1]. The standard
Student’s t-distribution implies that the bound for the second central moment of S1 and S2 is 3.

m /(α − 1.5) = 4s1.5

For S3 and S4  we adopt Pareto distribution  where the shape parameter is set as α = 2. We know
t θ∗ = αsm/(α − 1) implying sm = x⊤
x⊤
t θ∗/2. Then  we set ϵ = 0.5 leading to the bound of raw
moment as E%|yt|1.5& = αs1.5
m among all arms
as the bound of the 1.5-th raw moment. We generate arms and the parameter similar to S1 and S2.
In ﬁgures  we show the average of cumulative payoffs with time evolution over ten independent
repetitions for each dataset  and show error bars of a standard variance for comparing the robustness
of algorithms. For S1 and S2  we run MENU and MoM and set T = 2 × 104. For S3 and S4  we
run TOFU and CRT and set T = 1 × 104. For all algorithms  we set λ = 1.0  and δ = 0.1.
5.2 Results and Discussions

m . We take the maximum of 4s1.5

We show experimental results in Figure 2. From the ﬁgure  we clearly ﬁnd that our proposed two
algorithms outperform MoM and CRT  which is consistent with the theoretical results in Theorems 2
and 3. We also evaluate our algorithms with other synthetic datasets  as well as different λ and δ 
and observe similar superiority of MENU and TOFU. Finally  for further comparison on regret 
complexity and storage of four algorithms  we list the results shown in Table 2.

Table 2: Comparison on regret  complexity and storage of four algorithms.

algorithm

MoM

MENU

1+2ϵ

1+3ϵ )

!O(T

O(T )
O(1)

regret

complexity

storage

6 Conclusion

1

1+ϵ )

!O(T

O(T log T )
O(log T )

CRT
1
2 + 1

2(1+ϵ) )

O(T )
O(1)

!O(T

TOFU

1

!O(T

1+ϵ )
O(T 2)
O(T )

We have studied the problem of LinBET  where stochastic payoffs are characterized by ﬁnite (1 +
ϵ)-th moments with ϵ ∈ (0  1]. We broke the traditional assumption of sub-Gaussian noises in
payoffs of bandits  and derived theoretical guarantees based on the prior information of bounds
on ﬁnite moments. We rigorously analyzed the lower bound of LinBET  and developed two novel

8

(a) S1

(b) S2

(c) S3

(d) S4

Figure 2: Comparison of cumulative payoffs for synthetic datasets S1-S4 with four algorithms.

bandit algorithms with regret upper bounds matching the lower bound up to polylogarithmic factors.
Two novel algorithms were developed based on median of means and truncation. In the sense of
polynomial dependence on T   we provided optimal algorithms for the problem of LinBET  and
thus solved an open problem  which has been pointed out by Medina and Yang (2016). Finally  our
proposed algorithms have been evaluated based on synthetic datasets  and outperformed the state-of-
the-art results. Since both algorithms in this paper require a priori knowledge of ϵ  future directions
in this line of research include automatic learning of LinBET without information of distributional
moments  and evaluation of our proposed algorithms in real-world scenarios.

Acknowledgments

The work described in this paper was partially supported by the Research Grants Council of the Hong
Kong Special Administrative Region  China (No. CUHK 14208815 and No. CUHK 14210717 of the
General Research Fund)  and Microsoft Research Asia (2018 Microsoft Research Asia Collaborative
Research Award).

References

Y. Abbasi-Yadkori  D. Pál  and C. Szepesvári. Improved algorithms for linear stochastic bandits. In

Advances in Neural Information Processing Systems  pages 2312–2320  2011.

R. Agrawal. Sample mean based index policies by O(log n) regret for the multi-armed bandit

problem. Advances in Applied Probability  27(4):1054–1078  1995.

S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In

Conference on Learning Theory  pages 39–1  2012.

J.-Y. Audibert  O. Catoni  et al. Robust linear least squares regression. The Annals of Statistics  39

(5):2766–2794  2011.

P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learn-

ing Research  3(Nov):397–422  2002.

9

S. Bubeck. Bandits games and clustering foundations. PhD thesis  Université des Sciences et

Technologie de Lille-Lille I  2010.

S. Bubeck  N. Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochastic multi-armed

S. Bubeck  N. Cesa-Bianchi  and G. Lugosi. Bandits with heavy tail. IEEE Transactions on Infor-

bandit problems. Foundations and Trends R⃝ in Machine Learning  5(1):1–122  2012.
mation Theory  59(11):7711–7717  2013.

A. Carpentier and M. Valko. Extreme bandits.

In Advances in Neural Information Processing

Systems  pages 1089–1097  2014.

O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Advances in Neural

Information Processing Systems  pages 2249–2257  2011.

W. Chu  L. Li  L. Reyzin  and R. Schapire. Contextual bandits with linear payoff functions. In
Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics 
pages 208–214  2011.

R. Cont and J.-P. Bouchaud. Herd behavior and aggregate ﬂuctuations in ﬁnancial markets. Macroe-

conomic Dynamics  4(2):170–196  2000.

V. Dani  T. P. Hayes  and S. M. Kakade. Stochastic linear optimization under bandit feedback. In

Conference on Learning Theory  pages 355–366  2008a.

V. Dani  S. M. Kakade  and T. P. Hayes. The price of bandit information for online optimization. In

Advances in Neural Information Processing Systems  pages 345–352  2008b.

J. Gittins  K. Glazebrook  and R. Weber. Multi-armed bandit allocation indices. John Wiley & Sons 

2011.

D. Hsu and S. Sabato. Heavy-tailed regression with a generalized median-of-means. In International

Conference on Machine Learning  pages 37–45  2014.

D. Hsu and S. Sabato. Loss minimization and parameter estimation with heavy tails. The Journal of

Machine Learning Research  17(1):543–582  2016.

T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied

Mathematics  6(1):4–22  1985.

T. Lattimore. A scale free algorithm for stochastic bandits with bounded kurtosis. In Advances in

Neural Information Processing Systems  pages 1583–1592  2017.

T. Lattimore  K. Crammer  and C. Szepesvári. Optimal resource allocation with semi-bandit feed-
back. In Proceedings of the Thirtieth Conference on Uncertainty in Artiﬁcial Intelligence  pages
477–486. AUAI Press  2014.

L. Li  W. Chu  J. Langford  and R. E. Schapire. A contextual-bandit approach to personalized news
article recommendation. In Proceedings of the Nineteenth International Conference on World
Wide Web  pages 661–670. ACM  2010.

J. Liebeherr  A. Burchard  and F. Ciucu. Delay bounds in communication networks with heavy-tailed

and self-similar trafﬁc. IEEE Transactions on Information Theory  58(2):1010–1024  2012.

A. M. Medina and S. Yang. No-regret algorithms for heavy-tailed linear bandits. In International

Conference on Machine Learning  pages 1642–1650  2016.

R. Munos et al. From bandits to monte-carlo tree search: The optimistic principle applied to opti-

H. Robbins et al. Some aspects of the sequential design of experiments. Bulletin of the American

mization and planning. Foundations and Trends R⃝ in Machine Learning  7(1):1–129  2014.
Mathematical Society  58(5):527–535  1952.

J. A. Roberts  T. W. Boonstra  and M. Breakspear. The heavy tail of the human brain. Current

Opinion in Neurobiology  31:164–172  2015.

Y. Seldin  F. Laviolette  N. Cesa-Bianchi  J. Shawe-Taylor  and P. Auer. PAC-Bayesian inequalities

for martingales. IEEE Transactions on Information Theory  58(12):7086–7093  2012.

M. Shao and C. L. Nikias. Signal processing with fractional lower order moments: stable processes

and their applications. Proceedings of the IEEE  81(7):986–1010  1993.

W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the

evidence of two samples. Biometrika  25(3/4):285–294  1933.

S. Vakili  K. Liu  and Q. Zhao. Deterministic sequencing of exploration and exploitation for multi-
IEEE Journal of Selected Topics in Signal Processing  7(5):759–767 

armed bandit problems.
2013.

X. Yu  H. Shao  M. R. Lyu  and I. King. Pure exploration of multi-armed bandits with heavy-tailed
payoffs. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artiﬁcial Intelligence 
pages 937–946. AUAI Press  2018.

10

,Han Shao
Xiaotian Yu
Irwin King
Michael Lyu