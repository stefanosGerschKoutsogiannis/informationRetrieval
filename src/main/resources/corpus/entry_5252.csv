2013,Stochastic Optimization of PCA with Capped MSG,We study PCA as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as Matrix Stochastic Gradient'' (MSG)  as well as a practical variant  Capped MSG. We study the method both theoretically and empirically. ",Stochastic Optimization of PCA with Capped MSG

Raman Arora
TTI-Chicago

Chicago  IL USA

arora@ttic.edu

Andrew Cotter
TTI-Chicago

Chicago  IL USA

cotter@ttic.edu

Nathan Srebro

Technion  Haifa  Israel

and TTI-Chicago
nati@ttic.edu

Abstract

We study PCA as a stochastic optimization problem and propose a novel stochas-
tic approximation algorithm which we refer to as “Matrix Stochastic Gradient”
(MSG)  as well as a practical variant  Capped MSG. We study the method both
theoretically and empirically.

Introduction

1
Principal Component Analysis (PCA) is a ubiquitous tool used in many data analysis  machine learn-
ing and information retrieval applications. It is used to obtain a lower dimensional representation of
a high dimensional signal that still captures as much of the original signal as possible. Such a low di-
mensional representation can be useful for reducing storage and computational costs  as complexity
control in learning systems  or to aid in visualization.
PCA is typically phrased as a question about a ﬁxed data set: given n vectors in Rd  what is the
k-dimensional subspace that captures most of the variance in the data (or equivalently  that is best in
reconstructing the vectors  minimizing the sum squared distances  or residuals  from the subspace)?
It is well known that this subspace is the span of the leading k components of the singular value
decomposition of the data matrix (or equivalently of the empirical second moment matrix). Hence 
the study of computational approaches for PCA has mostly focused on methods for ﬁnding the SVD
(or leading components of the SVD) for a given n×d matrix (Oja & Karhunen  1985; Sanger  1989).
In this paper we approach PCA as a stochastic optimization problem  where the goal is to optimize
a “population objective” based on i.i.d. draws from the population. In this setting  we have some
unknown source (“population”) distribution D over Rd  and the goal is to ﬁnd the k-dimensional
subspace maximizing the (uncentered) variance of D inside the subspace (or equivalently  minimiz-
ing the average squared residual in the population)  based on i.i.d. samples from D. The main point
here is that the true objective is not how well the subspace captures the sample (i.e. the “training
error”)  but rather how well the subspace captures the underlying source distribution (i.e. the “gen-
eralization error”). Furthermore  we are not concerned with capturing some “true” subspace  and so
do not  for example  try to minimize the angle to such a subspace  but rather attampt to ﬁnd a “good”
subspace  i.e. one that is almost as good as the optimal one in terms of reconstruction error.
Of course  ﬁnding the subspace that best captures the sample is a very reasonable approach to PCA
on the population. This is essentially an Empirical Risk Minimization (ERM) approach. However 
when comparing it to alternative  perhaps computationally cheaper  approaches  we argue that one
should not compare the error on the sample  but rather the population objective. Such a view can jus-
tify and favor computational approaches that are far from optimal on the sample  but are essentially
as good as ERM on the population.
Such a population-based view of optimization has recently been advocated in machine learning 
and has been used to argue for crude stochastic approximation approaches (online-type methods)
over sophisticated deterministic optimization of the empirical (training) objective (i.e. “batch” meth-
ods)
(Bottou & Bousquet  2007; Shalev-Shwartz & Srebro  2008). A similar argument was also

1

made in the context of stochastic optimization  where Nemirovski et al. (2009) argues for stochastic
approximation (SA) approaches over ERM. approaches (a.k.a. ERM). Accordingly  SA approaches 
mostly variants of Stochastic Gradient Descent  are often the methods of choice for many learning
problems  especially when very large data sets are available (Shalev-Shwartz et al.  2007; Collins
et al.  2008; Shalev-Shwartz & Tewari  2009). We take the same view in order to advocate for  study 
and develop stochastic approximation approaches for PCA.
In an empirical study of stochastic approximation methods for PCA  a heuristic “incremental”
method showed very good empirical performance (Arora et al.  2012). However  no theoretical
guarantees or justiﬁcation were given for incremental PCA. In fact  it was shown that for some dis-
tributions it can converge to a suboptimal solution with high probability (see Section 5.2 for more
about this “incremental” algorithm). Also relevant is careful theoretical work on online PCA by
Warmuth & Kuzmin (2008)  in which an online regret guarantee was established. Using an online-
to-batch conversion  this online algorithm can be converted to a stochastic approximation algorithm
with good iteration complexity  however the runtime for each iteration is essentially the same as that
of ERM (i.e. of PCA on the sample)  and thus senseless as a stochastic approximation method (see
Section 3.3 for more on this algorithm).
In this paper we borrow from these two approaches and present a novel algorithm for stochastic
PCA—the Matrix Stochastic Gradient (MSG) algorithm. MSG enjoys similar iteration complex-
ity to Warmuth’s and Kuzmin’s algorithm  and in fact we present a uniﬁed view of both algo-
rithms as different instantiations of Mirror Descent for the same convex relaxation of PCA. We
then present the capped MSG algorithm  which is a more practical variant of MSG  has very similar
updates to those of the “incremental” method  works well in practice  and does not get stuck like
the “incremental” method. The Capped MSG algorithm is thus a clean  theoretically well founded
method  with interesting connections to other stochastic/online PCA methods  and excellent practi-
cal performance—a “best of both worlds” algorithm.
2 Problem Setup
We consider PCA as the problem of ﬁnding the maximal (uncentered) variance k-dimensional sub-
space with respect to an (unknown) distribution D over x ∈ Rd. We assume without loss of gener-
ality that the data are scaled in such a way that Ex∼D[(cid:107)x(cid:107)2] ≤ 1. For our analysis  we also require
that the fourth moment be bounded: Ex∼D[(cid:107)x(cid:107)4] ≤ 1. We represent a k-dimensional subspace by
an orthonormal basis  collected in the columns of a matrix U. With this parametrization  PCA is
deﬁned as the following stochastic optimization problem:

maximize : Ex∼D[xT U U T x]
subject to : U ∈ Rd×k  U T U = I.

(2.1)

In a stochastic optimization setting we do not have direct knowledge of the distribution D  and
instead may access it only through i.i.d. samples—these can be thought of as “training examples”.
As in other studies of stochastic approximation methods  we are less concerned with the number
of required samples  and instead care mostly about the overall runtime required to obtain an -
suboptimal solution.
The standard approach to Problem 2.1 is empirical risk minimization (ERM): given samples {xt}T
t=1
drawn from D  we compute the empirical covariance matrix ˆC = 1
t   and take the
columns of U to be the eigenvectors of ˆC corresponding to the top-k eigenvalues. This approach
requires O(d2) memory and O(d2) operations just in order to compute the covariance matrix  plus
some additional time for the SVD. We are interested in methods with much lower sample time and
space complexity  preferably linear rather than quadratic in d.
3 MSG and MEG
A natural stochastic approximation (SA) approach to PCA is projected stochastic gradient descent
(SGD) on Problem 2.1  with respect to U. This leads to the stochastic power method  for which  at
each iteration  the following update is performed:
U (t+1) = Porth

(cid:80)T

U (t) + ηxtxT
t

t=1 xtxT

T

(3.1)

(cid:17)

(cid:16)

2

is the gradient of the PCA objective w.r.t. U  η is a step size  and Porth (·) projects its
Here  xtxT
t
argument onto the set of matrices with orthonormal columns. Unfortunately  although SGD is well
understood for convex problems  Problem 2.1 is non-convex. Consequently  obtaining a theoretical
understanding of the stochastic power method  or of how the step size should be set  has proved
elusive. Under some conditions  convergence to the optimal solution can be ensured  but no rate is
known (Oja & Karhunen  1985; Sanger  1989; Arora et al.  2012).
Instead  we consider a re-parameterization of the PCA problem where the objective is convex. In-
stead of representing a linear subspace in terms of its basis matrix U  we parametrize it using the
corresponding projection matrix M = U U T . We can now reformulate the PCA problem as:

maximize : Ex∼D[xT M x]
subject to : M ∈ Rd×d  λi (M ) ∈ {0  1}   rank M = k

where λi (M ) is the ith eigenvalue of M.
We now have a convex (linear  in fact) objective  but the constraints are not convex. This prompts us
relax the objective by taking the convex hull of the feasible region:

(3.2)

(3.3)

maximize : Ex∼D[xT M x]
subject to : M ∈ Rd×d  0 (cid:22) M (cid:22) I  tr M = k

Since the objective is linear  and the feasible regiuon is the convex hull of that of Problem 3.2 
an optimal solution is always attained at a “vertex”  i.e. a point on the boundary of the original
constraints. The optima of the two objectives are thus the same (strictly speaking—every optimum
of Problem 3.2 is also an optimum of Problem 3.3)  and solving Problem 3.3 is equivalent to solving
Problem 3.2.
Furthermore  if a suboptimal solution for Problem 3.3 is not rank-k  i.e.
is not a feasible point
of Problem 3.2  we can easily sample from it to obtain a rank-k solution with the same objective
function value (in expectation). This is shown by the following result of Warmuth & Kuzmin (2008):
Lemma 3.1 (Rounding (Warmuth & Kuzmin  2008)). Any feasible solution of Problem 3.3 can be
expressed as a convex combination of at most d feasible solutions of Problem 3.2.

Algorithm 2 of Warmuth & Kuzmin (2008) shows how to efﬁciently ﬁnd such a convex combination.
Since the objective is linear  treating the coefﬁcients of the convex combination as deﬁning a discrete
distribution  and sampling according to this distribution  yields a rank-k matrix with the desired
expected objective function value.

3.1 Matrix Stochastic Gradient

Performing SGD on Problem 3.3 (w.r.t. the variable M) yields the following update rule:

(3.4)
The projection is now performed onto the (convex) constraints of Problem 3.3. This gives the Matrix
Stochastic Gradient (MSG) algorithm  which  in detail  consists of the following steps:

M (t) + ηxtxT
t

 

M (t+1) = P(cid:16)

(cid:17)

1. Choose a step-size η  iteration count T   and starting point M (0).
2. Iterate the update rule (Equation 3.4) T times  each time using an independent sample

xt ∼ D.

ous section.

3. Average the iterates as ¯M = 1
T
4. Sample a rank-k solution ˜M from ¯M using the rounding procedure discussed in the previ-

t=1 M (t).

Analyzing MSG is straightforward using a standard SGD analysis:
Theorem 1. After T iterations of MSG (on Problem 3.3)  with step size η =
M (0) = 0 

(cid:114)

E[Ex∼D[xT ˜M x]] ≥ Ex∼D[xT M∗x] − 1
2

k
T

 

where the expectation is w.r.t. the i.i.d. samples x1  . . .   xT ∼ D and the rounding  and M∗ is the
optimum of Problem 3.2.

(cid:113) k

T   and starting at

(cid:80)T

3

Algorithm 1 Matrix stochastic gradient (MSG) update: compute an eigendecomposition of M(cid:48)+ηxxT from a
rank-m eigendecomposition M(cid:48)= U(cid:48)diag(σ(cid:48))(U(cid:48))T and project the resulting solution onto the constraint set.
The computational cost is dominated by the matrix multiplication on lines 4 or 7 costing O(m2d) operations.

msg-step(cid:0)d  k  m : N  U(cid:48) : Rd×m  σ(cid:48) : Rm  x : Rd  η : R(cid:1)
ˆx ← √

η(U(cid:48))T x; x⊥ ← √

1
2
3
4
5
6
7
8
9
10

if r > 0

ηx − U(cid:48) ˆx; r ← (cid:107)x⊥(cid:107);
V  σ ← eig([diag(σ(cid:48)) + ˆxˆxT   rˆx; rˆxT   r2]);
U ← [U(cid:48)  x⊥/r]V ;
V  σ ← eig(diag(σ(cid:48)) + ˆxˆxT );
U ← U(cid:48)V ;

else

σ ← distinct eigenvalues in σ; κ ← corresponding multiplicities;
σ ← project (d  k  m  σ  κ);
return U  σ;

Proof. The SGD analysis of Nemirovski & Yudin (1983) yields that:

(cid:107)M∗ − M (0)(cid:107)2

F

√

k.

Ex∼D[(cid:107)g(cid:107)2

F ] +

E[xT M∗x − xT ¯M x] ≤ η
2

F = (cid:107)M∗(cid:107)2

2ηT
where g = xxT is the gradient of the PCA objective. Now  Ex∼D[(cid:107)g(cid:107)2

(3.5)
F ] = Ex∼D[(cid:107)x(cid:107)4] ≤ 1 and
F = k. In the last inequality  we used the fact that M∗ has k eigenvalues

(cid:13)(cid:13)M∗ − M (0)(cid:13)(cid:13)2

of value 1 each  and hence (cid:107)M∗(cid:107)F =
3.2 Efﬁcient Implementation and Projection
A na¨ıve implementation of the MSG update requires O(d2) memory and O(d2) operations per iter-
ation. In this section  we show how to perform this update efﬁciently by maintaining an up-to-date
eigendecomposition of M (t). Pseudo-code for the update may be found in Algorithm 1. Consider
the eigendecomposition M (t) = U(cid:48)diag(σ)(U(cid:48))T at the tth iteration  where rank(M (t)) = kt and
U(cid:48) ∈ Rd×kt. Given a new observation xt  the eigendecomposition of M (t) + ηxtxT
t can be updated
efﬁciently using a (kt+1)×(kt+1) SVD (Brand  2002; Arora et al.  2012) (steps 1-7 of Algorithm 1).
This rank-one eigen-update is followed by projection onto the constraints of Problem 3.3  invoked as
project in step 8 of Algorithm 1  discussed in the following paragraphs and given as Algorithm 2.
The projection procedure is based on the following lemma1. See supplementary material for a proof.
Lemma 3.2. Let M(cid:48) ∈ Rd×d be a symmetric matrix  with eigenvalues σ(cid:48)
d and associated
d. Its projection M = P (M(cid:48)) onto the feasible region of Problem 3.3 with
eigenvectors v(cid:48)
respect to the Frobenius norm  is the unique feasible matrix which has the same eigenvectors as M(cid:48) 
with the associated eigenvalues σ1  . . .   σd satisfying:

1  . . .   σ(cid:48)

1  . . .   v(cid:48)

with S ∈ R being chosen in such a way that(cid:80)d

σi = max (0  min (1  σ(cid:48)
i=1 σi = k.

i + S))

This result shows that projecting onto the feasible region amounts to ﬁnding the value of S such that 
after shifting the eigenvalues by S and clipping the results to [0  1]  the result is feasible. Importantly 
the projection operates only on the eigenvalues. Algorithm 2 contains pseudocode which ﬁnds S
from a list of eigenvalues. It is optimized to efﬁciently handle repeated eigenvalues—rather than
receiving the eigenvalues in a length-d list  it instead receives a length-n list containing only the
distinct eigenvalues  with κ containing the corresponding multiplicities. In Sections 4 and 5  we will
see why this is an important optimization. The central idea motivating the algorithm is that  in a
sorted array of eigenvalues  all elements with indices below some threshold i will be clipped to 0 
and all of those with indices above another threshold j will be clipped to 1. The pseudocode simply
searches over all possible pairs of such thresholds until it ﬁnds the one that works.
The rank-one eigen-update combined with the fast projection step yields an efﬁcient MSG update
that requires O(dkt) memory and O(dk2
t ) operations per iteration (recall that kt is the rank of the
1Our projection problem onto the capped simplex  even when seen in the vector setting  is substantially
different from Duchi et al. (2008). We project onto the set {0 ≤ σ ≤ 1 (cid:107)σ(cid:107)1 = k} in Problem 3.3 and {0 ≤
σ ≤ 1 (cid:107)σ(cid:107)1 = k (cid:107)σ(cid:107)0 ≤ K} in Problem 5.1 whereas Duchi et al. (2008) project onto {0 ≤ σ (cid:107)σ(cid:107)1 = k}.

4

κ(cid:48) contain the distinct eigenvalues and their multiplicities  respectively  of M(cid:48) (with(cid:80)n

Algorithm 2 Routine which ﬁnds the S of Lemma 3.2. It takes as parameters the dimension d  “target” sub-
space dimension k  and the number of distinct eigenvalues n of the current iterate. The length-n arrays σ(cid:48) and
i = d). Line 1 sorts
σ(cid:48) and re-orders κ(cid:48) so as to match this sorting. The loop will be run at most 2n times (once for each possible
increment to i or j on lines 12–15)  so the computational cost is dominated by that of the sort: O(n log n).

i=1 κ(cid:48)

project (d  k  n : N  σ(cid:48) : Rn  κ(cid:48) : Nn)

σ(cid:48)  κ(cid:48) ← sort(σ(cid:48)  κ(cid:48));
i ← 1; j ← 1; si ← 0; sj ← 0; ci ← 0; cj ← 0;
while i ≤ n
if (i < j)
S ← (k − (sj − si) − (d − cj))/(cj − ci);
b ← (
i + S ≥ 0) and (σ(cid:48)
(σ(cid:48)
and ((i ≤ 1) or (σ(cid:48)
and ((j ≥ n) or (σ(cid:48)

j−1 + S ≤ 1)
i−1 + S ≤ 0))
j+1 ≥ 1))

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

);
return S if b;
i ≤ 1)
if (j ≤ n) and (σ(cid:48)
j; cj ← cj + κ(cid:48)
sj ← sj + κ(cid:48)
jσ(cid:48)
i; ci ← ci + κ(cid:48)
si ← si + κ(cid:48)
iσ(cid:48)

j − σ(cid:48)

else

j; j ← j + 1;
i; i ← i + 1;

return error;

iterate M (t)). This is a signiﬁcant improvement over the O(d2) memory and O(d2) computation
required by a standard implementation of MSG  if the iterates have relatively low rank.

function  i.e. Ψh(M ) =(cid:80)

3.3 Matrix Exponentiated Gradient
Since M is constrained by its trace  and not by its Frobenius norm  it is tempting to consider mirror
descent (MD) (Beck & Teboulle  2003) instead of SGD updates for solving Problem 3.3. Recall that
Mirror Descent depends on a choice of “potential function” Ψ(·) which should be chosen according
to the geometry of the feasible set and the subgradients (Srebro et al.  2011). Using the squared
Frobenius norm as a potential function  i.e. Ψ(M ) = (cid:107)M(cid:107)2
F   yields SGD  i.e. the MSG updates
Equation 3.4. The trace-norm constraint suggests using the von Neumann entropy as the potential
i λi (M ) log λi (M ). This leads to multiplicative updates  yielding what
we refer to as the Matrix Exponentiated Gradient (MEG) algorithm  which is similar to that of (War-
muth & Kuzmin  2008). In fact  Warmuth and Kuzmin’s algorithm exactly corresponds to online
Mirror Descent on Problem 3.3 with this potential function  but takes the optimization variable to
be M⊥ = I − M (with the constraints tr M⊥ = d − k and 0 (cid:22) M⊥ (cid:22) I). In either case  using the
entropy potential  despite being well suited for the trace-geometry  does not actually lead to a better

dependence2 on d or k  and a Mirror Descent-based analysis again yields an excess loss of(cid:112)k/T .
(cid:19)

yields a total runtime of O( ¯k2dk/2)  where ¯k2 =(cid:80)T

Warmuth and Kuzmin present an “optimistic” analysis  with a dependence on the “reconstruction
error” L∗ = E[xT (I − M∗)x]  which yields an excess error of O
(their logarithmic term can be avoided by a more careful analysis).
4 MSG runtime and the rank of the iterates
As we saw in Sections 3.1 and 3.2  MSG requires O(k/2) iterations to obtain an -suboptimal
t d) operations  where kt is the rank of iterate M (t). This
solution  and each iteration costs O(k2
t . Clearly  the runtime for MSG depends
critically on the rank of the iterates. If kt is as large as d  then MSG achieves a runtime that is cubic
in the dimensionality. On the other hand  if the rank of the iterates is O(k)  the runtime is linear in
the dimensionality. Fortunately  in practice  each kt is typically much lower than d. The reason for
this is that the MSG update performs a rank-1 update followed by a projection onto the constraints.
t will have a larger trace than M (t) (i.e. tr M(cid:48) ≥ k)  the projection  as is
Since M(cid:48) = M (t) + ηxtxT
√
trM∗. Furthermore  the SGD analysis
depends on the Frobenius norm of the stochastic gradients  but since all stochastic gradients are rank one  this
is the same as their spectral norm  which comes up in the entropy-case analysis  and again there is no beneﬁt.

2This is because in our case  due to the other constraints  (cid:107)M∗(cid:107)F =

(cid:18)(cid:113) L∗k log(d/k)

+ k log(d/k)

t=1 k2

T

T

5

covariance matrices Σ = diag(σ/(cid:107)σ(cid:107))  where σi = τ−i/(cid:80)32

shown by Lemma 3.2  will subtract a quantity S from every eigenvalue of M(cid:48)  clipping each to 0 if
it becomes negative. Therefore  each MSG update will increase the rank of the iterate by at most 1 
and has the potential to decrease it  perhaps signiﬁcantly. It’s very difﬁcult to theoretically quantify
how the rank of the iterates will evolve over time  but we have observed empirically that the iterates
do tend to have relatively low rank.
We explore this issue in greater detail experimentally  on a distribution which we expect to be difﬁ-
cult for MSG. To this end  we generated data from known 32-dimensional distributions with diagonal
j=1 τ−j  for i = 1  . . .   32 and for
some τ > 1. Observe that Σ(k) has a smoothly-decaying set of eigenvalues and the rate of decay is
controlled by τ. As τ → 1  the spectrum becomes ﬂatter resulting in distributions that present chal-
lenging test cases for MSG. We experimented with τ = 1.1 and k ∈ {1  2  4}  where k is the desired
subspace dimension used by each algorithm. The data is generated by sampling the ith standard unit
basis vector ei with probability
Σii. We refer to this as the “orthogonal distribution”  since it is a
discrete distribution over 32 orthogonal vectors.
In Figure 1  we show the results with k = 4. We
can see from the left-hand plot that MSG main-
tains a subspace of dimension around 15. The
plot on the right shows how the set of nonzero
eigenvalues of the MSG iterates evolves over
time  from which we can see that many of the ex-
tra dimensions are “wasted” on very small eigen-
values  corresponding to directions which leave
the state matrix only a handful of iterations after
they enter. This suggests that constraining kt can
lead to signiﬁcant speedups and motivates capped
MSG updates discussed in the next section.

Figure 1: The ranks kt (left) and the eigenvalues
(right) of the MSG iterates M (t).

Iterations

Iterations

√

kt

Spectrum

5 Capped MSG
While  as was observed in the previous section  MSG’s iterates will tend to have ranks kt smaller
than d  they will nevertheless also be larger than k. For this reason  we recommend imposing a hard
constraint K on the rank of the iterates:

maximize : Ex∼D[xT M x]
subject to : M ∈ Rd×d  0 (cid:22) M (cid:22) I
tr M = k  rank M ≤ K

(5.1)

We will refer to MSG where the projection is replaced with a projection onto the constraints of
Problem 5.1 (i.e. where the iterates are SGD iterates on Problem 5.1) as “capped MSG”. As before 
as long as K ≥ k  Problem 5.1 and Problem 3.3 have the same optimum  it is achieved at a rank-k
matrix  and the extra rank constraint in Problem 5.1 is inactive at the optimum. However  the rank
constraint does affect the iterates  especially since Problem 5.1 is no longer convex. Nonetheless
if K > k (i.e. the hard rank-constraint K is strictly larger than the target rank k)  then we can
easily check if we are at a global optimum of Problem 5.1  and hence of Problem 3.3: if the capped
MSG algorithm converges to a solution of rank K  then the upper bound K should be increased.
Conversely  if it has converged to a rank-deﬁcient solution  then it must be the global optimum.
There is thus an advantage in using K > k  and we recommend setting K = k + 1  as we do in our
experiments  and increasing K only if a rank deﬁcient solution is not found in a timely manner.
If we take K = k  then the only way to satisfy the trace constraint is to have all non-zero eigenvalues
equal to one  and Problem 5.1 becomes identical to Problem 3.2. The detour through the convex
objective of Problem 3.3 allows us to increase the search rank K  allowing for more ﬂexibility in
the iterates  while still forcing each iterate to be low-rank  and each update to therefore be efﬁcient 
through the rank constraint.

Implementing the projection

5.1
The only difference between the implementation of MSG and capped MSG is in the projection step.
Similar reasoning to that which was used in the proof of Lemma 3.2 shows that if M (t+1) =P (M(cid:48))

6

1011021031040510152025303510110210310400.20.40.60.81k = 1

k = 2

k = 4

y
t
i
l
a
m

i
t
p
o
b
u
S

Iterations

Iterations

Iterations

Figure 2: Comparison on simulated data for different values of parameter k.

with M(cid:48) = M (t) + ηxtxT
t   then M (t) and M(cid:48) are simultaneously diagonalizable  and therefore
we can consider only how the projection acts on the eigenvalues. Hence  if we let σ(cid:48) be the vector
of the eigenvalues of M(cid:48)  and suppose that more than K of them are nonzero  then there will be
a a size-K subset of σ(cid:48) such that applying Algorithm 2 to this set gives the projected eigenvalues.
Since we perform only a rank-1 update at every iteration  we must check at most K possibilities 
at a total cost of O(K 2 log K) operations  which has no effect on the asymptotic runtime because
Algorithm 1 requires O(K 2d) operations.

5.2 Relationship to the incremental PCA method

The capped MSG algorithm with K = k is similar to the incremental algorithm of Arora et al.
(2012)  which maintains a rank-k approximation of the covariance matrix and updates according to:

(cid:16)

(cid:17)

M (t+1) = Prank-k

M (t) + xtxT
t

√

where the projection is onto the set of rank-k matrices. Unlike MSG  the incremental algorithm does
not have a step-size. Updates can be performed efﬁciently by maintaining an eigendecomposition
of each iterate  just as was done for MSG (see Section 3.2).
In a recent survey of stochastic algorithms for PCA (Arora et al.  2012)  the incremental algorithm
was found to perform extremely well in practice–it was the best  in fact  among the compared algo-
√
rithms. However  there exist cases in which it can get stuck at a suboptimal solution. For example 
If the data are drawn from a discrete distribution D which samples [
3  0]T with probability 1/3
and [0 
2]T with probability 2/3  and one runs the incremental algorithm with k = 1  then it will
converge to [1  0]T with probability 5/9  despite the fact that the maximal eigenvector is [0  1]T .
The reason for this failure is essentially that the orthogonality of the data interacts poorly with the
low-rank projection: any update which does not entirely displace the maximal eigenvector in one
iteration will be removed entirely by the projection  causing the algorithm to fail to make progress.
The capped MSG algorithm with K > k will not get stuck in such situations  since it will use
the additional dimensions to search in the new direction. Only as it becomes more conﬁdent in its
current candidate will the trace of M become increasingly concentrated on the top k directions. To
illustrate this empirically  we generalized this example by generating data using the 32-dimensional
“orthogonal” distribution described in Section 4. This distribution presents a challenging test-case
for MSG  capped MSG and the incremental algorithm. Figure 2 shows plots of individual runs of
MSG  capped MSG with K = k + 1  the incremental algorithm  and Warmuth and Kuzmin’s al-
gorithm  all based on the same sequence of samples drawn from the orthogonal distribution. We
compare algorithms in terms of the suboptimality on the population objective based on the largest
k eigenvalues of the state matrix M (t). The plots show the incremental algorithm getting stuck for
k ∈ {1  4}  and the others intermittently plateauing at intermediate solutions before beginning to
again converge rapidly towards the optimum. This behavior is to be expected on the capped MSG
algorithm  due to the fact that the dimension of the subspace stored at each iterate is constrained.
However  it is somewhat surprising that MSG and Warmuth and Kuzmin’s algorithm behaved simi-
larly  and barely faster than capped MSG.
6 Experiments
We also compared the algorithms on the real-world MNIST dataset  which consists of 70  000 binary
images of handwritten digits of size 28×28  resulting in a dimensionality of 784. We pre-normalized
the data by mean centering the feature vectors and scaling each feature by the product of its standard

7

10110210310400.20.40.60.8110110210310400.20.40.60.811.21.4  IncrementalWarmuth & KuzminMSGCapped MSG10110210310400.20.40.60.811.21.4k = 1

k = 4

k = 8

Iterations

Iterations

Iterations

y
t
i
l
a
m

i
t
p
o
b
u
S

y
t
i
l
a
m

i
t
p
o
b
u
S

iteration count  while the bottom row suboptimality as a function of estimated runtime(cid:80)t

Figure 3: Comparison on the MNIST dataset. The top row of plots shows suboptimality as a function of

Est. runtime

Est. runtime

Est. runtime

s=1(k(cid:48)

s)2.

deviation and the data dimension  so that each feature vector is zero mean and unit norm in expec-
tation. In addition to MSG  capped MSG  the incremental algorithm and Warmuth and Kuzmin’s
algorithm  we also compare to a Grassmannian SGD algorithm (Balzano et al.  2010). All algo-
√
rithms except the incremental algorithm have a step-size parameter. In these experiments  we ran
t for c ∈ {2−12  2−11  . . .   25} and picked the
each algorithm with decreasing step sizes ηt = c/
best c  in terms of the average suboptimality over the run  on a validation set. Since we cannot eval-
uate the true population objective  we estimate it by evaluating on a held-out test set. We use 40%
of samples in the dataset for training  20% for validation (tuning step-size)  and 40% for testing.
We are interested in learning a maximum variance subspace of dimension k ∈ {1  4  8} in a single
“pass” over the training sample. In order to compare MSG  capped MSG  the incremental algo-
rithm and Warmuth and Kuzmin’s algorithm in terms of runtime  we calculate the dominant term
s)2. The results are averaged over 100 random splits into

in the computational complexity:(cid:80)t

train-validation-test sets.
We can see from Figure 3 that the incremental algorithm makes the most progress per iteration and
is also the fastest of all algorithms. MSG is comparable to the incremental algorithm in terms of the
the progress made per iteration. However  its runtime is slightly worse because it will often keep
a slightly larger representation (of dimension kt). The capped MSG variant (with K = k + 1) is
signiﬁcantly faster–almost as fast as the incremental algorithm  while  as we saw in the previous
section  being less prone to getting stuck. Warmuth and Kuzmin’s algorithm fares well with k = 1 
but its performance drops for higher k. Inspection of the underlying data shows that  in the k ∈
{4  8} experiments  it also tends to have a larger kt than MSG in these experiments  and therefore
has a higher cost-per-iteration. Grassmannian SGD performs better than Warmuth and Kuzmin’s
algorithm  but much worse than MSG and capped MSG.

s=1(k(cid:48)

7 Conclusions

In this paper  we presented a careful development and analysis of MSG  a stochastic approximation
algorithm for PCA  which enjoys good theoretical guarantees and offers a computationally efﬁcient
variant  capped MSG. We show that capped MSG is well-motivated theoretically and that it does
not get stuck at a suboptimal solution. Capped MSG is also shown to have excellent empirical per-
formance and it therefore is a much better alternative to the recently proposed incremental PCA
algorithm of Arora et al. (2012). Furthermore  we provided a cleaner interpretation of PCA up-
dates of Warmuth & Kuzmin (2008) in terms of Matrix Exponentiated Gradient (MEG) updates
and showed that both MSG and MEG can be interpreted as mirror descent algorithms on the same
relaxation of the PCA optimization problem but with different distance generating functions.

8

10010110210310410500.511.522.53  IncrementalWarmuth & KuzminMSGCapped MSGGrassmannian10010110210310410501234567810010110210310410502468101210010110210310410510610710800.511.522.5100101102103104105106107108012345678100101102103104105106107108024681012References
Arora  Raman  Cotter  Andrew  Livescu  Karen  and Srebro  Nathan. Stochastic optimization for
PCA and PLS. In 50th Annual Allerton Conference on Communication  Control  and Computing 
2012.

Balzano  Laura  Nowak  Robert  and Recht  Benjamin. Online identiﬁcation and tracking of sub-
spaces from highly incomplete information. In 48th Annual Allerton Conference on Communica-
tion  Control  and Computing  2010.

Beck  A. and Teboulle  M. Mirror descent and nonlinear projected subgradient methods for convex

optimization. Operations Research Letters  31(3):167–175  2003.

Bottou  Leon and Bousquet  Olivier. The tradeoffs of large scale learning. In NIPS’07  pp. 161–168 

2007.

Boyd  Stephen and Vandenberghe  Lieven. Convex Optimization. Cambridge University Press  2004.
Brand  Matthew. Incremental singular value decomposition of uncertain data with missing values.

In ECCV  2002.

Collins  Michael  Globerson  Amir  Koo  Terry  Carreras  Xavier  and Bartlett  Peter L. Exponen-
tiated gradient algorithms for conditional random ﬁelds and max-margin markov networks. J.
Mach. Learn. Res.  9:1775–1822  June 2008.

Duchi  John  Shalev-Shwartz  Shai  Singer  Yoram  and Chandra  Tushar. Efﬁcient projections onto
the l1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning  ICML ’08  pp. 272–279  New York  NY  USA  2008. ACM.

Nemirovski  Arkadi and Yudin  David. Problem complexity and method efﬁciency in optimization.

John Wiley & Sons Ltd  1983.

Nemirovski  Arkadi  Juditsky  Anatoli  Lan  Guanghui  and Shapiro  Alexander. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization  19(4):1574–
1609  January 2009.

Oja  Erkki and Karhunen  Juha. On stochastic approximation of the eigenvectors and eigenvalues
of the expectation of a random matrix. Journal of Mathematical Analysis and Applications  106:
69–84  1985.

Sanger  Terence D. Optimal unsupervised learning in a single-layer linear feedforward neural net-

work. Neural Networks  12:459–473  1989.

Shalev-Shwartz  Shai and Srebro  Nathan. SVM optimization: Inverse dependence on training set

size. In ICML’08  pp. 928–935  2008.

Shalev-Shwartz  Shai and Tewari  Ambuj. Stochastic methods for l1 regularized loss minimization.
In Proceedings of the 26th Annual International Conference on Machine Learning  ICML’09  pp.
929–936  New York  NY  USA  2009. ACM.

Shalev-Shwartz  Shai  Singer  Yoram  and Srebro  Nathan. Pegasos: Primal Estimated sub-GrAdient

SOlver for SVM. In ICML’07  pp. 807–814  2007.

Srebro  N.  Sridharan  K.  and Tewari  A. On the universality of online mirror descent. Advances in

Neural Information Processing Systems  24  2011.

Warmuth  Manfred K. and Kuzmin  Dima. Randomized online PCA algorithms with regret bounds
that are logarithmic in the dimension. Journal of Machine Learning Research (JMLR)  9:2287–
2320  2008.

9

,Raman Arora
Andy Cotter
Nati Srebro
Dipan Pal
Ashwin Kannan
Gautam Arakalgud
Marios Savvides
Marylou Gabrié
Andre Manoel
Clément Luneau
jean barbier
Nicolas Macris
Florent Krzakala
Lenka Zdeborová
Boyi Li
Felix Wu
Kilian Weinberger
Serge Belongie