2019,Limitations of the empirical Fisher approximation for natural gradient descent,Natural gradient descent  which preconditions a gradient descent update
with the Fisher information matrix of the underlying statistical model 
is a way to capture partial second-order information. 
Several highly visible works have advocated an approximation known as the empirical Fisher 
drawing connections between approximate second-order methods and heuristics like Adam.
We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information.
We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian)
are unlikely to be met in practice  and that  even on simple optimization problems 
the pathologies of the empirical Fisher can have undesirable effects.,Limitations of the Empirical Fisher Approximation

for Natural Gradient Descent

Frederik Kunstner1 2 3

kunstner@cs.ubc.ca

Lukas Balles2 3

lballes@tue.mpg.de

Philipp Hennig2 3
ph@tue.mpg.de

École Polytechnique Fédérale de Lausanne (EPFL)  Switzerland1

University of Tübingen  Germany2

Max Planck Institute for Intelligent Systems  Tübingen  Germany3

Abstract

Natural gradient descent  which preconditions a gradient descent update with the
Fisher information matrix of the underlying statistical model  is a way to capture
partial second-order information. Several highly visible works have advocated
an approximation known as the empirical Fisher  drawing connections between
approximate second-order methods and heuristics like Adam. We dispute this
argument by showing that the empirical Fisher—unlike the Fisher—does not
generally capture second-order information. We further argue that the conditions
under which the empirical Fisher approaches the Fisher (and the Hessian) are
unlikely to be met in practice  and that  even on simple optimization problems  the
pathologies of the empirical Fisher can have undesirable effects.

(cid:80)

(cid:80)

L(θ) := −

Introduction

1
Consider a supervised machine learning problem of predicting outputs y ∈ Y from inputs x ∈ X.
We assume a probabilistic model for the conditional distribution of the form pθ(y|x) = p(y|f (x  θ)) 
where p(y|·) is an exponential family with natural parameters in F and f : X×RD → F is a prediction
function parameterized by θ ∈ RD. Given N iid training samples (xn  yn)N
n=1  we want to minimize
(1)
This framework covers common scenarios such as least-squares regression (Y = F = R and p(y|f ) =
N (y; f  σ2) with ﬁxed σ2) or C-class classiﬁcation with cross-entropy loss (Y = {1  . . .   C} 
i exp(fi)) with an arbitrary prediction function f. Eq. (1)
can be minimized by gradient descent  which updates θt+1 = θt − γt∇L(θt) with step size γt ∈ R.
This update can be preconditioned with a matrix Bt that incorporates additional information  such
−1∇L(θt). Choosing Bt to be the Hessian yields Newton’s
as local curvature  θt+1 = θt − γtBt
method  but its computation is often burdensome and might not be desirable for non-convex problems.
A prominent variant in machine learning is natural gradient descent [NGD; Amari  1998]. It adapts
to the information geometry of the problem by measuring the distance between parameters with the
Kullback–Leibler divergence between the resulting distributions rather than their Euclidean distance 
using the Fisher information matrix (or simply “Fisher”) of the model as a preconditioner 

F = RC and p(y = c|f ) = exp(fc)/(cid:80)

n log pθ(yn|xn) = −

n log p(yn|f (xn  θ)).

F(θ) :=(cid:80)

∇θ log pθ(y|xn) ∇θ log pθ(y|xn)(cid:62)(cid:3) .
(cid:2)

n Epθ(y|xn)

(2)
While this motivation is conceptually distinct from approximating the Hessian  the Fisher coincides
with a generalized Gauss-Newton [Schraudolph  2002] approximation of the Hessian for the problems
presented here. This gives NGD theoretical grounding as an approximate second-order method.
A number of recent works in machine learning have relied on a certain approximation of the Fisher 
which is often called the empirical Fisher (EF) and is deﬁned as

(cid:101)F(θ) :=(cid:80)

n ∇θ log pθ(yn|xn) ∇θ log pθ(yn|xn)(cid:62).

(3)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Fisher vs. empirical Fisher as preconditioners for linear least-squares regression on the data
shown in the left-most panel. The second plot shows the gradient vector ﬁeld of the (quadratic) loss
function and sample trajectories for gradient descent. The remaining plots depict the vector ﬁelds of
the natural gradient and the “EF-preconditioned” gradient  respectively. NGD successfully adapts to
the curvature whereas preconditioning with the empirical Fisher results in a distorted gradient ﬁeld.

At ﬁrst glance  this approximation is merely replacing the expectation over y in Eq. (2) with a sample
yn. However  yn is a training label and not a sample from the model’s predictive distribution pθ(y|xn).
Therefore  and contrary to what its name suggests  the empirical Fisher is not an empirical (i.e. Monte
Carlo) estimate of the Fisher. Due to the unclear relationship between the model distribution and the
data distribution  the theoretical grounding of the empirical Fisher approximation is dubious.
Adding to the confusion  the term “empirical Fisher” is used by different communities to refer to
different quantities. Authors closer to statistics tend to use “empirical Fisher” for Eq. (2)  while many
works in machine learning  some listed in Section 2  use “empirical Fisher” for Eq. (3). While the
statistical terminology is more accurate  we adopt the term “Fisher” for Eq. (2) and “empirical Fisher”
for Eq. (3)  which is the subject of this work  to be accessible to readers more familiar with this
convention. We elaborate on the different uses of the terminology in Section 3.1.
The main purpose of this work is to provide a detailed critical discussion of the empirical Fisher
approximation. While the discrepancy between the empirical Fisher and the Fisher has been
mentioned in the literature before [Pascanu and Bengio  2014  Martens  2014]  we see the need for
a detailed elaboration of the subtleties of this important issue. The intricacies of the relationship
between the empirical Fisher and the Fisher remain opaque from the current literature. Not all authors
using the EF seem to be fully aware of the heuristic nature of this approximation and overlook
its shortcomings  which can be seen clearly even on simple linear regression problems  see Fig. 1.
Natural gradients adapt to the curvature of the function using the Fisher while the empirical Fisher
distorts the gradient ﬁeld in a way that lead to worse updates than gradient descent.
The empirical Fisher approximation is so ubiquitous that it is sometimes just called the Fisher [e.g. 
Chaudhari et al.  2017  Wen et al.  2019]. Possibly as a result of this  there are examples of algorithms
involving the Fisher  such as Elastic Weight Consolidation [Kirkpatrick et al.  2017] and KFAC
[Martens and Grosse  2015]  which have been re-implemented by third parties using the empirical
Fisher. Interestingly  there is also at least one example of an algorithm that was originally developed
using the empirical Fisher and later found to work better with the Fisher [Wierstra et al.  2008 
Sun et al.  2009]. As the empirical Fisher is now used beyond optimization  for example as an
approximation of the Hessian in empirical works studying properties of neural networks [Chaudhari
et al.  2017  Jastrz˛ebski et al.  2018]  the pathologies of the EF approximation may lead the community
to erroneous conclusions—an arguably more worrysome outcome than a suboptimal preconditioner.
The poor theoretical grounding stands in stark contrast to the practical success that empirical Fisher-
based methods have seen. This paper is in no way meant to negate these practical advances but
rather points out that the existing justiﬁcations for the approximation are insufﬁcient and do not
stand the test of simple examples. This indicates that there are effects at play that currently elude our
understanding  which is not only unsatisfying  but might also prevent advancement of these methods.
We hope that this paper helps spark interest in understanding these effects; our ﬁnal section explores
a possible direction.

2

Datasety=θx+bGDNGDEF1.1 Overview and contributions

We ﬁrst provide a short but complete overview of natural gradient and the closely related generalized
Gauss-Newton method. Our main contribution is a critical discussion of the speciﬁc arguments used
to advocate the empirical Fisher approximation. A principal conclusion is that  while the empirical
Fisher follows the formal deﬁnition of a generalized Gauss-Newton matrix  it is not guaranteed to
capture any useful second-order information. We propose a clarifying amendment to the deﬁnition
of a generalized Gauss-Newton to ensure that all matrices satisfying it have useful approximation
properties. Furthermore  while there are conditions under which the empirical Fisher approaches
the true Fisher  we argue that these are unlikely to be met in practice. We illustrate that using the
empirical Fisher can lead to highly undesirable effects; Fig. 1 shows a ﬁrst example.
This raises the question: Why are methods based on the empirical Fisher practically successful?
We point to an alternative explanation  as an adaptation to gradient noise in stochastic optimization
instead of an adaptation to curvature.

2 Related work

The generalized Gauss-Newton [Schraudolph  2002] and natural gradient descent [Amari  1998]
methods have inspired a line of work on approximate second-order optimization [Martens  2010 
Botev et al.  2017  Park et al.  2000  Pascanu and Bengio  2014  Ollivier  2015]. A successful
example in modern deep learning is the KFAC algorithm [Martens and Grosse  2015]  which uses a
computationally efﬁcient structural approximation to the Fisher.
Numerous papers have relied on the empirical Fisher approximation for preconditioning and other
purposes. Our critical discussion is in no way intended as an invalidation of these works. All of them
provide important insights and the use of the empirical Fisher is usually not essential to the main
contribution. However  there is a certain degree of vagueness regarding the relationship between the
Fisher  the EF  Gauss-Newton matrices and the Hessian. Oftentimes  only limited attention is devoted
to possible implications of the empirical Fisher approximation.
The most prominent example of preconditioning with the EF is Adam  which uses a moving average
of squared gradients as “an approximation to the diagonal of the Fisher information matrix” [Kingma
and Ba  2015]. The EF has been used in the context of variational inference by various authors
[Graves  2011  Zhang et al.  2018  Salas et al.  2018  Khan et al.  2018  Mishkin et al.  2018]  some
of which have drawn further connections between NGD and Adam. There are also several works
building upon KFAC which substitute the EF for the Fisher [George et al.  2018  Osawa et al.  2019].
The empirical Fisher has also been used as an approximation of the Hessian for purposes other than
preconditioning. Chaudhari et al. [2017] use it to investigate curvature properties of deep learning
training objectives. It has also been employed to explain certain characteristics of SGD [Zhu et al. 
2019  Jastrz˛ebski et al.  2018] or as a diagnostic tool during training [Liao et al.  2020].
Le Roux et al. [2007] and Le Roux and Fitzgibbon [2010] have considered the empirical Fisher in its
interpretation as the (non-central) covariance matrix of stochastic gradients. While they refer to their
method as “Online Natural Gradient”  their goal is explicitly to adapt the update to the stochasticity
of the gradient estimate  not to curvature. We will return to this perspective in Section 5.
Before moving on  we want to re-emphasize that other authors have previously raised concerns about
the empirical Fisher approximation [e.g.  Pascanu and Bengio  2014  Martens  2014]. This paper
is meant as a detailed elaboration of this known but subtle issue  with novel results and insights.
Concurrent to our work  Thomas et al. [2019] investigated similar issues in the context of estimating
the generalization gap using information criteria.

3 Generalized Gauss-Newton and natural gradient descent

This section brieﬂy introduces natural gradient descent  adresses the difference in terminology for the
quantities of interest across ﬁelds  introduces the generalized Gauss-Newton (GGN) and reviews the
connections between the Fisher  the GGN  and the Hessian.

3

Quantity

F(cid:81)
F(cid:81)

˜F

n pθ(x y)
n pθ(y|xn)

Eq. (5)
Eq. (6)
Eq. (7)

Terminology in statistics
Fisher
empirical Fisher

and machine learning

Fisher
empirical Fisher

Table 1: Common terminology for the Fisher information and related matrices by authors closely
aligned with statistics  such as Amari [1998]  Park et al. [2000]  and Karakida et al. [2019]  or
machine learning  such as Martens [2010]  Schaul et al. [2013]  and Pascanu and Bengio [2014].

3.1 Natural gradient descent

Gradient descent follows the direction of “steepest descent”  the negative gradient. But the deﬁnition
of steepest depends on a notion of distance and the gradient is deﬁned with respect to the Euclidean
distance. The natural gradient is a concept from information geometry [Amari  1998] and applies
when the gradient is taken w.r.t. the parameters θ of a probability distribution pθ. Instead of measuring
the distance between parameters θ and θ(cid:48) with the Euclidean distance  we use the Kullback–Leibler
(KL) divergence between the distributions pθ and pθ(cid:48). The resulting steepest descent direction is the
negative gradient preconditioned with the Hessian of the KL divergence  which is exactly the Fisher
information matrix of pθ 

(cid:2)
∇θ log pθ(z)∇θ log pθ(z)T(cid:3) = Epθ(z)

(cid:2)
−∇2

θ logθ p(z)(cid:3) .

F(θ) := Epθ(z)

(4)
The second equality may seem counterintuitive; the difference between the outer product of gradients
and the Hessian cancels out in expectation with respect to the model distribution at θ  see Appendix A.
This equivalence highlights the relationship of the Fisher to the Hessian.

n pθ(x y)(θ) = N Ex y∼p(x)pθ(y|x)

F(cid:81)
n pθ(y|xn)(θ) =(cid:80)
F(cid:81)

This is what statisticians would call the “Fisher information” of the model pθ(x  y). However  we
typically do not know the distribution over inputs p(x)  so we use the empirical distribution over x

3.2 Difference in terminology across ﬁelds
In our setting  we only model the conditional distribution pθ(y|x) of the joint distribution pθ(x  y) =
p(x)pθ(y|x). The Fisher information of θ for N samples from the joint distribution pθ(x  y) is

(cid:2)
∇θ log pθ(y|x)∇θ log pθ(y|x)T(cid:3)  
instead and compute the Fisher information of the conditional distribution(cid:81)
∇θ log pθ(y|xn)∇θ log pθ(y|xn)T(cid:3) .
(cid:2)
[2014]  as it is the Fisher information for the distribution we are optimizing (cid:81)

(6)
This is Eq. (2)  which we call the “Fisher”. This is the terminology used by work on the application
of natural gradient methods in machine learning  such as Martens [2014] and Pascanu and Bengio
n pθ(y|xn). Work
closer to the statistics literature  following the seminal paper of Amari [1998]  such as Park et al.
[2000] and Karakida et al. [2019]  call this quantity the “empirical Fisher” due to the usage of the
empirical samples for the inputs. In constrast  we call Eq. (3) the “empirical Fisher”  restated here 

n Ey∼pθ(y|xn)

n pθ(y|xn);

(5)

˜F(θ) =(cid:80)

n ∇θ log pθ(yn|xn)∇θ log pθ(yn|xn)T  

(7)
where “empirical” describes the use of samples for both the inputs and the outputs. This expression 
however  does not have a direct interpretation as a Fisher information as it does not sample the output
according to the distribution deﬁned by the model. Neither is it a Monte-Carlo approximation of
Eq. (6)  as the samples yn do not come from pθ(y|xn) but from the data distribution p(y|xn). How
close the empirical Fisher (Eq. 7) is to the Fisher (Eq. 6) depends on how close the model pθ(y|xn)
is to the true data-generating distribution p(y|xn).
3.3 Generalized Gauss-Newton

One line of argument justifying the use of the empirical Fisher approximation uses the connection be-
tween the Hessian and the Fisher through the generalized Gauss-Newton (GGN) matrix [Schraudolph 
2002]. We give here a condensed overview of the deﬁnition and properties of the GGN.

4

2

(cid:123)(cid:122)

+(cid:80)
(cid:124)

(cid:123)(cid:122)
n rn∇2

(cid:80)
The original Gauss-Newton algorithm is an approximation to Newton’s method for nonlinear least
∇2 L(θ) =(cid:80)
n(f (xn  θ) − yn)2. By the chain rule  the Hessian can be written as
squares problems  L(θ) = 1
(cid:124)
(cid:125)
n ∇θf (xn  θ)∇θf (xn  θ)(cid:62)
(8)

(cid:125)
Schraudolph [2002] generalized this idea to objectives of the form L(θ) = (cid:80)
∇2 L(θ) =(cid:80)

bn : RD → RM and an : RM → R  for which the Hessian can be written as1

where rn = f (xn  θ) − yn are the residuals. The ﬁrst part  G(θ)  is the Gauss-Newton matrix. For
small residuals  R(θ) will be small and G(θ) will approximate the Hessian. In particular  when the
model perfectly ﬁts the data  the Gauss-Newton is equal to the Hessian.

(9)
The generalized Gauss-Newton matrix (GGN) is deﬁned as the part of the Hessian that ignores the
second-order information of bn 

ban(bn(θ)) (Jθbn(θ)) +(cid:80)

n m[∇ban(bn(θ))]m∇2

n an(bn(θ))  with

n(Jθbn(θ))(cid:62)

θf (xn  θ)

∇2

θb(m)

n (θ).

:=G(θ)

:=R(θ)

 

n[Jθbn(θ)](cid:62)

(10)
If an is convex  as is customary  the GGN is positive (semi-)deﬁnite even if the Hessian itself is not 
making it a popular curvature matrix in non-convex problems such as neural network training. The
GGN is ambiguous as it crucially depends on the “split” given by an and bn. As an example  consider
the two following possible splits for the least-squares problem from above:

∇2
ban(bn(θ)) [Jθbn(θ)].

an(b) = 1

2 (b − yn)2  bn(θ) = f (xn  θ) 

(11)
The ﬁrst recovers the classical Gauss-Newton  while in the second case  the GGN equals the Hessian.
While this is an extreme example  the split will be important for our discussion.

2 (f (xn  b) − yn)2  bn(θ) = θ.

an(b) = 1

or

3.4 Connections between the Fisher  the GGN and the Hessian

G(θ) :=(cid:80)

an(b) = − log p(yn|b) 

(cid:80)
n[Jθf (xn  θ)](cid:62)

While NGD is not explicitly motivated as an approximate second-order method  the following result 
noted by several authors 2 shows that the Fisher captures partial curvature information about the
problem deﬁned in Eq. (1).
Proposition 1 (Martens [2014]  §9.2). If p(y|f ) is an exponential family distribution with natural
parameters f  then the Fisher information matrix coincides with the GGN of Eq. (1) using the split
(12)

bn(θ) = f (xn  θ) 

and reads F(θ) = G(θ) = −
f log p(yn|f (xn  θ)) [Jθf (xn  θ)].
∇2
f log p(y|f ) does
For completeness  a proof can be found in Appendix A. The key insight is that ∇2
not depend on y for exponential families. One can see Eq. (12) as the “canonical” split  since it
matches the classical Gauss-Newton for the probabilistic interpretation of least-squares. From now
on  when referencing “the GGN” without further speciﬁcation  we mean this particular split.
The GGN  and under the assumptions of Proposition 1 also the Fisher  are well-justiﬁed approxi-
mations of the Hessian and we can bound their approximation error in terms of the (generalized)
residuals  mirroring the motivation behind the classical Gauss-Newton (Proof in Appendix C.2).
Proposition 2. Let L(θ) be deﬁned as in Eq. (1) with F = RM . Denote by f (m)
of f (xn ·) : RD→ RM and assume each f (m)
where r(θ) =(cid:80)N

the m-th component
is β-smooth. Let G(θ) be the GGN (Eq. 10). Then 
(13)

n=1 (cid:107)∇f log p(yn|f (xn  θ))(cid:107)1 and (cid:107) · (cid:107)2 denotes the spectral norm.

(cid:107)∇2 L(θ) − G(θ)(cid:107)2

2 ≤ r(θ)β 

n

n

[·]m selects the m-th component of a vector; and b

1Jθbn(θ) ∈ RM×D is the Jacobian of bn; we use the shortened notation ∇2

The approximation improves as the residuals in r(θ) diminish  and is exact if the data is perfectly ﬁt.
ban(b)|b=bn(θ);
2Heskes [2000] showed this for regression with squared loss  Pascanu and Bengio [2014] for classiﬁcation
with cross-entropy loss  and Martens [2014] for general exponential families. However  this has been known
earlier in the statistics literature in the context of “Fisher Scoring” (see Wang [2010] for a review).

denotes the m-th component function of bn.

ban(bn(θ)) := ∇2

(m)
n

5

4 Critical discussion of the empirical Fisher

Two arguments have been put forward to advocate the empirical Fisher approximation. Firstly  it
has been argued that it follows the deﬁnition of a generalized Gauss-Newton matrix  making it an
approximate curvature matrix in its own right. We examine this relation in §4.1 and show that  while
technically correct  it does not entail the approximation guarantee usually associated with the GGN.
Secondly  a popular argument is that the empirical Fisher approaches the Fisher at a minimum if the
model “is a good ﬁt for the data”. We discuss this argument in §4.2 and point out that it requires
strong additional assumptions  which are unlikely to be met in practical scenarios. In addition  this
argument only applies close to a minimum  which calls into question the usefulness of the empirical
Fisher in optimization. We discuss this in §4.3  showing that preconditioning with the empirical
Fisher leads to adverse effects on the scaling and the direction of the updates far from an optimum.
We use simple examples to illustrate our arguments. We want to emphasize that  as these are counter-
examples to arguments found in the existing literature  they are designed to be as simple as possible 
and deliberately do not involve intricate state-of-the art models that would complicate analysis. On a
related note  while contemporary machine learning often relies on stochastic optimization  we restrict
our considerations to the deterministic (full-batch) setting to focus on the adaptation to curvature.

4.1 The empirical Fisher as a generalized Gauss-Newton matrix

The ﬁrst justiﬁcation for the empirical Fisher is that it matches the construction of a generalized
Gauss-Newton (Eq. 10) using the split [Bottou et al.  2018]

an(b) = − log b 

bn(θ) = p(yn|f (xn  θ)).

(14)

Although technically correct 3 we argue that this split does not provide a reasonable approximation.
For example  consider a least-squares problem which corresponds to the log-likelihood log p(y|f ) =
2 (y − f )2]. In this case  Eq. (14) splits the identity function  log exp(·)  and takes into
log exp[− 1
account the curvature from the log while ignoring that of exp. This questionable split runs counter to
the basic motivation behind the classical Gauss-Newton matrix  that small residuals lead to a good
approximation to the Hessian: The empirical Fisher

n ∇θ log pθ(yn|xn) ∇θ log pθ(yn|xn)(cid:62) =(cid:80)

(cid:101)F(θ) =(cid:80)
F (θ) = (cid:80)
be given by ∇2 L(θ) = F (θ) +(cid:80)

(15)
approaches zero as the residuals rn = f (xn  θ) − yn become small. In that same limit  the Fisher
n ∇f (xn  θ)∇f (xn  θ)(cid:62) does approach the Hessian  which we recall from Eq. (8) to
θf (xn  θ). This argument generally applies for problems
where we can ﬁt all training samples such that ∇θ log pθ(yn|xn) = 0 for all n. In such cases  the EF
goes to zero while the Fisher (and the corresponding GGN) approaches the Hessian (Prop. 2).
For the generalized Gauss-Newton  the role of the “residual” is played by the gradient ∇ban(b);
compare Equations (8) and (9). To retain the motivation behind the classical Gauss-Newton  the
split should be chosen such that this gradient can in principle attain zero  in which case the residual
curvature not captured by the GGN in (9) vanishes. The EF split (Eq. 14) does not satisfy this
property  as ∇b log b can never go to zero for a probability b ∈ [0  1]. It might be desirable to amend
the deﬁnition of a generalized Gauss-Newton to enforce this property (addition in bold):
n an(bn(θ)) with convex an  leads to a

Deﬁnition 1 (Generalized Gauss-Newton). A split L(θ) =(cid:80)

n ∇θf (xn  θ) ∇θf (xn  θ)(cid:62) 
n r2

n rn∇2

generalized Gauss-Newton matrix of L  deﬁned as

n Gn(θ) 

Gn(θ) := [Jθbn(θ)](cid:62)
n ∈ Im(bn) such that ∇ban(b)|b=b∗

∇2
ban(bn(θ)) [Jθbn(θ)] 

if the split an  bn is such that there is b∗
Under suitable smoothness conditions  a split satisfying this condition will have a meaningful error
bound akin to Proposition 2. To avoid confusion  we want to note that this condition does not assume
the existence of θ∗ such that bn(θ∗) = b∗
n for all n; only that the residual gradient for each data point
can  in principle  go to zero.

= 0.

n

(16)

G(θ) =(cid:80)

3The equality can easily be veriﬁed by plugging the split (14) into the deﬁnition of the GGN (Eq. 10) and

ban(b) = ∇ban(b) ∇ban(b)(cid:62) as a special property of the choice an(b) = − log(b).

observing that ∇2

6

Figure 2: Quadratic approximations of the loss function using the Fisher and the empirical Fisher
on a logistic regression problem. Logistic regression implicitly assumes identical class-conditional
covariances [Hastie et al.  2009  §4.4.5]. The EF is a good approximation of the Fisher at the
minimum if this assumption is fulﬁlled (left panel)  but can be arbitrarily wrong if the assumption
is violated  even at the minimum and with large N. Note: we achieve classiﬁcation accuracies of
≥ 85% in the misspeciﬁed cases compared to 73% in the well-speciﬁed case  which shows that a
well-performing model is not necessarily a well-speciﬁed one.

4.2 The empirical Fisher near a minimum

N

N ) N→∞
−→ 1

N F(θ(cid:63)

An often repeated argument is that the empirical Fisher converges to the true Fisher when the model
is a good ﬁt for the data [e.g.  Jastrz˛ebski et al.  2018  Zhu et al.  2019]. Unfortunately  this is
often misunderstood to simply mean “near the minimum”. The above statement has to be carefully
formalized and requires additional assumptions  which we detail in the following.
Assume that the training data consists of iid samples from some data-generating distribution
ptrue(x  y) = ptrue(y|x)ptrue(x).
If the model is realizable  i.e.  there exists a parameter setting
θT such that pθT (y|x) = ptrue(y|x)  then clearly by a Monte Carlo sampling argument  as the number
(y|x) converges to ptrue(y|x) as N → ∞ 
estimate for N samples θ(cid:63)
(17)
N ).

of data points N goes to inﬁnity (cid:101)F(θT)/N → F(θT)/N. Additionally  if the maximum likelihood

N is consistent in the sense that pθ(cid:63)

N(cid:101)F(θ(cid:63)

1

That is  the empirical Fisher converges to the Fisher at the minimum as the number of data points
grows. (Both approach the Hessian  as can be seen from the second equality in Eq. 4 and detailed
in Appendix C.2.) For the EF to be a useful approximation  we thus need (i) a “correctly-speciﬁed”
model in the sense of the realizability condition  and (ii) enough data to recover the true parameters.
Even under the assumption that N is sufﬁciently large  the model needs to be able to realize the true
data distribution. This requires that the likelihood p(y|f ) is well-speciﬁed and that the prediction
function f (x  θ) captures all relevant information. This is possible in classical statistical modeling of 
say  scientiﬁc phenomena where the effect of x on y is modeled based on domain knowledge. But it
is unlikely to hold when the model is only approximate  as is most often the case in machine learning.
Figure 2 shows examples of model misspeciﬁcation and the effect on the empirical and true Fisher.
It is possible to satisfy the realizability condition by using a very ﬂexible prediction function f (x  θ) 
such as a deep network. However  “enough” data has to be seen relative to the model capacity. The
massively overparameterized models typically used in deep learning are able to ﬁt the training data
almost perfectly  even when regularized [Zhang et al.  2017]. In such settings  the individual gradients 
and thus the EF  will be close to zero at a minimum  whereas the Hessian will generally be nonzero.

4.3 Preconditioning with the empirical Fisher far from an optimum
The relationship discussed in §4.2 only holds close to the minimum. Any similarity between pθ(y|x)
and ptrue(y|x) is very unlikely when θ has not been adapted to the data  for example  at the beginning
of an optimization procedure. This makes the empirical Fisher a questionable preconditioner.

7

DatasetCorrectMisspeciﬁed(A)Misspeciﬁed(B)QuadraticapproximationLosscontourFisheremp.FisherMinimumFigure 3: Fisher (NGD) vs. empirical Fisher (EFGD) as preconditioners (with damping) on linear
classiﬁcation (BreastCancer  a1a) and regression (Boston). While the EF can be a good approximation
for preconditioning on some problems (e.g.  a1a)  it is not guaranteed to be. The second row shows
the cosine similarity between the EF direction and the natural gradient  over the path taken by EFGD 
showing that the EF can lead to update directions that are opposite to the natural gradient (see Boston).
Even when the direction is correct  the magnitude of the steps can lead to poor performance (see
BreastCancer). See Appendix D for details and additional experiments.

In fact  the empirical Fisher can cause severe  adverse distortions of the gradient ﬁeld far from the
optimum  as evident even on the elementary linear regression problem of Fig. 1. As a consequence 
EF-preconditioned gradient descent compares unfavorably to NGD even on simple linear regression
and classiﬁcation tasks  as shown in Fig. 3. The cosine similarity plotted in Fig. 3 shows that the
empirical Fisher can be arbitrarily far from the Fisher in that the two preconditioned updates point in
almost opposite directions.
One particular issue is the scaling of EF-preconditioned updates. As the empirical Fisher is the sum
of “squared” gradients (Eq. 3)  multiplying the gradient by the inverse of the EF leads to updates of
magnitude almost inversely proportional to that of the gradient  at least far from the optimum. This
effect has to be counteracted by adapting the step size  which requires manual tuning and makes the
selected step size dependent on the starting point; we explore this aspect further in Appendix E.

5 Variance adaptation

N(cid:101)F(θ) = Σ(θ) + ∇L(θ) ∇L(θ)(cid:62) 

Σ(θ) := cov[g(θ)].

The previous sections have shown that  interpreted as a curvature matrix  the empirical Fisher is a
questionable choice at best. Another perspective on the empirical Fisher is that  in contrast to the
Fisher  it contains useful information to adapt to the gradient noise in stochastic optimization.
In stochastic gradient descent [SGD; Robbins and Monro  1951]  we sample n ∈ [N ] uniformly
at random and use a stochastic gradient g(θ) = −N ∇θ log pθ(yn|xn) as an inexpensive but noisy
estimate of ∇L(θ). The empirical Fisher  as a sum of outer products of individual gradients  coincides
with the non-central second moment of this estimate and can be written as
(18)
Gradient noise is a major hindrance to SGD and the covariance information encoded in the EF may
be used to attenuate its harmful effects  e.g.  by scaling back the update in high-noise directions.
A small number of works have explored this idea before. Le Roux et al. [2007] showed that the
update direction Σ(θ)−1g(θ) maximizes the probability of decreasing in function value  while Schaul
et al. [2013] proposed a diagonal rescaling based on the signal-to-noise ratio of each coordinate 
i + Σ(θ)ii). Balles and Hennig [2018] identiﬁed these factors as
Dii := [∇L(θ)]2
A straightforward extension of this argument to full matrices yields the variance adaptation matrix

optimal in that they minimize the expected error E(cid:2)
M =(cid:0)Σ(θ) + ∇L(θ) ∇L(θ)(cid:62)(cid:1)−1

∇L(θ) ∇L(θ)(cid:62) = (N(cid:101)F(θ))−1∇L(θ) ∇L(θ)(cid:62).

(19)
In that sense  preconditioning with the empirical Fisher can be understood as an adaptation to gradient
noise instead of an adaptation to curvature. The multiplication with ∇L(θ)∇L(θ)(cid:62) in Eq. (19) will
counteract the poor scaling discussed in §4.3.
This perspective on the empirical Fisher is currently not well studied. Of course  there are obvious
difﬁculties ahead: Computing the matrix in Eq. (19) requires the evaluation of all gradients  which

(cid:3) for a diagonal matrix D.

(cid:107)Dg(θ) − ∇L(θ)(cid:107)2

2

i / ([∇L(θ)]2

8

10−2100LossBreastCancer101102Boston100a1a02575100Iteration-11Cosine(NGD EFG)02575100Iteration-11050Iteration-11GDNGDEFGDdefeats its purpose. It is not obvious how to obtain meaningful estimates of this matrix from  say  a
mini-batch of gradients  that would provably attenuate the effects of gradient noise. Nevertheless 
we believe that variance adaptation is a possible explanation for the practical success of existing
methods using the EF and an interesting avenue for future research. To put it bluntly: it may just be
that the name “empirical Fisher” is a fateful historical misnomer  and the quantity should instead just
be described as the gradient’s non-central second moment.
As a ﬁnal comment  it is worth pointing out that some methods precondition with the square-root of
the EF  the prime example being Adam. While this avoids the “inverse gradient” scaling discussed in
§4.3  it further widens the conceptual gap between those methods and natural gradient. In fact  such a
preconditioning effectively cancels out the gradient magnitude  which has recently been examined
more closely as “sign gradient descent” [Balles and Hennig  2018  Bernstein et al.  2018].

6 Conclusions

We offered a critical discussion of the empirical Fisher approximation  summarized as follows:

• While the EF follows the formal deﬁnition of a generalized Gauss-Newton matrix  the
underlying split does not retain useful second-order information. We proposed a clarifying
amendment to the deﬁnition of the GGN.

• A clear relationship between the empirical Fisher and the Fisher only exists at a minimum
under strong additional assumptions: (i) a correct model and (ii) enough data relative to
model capacity. These conditions are unlikely to be met in practice  especially when using
overparametrized general function approximators and settling for approximate minima.

• Far from an optimum  EF preconditioning leads to update magnitudes which are inversely
proportional to that of the gradient  complicating step size tuning and often leading to poor
performance even for linear models.

• As a possible alternative explanation of the practical success of EF preconditioning  and an
interesting avenue for future research  we have pointed to the concept of variance adaptation.

The existing arguments do not justify the empirical Fisher as a reasonable approximation to the Fisher
or the Hessian. Of course  this does not rule out the existence of certain model classes for which
the EF might give reasonable approximations. However  as long as we have not clearly identiﬁed
and understood these cases  the true Fisher is the “safer” choice as a curvature matrix and should be
preferred in virtually all cases.
Contrary to conventional wisdom  the Fisher is not inherently harder to compute than the EF. As shown
by Martens and Grosse [2015]  an unbiased estimate of the true Fisher can be obtained at the same
computational cost as the empirical Fisher by replacing the expectation in Eq. (2) with a single sample
˜yn from the model’s predictive distribution pθ(y|xn). Even exact computation of the Fisher is feasible
in many cases. We discuss computational aspects further in Appendix B. The apparent reluctance to
compute the Fisher might have more to do with the current lack of convenient implementations in
deep learning libraries. We believe that it is misguided—and potentially dangerous—to accept the
poor theoretical grounding of the EF approximation purely for implementational convenience.

Acknowledgements

We thank Matthias Bauer  Felix Dangel  Filip de Roos  Diego Fioravanti  Jason Hartford  Si Kai
Lee  and Frank Schneider for their helpful comments on the manuscript. We thank Emtiyaz Khan 
Aaron Mishkin  and Didrik Nielsen for many insightful conversations that lead to this work  and the
anonymous reviewers for their constructive feedback.
Lukas Balles kindly acknowledges the support of the International Max Planck Research School
for Intelligent Systems (IMPRS-IS). The authors gratefully acknowledge ﬁnancial support by the
European Research Council through ERC StG Action 757275 / PANAMA and the DFG Cluster
of Excellence “Machine Learning - New Perspectives for Science”  EXC 2064/1  project number
390727645  the German Federal Ministry of Education and Research (BMBF) through the Tübingen
AI Center (FKZ: 01IS18039A) and funds from the Ministry of Science  Research and Arts of the
State of Baden-Württemberg.

9

References
Shun-ichi Amari. Natural gradient works efﬁciently in learning. Neural computation  10(2):251–276 

1998.

Lukas Balles and Philipp Hennig. Dissecting Adam: The sign  magnitude and variance of stochastic
gradients. In Jennifer G. Dy and Andreas Krause  editors  Proceedings of the 35th International
Conference on Machine Learning  ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15 
2018  volume 80 of Proceedings of Machine Learning Research  pages 413–422. PMLR  2018.

Jeremy Bernstein  Yu-Xiang Wang  Kamyar Azizzadenesheli  and Anima Anandkumar. signSGD:
compressed optimisation for non-convex problems.
In Jennifer G. Dy and Andreas Krause 
editors  Proceedings of the 35th International Conference on Machine Learning  ICML 2018 
Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  volume 80 of Proceedings of Machine
Learning Research  pages 559–568. PMLR  2018.

Aleksandar Botev  Hippolyt Ritter  and David Barber. Practical Gauss-Newton optimisation for deep
learning. In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th International Con-
ference on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  volume 70
of Proceedings of Machine Learning Research  pages 557–565. PMLR  2017.

Léon Bottou  Frank E. Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. SIAM Reviews  60(2):223–311  2018.

Pratik Chaudhari  Anna Choromanska  Stefano Soatto  Yann LeCun  Carlo Baldassi  Christian Borgs 
Jennifer Chayes  Levent Sagun  and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. In 5th International Conference on Learning Representations  ICLR 2017 
Toulon  France  April 24-26  2017  Conference Track Proceedings. OpenReview.net  2017.

Thomas George  César Laurent  Xavier Bouthillier  Nicolas Ballas  and Pascal Vincent. Fast
approximate natural gradient descent in a Kronecker-factored eigenbasis. In Samy Bengio  Hanna
Wallach  Hugo Larochelle  Kristen Grauman  Nicolò Cesa-Bianchi  and Roman Garnett  editors 
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal  Canada.  pages 9573–
9583  2018.

Alex Graves. Practical variational inference for neural networks. In John Shawe-Taylor  Richard S.
Zemel  Peter L. Bartlett  Fernando C. N. Pereira  and Kilian Q. Weinberger  editors  Advances
in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information
Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011  Granada  Spain. 
pages 2348–2356  2011.

Trevor Hastie  Robert Tibshirani  and Jerome Friedman. The elements of statistical learning: data

mining  inference  and prediction. Springer Verlag  2009.

Tom Heskes. On “natural” learning and pruning in multilayered perceptrons. Neural Computation 

12(4):881–901  2000.

Stanisław Jastrz˛ebski  Zac Kenton  Devansh Arpit  Nicolas Ballas  Asja Fischer  Amos Storkey 
and Yoshua Bengio. Three factors inﬂuencing minima in SGD. In International Conference on
Artiﬁcial Neural Networks  2018.

Eric Jones  Travis Oliphant  Pearu Peterson  et al. SciPy: Open source scientiﬁc tools for Python 

2001. URL http://www.scipy.org/.

Ryo Karakida  Shotaro Akaho  and Shun ichi Amari. Universal statistics of ﬁsher information in deep
neural networks: Mean ﬁeld approach. In Kamalika Chaudhuri and Masashi Sugiyama  editors 
The 22nd International Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2019  16-18
April 2019  Naha  Okinawa  Japan  volume 89 of Proceedings of Machine Learning Research 
pages 1032–1041. PMLR  2019.

10

Mohammad Emtiyaz Khan  Didrik Nielsen  Voot Tangkaratt  Wu Lin  Yarin Gal  and Akash Srivastava.
Fast and scalable Bayesian deep learning by weight-perturbation in Adam. In Jennifer G. Dy and
Andreas Krause  editors  Proceedings of the 35th International Conference on Machine Learning 
ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  volume 80 of Proceedings
of Machine Learning Research  pages 2616–2625. PMLR  2018.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun  editors  3rd International Conference on Learning Representations  ICLR 2015 
San Diego  CA  USA  May 7-9  2015  Conference Track Proceedings  2015.

James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  Joel Veness  Guillaume Desjardins  Andrei A.
Rusu  Kieran Milan  John Quan  Tiago Ramalho  Agnieszka Grabska-Barwinska  Demis Hassabis 
Claudia Clopath  Dharshan Kumaran  and Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences  114(13):3521–3526  2017.

Nicolas Le Roux and Andrew Fitzgibbon. A fast natural Newton method. In Johannes Fürnkranz
and Thorsten Joachims  editors  Proceedings of the 27th International Conference on Machine
Learning (ICML-10)  June 21-24  2010  Haifa  Israel  pages 623–630. Omnipress  2010.

Nicolas Le Roux  Pierre-Antoine Manzagol  and Yoshua Bengio. Topmoumoute online natural
gradient algorithm. In John C. Platt  Daphne Koller  Yoram Singer  and Sam T. Roweis  editors 
Advances in Neural Information Processing Systems 20  Proceedings of the Twenty-First Annual
Conference on Neural Information Processing Systems  Vancouver  British Columbia  Canada 
December 3-6  2007  pages 849–856. Curran Associates  Inc.  2007.

Zhibin Liao  Tom Drummond  Ian Reid  and Gustavo Carneiro. Approximate Fisher information
matrix to characterise the training of deep neural networks. IEEE Transactions on Pattern Analysis
and Machine Intelligence  42(1):15–26  2020.

James Martens. Deep learning via Hessian-free optimization. In Johannes Fürnkranz and Thorsten
Joachims  editors  Proceedings of the 27th International Conference on Machine Learning (ICML-
10)  June 21-24  2010  Haifa  Israel  pages 735–742. Omnipress  2010.

James Martens. New insights and perspectives on the natural gradient method. CoRR  abs/1412.1193 

2014.

James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate
curvature. In Francis Bach and David M. Blei  editors  Proceedings of the 32nd International
Conference on Machine Learning  ICML 2015  Lille  France  6-11 July 2015  volume 37 of JMLR
Workshop and Conference Proceedings  pages 2408–2417  2015.

Aaron Mishkin  Frederik Kunstner  Didrik Nielsen  Mark Schmidt  and Mohammad Emtiyaz Khan.
SLANG: Fast structured covariance approximations for Bayesian deep learning with natural
gradient. In Samy Bengio  Hanna Wallach  Hugo Larochelle  Kristen Grauman  Nicolò Cesa-
Bianchi  and Roman Garnett  editors  Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December
2018  Montréal  Canada.  pages 6248–6258. 2018.

Yann Ollivier. Riemannian metrics for neural networks I: feedforward networks. Information and

Inference: A Journal of the IMA  4(2):108–153  2015.

Kazuki Osawa  Yohei Tsuji  Yuichiro Ueno  Akira Naruse  Rio Yokota  and Satoshi Matsuoka.
Large-scale distributed second-order optimization using Kronecker-factored approximate curvature
for deep convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 12359–12367. Computer Vision Foundation / IEEE  June 2019.

Hyeyoung Park  Shun-ichi Amari  and Kenji Fukumizu. Adaptive natural gradient learning algorithms

for various stochastic models. Neural Networks  13(7):755–764  2000.

Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In Yoshua Bengio
and Yann LeCun  editors  2nd International Conference on Learning Representations  ICLR 2014 
Banff  AB  Canada  April 14-16  2014  Conference Track Proceedings  2014.

11

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical

Statistics  pages 400–407  1951.

Arnold Salas  Stefan Zohren  and Stephen Roberts. Practical Bayesian learning of neural networks

via adaptive subgradient methods. CoRR  abs/1811.03679  2018.

Tom Schaul  Sixin Zhang  and Yann LeCun. No more pesky learning rates. In Proceedings of the
30th International Conference on Machine Learning  ICML 2013  Atlanta  GA  USA  16-21 June
2013  volume 28 of JMLR Workshop and Conference Proceedings  pages 343–351  2013.

Nicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.

Neural Computation  14(7):1723–1738  2002.

Yi Sun  Daan Wierstra  Tom Schaul  and Jürgen Schmidhuber. Efﬁcient natural evolution strategies.
In Proceedings of the 11th Annual conference on Genetic and evolutionary computation  pages
539–546  2009.

Valentin Thomas  Fabian Pedregosa  Bart van Merriënboer  Pierre-Antoine Manzagol  Yoshua Bengio 

and Nicolas Le Roux. Information matrices and generalization. CoRR  abs/1906.07774  2019.

Yong Wang. Fisher scoring: An interpolation family and its Monte Carlo implementations. Computa-

tional Statistics & Data Analysis  54(7):1744–1755  2010.

Yeming Wen  Kevin Luk  Maxime Gazeau  Guodong Zhang  Harris Chan  and Jimmy Ba. Interplay
between optimization and generalization of stochastic gradient descent with covariance noise.
CoRR  abs/1902.08234  2019. To appear in the 23rd International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS)  2020.

Daan Wierstra  Tom Schaul  Jan Peters  and Jürgen Schmidhuber. Natural evolution strategies. In
2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational
Intelligence)  pages 3381–3387  2008.

Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations  ICLR 2017  Toulon  France  April 24-26  2017  Conference Track Proceedings.
OpenReview.net  2017.

Guodong Zhang  Shengyang Sun  David Duvenaud  and Roger Grosse. Noisy natural gradient
In Jennifer G. Dy and Andreas Krause  editors  Proceedings of the
as variational inference.
35th International Conference on Machine Learning  ICML 2018  Stockholmsmässan  Stockholm 
Sweden  July 10-15  2018  volume 80 of Proceedings of Machine Learning Research  pages
5847–5856. PMLR  2018.

Zhanxing Zhu  Jingfeng Wu  Bing Yu  Lei Wu  and Jinwen Ma. The anisotropic noise in stochastic
In
gradient descent: Its behavior of escaping from sharp minima and regularization effects.
Kamalika Chaudhuri and Ruslan Salakhutdinov  editors  Proceedings of the 36th International
Conference on Machine Learning  ICML 2019  9-15 June 2019  Long Beach  California  USA 
volume 97 of Proceedings of Machine Learning Research  pages 7654–7663. PMLR  2019.

12

,Anirudh Goyal ALIAS PARTH GOYAL
Nan Rosemary Ke
Surya Ganguli
Yoshua Bengio
Frederik Kunstner
Philipp Hennig
Lukas Balles