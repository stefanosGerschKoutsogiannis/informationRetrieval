2010,Transduction with Matrix Completion: Three Birds with One Stone,We pose transductive classification as a matrix completion problem. By assuming the underlying matrix has a low rank  our formulation is able to handle three problems simultaneously: i) multi-label learning  where each item has more than one label  ii) transduction  where most of these labels are unspecified  and iii) missing data  where a large number of features are missing. We obtained satisfactory results on several real-world tasks  suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modified fixed-point continuation method that is guaranteed to find the global optimum.,Transduction with Matrix Completion:

Three Birds with One Stone

Andrew B. Goldberg1  Xiaojin Zhu1  Benjamin Recht1  Jun-Ming Xu1  Robert Nowak2

Department of {1Computer Sciences  2Electrical and Computer Engineering}

{goldberg  jerryzhu  brecht  xujm}@cs.wisc.edu  nowak@ece.wisc.edu

University of Wisconsin-Madison  Madison  WI 53706

Abstract

We pose transductive classiﬁcation as a matrix completion problem. By assuming
the underlying matrix has a low rank  our formulation is able to handle three prob-
lems simultaneously: i) multi-label learning  where each item has more than one
label  ii) transduction  where most of these labels are unspeciﬁed  and iii) miss-
ing data  where a large number of features are missing. We obtained satisfactory
results on several real-world tasks  suggesting that the low rank assumption may
not be as restrictive as it seems. Our method allows for different loss functions to
apply on the feature and label entries of the matrix. The resulting nuclear norm
minimization problem is solved with a modiﬁed ﬁxed-point continuation method
that is guaranteed to ﬁnd the global optimum.

1

Introduction

Semi-supervised learning methods make assumptions about how unlabeled data can help in the
learning process  such as the manifold assumption (data lies on a low-dimensional manifold) and
the cluster assumption (classes are separated by low density regions) [4  16].
In this work  we
present two transductive learning methods under the novel assumption that the feature-by-item and
label-by-item matrices are jointly low rank. This assumption effectively couples different label pre-
diction tasks  allowing us to implicitly use observed labels in one task to recover unobserved labels
in others. The same is true for imputing missing features. In fact  our methods learn in the difﬁ-
cult regime of multi-label transductive learning with missing data that one sometimes encounters in
practice. That is  each item is associated with many class labels  many of the items’ labels may be
unobserved (some items may be completely unlabeled across all labels)  and many features may also
be unobserved. Our methods build upon recent advances in matrix completion  with efﬁcient algo-
rithms to handle matrices with mixed real-valued features and discrete labels. We obtain promising
experimental results on a range of synthetic and real-world data.

2 Problem Formulation
Let x1 . . . xn ∈ Rd be feature vectors associated with n items. Let X = [x1 . . . xn] be the d × n
feature matrix whose columns are the items. Let there be t binary classiﬁcation tasks  y1 . . . yn ∈
{−1  1}t be the label vectors  and Y = [y1 . . . yn] be the t × n label matrix. Entries in X or Y can
be missing at random. Let ΩX be the index set of observed features in X  such that (i  j) ∈ ΩX if
and only if xij is observed. Similarly  let ΩY be the index set of observed labels in Y. Our main goal
is to predict the missing labels yij for (i  j) /∈ ΩY. Of course  this reduces to standard transductive
learning when t = 1  |ΩX| = nd (no missing features)  and 1 < |ΩY| < n (some missing labels).
In our more general setting  as a side product we are also interested in imputing the missing features 
and de-noising the observed features  in X.

1

2.1 Model Assumptions

The above problem is in general ill-posed. We now describe our assumptions to make it a well-
deﬁned problem. In a nutshell  we assume that X and Y are jointly produced by an underlying
low rank matrix. We then take advantage of the sparsity to ﬁll in the missing labels and features
using a modiﬁed method of matrix completion. Speciﬁcally  we assume the following generative
story. It starts from a d × n low rank “pre”-feature matrix X0  with rank(X0) (cid:28) min(d  n). The
actual feature matrix X is obtained by adding iid Gaussian noise to the entries of X0: X = X0 +  
where ij ∼ N(0  σ2
j ∈ Rt of item j are
j + b  where W is a t × d weight matrix  and b ∈ Rt is a bias vector. Let
produced by y0
1 . . . y0
n

 ). Meanwhile  the t “soft” labels(cid:0)y0
(cid:3) be the soft label matrix. Note the combined (t + d) × n matrix(cid:2)Y0; X0(cid:3) is low
Y0 =(cid:2)y0
rank too: rank((cid:2)Y0; X0(cid:3)) ≤ rank(X0) + 1. The actual label yij ∈ {−1  1} is generated randomly
ij) = 1/(cid:0)1 + exp(−yijy0
ij)(cid:1). Finally  two random masks ΩX  ΩY

via a sigmoid function: P (yij|y0
are applied to expose only some of the entries in X and Y  and we use ω to denote the percentage of
observed entries. This generative story may seem restrictive  but our approaches based on it perform
well on synthetic and real datasets  outperforming several baselines with linear classiﬁers.

(cid:1)> ≡ y0

j = Wx0

1j . . . y0
tj

2.2 Matrix Completion for Heterogeneous Matrix Entries

With the above data generation model  our task can be deﬁned as follows. Given the partially
observed features and labels as speciﬁed by X  Y  ΩX  ΩY  we would like to recover the interme-

diate low rank matrix(cid:2)Y0; X0(cid:3). Then  X0 will contain the denoised and completed features  and
The key assumption is that the (t + d) × n stacked matrix(cid:2)Y0; X0(cid:3) is of low rank. We will start

sign(Y0) will contain the completed and correct labels.

from a “hard” formulation that is illustrative but impractical  then relax it.

argmin
Z∈R(t+d)×n
s.t.

rank(Z)
sign(zij) = yij  ∀(i  j) ∈ ΩY;

Here  Z is meant to recover(cid:2)Y0; X0(cid:3) by directly minimizing the rank while obeying the observed

features and labels. Note the indices (i  j) ∈ ΩX are with respect to X  such that i ∈ {1  . . .   d}. To
index the corresponding element in the larger stacked matrix Z  we need to shift the row index by t
to skip the part for Y0  and hence the constraints z(i+t)j = xij. The above formulation assumes that
there is no noise in the generation processes X0 → X and Y0 → Y. Of course  there are several
issues with formulation (1)  and we handle them as follows:

z(i+t)j = xij  ∀(i  j) ∈ ΩX

(1)

k=1

Pmin(t+d n)

• rank() is a non-convex function and difﬁcult to optimize. Following recent work in
matrix completion [3  2]  we relax rank() with the convex nuclear norm kZk∗ =
σk(Z)  where σk’s are the singular values of Z. The relationship between
rank(Z) and kZk∗ is analogous to that of ‘0-norm and ‘1-norm for vectors.
• There is feature noise from X0 to X. Instead of the equality constraints in (1)  we minimize
2(u − v)2 in this
• Similarly  there is label noise from Y0 to Y. The observed labels are of a different type
than the observed features. We therefore introduce another loss function cy(zij  yij) to
In this work  we use the logistic loss cy(u  v) =
account for the heterogeneous data.
log(1 + exp(−uv)).

a loss function cx(z(i+t)j  xij). We choose the squared loss cx(u  v) = 1
work  but other convex loss functions are possible too.

In addition to these changes  we will model the bias b either explicitly or implicitly  leading to two
alternative matrix completion formulations below.
Formulation 1 (MC-b). In this formulation  we explicitly optimize the bias b ∈ Rt in addition to

Z ∈ R(t+d)×n  hence the name. Here  Z corresponds to the stacked matrix(cid:2)WX0; X0(cid:3) instead of
(cid:2)Y0; X0(cid:3)  making it potentially lower rank. The optimization problem is
X

X

cy(zij + bi  yij) +

cx(z(i+t)j  xij) 

(2)

argmin

Z b

µkZk∗ + λ
|ΩY|

(i j)∈ΩY

1
|ΩX|

(i j)∈ΩX

2

where µ  λ are positive trade-off weights. Notice the bias b is not regularized. This is a convex
problem  whose optimization procedure will be discussed in section 3. Once the optimal Z  b are
found  we recover the task-i label of item j by sign(zij + bi)  and feature k of item j by z(k+t)j.
Formulation 2 (MC-1). In this formulation  the bias is modeled implicitly within Z. Similar to how
bias is commonly handled in linear classiﬁers  we append an additional feature with constant value

one to each item. The corresponding pre-feature matrix is augmented into(cid:2)X0; 1>(cid:3)  where 1 is the
Y0 are linear combinations of rows in(cid:2)X0; 1>(cid:3)  i.e.  rank((cid:2)Y0; X0; 1>(cid:3)) = rank((cid:2)X0; 1>(cid:3)). We
then let Z correspond to the (t + d + 1) × n stacked matrix(cid:2)Y0; X0; 1>(cid:3)  by forcing its last row to

all-1 vector. Under the same label assumption y0

j + b  the rows of the soft label matrix

j = Wx0

be 1> (hence the name):

argmin

Z∈R(t+d+1)×n

s.t.

µkZk∗ + λ
|ΩY|
z(t+d+1)· = 1>.

X

(i j)∈ΩY

cy(zij  yij) +

1
|ΩX|

cx(z(i+t)j  xij)

(3)

X

(i j)∈ΩX

This is a constrained convex optimization problem. Once the optimal Z is found  we recover the
task-i label of item j by sign(zij)  and feature k of item j by z(k+t)j.
MC-b and MC-1 differ mainly in what is in Z  which leads to different behaviors of the nuclear norm.
Despite the generative story  we do not explicitly recover the weight matrix W in these formulations.

Other formulations are certainly possible. One way is to let Z correspond to(cid:2)Y0; X0(cid:3) directly 

without introducing bias b or the all-1 row  and hope nuclear norm minimization will prevail. This
is inferior in our preliminary experiments  and we do not explore it further in this paper.

3 Optimization Techniques

We solve MC-b and MC-1 using modiﬁcations of the Fixed Point Continuation (FPC) method of Ma 
Goldfarb  and Chen [10].1 While nuclear norm minimization can be converted into a semideﬁnite
programming (SDP) problem [2]  current SDP solvers are severely limited in the size of problems
they can solve. Instead  the basic ﬁxed point approach is a computationally efﬁcient alternative 
which provably converges to the globally optimal solution and has been shown to outperform SDP
solvers in terms of matrix recoverability.

3.1 Fixed Point Continuation for MC-b

We ﬁrst describe our modiﬁed FPC method for MC-b. It differs from [10] in the extra bias variables
and multiple loss functions. Our ﬁxed point iterative algorithm to solve the unconstrained problem
of (2) consists of two alternating steps for each iteration k:

1. (gradient step) bk+1 = bk − τbg(bk)  Ak = Zk − τZg(Zk)
2. (shrinkage step) Zk+1 = SτZµ(Ak).

In the gradient step  τb and τZ are step sizes whose choice will be discussed next. Overloading
notation a bit  g(bk) is the vector gradient  and g(Zk) is the matrix gradient  respectively  of the two
loss terms in (2) (i.e.  excluding the nuclear norm term):
−yij

X

g(bi) =

g(zij) =

λ
|ΩY|

 λ|ΩY|

j:(i j)∈ΩY

1 + exp(yij(zij + bi))
−yij

1+exp(yij (zij +bi))  

1|ΩX|(zij − x(i−t)j) 
0 

i ≤ t and (i  j) ∈ ΩY
i > t and (i − t  j) ∈ ΩX
otherwise

(4)

(5)

Note for g(zij)  i > t  we need to shift down (un-stack) the row index by t in order to map the
element in Z back to the item x(i−t)j.

1While the primary method of [10] is Fixed Point Continuation with Approximate Singular Value Decom-
position (FPCA)  where the approximate SVD is used to speed up the algorithm  we opt to use an exact SVD
for simplicity and will refer to the method simply as FPC.

3

Input: Initial matrix Z0  bias b0 
parameters µ  λ  Step sizes τb  τZ
Determine µ1 > µ2 > ··· > µL = µ > 0.
Set Z = Z0  b = b0.
foreach µ = µ1  µ2  . . .   µL do

while Not converged do

Compute b = b − τbg(b)  A = Z − τZg(Z)
Compute SVD of A = UΛV>
Compute Z = U max(Λ − τZµ  0)V>

end

end
Output: Recovered matrix Z  bias b

Algorithm 1: FPC algorithm for MC-b.

parameters µ  λ  Step sizes τZ

Input: Initial matrix Z0 
Determine µ1 > µ2 > ··· > µL = µ > 0.
Set Z = Z0.
foreach µ = µ1  µ2  . . .   µL do

while Not converged do

Compute A = Z − τZg(Z)
Compute SVD of A = UΛV>
Compute Z = U max(Λ − τZµ  0)V>
Project Z to feasible region z(t+d+1)· = 1>

end

end
Output: Recovered matrix Z

Algorithm 2: FPC algorithm for MC-1.

In the shrinkage step  SτZµ(·) is a matrix shrinkage operator. Let Ak = UΛV> be the SVD of
Ak. Then SτZµ(Ak) = U max(Λ − τZµ  0)V>  where max is elementwise. That is  the shrinkage
operator shifts the singular values down  and truncates any negative values to zero. This step reduces
the nuclear norm.

Even though the problem is convex  convergence can be slow. We follow [10] and use a con-
tinuation or homotopy method to improve the speed. This involves beginning with a large value
µ1 > µ and solving a sequence of subproblems  each with a decreasing value and using the pre-
vious solution as its initial point. The sequence of values is determined by a decay parameter ηµ:
k = 1  . . .   L − 1  where µ is the ﬁnal value to use  and L is the number
µk+1 = max{µkηµ  µ} 
of rounds of continuation. The complete FPC algorithm for MC-b is listed in Algorithm 1.

A minor modiﬁcation of the argument in [10] reveals that as long as we choose non-negative step
sizes satisfying τb < 4|ΩY|/(λn) and τZ < min{4|ΩY|/λ |ΩX|}  the algorithms MC-b will be
guaranteed to converge to a global optimum. Indeed  to guarantee convergence  we only need that
the gradient step is non-expansive in the sense that
kb1−τbg(b1)−b2 +τbg(b2)k2 +kZ1−τZg(Z1)−Z2 +τZg(Z2)k2
F ≤ kb1−b2k2 +kZ1−Z2k2
F
for all b1  b2  Z1  and Z2. Our choice of τb and τZ guarantee such non-expansiveness. Once this
non-expansiveness is satisﬁed  the remainder of the convergence analysis is the same as in [10].

3.2 Fixed Point Continuation for MC-1

Our modiﬁed FPC method for MC-1 is similar except for two differences. First  there is no bias
variable b. Second  the shrinkage step will in general not satisfy the all-1-row constraints in (3).
Thus  we add a third projection step at the end of each iteration to project Zk+1 back to the feasible
region  by simply setting its last row to all 1’s. The complete algorithm for MC-1 is given in Algo-
rithm 2. We were unable to prove convergence for this gradient + shrinkage + projection algorithm.
Nonetheless  in our empirical experiments  Algorithm 2 always converges and tends to outperform
MC-b. The two algorithms have about the same convergence speed.

4 Experiments

We now empirically study the ability of matrix completion to perform multi-class transductive clas-
siﬁcation when there is missing data. We ﬁrst present a family of 24 experiments on a synthetic
task by systematically varying different aspects of the task  including the rank of the problem  noise
level  number of items  and observed label and feature percentage. We then present experiments on
two real-world datasets: music emotions and yeast microarray. In each experiments  we compare
MC-b and MC-1 against four other baseline algorithms. Our results show that MC-1 consistently
outperforms other methods  and MC-b follows closely.
Parameter Tuning and Other Settings for MC-b and MC-1: To tune the parameters µ and λ 
we use 5-fold cross validation (CV) separately for each experiment. Speciﬁcally  we randomly

4

λ

 |ΩX|)  τb = 3.8|ΩY|

5 of the observed entries  measure its performance on the remaining 1

divide ΩX and ΩY into ﬁve disjoint subsets each. We then run our matrix completion algorithms
using 4
5   and average over
the ﬁve folds. Since our main goal is to predict unobserved labels  we use label error as the CV
performance criterion to select parameters. Note that tuning µ is quite efﬁcient since all values
under consideration can be evaluated in one run of the continuation method. We set ηµ = 0.25 and 
as in [10]  consider µ values starting at σ1ηµ  where σ1 is the largest singular value of the matrix
of observed entries in [Y; X] (with the unobserved entries set to 0)  and decrease µ until 10−5.
The range of λ values considered was {10−3  10−2  10−1  1}. We initialized b0 to be all zero and
Z0 to be the rank-1 approximation of the matrix of observed entries in [Y; X] (with unobserved
entries set to 0) obtained by performing an SVD and reconstructing the matrix using only the largest
singular value and corresponding left and right singular vectors. The step sizes were set as follows:
τZ = min( 3.8|ΩY|
λn . Convergence was deﬁned as relative change in objective
functions (2)(3) smaller than 10−5.
Baselines: We compare to the following baselines  each consisting of some missing feature impu-
tation step on X ﬁrst  then using a standard SVM to predict the labels: [FPC+SVM] Matrix com-
pletion on X alone using FPC [10]. [EM(k)+SVM] Expectation Maximization algorithm to impute
missing X entries using a mixture of k Gaussian components. As in [9]  missing features  mixing
component parameters  and the assignments of items to components are treated as hidden variables 
which are estimated in an iterative manner to maximize the likelihood of the data. [Mean+SVM]
Impute each missing feature by the mean of the observed entries for that feature. [Zero+SVM]
Impute missing features by ﬁlling in zeros.
After imputation  an SVM is trained using the available (noisy) labels in ΩY for that task  and
predictions are made for the rest of the labels. All SVMs are linear  trained using SVMlin2  and the
regularization parameter is tuned using 5-fold cross validation separately for each task. The range
of parameter values considered was {10−8  10−7  . . .   107  108}.
Evaluation Method: To evaluate performance  we consider two measures: transductive label error 
i.e.  the percentage of unobserved labels predicted incorrectly; and relative feature imputation error
ij  where ˆx is the predicted feature value. In the tables below 
x2
for each parameter setting  we report the mean performance (and standard deviation in parenthesis)
of different algorithms over 10 random trials. The best algorithm within each parameter setting 
as well as any statistically indistinguishable algorithms via a two-tailed paired t-test at signiﬁcance
level α = 0.05  are marked in bold.

(xij − ˆxij)2(cid:17)

/P

(cid:16)P

ij /∈ΩX

ij /∈ΩX

4.1 Synthetic Data Experiments

Synthetic Data Generation: We generate a family of synthetic datasets to systematically explore
the performance of the algorithms. We ﬁrst create a rank-r matrix X0 = LR>  where L ∈ Rd×r
and R ∈ Rn×r with entries drawn iid from N (0  1). We then normalize X0 such that its entries
have variance 1. Next  we create a weight matrix W ∈ Rt×d and bias vector b ∈ Rt  with all entries
drawn iid from N (0  10). We then produce X  Y0  Y according to section 2.1. Finally  we produce
the random ΩX  ΩY masks with ω percent observed entries.
Using the above procedure  we vary ω = 10%  20%  40%  n = 100  400  r = 2  4  and σ2
 =
0.01  0.1  while ﬁxing t = 10  d = 20  to produce 24 different parameter settings. For each setting 
we generate 10 trials  where the randomness is in the data and mask.
Synthetic experiment results: Table 1 shows the transductive label errors  and Table 2 shows the
relative feature imputation errors  on the synthetic datasets. We make several observations.

Observation 1: MC-b and MC-1 are the best for feature imputation  as Table 2 shows. However 
the imputations are not perfect  because in these particular parameter settings the ratio between the
number of observed entries over the degrees of freedom needed to describe the feature matrix (i.e. 
r(d + n − r)) is below the necessary condition for perfect matrix completion [2]  and because there
is some feature noise. Furthermore  our CV tuning procedure selects parameters µ  λ to optimize
label error  which often leads to suboptimal imputation performance. In a separate experiment (not
reported here) when we made the ratio sufﬁciently large and without noise  and speciﬁcally tuned for

2http://vikas.sindhwani.org/svmlin.html

5

Table 1: Transductive label error of six algorithms on the 24 synthetic datasets. The varying pa-
   rank(X0) = r  number of items n  and observed label and feature
rameters are feature noise σ2
percentage ω. Each row is for a unique parameter combination. Each cell shows the mean(standard
deviation) of transductive label error (in percentage) over 10 random trials. The “meta-average” row
is the simple average over all parameter settings and all trials.

σ2

0.01

r
2

n
100

400

4

100

400

0.1

2

100

400

4

100

400

ω

MC-b
10% 37.8(4.0)
20% 23.5(2.9)
40% 15.1(3.1)
10% 26.5(2.0)
20% 15.9(2.5)
40% 11.7(2.0)
10% 42.5(4.0)
20% 33.2(2.3)
40% 19.6(3.1)
10% 35.3(3.1)
20% 24.4(2.3)
40% 14.6(1.8)
10% 39.6(5.5)
20% 25.2(2.6)
40% 15.7(3.1)
10% 27.6(2.1)
20% 18.0(2.2)
40% 12.0(2.1)
10% 42.5(4.3)
20% 33.3(1.9)
40% 21.4(2.7)
10% 36.3(2.7)
20% 25.5(2.0)
40% 16.0(1.8)

MC-1
31.8(4.3)
17.0(2.2)
10.8(1.8)
19.9(1.7)
11.7(1.9)
8.0(1.6)
40.8(4.4)
26.2(2.8)
14.3(2.7)
32.1(1.6)
19.1(1.3)
9.5(0.5)
34.6(3.5)
20.1(1.7)
12.6(1.4)
22.6(1.9)
15.2(1.7)
10.1(1.3)
41.5(2.5)
29.0(2.2)
18.4(3.1)
34.0(1.7)
21.8(1.0)
12.8(0.8)
21.4

FPC+SVM EM1+SVM Mean+SVM
34.8(7.0)
40.5(5.7)
17.6(2.1)
28.7(4.1)
9.6(1.5)
16.5(2.5)
32.4(2.9)
23.7(1.7)
20.0(1.9)
12.6(2.2)
7.2(1.8)
12.2(1.8)
41.5(2.6)
43.5(2.9)
26.7(1.7)
35.5(1.4)
13.6(2.6)
22.5(2.0)
33.4(1.6)
37.7(1.2)
26.9(1.5)
20.5(1.4)
16.4(1.2)
9.2(0.9)
37.3(6.4)
41.5(6.0)
21.6(2.6)
31.8(4.7)
13.2(2.0)
18.5(2.7)
34.5(3.3)
27.6(2.4)
22.6(2.4)
16.8(2.3)
10.4(2.1)
14.1(2.0)
42.3(2.0)
44.6(2.9)
30.9(3.1)
36.2(2.3)
18.7(2.4)
23.9(2.0)
35.1(1.2)
38.7(1.3)
23.8(1.5)
28.4(1.7)
18.3(1.2)
13.9(1.2)
22.6
28.6

34.6(3.9)
19.7(2.4)
10.4(1.0)
24.2(1.9)
12.0(1.9)
7.3(1.4)
43.2(2.2)
30.8(2.7)
14.1(2.4)
34.2(1.8)
19.8(1.1)
8.6(1.1)
40.2(5.3)
26.8(3.7)
15.1(2.4)
28.8(2.6)
18.4(2.5)
11.1(1.9)
45.6(1.9)
34.9(3.0)
21.6(2.4)
36.3(1.4)
25.1(1.4)
14.7(1.3)
24.1

Zero+SVM
40.5(5.1)
27.4(4.4)
15.4(2.3)
31.5(2.7)
19.7(1.7)
12.1(2.0)
42.9(2.9)
33.9(1.5)
21.7(2.3)
38.2(1.4)
26.9(1.3)
16.5(1.3)
41.0(5.7)
29.9(4.0)
17.2(2.4)
33.6(2.8)
21.8(2.5)
14.0(2.4)
43.6(2.3)
35.4(1.6)
23.3(2.5)
39.1(1.2)
28.4(1.8)
18.2(1.2)
28.0

meta-average

25.6

imputation error  both MC-b and MC-1 did achieve perfect feature imputation. Also  FPC+SVM is
slightly worse in feature imputation. This may seem curious as FPC focuses exclusively on imputing
X. We believe the fact that MC-b and MC-1 can use information in Y to enhance feature imputation
in X made them better than FPC+SVM.
Observation 2: MC-1 is the best for multi-label transductive classiﬁcation  as suggested by Table 1.
Surprisingly  the feature imputation advantage of MC-b did not translate into classiﬁcation  and
FPC+SVM took second place.

Observation 3: The same factors that affect standard matrix completion also affect classiﬁcation
performance of MC-b and MC-1. As the tables show  everything else being equal  less feature noise
(smaller σ2
 )  lower rank r  more items  or more observed features and labels  reduce label error.
Beneﬁcial combination of these factors (the 6th row) produces the lowest label errors.
Matrix completion beneﬁts from more tasks. We performed one additional synthetic data exper-
iment examining the effect of t (the number of tasks) on MC-b and MC-1  with the remaining data
parameters ﬁxed at ω = 10%  n = 400  r = 2  d = 20  and σ2
 = 0.01. Table 3 reveals that both
MC methods achieve statistically signiﬁcantly better label prediction and imputation performance
with t = 10 than with only t = 2 (as determined by two-sample t-tests at signiﬁcance level 0.05).

4.2 Music Emotions Data Experiments

In this task introduced by Trohidis et al. [14]  the goal is to predict which of several types of emotion
are present in a piece of music. The data3 consists of n = 593 songs of a variety of musical genres 
each labeled with one or more of t = 6 emotions (i.e.  amazed-surprised  happy-pleased  relaxing-
calm  quiet-still  sad-lonely  and angry-fearful). Each song is represented by d = 72 features (8
rhythmic  64 timbre-based) automatically extracted from a 30-second sound clip.

3Available at http://mulan.sourceforge.net/datasets.html

6

Table 2: Relative feature imputation error on the synthetic datasets. The algorithm Zero+SVM is
not shown because it by deﬁnition has relative feature imputation error 1.

σ2

0.01

r
2

n
100

400

4

100

400

0.1

2

100

400

4

100

400

ω

MC-b
10% 0.84(0.04)
20% 0.54(0.08)
40% 0.29(0.06)
10% 0.73(0.03)
20% 0.43(0.04)
40% 0.30(0.10)
10% 0.99(0.04)
20% 0.77(0.05)
40% 0.42(0.07)
10% 0.87(0.04)
20% 0.69(0.07)
40% 0.34(0.05)
10% 0.92(0.05)
20% 0.69(0.07)
40% 0.51(0.05)
10% 0.79(0.03)
20% 0.64(0.06)
40% 0.48(0.04)
10% 1.01(0.04)
20% 0.84(0.03)
40% 0.59(0.03)
10% 0.90(0.02)
20% 0.75(0.04)
40% 0.56(0.03)

MC-1
0.87(0.06)
0.57(0.06)
0.27(0.06)
0.72(0.04)
0.46(0.05)
0.22(0.04)
0.96(0.03)
0.78(0.05)
0.40(0.03)
0.88(0.03)
0.67(0.04)
0.34(0.03)
0.93(0.04)
0.72(0.06)
0.52(0.05)
0.80(0.03)
0.64(0.06)
0.45(0.05)
0.97(0.03)
0.85(0.03)
0.61(0.04)
0.92(0.02)
0.77(0.02)
0.55(0.04)
0.66

FPC+SVM
0.88(0.06)
0.57(0.07)
0.27(0.06)
0.76(0.03)
0.50(0.04)
0.24(0.05)
0.96(0.03)
0.77(0.04)
0.42(0.04)
0.89(0.01)
0.69(0.03)
0.38(0.03)
0.93(0.05)
0.74(0.06)
0.53(0.05)
0.84(0.03)
0.67(0.04)
0.49(0.05)
0.97(0.03)
0.85(0.03)
0.63(0.04)
0.92(0.01)
0.79(0.03)
0.59(0.04)
0.68

EM1+SVM
1.01(0.12)
0.67(0.13)
0.34(0.03)
0.79(0.07)
0.45(0.04)
0.21(0.04)
1.22(0.11)
0.92(0.07)
0.49(0.04)
1.00(0.08)
0.66(0.03)
0.29(0.02)
1.18(0.10)
0.94(0.07)
0.67(0.08)
0.96(0.07)
0.73(0.07)
0.57(0.07)
1.25(0.05)
1.07(0.06)
0.80(0.09)
1.08(0.07)
0.86(0.05)
0.66(0.06)
0.78

Mean+SVM
1.06(0.02)
1.03(0.02)
1.01(0.01)
1.02(0.01)
1.01(0.00)
1.00(0.00)
1.05(0.01)
1.02(0.01)
1.01(0.01)
1.01(0.00)
1.01(0.00)
1.00(0.00)
1.06(0.02)
1.03(0.02)
1.02(0.01)
1.02(0.01)
1.01(0.00)
1.00(0.00)
1.05(0.02)
1.02(0.01)
1.01(0.01)
1.01(0.01)
1.01(0.00)
1.00(0.00)
1.02

meta-average

0.66

Table 3: More tasks help matrix completion (ω = 10%  n = 400  r = 2  d = 20  σ2
FPC+SVM
0.76(0.03)
0.76(0.03)

MC-1
0.78(0.04)
0.72(0.04)

MC-b
0.78(0.07)
0.73(0.03)

MC-1
22.9(2.2)
19.9(1.7)

FPC+SVM
20.5(2.5)
23.7(1.7)

t
2
10

MC-b
30.1(2.8)
26.5(2.0)

 = 0.01).

transductive label error

relative feature imputation error

Table 4: Performance on the music emotions data.

ω =40%
28.0(1.2)
27.4(0.8)
26.9(0.7)
26.0(1.1)
26.2(0.9)
26.3(0.8)
30.3(0.6)

60%
25.2(1.0)
23.7(1.6)
25.2(1.6)
23.6(1.1)
23.1(1.2)
24.2(1.0)
28.9(1.1)

80%
22.2(1.6)
19.8(2.4)
24.4(2.0)
21.2(2.3)
21.6(1.6)
22.6(1.3)
25.7(1.4)

Algorithm

MC-b
MC-1

FPC+SVM
EM1+SVM
EM4+SVM
Mean+SVM
Zero+SVM

ω =40%
0.69(0.05)
0.60(0.05)
0.64(0.01)
0.46(0.09)
0.49(0.10)
0.18(0.00)
1.00(0.00)

60%
0.54(0.10)
0.46(0.12)
0.46(0.02)
0.23(0.04)
0.27(0.04)
0.19(0.00)
1.00(0.00)

80%
0.41(0.02)
0.25(0.03)
0.31(0.03)
0.13(0.01)
0.15(0.02)
0.20(0.01)
1.00(0.00)

transductive label error

relative feature imputation error

We vary the percentage of observed entries ω = 40%  60%  80%. For each ω  we run 10 random
trials with different masks ΩX  ΩY. For this dataset  we tuned only µ with CV  and set λ = 1.
The results are in Table 4. Most importantly  these results show that MC-1 is useful for this real-
world multi-label classiﬁcation problem  leading to the best (or statistically indistinguishable from
the best) transductive error performance with 60% and 80% of the data available  and close to the
best with only 40%.

We also compared these algorithms against an “oracle baseline” (not shown in the table). In this
baseline  we give 100% features (i.e.  no indices are missing from ΩX) and the training labels
in ΩY to a standard SVM  and let it predict the unspeciﬁed labels. On the same random tri-
als  for observed percentage ω = 40%  60%  80%  the oracle baseline achieved label error rate
22.1(0.8)  21.3(0.8)  20.5(1.8) respectively. Interestingly  MC-1 with ω = 80% (19.8) is statisti-
cally indistinguishable from the oracle baseline.

7

4.3 Yeast Microarray Data Experiments

This dataset comes from a biological domain and involves the problem of Yeast gene functional
classiﬁcation. We use the data studied by Elisseeff and Weston [5]  which contains n = 2417
examples (Yeast genes) with d = 103 input features (results from microarray experiments).4 We
follow the approach of [5] and predict each gene’s membership in t = 14 functional classes. For
this larger dataset  we omitted the computationally expensive EM4+SVM methods  and tuned only
µ for matrix completion while ﬁxing λ = 1.
Table 5 reveals that MC-b leads to statistically signiﬁcantly lower transductive label error for this bi-
ological dataset. Although not highlighted in the table  MC-1 is also statistically better than the SVM
methods in label error. In terms of feature imputation performance  the MC methods are weaker than
FPC+SVM. However  it seems simultaneously predicting the missing labels and features appears to
provide a large advantage to the MC methods. It should be pointed out that all algorithms except
Zero+SVM in fact have small but non-zero standard deviation on imputation error  despite what the
ﬁxed-point formatting in the table suggests. For instance  with ω = 40%  the standard deviation is
0.0009 for MC-1  0.0011 for FPC+SVM  and 0.0001 for Mean+SVM.
Again  we compared these algorithms to an oracle SVM baseline with 100% observed entries in ΩX.
The oracle SVM approach achieves label error of 20.9(0.1)  20.4(0.2)  and 20.1(0.3) for ω =40% 
60%  and 80% observed labels  respectively. Both MC-b and MC-1 signiﬁcantly outperform this
oracle under paired t-tests at signiﬁcance level 0.05. We attribute this advantage to a combination
of multi-label learning and transduction that is intrinsic to our matrix completion methods.

ω =40%
16.1(0.3)
16.7(0.3)
21.5(0.3)
22.0(0.2)
21.7(0.2)
21.6(0.2)

60%
12.2(0.3)
13.0(0.2)
20.8(0.3)
21.2(0.2)
21.1(0.2)
21.1(0.2)

80%
8.7(0.4)
8.5(0.4)
20.3(0.3)
20.4(0.2)
20.5(0.4)
20.5(0.4)

Algorithm

MC-b
MC-1

FPC+SVM
EM1+SVM
Mean+SVM
Zero+SVM

Table 5: Performance on the yeast data.

ω =40%
0.83(0.02)
0.86(0.00)
0.81(0.00)
1.15(0.02)
1.00(0.00)
1.00(0.00)

60%
0.76(0.00)
0.92(0.00)
0.76(0.00)
1.04(0.02)
1.00(0.00)
1.00(0.00)

80%
0.73(0.02)
0.74(0.00)
0.72(0.00)
0.77(0.01)
1.00(0.00)
1.00(0.00)

transductive label error

relative feature imputation error

5 Discussions and Future Work

We have introduced two matrix completion methods for multi-label transductive learning with miss-
ing features  which outperformed several baselines. In terms of problem formulation  our methods
differ considerably from sparse multi-task learning [11  1  13] in that we regularize the feature and
label matrix directly  without ever learning explicit weight vectors. Our methods also differ from
multi-label prediction via reduction to binary classiﬁcation or ranking [15]  and via compressed
sensing [7]  which assumes sparsity in that each item has a small number of positive labels  rather
than the low-rank nature of feature matrices. These methods do not naturally allow for missing fea-
tures. Yet other multi-label methods identify a subspace of highly predictive features across tasks in
a ﬁrst stage  and learn in this subspace in a second stage [8  12]. Our methods do not require separate
stages. Learning in the presence of missing data typically involves imputation followed by learning
with completed data [9]. Our methods perform imputation plus learning in one step  similar to EM
on missing labels and features [6]  but the underlying model assumption is quite different.

A drawback of our methods is their restriction to linear classiﬁers only. One future extension is to
explicitly map the partial feature matrix to a partially observed polynomial (or other) kernel Gram
matrix  and apply our methods there. Though such mapping proliferates the missing entries  we
hope that the low-rank structure in the kernel matrix will allow us to recover labels that are nonlinear
functions of the original features.

Acknowledgements: This work is supported in part by NSF IIS-0916038  NSF IIS-0953219  AFOSR FA9550-
09-1-0313  and AFOSR A9550-09-1-0423. We also wish to thank Brian Eriksson for useful discussions and
source code implementing EM-based imputation.

4Available at http://mulan.sourceforge.net/datasets.html

8

References

[1] Andreas Argyriou  Charles A. Micchelli  and Massimiliano Pontil. On spectral learning. Jour-

nal of Machine Learning Research  11:935–953  2010.

[2] Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational Mathematics  9:717–772  2009.

[3] Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix

completion. IEEE Transactions on Information Theory  56:2053–2080  2010.

[4] Olivier Chapelle  Alexander Zien  and Bernhard Sch¨olkopf  editors. Semi-supervised learning.

MIT Press  2006.

[5] Andr´e Elisseeff and Jason Weston. A kernel method for multi-labelled classiﬁcation.

In
Thomas G. Dietterich  Suzanna Becker  and Zoubin Ghahramani  editors  NIPS  pages 681–
687. MIT Press  2001.

[6] Zoubin Ghahramani and Michael I. Jordan. Supervised learning from incomplete data via
an EM approach. In Advances in Neural Information Processing Systems 6  pages 120–127.
Morgan Kaufmann  1994.

[7] Daniel Hsu  Sham Kakade  John Langford  and Tong Zhang. Multi-label prediction via com-
pressed sensing. In Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams  and A. Culotta 
editors  Advances in Neural Information Processing Systems 22  pages 772–780. 2009.

[8] Shuiwang Ji  Lei Tang  Shipeng Yu  and Jieping Ye. Extracting shared subspace for multi-label
classiﬁcation. In KDD ’08: Proceeding of the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining  pages 381–389  New York  NY  USA  2008. ACM.

[9] Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data. Wiley-

Interscience  2nd edition  September 2002.

[10] Shiqian Ma  Donald Goldfarb  and Lifeng Chen. Fixed point and Bregman iterative methods
for matrix rank minimization. Mathematical Programming Series A  to appear (published
online September 23  2009).

[11] Guillaume Obozinski  Ben Taskar  and Michael I. Jordan. Joint covariate selection and joint
subspace selection for multiple classiﬁcation problems. Statistics and Computing  20(2):231–
252  2010.

[12] Piyush Rai and Hal Daume. Multi-label prediction via sparse inﬁnite CCA.

In Y. Bengio 
D. Schuurmans  J. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in Neural
Information Processing Systems 22  pages 1518–1526. 2009.

[13] Nathan Srebro and Adi Shraibman. Rank  trace-norm and max-norm. In Proceedings of the

18th Annual Conference on Learning Theory  pages 545–560. Springer-Verlag  2005.

[14] K. Trohidis  G. Tsoumakas  G. Kalliris  and I. Vlahavas. Multilabel classiﬁcation of music
into emotions. In Proc. 9th International Conference on Music Information Retrieval (ISMIR
2008)  Philadelphia  PA  USA  2008  2008.

[15] G. Tsoumakas  I. Katakis  and I. Vlahavas. Mining multi-label data.

Knowledge Discovery Handbook. Springer  2nd edition  2010.

In Data Mining and

[16] Xiaojin Zhu and Andrew B. Goldberg. Introduction to Semi-Supervised Learning. Morgan &

Claypool  2009.

9

,Hakan Bilen
Andrea Vedaldi