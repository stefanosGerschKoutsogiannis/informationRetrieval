2019,Unsupervised State Representation Learning in Atari,State representation learning  or the ability to capture latent generative factors of an environment is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations in an unsupervised manner without supervision from rewards is an open problem. We introduce a method that tries to learn better state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally  we compare our technique with other state-of-the-art generative and contrastive representation learning methods.,Unsupervised State Representation Learning in Atari

Ankesh Anand⇤

Mila  Université de Montréal

Microsoft Research

Evan Racah⇤

Mila  Université de Montréal

Sherjil Ozair⇤

Mila  Université de Montréal

Yoshua Bengio

Mila  Université de Montréal

Marc-Alexandre Côté

Microsoft Research

R Devon Hjelm
Microsoft Research

Mila  Université de Montréal

Abstract

State representation learning  or the ability to capture latent generative factors
of an environment  is crucial for building intelligent agents that can perform a
wide variety of tasks. Learning such representations without supervision from
rewards is a challenging open problem. We introduce a method that learns
state representations by maximizing mutual information across spatially and tem-
porally distinct features of a neural encoder of the observations. We also in-
troduce a new benchmark based on Atari 2600 games where we evaluate rep-
resentations based on how well they capture the ground truth state variables.
We believe this new framework for evaluating representation learning models
will be crucial for future representation learning research. Finally  we com-
pare our technique with other state-of-the-art generative and contrastive repre-
sentation learning methods. The code associated with this work is available at
https://github.com/mila-iqia/atari-representation-learning

1

Introduction

The ability to perceive and represent visual sensory data into useful and concise descriptions is con-
sidered a fundamental cognitive capability in humans [1  2]  and thus crucial for building intelligent
agents [3]. Representations that concisely capture the true state of the environment should empower
agents to effectively transfer knowledge across different tasks in the environment  and enable learning
with fewer interactions [4].
Recently  deep representation learning has led to tremendous progress in a variety of machine learning
problems across numerous domains [5  6  7  8  9]. Typically  such representations are often learned
via end-to-end learning using the signal from labels or rewards  which makes such techniques often
very sample-inefﬁcient. Human perception in the natural world  however  appears to require almost
no explicit supervision [10].
Unsupervised [11  12  13] and self-supervised representation learning [14  15  16] have emerged as
an alternative to supervised versions which can yield useful representations with reduced sample
complexity. In the context of learning state representations [17]  current unsupervised methods rely
on generative decoding of the data using either VAEs [18  19  20  21] or prediction in pixel-space
[22  23]. Since these objectives are based on reconstruction error in the pixel space  they are not
incentivized to capture abstract latent factors and often default to capturing pixel level details.
In this work  we leverage recent advances in self-supervision that rely on scalable estimation of
mutual information [24  25  26  27]  and propose a new contrastive state representation learning

⇤Equal contribution. {anandank  racaheva  ozairs}@mila.quebec

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

method named Spatiotemporal DeepInfomax (ST-DIM)  which maximizes the mutual information
across both the spatial and temporal axes.

Score
Lives
Items in 
Inventory

Room number

Agent’s x  y
 location 

Skull’s x  y
 location 

Num dots 

eaten

Num Ghosts
Sue’s x  y
 location 
Inky’s x  y
 location 
Blinky’s x  y
 location 

Agent’s x  y
 location 

Lives
Score

Figure 1: We use a collection of 22 Atari 2600 games to evaluate state representations. We leveraged
the source code of the games to annotate the RAM states with important state variables such as the
location of various objects in the game. We compare various unsupervised representation learning
techniques based on how well the representations linearly-separate the state variables. Shown above
are examples of state variables annotated for Montezuma’s Revenge and MsPacman.

To systematically evaluate the ability of different representation learning methods at capturing the true
underlying factors of variation  we propose a benchmark based on Atari 2600 games using the Arcade
Learning Environment [ALE  28]. A simulated environment provides access to the underlying
generative factors of the data  which we extract using the source code of the games. These factors
include variables such as the location of the player character  location of various items of interest
(keys  doors  etc.)  and various non-player characters  such as enemies (see ﬁgure 2). Performance of
a representation learning technique in the Atari representation learning benchmark is then evaluated
using linear probing [29]  i.e. the accuracy of linear classiﬁers trained to predict the latent generative
factors from the learned representations.
Our contributions are the following

1. We propose a new self-supervised state representation learning technique which exploits the

spatial-temporal nature of visual observations in a reinforcement learning setting.

2. We propose a new state representation learning benchmark using 22 Atari 2600 games based

on the Arcade Learning Environment (ALE).

3. We conduct extensive evaluations of existing representation learning techniques on the

proposed benchmark and compare with our proposed method.

2 Spatiotemporal Deep Infomax

We assume a setting where an agent interacts with an environment and observes a set of high-
dimensional observations X = {x1  x2  . . .   xN} across several episodes. Our goal is to learn an
abstract representation of the observation that captures the underlying latent generative factors of the
environment.
This representations should focus on high-level semantics (e.g.  the concept of agents  enemies 
objects  score  etc.) and ignore the low-level details such as the precise texture of the background 
which warrants a departure from the class of methods that rely on a generative decoding of the full
observation. Prior work in neuroscience [30  31] has suggested that the brain maximizes predictive
information [32] at an abstract level to avoid sensory overload. Predictive information  or the mutual
information between consecutive states  has also been shown to be the organizing principle of
retinal ganglion cells in salamander brains [33]. Thus our representation learning approach relies
on maximizing an estimate based on a lower bound on the mutual information over consecutive
observations xt and xt+1.

2

2.1 Maximizing mutual information across space and time

Figure 2: A schematic overview of SpatioTemporal DeepInfoMax (ST-DIM). Left: The two different
mutual information objectives: local-local infomax and global-local infomax. Right: A simpliﬁed
version of the global-local contrastive task. In practice  we use multiple negative samples.

Given a mutual information estimator  we follow DIM [26] and maximize a sum of patch-level mutual
information objectives. The global-local objective in equation 2 maximize the mutual information
between the full observation at time t with small patches of the observation at time t + 1. The
representations of the small image patches are taken to be the hidden activations of the convolutional
encoder applied to the full observation. The layer is picked appropriately to ensure that the hidden
activations only have a limited receptive ﬁeld corresponding to 1/16th the size of the full observations.
The local-local objective in equation 3 maximizes the mutual information between the local feature
at time t with the corresponding local feature at time t + 1. Figure ?? is a visual depiction of our
model which we call Spatiotemporal Deep Infomax (ST-DIM).
It has been shown that mutual information bounds can be loose for large values of the mutual
information [34] and in practice fail to capture all the relevant features in the data [35] when used
to learn representations. To alleviate this issue  our approach constructs multiple small mutual
information objectives (rather than a single large one) which are easier to estimate via lower bounds 
which has been concurrently found to work well in the context of semi-supervised learning [36].
For the mutual information estimator  we use infoNCE [25]  a multi-sample variant of noise-
contrastive estimation [37] that was also shown to work well with DIM. Let {(xi  yi)}N
i=1 be a
paired dataset of N samples from some joint distribution p(x  y). For any index i  (xi  yi) is a sample
from the joint p(x  y) which we refer to as positive examples  and for any i 6= j  (xi  yj) is a sample
from the product of marginals p(x)p(y)  which we refer to as negative examples. The InfoNCE
objective learns a score function f (x  y) which assigns large values to positive examples and small
values to negative examples by maximizing the following bound [see 25  38  for more details on this
bound] 

IN CE({(xi  yi)}N

i=1) =

(1)

The above objective has also been referred to as multi-class n-pair loss [39  40] and ranking-based
NCE [41]  and is similar to MINE [24] and the JSD-variant of DIM [26].

exp f (xi  yi)
j=1 exp f (xi  yj)

PN

log

NXi=1

3

conv layersconv layersconv layersFeature mapsAnchorNegativePositiveMLPglobal featurepositive local feature (+)negative local feature (-)Discriminatorlocal-local infomaxglobal-local infomaxconv layersconv layersdense layersBilinearBilinearFollowing van den Oord et al. [25] we use a bilinear model for the score function f (x  y) =
(x)T W (y)  where  is the representation encoder. The bilinear model combined with the InfoNCE
objective forces the encoder to learn linearly predictable representations  which we believe helps in
learning representations at the semantic level.
Let X = {(xt  xt+1)i}B
i=1 be a minibatch of consecutive observations that are randomly sampled
from several collected episodes. Let Xnext = X[:  1] correspond to the set of next observations.
In our context  the positive samples correspond to pairs of consecutive observations (xt  xt+1) and
negative samples correspond to pairs of non-consecutive observations (xt  xt⇤)  where xt⇤ is a
randomly sampled observation from the same minibatch.
As mentioned above  in ST-DIM  we construct two losses: the global-local objective (GL) and the
local-local objective (LL). The global-local objective is as follows:

LGL =

MXm=1

NXn=1

 log

exp(gm n(xt  xt+1))

exp(gm n(xt  xt⇤))

Px⇤t 2Xnext

where the score function for the global-local objective  gm n(xt  xt+1) = (xt)T Wgm n(xt+1) and
m n is the local feature vector produced by an intermediate layer in  at the (m  n) spatial location.
The local-local objective is as follows:

(2)

(3)

LLL =

MXm=1

NXn=1

 log

exp(fm n(xt  xt+1))

exp(fm n(xt  xt⇤))

Px⇤t 2Xnext

where the score function of the local-local objective is fm n(xt  xt+1) = m n(xt)T Wlm n(xt+1)

3 The Atari Annotated RAM Interface (AtariARI)

Measuring the usefulness of a representation is still an open problem  as a core utility of represen-
tations is their use as feature extractors in tasks that are different from those used for training (e.g. 
transfer learning). Measuring classiﬁcation performance  for example  may only reveal the amount
of class-relevant information in a representation  but may not reveal other information useful for
segmentation. It would be useful  then  to have a more general set of measures on the usefulness of a
representation  such as ones that may indicate more general utility across numerous real-world tasks.
In this vein  we assert that in the context of dynamic  visual  interactive environments  the capability
of a representation to capture the underlying high-level factors of the state of an environment will be
generally useful for a variety of downstream tasks such as prediction  control  and tracking.
We ﬁnd video games to be a useful candidate for evaluating visual representation learning algorithms
primarily because they are spatiotemporal in nature  which is (1) more realistic compared to static
i.i.d. datasets and (2) prior work [42  43] have argued that without temporal structure  recovering
the true underlying latent factors is undecidable. Apart from this  video games also provide ready
access to the underlying ground truth states  unlike real-world datasets  which we need to evaluate
performance of different techniques.

Annotating Atari RAM: ALE does not explicitly expose any ground truth state information.
However  ALE does expose the RAM state (128 bytes per timestep) which are used by the game
programmer to store important state information such as the location of sprites  the state of the
clock  or the current room the agent is in. To extract these variables  we consulted commented
disassemblies [44] (or source code) of Atari 2600 games which were made available by Engelhardt
[45] and Jentzsch and CPUWIZ [46]. We were able to ﬁnd and verify important state variables for a
total of 22 games. Once this information is acquired  combining it with the ALE interface produces
a wrapper that can automatically output a state label for every example frame generated from the
game. We make this available with an easy-to-use gym wrapper  which returns this information with
no change to existing code using gym interfaces. Table 1 lists the 22 games along with the categories
of variables for each game. We describe the meaning of each category in the next section.

State variable categories: We categorize the state variables of all the games among six major
categories: agent localization  small object localization  other localization  score/clock/lives/display 

4

Table 1: Number of ground truth labels available in the benchmark for each game across each category.
Localization is shortened for local. See section 3 for descriptions and examples for each category.

GAME

ASTEROIDS
BERZERK
BOWLING
BOXING
BREAKOUT
DEMONATTACK
FREEWAY
FROSTBITE
HERO
MONTEZUMAREVENGE
MSPACMAN
PITFALL
PONG
PRIVATEEYE
QBERT
RIVERRAID
SEAQUEST
SPACEINVADERS
TENNIS
VENTURE
VIDEOPINBALL
YARSREVENGE

TOTAL

AGENT
LOCAL.

SMALL
OBJECT
LOCAL.

OTHER
LOCAL.

SCORE/CLOCK

LIVES

DISPLAY

MISC

OVERALL

2
2
2
2
1
1
1
2
2
2
2
2
1
2
3
1
2
1
2
2
2
2
39

4
4
2
0
2
1
0
0
0
0
0
0
2
0
0
2
1
1
2
0
2
4
27

30
19
0
2
0
6
10
9
0
4
10
3
1
2
2
0
8
2
2
12
0
2
124

3
4
2
3
1
1
1
4
3
4
2
0
2
4
0
2
4
2
2
3
2
0
49

3
5
10
0
31
1
0
2
3
5
3
0
0
2
0
0
3
1
0
1
0
0
70

41
34
16
7
35
10
12
17
8
15
17
5
6
10
5
5
18
7
8
18
6
8
308

and miscellaneous. Agent Loc. (agent localization) refers to state variables that represent the x
or y coordinates on the screen of any sprite controllable by actions. Small Loc. (small object
localization) variables refer to the x or y screen position of small objects  like balls or missiles.
Prominent examples include the ball in Breakout and Pong  and the torpedo in Seaquest. Other
Loc. (other localization) denotes the x or y location of any other sprites  including enemies or large
objects to pick up. For example  the location of ghosts in Ms. Pacman or the ice ﬂoes in Frostbite.
Score/Clock/Lives/Display refers to variables that track the score of the game  the clock  or the
number of remaining lives the agent has  or some other display variable  like the oxygen meter in
Seaquest. Misc. (Miscellaneous) consists of state variables that are largely speciﬁc to a game  and
don’t fall within one of the above mentioned categories. Examples include the existence of each
block or pin in Breakout and Bowling  the room number in Montezuma’s Revenge  or Ms. Pacman’s
facing direction.

Probing: Evaluating representation learning methods is a challenging open problem. The notion of
disentanglement [47  48] has emerged as a way to measure the usefulness of a representation [49  50].
In this work  we focus only on explicitness  i.e the degree to which underlying generative factors
can be recovered using a linear transformation from the learned representation. This is standard
methodology in the self-supervised representation learning literature [15  25  51  16  26]. Speciﬁcally 
to evaluate a representation we train linear classiﬁers predicting each state variable  and we report the
mean F1 score.

4 Related Work

Unsupervised representation learning via mutual information objectives: Recent work in unsu-
pervised representation learning have focused on extracting latent representations by maximizing a
lower bound on the mutual information between the representation and the input. Belghazi et al. [24]
estimate the mutual information with neural networks using the Donsker-Varadhan representation of
the KL divergence [52]  while Chen et al. [53] use the variational bound from Barber and Agakov
[54] to learn discrete latent representations. Hjelm et al. [26] learn representations by maximiz-
ing the Jensen-Shannon divergence between joint and product of marginals of an image and its

5

patches. van den Oord et al. [25] maximize mutual information using a multi-sample version of
noise contrastive estimation [37  41]. See [38] for a review of different variational bounds for mutual
information.

State representation learning: Learning better state representations is an active area of research
within robotics and reinforcement learning. Recently  Cuccu et al. [55] and Eslami et al. [4] show
that visual processing and policy learning can be effectively decoupled in pixel-based environments.
Jonschkowski and Brock [56] and Jonschkowski et al. [57] propose to learn representations using
a set of handcrafted robotic priors. Several prior works use a VAE and its variations to learn a
mapping from observations to state representations [50  18  58]. Single-view TCN [40] and TDC
[59] learn state representations using self-supervised objectives that leverage temporal information in
demonstrations. ST-DIM can be considered as an extension of TDC and TCN that also leverages the
local spatial structure (see Figure 3b for an ablation of spatial losses in ST-DIM).
A few works have focused on learning state representations that capture factors of an environment that
are under the agent’s control in order to guide exploration [60  61] or unsupervised control [62]. [EMI 
61] harnesses mutual information between state embeddings and actions to learn representations
that capture just the controllable factors of the environment  like the agent’s position. ST-DIM  on
the other hand  aims to capture every temporally evolving factor (not just the controllable ones)
in an environment  like the position of enemies  score  balls  missiles  moving obstacles  and the
agent position. The ST-DIM objective is also different from EMI in that it maximizes the mutual
information between global and local representations in consecutive time steps  whereas EMI just
considers mutual information between global representations. Lastly  ST-DIM uses an InfoNCE
objective instead of the JSD one used in EMI. Our work is also closely related to recent work in
learning object-oriented representations [63  64  65].

Evaluation frameworks of representations: Evaluating representations is an open problem  and
doing so is usually domain speciﬁc. In vision tasks  it is common to evaluate based on the presence
of linearly separable label-relevant information  either in the domain the representation was learned
on [66] or in transfer learning tasks [67  68]. In NLP  the SentEval [69] and GLUE [70] benchmarks
provide a means of providing a more linguistic-speciﬁc understanding of what the model has learned 
and these have become a standard tool in NLP research. Such et al. [71] has shown initial quantitative
and qualitative comparisons between the performance and representations of several DRL algorithms.
Our evaluation framework can be thought of as a GLUE-like benchmarking tool for RL  providing a
ﬁne-grained understanding of how well the RL agent perceives the objects in the scene. Analogous to
GLUE in NLP  we anticipate that our benchmarking tool will be useful in RL research in order to
better design components of agent learning.

5 Experimental Setup

We evaluate the performance of different representation learning methods on our benchmark. Our
experimental pipeline consists of ﬁrst training an encoder  then freezing its weights and evaluating its
performance on linear probing tasks. For each identiﬁed generative factor in each game  we construct
a linear probing task where the representation is trained to predict the ground truth value of that factor.
Note that the gradients are not backpropagated through the encoder network  and only used to train
the linear classiﬁer on top of the representation.

5.1 Data preprocessing and acquisition

We consider two different modes for collecting the data: (1) using a random agent (steps through
the environment by selecting actions randomly)  and (2) using a PPO [72] agent trained for 50M
timesteps. For both these modes  we ensure there is enough data diversity by collecting data using 8
differently initialized workers. We also add additional stochasticity to the pretrained PPO agent by
using an ✏-greedy like mechanism wherein at each timestep we take a random action with probability
✏ 2.

2For all our experiments  we used ✏ = 0.2.

6

5.2 Methods

In our evaluations  we compare the following methods:

1. Randomly-initialized CNN encoder (RANDOM-CNN).
2. Variational autoencoder (VAE) [12] on raw observations.
3. Next-step pixel prediction model (PIXEL-PRED) inspired by the "No-action Feedforward"

model from [22].

4. Contrastive Predictive Coding (CPC) [25]  which maximizes the mutual information between

current latents and latents at a future timestep.

5. SUPERVISED model which learns the encoder and the linear probe using the labels. The
gradients are backpropagated through the encoder in this case  so this provides a best-case
performance bound.

All methods use the same base encoder architecture  which is the CNN from [73]  but adapted for the
full 160x210 Atari frame size. To ensure a fair comparison  we use a representation size of 256 for
each method. As a sanity check  we include a blind majority classiﬁer (MAJ-CLF)  which predicts
label values based on the mode of the train set. More details in Appendix  section A.

5.3 Probing
We train a different 256-way3 linear classiﬁer with the representation under consideration as input.
We ensure the distribution of realizations of each state variable has high entropy by pruning any
variable with entropy less than 0.6. We also ensure there are no duplicates between the train and
test set. We train each linear probe with 35 000 frames and use 5 000 and 10 000 frames each for
validation and test respectively. We use early stopping and a learning rate scheduler based on plateaus
in the validation loss.

6 Results

Table 2: Probe F1 scores averaged across categories for each game (data collected by random agents)

GAME
ASTEROIDS
BERZERK
BOWLING
BOXING
BREAKOUT
DEMONATTACK
FREEWAY
FROSTBITE
HERO
MONTEZUMAREVENGE
MSPACMAN
PITFALL
PONG
PRIVATEEYE
QBERT
RIVERRAID
SEAQUEST
SPACEINVADERS
TENNIS
VENTURE
VIDEOPINBALL
YARSREVENGE

MEAN

MAJ-CLF
0.28
0.18
0.33
0.01
0.17
0.16
0.01
0.08
0.22
0.08
0.10
0.07
0.10
0.23
0.29
0.04
0.29
0.14
0.09
0.09
0.09
0.01
0.14

RANDOM-CNN
0.34
0.43
0.48
0.19
0.51
0.26
0.50
0.57
0.75
0.68
0.49
0.34
0.17
0.70
0.49
0.34
0.57
0.41
0.41
0.36
0.37
0.22
0.44

VAE
0.36
0.45
0.50
0.20
0.57
0.26
0.01
0.51
0.69
0.38
0.56
0.35
0.09
0.71
0.49
0.26
0.56
0.52
0.29
0.38
0.45
0.08
0.40

PIXEL-PRED
0.34
0.55
0.81
0.44
0.70
0.32
0.81
0.72
0.74
0.74
0.74
0.44
0.70
0.83
0.52
0.41
0.62
0.57
0.57
0.46
0.57
0.19
0.58

CPC
0.42
0.56
0.90
0.29
0.74
0.57
0.47
0.76
0.90
0.75
0.65
0.46
0.71
0.81
0.65
0.40
0.66
0.54
0.60
0.51
0.58
0.39
0.61

ST-DIM SUPERVISED
0.52
0.68
0.95
0.83
0.94
0.83
0.98
0.85
0.98
0.87
0.87
0.83
0.87
0.97
0.76
0.57
0.85
0.75
0.81
0.68
0.82
0.74
0.82

0.49
0.53
0.96
0.58
0.88
0.69
0.81
0.75
0.93
0.78
0.72
0.60
0.81
0.91
0.73
0.36
0.67
0.57
0.60
0.58
0.61
0.42
0.68

3Each RAM variable is a single byte thus has 256 possible values ranging from 0 to 255.

7

Table 3: Probe F1 scores for different methods averaged across all games for each category (data
collected by random agents)

CATEGORY
SMALL LOC.
AGENT LOC.
OTHER LOC.
SCORE/CLOCK/LIVES/DISPLAY
MISC.

MAJ-CLF
0.14
0.12
0.14
0.13
0.26

RANDOM
CNN
0.19
0.31
0.50
0.58
0.59

VAE
0.18
0.32
0.39
0.54
0.63

PIXEL-PRED
0.31
0.48
0.61
0.76
0.70

CPC
0.42
0.43
0.66
0.83
0.71

ST-DIM SUPERVISED
0.66
0.81
0.80
0.91
0.83

0.51
0.58
0.69
0.87
0.75

(a) InfoNCE vs JSD

(b) Effect of Spatial Loss

Figure 3: Different ablations for the ST-DIM model

We report the F1 averaged across all categories for each method and for each game in Table 2 for data
collected by random agent. In addition  we provide a breakdown of probe results in each category 
such as small object localization or score/lives classiﬁcation in Table 3 for the random agent. We
include the corresponding tables for these results with data collected by a pretrained PPO agent in
tables 6 and 7. The results in table 2 show that ST-DIM largely outperforms other methods in terms of
mean F1 score. In general  contrastive methods (ST-DIM and CPC) methods seem to perform better
than generative methods (VAE and PIXEL-PRED) at these probing tasks. We ﬁnd that RandomCNN
is a strong prior in Atari games as has been observed before [74]  possibly due to the inductive bias
captured by the CNN architecture empirically observed in [75]. We ﬁnd similar trends to hold on
results with data collected by a PPO agent. Despite contrastive methods performing well  there is still
a sizable gap between ST-DIM and the fully supervised approach  leaving room for improvement
from new unsupervised representation learning techniques for the benchmark.

7 Discussion

Ablations: We investigate two ablations of our ST-DIM model: Global-T-DIM  which only maxi-
mizes the mutual information between the global representations (similar in construction to PCL [76])
and JSD-ST-DIM  which uses the NCE loss [77] instead of the InfoNCE loss  which is equivalent to
maximizing the Jensen Shannon Divergence between representations. We report results from these
ablations in Figure 3. We see from the results in that 1) the InfoNCE loss performs better than the
JSD loss and 2) contrasting spatiotemporally (and not just temporally) is important across the board
for capturing all categories of latent factors.
We found ST-DIM has two main advantages which explain its superior performance over other
methods and over its own ablations. It captures small objects much better than other methods  and is
more robust to the presence of easy-to-exploit features which hurts other contrastive methods. Both
these advantages are due to ST-DIM maximizing mutual information of patch representations.

8

Capturing small objects: As we can see in Table 3  ST-DIM performs better at capturing small
objects than other methods  especially generative models like VAE and pixel prediction methods.
This is likely because generative models try to model every pixel  so they are not penalized much if
they fail to model the few pixels that make up a small object. Similarly  ST-DIM holds this same
advantage over Global-T-DIM (see Table 9)  which is likely due to the fact that Global-T-DIM is not
penalized if its global representation fails to capture features from some patches of the frame.

Robust to presence of easy-to-exploit features: Representation learning with mutual information
or contrastive losses often fail to capture all salient features if a few easy-to-learn features are
sufﬁcient to saturate the objective. This phenomenon has been linked to the looseness of mutual
information lower bounds [34  35] and gradient starvation [78]. We see the most prominent example
of this phenomenon in Boxing. The observations in Boxing have a clock showing the time remaining
in the round. A representation which encodes the shown time can perform near-perfect predictions
without learning any other salient features in the observation. Table 4 shows that CPC  Global T-DIM 
and ST-DIM perform well at predicting the clock variable. However only ST-DIM does well on
encoding the other variables such as the score and the position of the boxers.
We also observe that the best generative model (PIXEL-PRED) does not suffer from this problem.
It performs its worst on high-entropy features such as the clock and player score (where ST-DIM
excels)  and does slightly better than ST-DIM on low-entropy features which have a large contribution
in the pixel space such as player and enemy locations. This sheds light on the qualitative difference
between contrastive and generative methods: contrastive methods prefer capturing high-entropy
features (irrespective of contribution to pixel space) while generative methods do not  and generative
methods prefer capturing large objects which have low entropy. This complementary nature suggests
hybrid models as an exciting direction of future work.

Table 4: Breakdown of F1 Scores for every state variable in Boxing for ST-DIM  CPC  and Global-T-
DIM  an ablation of ST-DIM that removes the spatial contrastive constraint  for the game Boxing

METHOD

CLOCK
ENEMY_SCORE
ENEMY_X
ENEMY_Y
PLAYER_SCORE
PLAYER_X
PLAYER_Y

VAE
0.03
0.19
0.32
0.22
0.08
0.33
0.16

PIXEL-PRED
0.27
0.58
0.49
0.42
0.32
0.54
0.43

CPC
0.79
0.59
0.15
0.04
0.56
0.19
0.04

GLOBAL-T-DIM ST-DIM
0.92
0.70
0.51
0.38
0.88
0.56
0.37

0.81
0.74
0.17
0.16
0.45
0.13
0.14

8 Conclusion

We present a new representation learning technique which maximizes the mutual information of
representations across spatial and temporal axes. We also propose a new benchmark for state
representation learning based on the Atari 2600 suite of games to emphasize learning multiple
generative factors. We demonstrate that the proposed method excels at capturing the underlying
latent factors of a state even for small objects or when a large number of objects are present  which
prove difﬁcult for generative and other contrastive techniques  respectively. We have shown that
our proposed benchmark can be used to study qualitative and quantitative differences between
representation learning techniques  and hope that it will encourage more research in the problem of
state representation learning.

Acknowledgements

We are grateful for the collaborative research environment provided by Mila and Microsoft Research.
We thank Aaron Courville  Chris Pal  Remi Tachet  Eric Yuan  Chinwei-Huang  Khimya Khetrapal 
Tristan Deleu  and Aravind Srinivas for helpful discussions and feedback during the course of this
work. We would also like to thank the developers of PyTorch [79] and Weights&Biases.

9

References
[1] David Marr. Vision: A Computational Investigation into the Human Representation and

Processing of Visual Information. Henry Holt and Co.  Inc.  1982. ISBN 0716715678.

[2] Robert D Gordon and David E Irwin. What’s in an object ﬁle? evidence from priming studies.

Perception & Psychophysics  58(8):1260–1277  1996.

[3] Brenden M Lake  Tomer D Ullman  Joshua B Tenenbaum  and Samuel J Gershman. Building

machines that learn and think like people. Behavioral and brain sciences  40  2017.

[4] SM Ali Eslami  Danilo Jimenez Rezende  Frederic Besse  Fabio Viola  Ari S Morcos  Marta
Garnelo  Avraham Ruderman  Andrei A Rusu  Ivo Danihelka  Karol Gregor  et al. Neural scene
representation and rendering. Science  360(6394):1204–1210  2018.

[5] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[6] Dario Amodei  Sundaram Ananthanarayanan  Rishita Anubhai  Jingliang Bai  Eric Battenberg 
Carl Case  Jared Casper  Bryan Catanzaro  Qiang Cheng  Guoliang Chen  et al. Deep speech
2: End-to-end speech recognition in english and mandarin. In International conference on
machine learning  pages 173–182  2016.

[7] Yonghui Wu  Mike Schuster  Zhifeng Chen  Quoc V Le  Mohammad Norouzi  Wolfgang
Macherey  Maxim Krikun  Yuan Cao  Qin Gao  Klaus Macherey  et al. Google’s neural machine
translation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144  2016.

[8] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[9] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driess-
che  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al.
Mastering the game of go with deep neural networks and tree search. nature  529(7587):484 
2016.

[10] Jerzy Konorski. Integrative activity of the brain; an interdisciplinary approach. 1967.

[11] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Olivier Mastropietro  Alex Lamb  Martin
Arjovsky  and Aaron Courville. Adversarially learned inference. International Conference on
Learning Representations (ICLR)  2017.

[12] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Confer-

ence on Learning Representations (ICLR)  2014.

[13] Laurent Dinh  Jascha Sohl-Dickstein  and Samy Bengio. Density estimation using real nvp.

International Conference on Learning Representations (ICLR)  2017.

[14] Deepak Pathak  Philipp Krähenbühl  Jeff Donahue  Trevor Darrell  and Alexei A. Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  2016.

[15] Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In Proceedings

of the IEEE International Conference on Computer Vision  pages 2051–2060  2017.

[16] Alexander Kolesnikov  Xiaohua Zhai  and Lucas Beyer. Revisiting self-supervised visual

representation learning. arXiv preprint arXiv:1901.09005  2019.

[17] Timothée Lesort  Natalia Díaz-Rodríguez  Jean-Franois Goudou  and David Filliat. State

representation learning for control: An overview. Neural Networks  2018.

10

[18] Manuel Watter  Jost Springenberg  Joschka Boedecker  and Martin Riedmiller. Embed to
control: A locally linear latent dynamics model for control from raw images. In Advances in
neural information processing systems  pages 2746–2754  2015.

[19] Irina Higgins  Arka Pal  Andrei Rusu  Loic Matthey  Christopher Burgess  Alexander Pritzel 
Matthew Botvinick  Charles Blundell  and Alexander Lerchner. Darla: Improving zero-shot
transfer in reinforcement learning. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70  pages 1480–1490. JMLR. org  2017.

[20] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In

Advances in Neural Information Processing Systems  pages 2450–2462  2018.

[21] Wuyang Duan. Learning state representations for robotic control: Information disentangling

and multi-modal learning. Master’s thesis  Delft University of Technology  2017.

[22] Junhyuk Oh  Xiaoxiao Guo  Honglak Lee  Richard L Lewis  and Satinder Singh. Action-
In Advances in neural

conditional video prediction using deep networks in atari games.
information processing systems  pages 2863–2871  2015.

[23] Chelsea Finn  Ian Goodfellow  and Sergey Levine. Unsupervised learning for physical interac-
tion through video prediction. In Advances in neural information processing systems  pages
64–72  2016.

[24] Mohamed Ishmael Belghazi  Aristide Baratin  Sai Rajeshwar  Sherjil Ozair  Yoshua Bengio 
Aaron Courville  and Devon Hjelm. Mutual information neural estimation. In Proceedings
of the 35th International Conference on Machine Learning  pages 531–540  2018. URL
http://proceedings.mlr.press/v80/belghazi18a.html.

[25] Aaron van den Oord  Yazhe Li  and Oriol Vinyals. Representation learning with contrastive

predictive coding. arXiv preprint arXiv:1807.03748  2018.

[26] R Devon Hjelm  Alex Fedorov  Samuel Lavoie-Marchildon  Karan Grewal  Phil Bachman 
Adam Trischler  and Yoshua Bengio. Learning deep representations by mutual information
estimation and maximization. International Conference on Learning Representations (ICLR) 
2019.

[27] Petar Veliˇckovi´c  William Fedus  William L Hamilton  Pietro Liò  Yoshua Bengio  and R Devon

Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341  2018.

[28] Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence
Research  47:253–279  2013.

[29] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classiﬁer

probes. International Conference on Learning Representations (Workshop Track)  2017.

[30] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B:

Biological sciences  360(1456):815–836  2005.

[31] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-ﬁeld effects. Nature neuroscience  2(1):79 
1999.

[32] William Bialek and Naftali Tishby. Predictive information. arXiv preprint cond-mat/9902341 

1999.

[33] Stephanie E Palmer  Olivier Marre  Michael J Berry  and William Bialek. Predictive information
in a sensory population. Proceedings of the National Academy of Sciences  112(22):6908–6913 
2015.

[34] David McAllester and Karl Statos. Formal limitations on the measurement of mutual information.

arXiv preprint arXiv:1811.04251  2018.

11

[35] Sherjil Ozair  Corey Lynch  Yoshua Bengio  Aaron Van den Oord  Sergey Levine  and Pierre
Sermanet. Wasserstein dependency measure for representation learning. arXiv preprint
arXiv:1903.11780  2019.

[36] Philip Bachman  R Devon Hjelm  and William Buchwalter. Learning representations by

maximizing mutual information across views. arXiv preprint arXiv:1906.00910  2019.

[37] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation
principle for unnormalized statistical models. In Proceedings of the Thirteenth International
Conference on Artiﬁcial Intelligence and Statistics  pages 297–304  2010.

[38] Ben Poole  Sherjil Ozair  Aäron Van den Oord  Alexander A Alemi  and George Tucker. On
variational bounds of mutual information. In International Conference on Machine Learning 
2019.

[39] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Advances

in Neural Information Processing Systems  pages 1857–1865  2016.

[40] Pierre Sermanet  Corey Lynch  Yevgen Chebotar  Jasmine Hsu  Eric Jang  Stefan Schaal  Sergey
Levine  and Google Brain. Time-contrastive networks: Self-supervised learning from video. In
2018 IEEE International Conference on Robotics and Automation (ICRA)  pages 1134–1141.
IEEE  2018.

[41] Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for
conditional models: Consistency and statistical efﬁciency. arXiv preprint arXiv:1809.01812 
2018.

[42] Aapo Hyvärinen  Juha Karhunen  and Erkki Oja. Independent component analysis  volume 46.

John Wiley & Sons  2004.

[43] Francesco Locatello  Stefan Bauer  Mario Lucic  Sylvain Gelly  Bernhard Schölkopf  and
Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled
representations. International Conference on Machine Learning  2019.

[44] Zach Whalen and Laurie N Taylor. Playing the past. History and Nostalgia in Video Games.

Nashville  TN: Vanderbilt University Press  2008.

[45] Steve Engelhardt. BJARS.com Atari Archives. http://bjars.com  2019. [Online; accessed

1-March-2019].

[46] Thomas Jentzsch and CPUWIZ. Atariage atari 2600 forums  2019. URL http://atariage.

com/forums/forum/16-atari-2600/.

[47] Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine

Learning  2(1):1–127  2009.

[48] Yoshua Bengio  Aaron Courville  and Pascal Vincent. Representation learning: A review
and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI)  35(8):
1798–1828  2013.

[49] Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of
disentangled representations. International Conference on Learning Representations (ICLR) 
2018.

[50] Irina Higgins  David Amos  David Pfau  Sebastien Racaniere  Loic Matthey  Danilo Rezende 
and Alexander Lerchner. Towards a deﬁnition of disentangled representations. arXiv preprint
arXiv:1812.02230  2018.

[51] Mathilde Caron  Piotr Bojanowski  Armand Joulin  and Matthijs Douze. Deep clustering
for unsupervised learning of visual features. In Proceedings of the European Conference on
Computer Vision (ECCV)  pages 132–149  2018.

[52] Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov
process expectations for large time. iv. Communications on Pure and Applied Mathematics  36
(2):183–212  1983.

12

[53] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems  pages 2172–2180  2016.

[54] David Barber and Felix Agakov. The im algorithm: A variational approach to information
maximization. In Proceedings of the 16th International Conference on Neural Information
Processing Systems  NIPS’03  pages 201–208  Cambridge  MA  USA  2003. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2981345.2981371.

[55] Giuseppe Cuccu  Julian Togelius  and Philippe Cudré-Mauroux. Playing atari with six neurons.

International Conference on Autonomous Agents and Multiagent Systems  2019.

[56] Rico Jonschkowski and Oliver Brock. Learning state representations with robotic priors.

Autonomous Robots  39(3):407–428  2015.

[57] Rico Jonschkowski  Roland Hafner  Jonathan Scholz  and Martin Riedmiller. Pves: Position-
velocity encoders for unsupervised learning of structured state representations. arXiv preprint
arXiv:1705.09805  2017.

[58] Herke van Hoof  Nutan Chen  Maximilian Karl  Patrick van der Smagt  and Jan Peters. Stable
reinforcement learning with autoencoders for tactile and visual data.
In 2016 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS)  pages 3928–3934. IEEE 
2016.

[59] Yusuf Aytar  Tobias Pfaff  David Budden  Thomas Paine  Ziyu Wang  and Nando de Freitas.
Playing hard exploration games by watching youtube. In Advances in Neural Information
Processing Systems  pages 2930–2941  2018.

[60] Jongwook Choi  Yijie Guo  Marcin Moczulski  Junhyuk Oh  Neal Wu  Mohammad Norouzi 
and Honglak Lee. Contingency-aware exploration in reinforcement learning. arXiv preprint
arXiv:1811.01483  2018.

[61] Hyoungseok Kim  Jaekyeom Kim  Yeonwoo Jeong  Sergey Levine  and Hyun Oh Song. Emi:
Exploration with mutual information. In International Conference on Machine Learning  pages
3360–3369  2019.

[62] David Warde-Farley  Tom Van de Wiele  Tejas Kulkarni  Catalin Ionescu  Steven Hansen  and
Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv
preprint arXiv:1811.11359  2018.

[63] Christopher P Burgess  Loic Matthey  Nicholas Watters  Rishabh Kabra  Irina Higgins  Matt
Botvinick  and Alexander Lerchner. Monet: Unsupervised scene decomposition and representa-
tion. arXiv preprint arXiv:1901.11390  2019.

[64] Guangxiang Zhu  Zhiao Huang  and Chongjie Zhang. Object-oriented dynamics predictor. In

Advances in Neural Information Processing Systems  pages 9804–9815  2018.

[65] Klaus Greff  Raphaël Lopez Kaufmann  Rishab Kabra  Nick Watters  Chris Burgess  Daniel
Zoran  Loic Matthey  Matthew Botvinick  and Alexander Lerchner. Multi-object representation
learning with iterative variational inference. arXiv preprint arXiv:1903.00450  2019.

[66] Adam Coates  Andrew Ng  and Honglak Lee. An analysis of single-layer networks in unsuper-
vised feature learning. In Proceedings of the fourteenth international conference on artiﬁcial
intelligence and statistics  pages 215–223  2011.

[67] Yongqin Xian  Christoph H. Lampert  Bernt Schiele  and Zeynep Akata. Zero-shot learning -
a comprehensive evaluation of the good  the bad and the ugly. IEEE Transactions on Pattern
Analysis and Machine Intelligence  page 1–1  2018. ISSN 1939-3539. doi: 10.1109/tpami.2018.
2857768. URL http://dx.doi.org/10.1109/TPAMI.2018.2857768.

[68] Eleni Triantaﬁllou  Richard Zemel  and Raquel Urtasun. Few-shot learning through an informa-

tion retrieval lens  2017.

13

[69] Alexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence repre-
sentations. In Proceedings of the Eleventh International Conference on Language Resources
and Evaluation (LREC-2018)  2018.

[70] Alex Wang  Amanpreet Singh  Julian Michael  Felix Hill  Omer Levy  and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
International Conference on Learning Representations  2019. URL https://openreview.
net/forum?id=rJ4km2R5t7.

[71] Felipe Petroski Such  Vashisht Madhavan  Rosanne Liu  Rui Wang  Pablo Samuel Castro  Yulun
Li  Ludwig Schubert  Marc Bellemare  Jeff Clune  and Joel Lehman. An atari model zoo for
analyzing  visualizing  and comparing deep reinforcement learning agents. arXiv preprint
arXiv:1812.07069  2018.

[72] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

[73] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan
Wierstra  and Martin Riedmiller. Playing atari with deep reinforcement learning. In NIPS Deep
Learning Workshop. MIT Press  2013.

[74] Yuri Burda  Harri Edwards  Deepak Pathak  Amos Storkey  Trevor Darrell  and Alexei A
Efros. Large-scale study of curiosity-driven learning. International Conference on Learning
Representations (ICLR)  2019.

[75] Dmitry Ulyanov  Andrea Vedaldi  and Victor Lempitsky. Deep image prior. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  pages 9446–9454  2018.

[76] AJ Hyvarinen and Hiroshi Morioka. Nonlinear ica of temporally dependent stationary sources.

Proceedings of Machine Learning Research  2017.

[77] Aapo Hyvärinen and Petteri Pajunen. Nonlinear independent component analysis: Existence

and uniqueness results. Neural Networks  12(3):429–439  1999.

[78] Remi Tachet des Combes  Mohammad Pezeshki  Samira Shabanian  Aaron Courville  and
arXiv preprint

Yoshua Bengio. On the learning dynamics of deep neural networks.
arXiv:1809.06848  2018.

[79] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[80] Ken Kansky  Tom Silver  David A Mély  Mohamed Eldawy  Miguel Lázaro-Gredilla  Xinghua
Lou  Nimrod Dorfman  Szymon Sidor  Scott Phoenix  and Dileep George. Schema networks:
Zero-shot transfer with a generative causal model of intuitive physics. In Proceedings of the
34th International Conference on Machine Learning-Volume 70  pages 1809–1818. JMLR. org 
2017.

[81] Amy Zhang  Yuxin Wu  and Joelle Pineau. Natural environment benchmarks for reinforcement

learning. arXiv preprint arXiv:1811.06032  2018.

14

,Ankesh Anand
Evan Racah
Sherjil Ozair
Yoshua Bengio
Marc-Alexandre Côté
R Devon Hjelm