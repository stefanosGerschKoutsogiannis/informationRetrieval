2017,Elementary Symmetric Polynomials for Optimal Experimental Design,We revisit the classical problem of optimal experimental design (OED) under a new mathematical model grounded in a geometric motivation. Specifically  we introduce models based on elementary symmetric polynomials; these polynomials capture "partial volumes" and offer a graded interpolation between the widely used A-optimal and D-optimal design models  obtaining each of them as special cases. We analyze properties of our models  and derive both greedy and convex-relaxation algorithms for computing the associated designs. Our analysis establishes approximation guarantees on these algorithms  while our empirical results substantiate our claims and demonstrate a curious phenomenon concerning our greedy algorithm. Finally  as a byproduct  we obtain new results on the theory of elementary symmetric polynomials that may be of independent interest.,Elementary Symmetric Polynomials for Optimal

Experimental Design

Massachusetts Institute of Technology

Massachusetts Institute of Technology

Zelda Mariet

Cambridge  MA 02139
zelda@csail.mit.edu

Suvrit Sra

Cambridge  MA 02139

suvrit@mit.edu

Abstract

We revisit the classical problem of optimal experimental design (OED) under a
new mathematical model grounded in a geometric motivation. Speciﬁcally  we
introduce models based on elementary symmetric polynomials; these polynomials
capture “partial volumes” and offer a graded interpolation between the widely
used A-optimal design and D-optimal design models  obtaining each of them as
special cases. We analyze properties of our models  and derive both greedy and
convex-relaxation algorithms for computing the associated designs. Our analysis
establishes approximation guarantees on these algorithms  while our empirical
results substantiate our claims and demonstrate a curious phenomenon concerning
our greedy method. Finally  as a byproduct  we obtain new results on the theory
of elementary symmetric polynomials that may be of independent interest.

1 Introduction
Optimal Experimental Design (OED) develops the theory of selecting experiments to perform in
order to estimate a hidden parameter as well as possible.
It operates under the assumption that
experiments are costly and cannot be run as many times as necessary or run even once without
tremendous difﬁculty [33]. OED has been applied in a large number of experimental settings [35  9 
28  46  36]  and has close ties to related machine-learning problems such as outlier detection [15  22] 
active learning [19  18]  Gaussian process driven sensor placement [27]  among others.
We revisit the classical setting where each experiment depends linearly on a hidden parameter (cid:18) 2
Rm. We assume there are n possible experiments whose outcomes yi 2 R can be written as

yi = x

⊤
i (cid:18) + ϵi

1 (cid:20) i (cid:20) n;

where the xi 2 Rm and ϵi are independent  zero mean  and homoscedastic noises. OED seeks to
answer the question: how to choose a set S of k experiments that allow us to estimate (cid:18) without bias
and with minimal variance?
is invertible)  the Gauss-Markov theorem
Given a feasible set S of experiments (i.e. 
(cid:0)1. How-
shows that the lowest variance for an unbiased estimate ^(cid:18) satisﬁes Var[^(cid:18)] = (
ever  Var[^(cid:18)] is a matrix  and matrices do not admit a total order  making it difﬁcult to compare
(cid:3)
different designs. Hence  OED is cast as an optimization problem that seeks an optimal design S

i2S xix

i2S xix

∑

∑

⊤
i )

⊤
i

((∑

)

)(cid:0)1

S

(cid:3) 2 argmin
S2[n];jSj(cid:20)k

(cid:8)

i2S

⊤
xix
i

;

(1.1)

where (cid:8) maps positive deﬁnite matrices to R to compare the variances for each design  and may
help elicit different properties that a solution should satisfy  either statistical or structural.
Elfving [16] derived some of the earliest theoretical results for the linear dependency setting  fo-
cusing on the case where one is interested in reconstructing a predeﬁned linear combination of the

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

⊤

(cid:18) (C-optimal design). Kiefer [26] introduced a more general approach
underlying parameters c
to OED  by considering matrix means on positive deﬁnite matrices as a general way of evaluating
optimality [33  Ch. 6]  and Yu [48] derived general conditions for a map (cid:8) under which a class of
multiplicative algorithms for optimal design has guaranteed monotonic convergence.
Nonetheless  the theory of OED branches into multiple variants of (1.1) depending on the choice of
(cid:8)  among which A-optimal design ((cid:8) = trace) and D-optimal design ((cid:8) = determinant) are prob-
ably the two most popular choices. Each of these choices has a wide range of applications as well
as statistical  algorithmic  and other theoretical results. We refer the reader to the classic book [33] 
which provides an excellent overview and introduction to the topic; see also the summaries in [1  35].
For A-optimal design  recently Wang et al. [44] derived greedy and convex-relaxation approaches;
[11] considers the problem of constrained adaptive sensing  where (cid:18) is supposed sparse. D-optimal
design has historically been more popular  with several approaches to solving the related optimiza-
tion problem [17  38  31  20]. The dual problem of D-optimality  Minimum Volume Covering Ellip-
soid (MVCE) is also a well-known and deeply studied optimization problem [3  34  43  41  14  42].
Experimental design has also been studied in more complex settings: [8] considers Bayesian optimal
design; under certain conditions  non-linear settings can be approached with linear OED [13  25].
Due to the popularity of A- and D-optimal design  the theory surrounding these two sub-problems
has diverged signiﬁcantly. However  both the trace and the determinant are special cases of fun-
damental spectral polynomials of matrices: elementary symmetric polynomials (ESP)  which have
been extensively studied in matrix theory  combinatorics  information theory  and other areas due to
their importance in the theory of polynomials [24  30  21  6  23  4].
These considerations motivate us to derive a broader view of optimal design which we call ESP-
Design  where (cid:8) is obtained from an elementary symmetric polynomial. This allows us to consider
A-optimal design and D-optimal design as special cases of ESP-design  and thus treat the entire
ESP-class in a uniﬁed manner. Let us state the key contributions of this paper more precisely below.
Contributions
(cid:15) We introduce ESP-design  a new  general framework for OED that leverages geometric proper-
ties of positive deﬁnite matrices to interpolate between A- and D-optimality. ESP-design offers
an intuitive setting in which to gradually scale between A-optimal and D-optimal design.

(cid:15) We develop a convex relaxation as well as greedy algorithms to compute the associated designs.
As a byproduct of our convex relaxation  we prove that ESPs are geodesically log-convex on the
Riemannian manifold of positive deﬁnite matrices; this result may be of independent interest.

(cid:15) We extend a result of Avron and Boutsidis [2] on determinantal column-subset selection to ESPs;
as a consequence we obtain a greedy algorithm with provable optimality bounds for ESP-design.

Experiments on synthetic and real data illustrate the performance of our algorithms and conﬁrm that
ESP-design can be used to obtain designs with properties that scale between those of both A- and
D-optimal designs  allowing users to tune trade-offs between their different beneﬁts (e.g. predictive
error  sparsity  etc.). We show that our greedy algorithm generates designs of equal quality to the
famous Fedorov exchange algorithm [17]  while running in a fraction of the time.
2 Preliminaries
We begin with some background material that also serves to set our notation. We omit proofs for
brevity  as they can be found in standard sources such as [6].
We deﬁne [n] ≜ f1; 2; : : : ; ng. For S (cid:18) [n] and M 2 Rn(cid:2)m  we write MS the jSj (cid:2) m matrix
created by keeping only the rows of M indexed by S  and M [SjS
] the submatrix with rows indexed
′; by x(i) we denote the vector x with its i-th component removed.
∑
by S and columns indexed by S
For a vector v 2 Rm  the elementary symmetric polynomial (ESP) of order ℓ 2 N is deﬁned by

∏ℓ
1(cid:20)i1<:::<iℓ(cid:20)m
where eℓ (cid:17) 0 for ℓ = 0 and ℓ > m. Let S+
m ) be the cone of positive semideﬁnite (positive
deﬁnite) matrices of order m. We denote by (cid:21)(M ) the eigenvalues (in decreasing order) of a sym-
metric matrix M. Def. (2.1) extends to matrices naturally; ESPs are spectral functions  as we set

I(cid:18)[m];jIj=ℓ

eℓ(v) ≜

m (S++

∑

vij =

j=1

vj;

j2I

(2.1)

′

∏

2

Eℓ(M ) ≜ eℓ◦ (cid:21)(M ); additionally  they enjoy another representation that allows us to interpret them
as “partial volumes”  namely 

∑

Eℓ(M ) =

S(cid:18)[n];jSj=ℓ

det(M [SjS]):

(2.2)

The following proposition captures basic properties of ESPs that we will require in our analysis.
Proposition 2.1. Let M 2 Rm(cid:2)m be symmetric and 1 (cid:20) ℓ (cid:20) m; also let A; B 2 S+
following properties:
then Eℓ(M

m. We have the
(i) If A ⪰ B in Löwner order  then Eℓ(A) (cid:21) Eℓ(B); (ii) If M is invertible 
(cid:0)1)Em(cid:0)ℓ(M ); (iii) ∇eℓ((cid:21)) = [eℓ(cid:0)1((cid:21)(i))]1(cid:20)i(cid:20)m.

(cid:0)1) = det(M

3 ESP-design
A-optimal design uses (cid:8) (cid:17) tr in (1.1)  and thus selects designs with low average variance. Geo-
metrically  this translates into selecting conﬁdence ellipsoids whose bounding boxes have a small
diameter. Conversely  D-optimal design uses (cid:8) (cid:17) det in (1.1)  and selects vectors that correspond
to the ellipsoid with the smallest volume; as a result it is more sensitive to outliers in the data1. We
introduce a natural model that scales between A- and D-optimal design. Indeed  by recalling that
both the trace and the determinant are special cases of ESPs  we obtain a new model as fundamental
as A- and D-optimal design  while being able to interpolate between the two in a graded manner.
Unless otherwise indicated  we consider that we are selecting experiments without repetition.
3.1 Problem formulation
Let X 2 Rn(cid:2)m (m ≪ n) be a design matrix with full column rank  and k 2 N be the budget
(
(m (cid:20) k (cid:20) n). Deﬁne (cid:0)k = fS (cid:18) [n] s.t. jSj (cid:20) k; X
S XS ≻ 0g to be the set of feasible designs
⊤
that allow unbiased (cid:18) estimates. For ℓ 2 f1; : : : ; mg  we introduce the ESP-design model:

)

fℓ(S) ≜ 1

ℓ log Eℓ

(X

⊤
S XS)

(cid:0)1

:

min
S2(cid:0)k

(3.1)

We keep the 1=ℓ-factor in (3.1) to highlight the homogeneity (Eℓ is a polynomial of degree ℓ) of our
design criterion  as is advocated in [33  Ch. 6].
For ℓ = 1  (3.1) yields A-optimal design  while for ℓ = m  it yields D-optimal design. For 1 < ℓ <
m  ESP-design interpolates between these two extremes. Geometrically  we may view it as seeking
an ellipsoid with the smallest average volume for ℓ-dimensional slices (taken across sets of size ℓ).
Alternatively  ESP-design can be also be interpreted as a regularized version of D-optimal design
via Prop. 2.1-(ii). In particular  for ℓ = m (cid:0) 1  we recover a form of regularized D-optimal design:

[

(

)

]

fm(cid:0)1(S) = 1
m(cid:0)1

log det

(X

⊤
S XS)

(cid:0)1

+ log ∥XS∥2

2

:

(3.1) is a known hard combinatorial optimization problem (in particular for ℓ = m)  which precludes
an exact optimal solution. However  its objective enjoys remarkable properties that help us derive
efﬁcient algorithms for its approximate solution. The ﬁrst one of these is based on a natural convex
relaxation obtained below.

3.2 Continuous relaxation
We describe below a traditional approach of relaxing (3.1) by relaxing the constraint on S  allowing
elements in the set to have fractional multiplicities. The new optimization problem takes the form

(

)

⊤

Diag(z)X)

(cid:0)1

;

(3.2)

min
z2(cid:0)c

k

1
ℓ log Eℓ

(X

where we (cid:0)c
invertible and 1
Proposition 3.1. Let z

⊤

k denotes the set of vectors fz 2 Rn j 0 (cid:20) zi (cid:20) 1g such that X
⊤
z (cid:20) k. The following is a direct consequence of Prop 2.1-(i):

(cid:3) be the optimal solution to (3.2). Then ∥z

(cid:3)∥1 = k.

Diag(z)X remains

Convexity of fℓ on (cid:0)c
k (where by abuse of notation  fℓ also denotes the continuous relaxation in (3.2))
can be obtained as a consequence of [32]; however  we obtain it as a corollary Lemma 3.3  which
shows that log Eℓ is geodesically convex; this result seems to be new  and is stronger than convexity
of fℓ; hence it may be of independent interest.

1For a more in depth discussion of the geometric interpretation of various optimal designs  refer to e.g. [7 

Section 7.5].

3

Deﬁnition 3.2 (geodesic-convexity). A function f : S++
S++
m is called geodesically convex if it satisﬁes

f (P #tQ) (cid:20) (1 (cid:0) t)f (P ) + tf (Q);

m

! R deﬁned on the Riemannian manifold
t 2 [0; 1]; and P; Q ≻ 0:
(cid:0)1=2QP

(cid:0)1=2)tP 1=2 to denote the geodesic

ℓ

⊤

((X

M X)

(cid:0)1Y ).

(cid:0)1XP

m and their optimization  we

m under the Riemannian metric gP (X; Y ) = tr(P

(cid:0)1) is log-convex on the set of PD matrices.

where we use the traditional notation P #tQ := P 1=2(P
between P and Q 2 S++
Lemma 3.3. The function Eℓ is geodesically log-convex on the set of positive deﬁnite matrices.
Corollary 3.4. The map M 7! E1=ℓ
For further details on the theory of geodesically convex functions on S+
refer the reader to [40]. We prove Lemma 3.3 and Corollary 3.4 in Appendix A.
From Corollary 3.4  we immediately obtain that (3.2) is a convex optimization problem  and can
therefore be solved using a variety of efﬁcient algorithms. Projected gradient descent turns out to
be particularly easy to apply because we only require projection onto the intersection of the cube
1 = kg (as a consequence of Prop 3.1). Projection onto this
0 (cid:20) z (cid:20) 1 and the plane fz j z
intersection is a special case of the so-called continuous quadratic knapsack problem  which is a
very well-studied problem and can be solved essentially in linear time [10  12].
Remark 3.5. The convex relaxation remains log-convex when points can be chosen with multiplic-
ity  in which case the projection step is also signiﬁcantly simpler  requiring only z (cid:21) 0.
We conclude the analysis of the continuous relaxation by showing a bound on the support of its
solution under some mild assumptions:
Theorem 3.6. Let ϕ be the mapping from Rm to Rm(m+1)=2 such that ϕ(x) = ((cid:24)ijxixj))1(cid:20)i;j(cid:20)m
with (cid:24)ij = 1 if i = j and 2 otherwise. Let ~ϕ(x) = (ϕ(x); 1) be the afﬁne version of ϕ. If for any
set of m(m + 1)=2 distinct rows of X  the mapping under ~ϕ is independent  then the support of the
optimum z

(cid:3)of (3.2) satisﬁes ∥z

(cid:3)∥0 (cid:20) k + m(m+1)

⊤

.

2

The proof is identical to that of [44  Lemma 3.5]  which shows such a result for A-optimal design;
we relegate it to Appendix B.

4 Algorithms and analysis

Solving the convex relaxation (3.2) does not directly provide a solution to (3.1); ﬁrst  we must round
k to a discrete solution S 2 (cid:0)k. We present two possibilities: (i) rounding
the relaxed solution z
the solution of the continuous relaxation (§4.1); and (ii) a greedy approach (§4.2).

(cid:3) 2 (cid:0)c

4.1 Sampling from the continuous relaxation

For conciseness  we concentrate on sampling without replacement  but note that these results extend
with minor changes to with replacement sampling (see [44]). Wang et al. [44] discuss the sampling
scheme described in Alg. 1) for A-optimal design; the same idea easily extends to ESP-design. In
particular  Alg. 1  applied to a solution of (3.2)  provides the same asymptotic guarantees as those
proven in [44  Lemma 3.2] for A-optimal design.

(cid:3)

(cid:3) 2 Rn

Algorithm 1: Sample from z
Data: budget k  z
Result: S of size k
S ∅
while jSj < k do

Sample i 2 [n] n S uniformly at random
Sample x (cid:24) Bernoulli(z
(cid:3)
i )
if x = 1 then S S [ fig

return S
Theorem 4.1. Let (cid:6)(cid:3) = X
S constructed by sampling as above veriﬁes with probability p = 0:8

)X. Suppose ∥(cid:6)

Diag(z

⊤

(cid:3)

)(cid:0)1

)1=ℓ (cid:20) O(1) (cid:1) Eℓ

((
(cid:0)1(cid:3) ∥2(cid:20)((cid:6)(cid:3))∥X∥21 log m = O(1). The subset

)1=ℓ

)(cid:0)1

⊤
S(cid:3)XS(cid:3)

X

:

((

Eℓ

⊤
S XS

X

4

Theorem 4.1 shows that under reasonable conditions  we can probabilistically construct a good
(cid:3) to the convex relaxation.
approximation to the optimal solution in linear time  given the solution z

4.2 Greedy approach

In addition to the solution based on convex relaxation  ESP-design admits an intuitive greedy ap-
proach  despite not being a submodular optimization problem in general. Here  elements are re-
moved one-by-one from a base set of experiments; this greedy removal  as opposed to greedy ad-
dition  turns out to be much more practical. Indeed  since fℓ is not deﬁned for sets of size smaller
than k  it is hard to greedily add experiments to the empty set and then bound the objective function
after k items have been added. This difﬁculty precludes analyses such as [45  39] for optimizing
non-submodular set functions by bounding their “curvature”.

Algorithm 2: Greedy algorithm
Data: matrix X  budget k  initial set S0
Result: S of size k
S S0
while jSj > k do
Find i 2 S such that S n fig is feasible and i minimizes fℓ(S n fig)
S S n fig

return S

Bounding the performance of Algorithm 2 relies on the following lemma.
Lemma 4.2. Let X 2 Rn(cid:2)m(n (cid:21) m) be a matrix with full column rank  and let k be a budget
m (cid:20) k (cid:20) n. Let S of size k be subset of [n] drawn with probability P / det(X

⊤
S XS). Then

)]

((

[
∏ℓ
ES(cid:24)P
S XS ≻ 0 for all subsets S of size k.
⊤

)(cid:0)1

⊤
S XS

(cid:20)

Eℓ

i=1

X

((

)

)(cid:0)1

n (cid:0) m + i
k (cid:0) m + i

(cid:1) Eℓ

⊤

X

X

with equality if X

;

(4.1)

(cid:20)

Eℓ

X

((

)(cid:0)1

⊤
S+XS+

)
∏ℓ
)1=ℓ (cid:20) n (cid:0) m + ℓ

((
)(cid:0)1

Lemma 4.2 extends a result from [2  Lemma 3.9] on column-subset selection via volume sampling
to all ESPs. In particular  it follows that removing one element (by volume sampling a set of size
n (cid:0) 1) will in expectation decrease f by a multiplicative factor which is clearly also attained by a
greedy minimization. This argument then entails the following bound on Algorithm 2’s performance.
Proofs of both results are in Appendix C.
Theorem 4.3. Algorithm 2 initialized with a set S0 of size n0 produces a set S+ of size k such that
(4.2)

)(cid:0)1
((
(cid:3) denotes the optimal set):
f (f1; : : : ; ng) (cid:20) n (cid:0) m + ℓ
k (cid:0) m + 1
Eℓ
(4.3)
(cid:3)∥0 of the convex relaxation
However  this naive initialization can be replaced by the support ∥z
solution; in the common scenario described by Theorem 3.6  we then obtain the following result:
Theorem 4.4. Let ~ϕ be the mapping deﬁned in 3.6  and assume that all choices of m(m + 1)=2
distinct rows of X always have their mapping independent mappings for ~ϕ. Then the outcome of the
greedy algorithm initialized with the support of the solution to the continuous relaxation veriﬁes

As Wang et al. [44] note regarding A-optimal design  (4.2) provides a trivial optimality bound on
the greedy algorithm when initialized with S0 = f1; : : : ; ng (S

n0 (cid:0) m + j
k (cid:0) m + j

k (cid:0) m + 1

)1=ℓ

⊤
S+XS+

)(cid:0)1

⊤
S(cid:3)XS(cid:3)

((

(cid:1) Eℓ

)

⊤
S0

X

Eℓ

X

XS0

j=1

X

)

(

k + m(m (cid:0) 1)=2 + ℓ

k (cid:0) m + 1

+ fℓ(S

(cid:3)

):

fℓ(S+) (cid:20) log

4.3 Computational considerations

Computing the ℓ-th elementary symmetric polynomial on a vector of size m can be done in
O(m log2 ℓ) using Fast Fourier Transform for polynomial multiplication  due to the construction
introduced by Ben-Or (see [37]); hence  computing fℓ(S) requires O(nm2) time  where the cost is
dominated by computing X

S XS. Alg. 1 runs in expectation in O(n); Alg. 2 costs O(m2n3).
⊤

5

5 Further Implications

We close our theoretical presentation by discussing a potentially important geometric problem re-
lated to ESP-design. In particular  our motivation here is the dual problem of D-optimal design (i.e. 
dual to the convex relaxation of D-optimal design): this is nothing but the well-known Minimum Vol-
ume Covering Ellipsoid (MVCE) problem  which is a problem of great interest to the optimization
community in its own right—see the recent book [42] for an excellent account.
With this motivation  we develop the dual formulation for ESP-design now. We start by deriving
∇Eℓ(A)  for which we recall that Eℓ((cid:1)) is a spectral function  whereby the spectral calculus of Lewis
[29] becomes applicable  saving us from intractable multilinear algebra [23]. More precisely  say
⊤
U

(cid:3)U is the eigendecomposition of A  with U unitary. Then  as Eℓ(A) = eℓ ◦ (cid:21)(A) 

(5.1)
We can now derive the dual of ESP-design (we consider only z (cid:21) 0); in this case problem (3.2) is

Diag(eℓ(cid:0)1((cid:3)

∇Eℓ(A) = U

⊤

Diag(∇eℓ((cid:3)))U = U

⊤

(cid:0)(i)))U:

(cid:0)1 (cid:0) X

⊤

Diag(z)X)) (cid:0) (cid:22)(1

⊤

z (cid:0) k);

+ tr(HX

⊤

Diag(z)X) (cid:0) (cid:22)(1
⊤

z (cid:0) k):

(5.2)

sup

A≻0;z(cid:21)0

inf
(cid:22)2R;H

which admits as dual

inf
(cid:22)2R;H

sup

A≻0;z(cid:21)0

|

(cid:0) 1
ℓ log Eℓ(A) (cid:0) tr(HA

{z

ℓ log Eℓ(A) (cid:0) tr(H(A
(cid:0) 1
}
(

(cid:0)1)

(cid:3)U  we have
∇g(A) = 0 () (cid:3) Diag

g(A)

We easily show that H ⪰ 0 and that g reaches its maximum on S++
Rewriting A = U

⊤

m for A such that ∇g = 0.

)

eℓ(cid:0)1((cid:3)(i))

(cid:3) = eℓ((cid:3))U HU

⊤

:

In particular  H and A are co-diagonalizable  with (cid:3) Diag(eℓ(cid:0)1((cid:3)(i)))(cid:3) = Diag(h1; : : : ; hm). The
eigenvalues of A must thus satisfy the system of equations

i eℓ(cid:0)1((cid:21)1; : : : ; (cid:21)i(cid:0)1; (cid:21)i+1; : : : ; (cid:21)m) = hieℓ((cid:21)1; : : : ; (cid:21)m);
(cid:21)2

Let a(H) be such a matrix (notice  a(H) = ∇g
where f ⋆

(cid:3)

ℓ is the Fenchel conjugate of fℓ . Finally  the dual optimization problem is given by

(0)). Since fℓ is convex  g(a(H)) = f ⋆

ℓ ((cid:0)H)

1 (cid:20) i (cid:20) m:

sup

i Hxi(cid:20)1;H⪰0
⊤

x

ℓ ((cid:0)H) =
f ⋆

sup

i Hxi(cid:20)1;H⪰0
⊤

x

1
ℓ log Eℓ(a(H))

Details of the calculation are provided in Appendix D. In the general case  deriving a(H) or even
Eℓ(a(H)) does not admit a closed form that we know of. Nevertheless  we recover the well-known
duals of A-optimal design and D-optimal design as special cases.
Corollary 5.1. For ℓ = 1  a(H) = tr(H 1=2)H 1=2 and for ℓ = m  a(H) = H. Consequently  we
recover the dual formulations of A- and D-optimal design.

6 Experimental results

We compared the following methods to solving (3.1):

– UNIF / UNIFFDV: k experiments are sampled uniformly / with Fedorov exchange
– GREEDY / GREEDYFDV: greedy algorithm (relaxed init.) / with Fedorov exchange
– SAMPLE: sampling (relaxed init.) as in Algorithm 1.

We also report the results for solution of the continuous relaxation (RELAX); the convex optimization
was solved using projected gradient descent  the projection being done with the code from [12].

6.1 Synthetic experiments: optimization comparison

We generated the experimental matrix X by sampling n vectors of size m from the multivariate
(cid:0)1 (density d ranging from 0.3 to 0.9). Due
Gaussian distribution of mean 0 and sparse precision (cid:6)
to the runtime of Fedorov methods  results are reported for only one run; results averaged over
multiple iterations (as well as for other distributions over X) are provided in Appendix E.

6

As shown in Fig. 1  the greedy algorithm applied to the convex relaxation’s support outperforms
sampling from the convex relaxation solution  and does as well as the usual Fedorov algorithm
UNIFFDV; GREEDYFDV marginally improves upon the greedy algorithm and UNIFFDV. Strik-
ingly  GREEDY provides designs of comparable quality to UNIFFDV; furthermore  as very few local
exchanges improve upon its design  running the Fedorov algorithm with GREEDY initialization is
much faster (Table 1); this is conﬁrmed by Table 2  which shows the number of experiments in com-
mon for different algorithms: GREEDY and GREEDYFDV only differ on very few elements. As the
budget k increases  the difference in performances between SAMPLE  GREEDY and the continuous
relaxation decreases  and the simpler SAMPLE algorithm becomes competitive. Table 3 reports the
support of the continuous relaxation solution for ESP-design with ℓ = 10.

Table 1: Runtimes (s) (ℓ = 10  d = 0:6)

k

40

80

120

160

200

GREEDY

GREEDYFDV

UNIFFDV

2.8 101
6.6 101
1.6 103

2.7 101
2.2 102
4.1 103

3.1 101
3.2 102
6.0 103

4.0 101
1.2 102
6.2 103

5.2 101
1.3 102
4.7 103

Table 2: Common items between solutions (ℓ = 10  d = 0:6)

k

|GREEDY \ UNIFFDV|
|GREEDY \ GREEDYFDV|
|UNIFFDV \ GREEDYFDV|

Table 3: ∥z

40
26
40
26

80
76
78
75

120
114
117
113
(cid:3)∥0 (ℓ = 10  d = 0:6)

160
155
160
155

200
200
200
200

k

d = 0:3
d = 0:6
d = 0:9

40
93 (cid:6) 3
92 (cid:6) 7
88 (cid:6) 3

80

117 (cid:6) 3
117 (cid:6) 4
116 (cid:6) 3

120
148 (cid:6) 2
145 (cid:6) 4
147 (cid:6) 4

160
181 (cid:6) 3
180 (cid:6) 3
179 (cid:6) 3

200
213 (cid:6) 2
214 (cid:6) 4
214 (cid:6) 1

6.2 Real data

We used the Concrete Compressive Strength dataset [47] (with column normalization) from the UCI
repository to evaluate ESP-design on real data; this dataset consists in 1030 possible experiments to
model concrete compressive strength as a linear combination of 8 physical parameters. In Figure 2
(a)  OED chose k experiments to run to estimate (cid:18)  and we report the normalized prediction error on
the remaining n (cid:0) k experiments. The best choice of OED for this problem is of course A-optimal
design  which shows the smallest predictive error. In Figure 2 (b)  we report the fraction of non-zero
entries in the design matrix XS; higher values of ℓ correspond to increasing sparsity. This conﬁrms
that OED allows us to scale between the extremes of A-optimal design and D-optimal design to tune
desirable side-effects of the design; for example  sparsity in a design matrix can indicate not needing
to tune a potentially expensive experimental parameter  which is instead left at its default value.

7 Conclusion and future work

We introduced the family of ESP-design problems  which evaluate the quality of an experimental
design using elementary symmetric polynomials  and showed that typical approaches to optimal
design such as continuous relaxation and greedy algorithms can be extended to this broad family of
problems  which covers A-optimal design and D-optimal design as special cases.
We derived new properties of elementary symmetric polynomials: we showed that they are geodesi-
cally log-convex on the space of positive deﬁnite matrices  enabling fast solutions to solving the
relaxed ESP optimization problem. We furthermore showed in Lemma 4.2 that volume sampling 
applied to the columns of the design matrix X has a constant multiplicative impact on the objec-
tive function Eℓ(
)  extending Avron and Boutsidis [2]’s result from the trace to all el-

)(cid:0)1

(

X

⊤
S XS

7

GREEDY

GREEDYFDV

SAMPLE

RELAX

UNIF

UNIFFDV

..
.
.
.
.
.

)
t
p
O
A

-

(

1
=
ℓ

..
.
.
.
.
.
.
.
.
..

)
S
(
ℓ
f

2.0

1.0

0.0

0.0

-1.0

-2.0

40

80

120

160

200

40

80

120

160

200

2.0

1.0

0.0
..
.
.
.
.
.
.
.

0.0

-1.0

..
.
.
.
.
.
.
-1.0

-2.0

-3.0

40

80

120

160

200

40

80

120

160

200

2.0

1.0

..
.
.
.
.
.
.

0.0

-1.0

..
.
.
.
.
.
.

-1.0

-2.0

-3.0

40

80

120

160

200

40

80

120

160

200

40

80

120

160

200

40

80

120

160

200

40

80

120

160

200

budget k
d = 0:3

..
.
.
.
.
.
.
.
.
.

budget k
d = 0:6

..
.
.
.
.
.
.
.
.
.

budget k
d = 0:9

0
1
=
ℓ

)
S
(
ℓ
f

)
S
(
ℓ
f

-2.0

-3.0

..
.
.
.
.
.
.
.
.
..

)
t
p
O
D

-

(

0
2
=
ℓ

..
.
.
.
.
.
.
.
.
..
.

Figure 1: Synthetic experiments  n = 500  m = 30. The greedy algorithm performs as well as the
classical Fedorov approach; as k increases  all designs except UNIF converge towards the continuous
relaxation  making SAMPLE the best approach for large designs.

ℓ = 1 (A-opt)

ℓ = 3

ℓ = 6

ℓ = 8 (D-opt)

0:82

0:81

0:80

s
e
i
r
t
n
e

o
r
e
z

n
o
n

f
o

o
i
t
a
r

..
.
.
.

(cid:2)10

(cid:0)4

r
o
r
r
e

e
v
i
t
c
i
d
e
r
p

3:2

3:0

2:8

..
.
.
.
.
.
.
.
.
.
.
.
.

100

120

140

160

180

200

100

120

140

160

180

200

budget k
(a) MSE

..
.
.
.
.
.
.
.
.
.
.
.

budget k

(b) Sparsity

Figure 2: Predicting concrete compressive strength via the greedy method; higher ℓ increases the
sparsity of the design matrix XS  at the cost of marginally decreasing predictive performance.

ementary symmetric polynomials. This allows us to derive a greedy algorithm with performance
guarantees  which empirically performs as well as Fedorov exchange  in a fraction of the runtime.
However  our work still includes some open questions: in deriving the Lagrangian dual of the op-
timization problem  we had to introduce the function a(H) which maps S++
m ; however  although
a(H) is known for ℓ = 1; m  its form for other values of ℓ is unknown  making the dual form
a purely theoretical object in the general case. Whether the closed form of a can be derived  or
whether Eℓ(a(H)) can be obtained with only knowledge of H  remains an open problem. Due to
the importance of the dual form of D-optimal design as the Minimum Volume Covering Ellipsoid 
we believe that further investigation of the general dual form of ESP-design will provide valuable
insight  both into optimal design and for the general theory of optimization.

8

ACKNOWLEDGEMENTS
Suvrit Sra acknowledges support from NSF grant IIS-1409802 and DARPA Fundamental Limits of
Learning grant W911NF-16-1-0551.

References

[1] A. Atkinson  A. Donev  and R. Tobias. Optimum Experimental Designs  With SAS. Oxford Statistical

Science Series. OUP Oxford  2007.

[2] H. Avron and C. Boutsidis. Faster subset selection for matrices and applications. SIAM J. Matrix Analysis

Applications  34(4):1464–1499  2013.

[3] E. R. Barnes. An algorithm for separating patterns by ellipsoids. IBM Journal of Research and Develop-

ment  26:759–764  1982.

[4] H. H. Bauschke  O. Güler  A. S. Lewis  and H. S. Sendov. Hyperbolic polynomials and convex analysis.

Canad. J. Math.  53(3):470–488  2001.

[5] R. Bhatia. Matrix Analysis. Springer  1997.
[6] R. Bhatia. Positive Deﬁnite Matrices. Princeton University Press  2007.
[7] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[8] K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science  10:273–304 

1995.

[9] D. A. Cohn. Neural network exploration using optimal experiment design. In Neural Networks  pages

679–686. Morgan Kaufmann  1994.

[10] R. Cominetti  W. F. Mascarenhas  and P. J. S. Silva. A newton’s method for the continuous quadratic

knapsack problem. Mathematical Programming Computation  6(2):151–169  2014.

[11] M. A. Davenport  A. K. Massimino  D. Needell  and T. Woolf. Constrained adaptive sensing.

Transactions on Signal Processing  64(20):5437–5449  2016.

IEEE

[12] T. A. Davis  W. W. Hager  and J. T. Hungerford. An efﬁcient hybrid algorithm for the separable convex

quadratic knapsack problem. ACM Trans. Math. Softw.  42(3):22:1–22:25  2016.

[13] H. Dette  V. B. Melas  and W. K. Wong. Locally d-optimal designs for exponential regression models.

Statistica Sinica  16(3):789–803  2006.

[14] A. N. Dolia  T. De Bie  C. J. Harris  J. Shawe-Taylor  and D. M. Titterington. The minimum volume cover-
ing ellipsoid estimation in kernel-deﬁned feature spaces. In European Conference on Machine Learning 
pages 630–637  2006.

[15] E. N. Dolia  N. M. White  and C. J. Harris. D-optimality for minimum volume ellipsoid with outliers. In
In Proceedings of the Seventh International Conference on Signal/Image Processing and Pattern Recog-
nition  pages 73–76  2004.

[16] G. Elfving. Optimum allocation in linear regression theory. Ann. Math. Statist.  23(2):255–262  1952.
[17] V. Fedorov. Theory of optimal experiments. Probability and mathematical statistics. Academic Press 

1972.

[18] Y. Gu and Z. Jin. Neighborhood preserving d-optimal design for active learning and its application to

terrain classiﬁcation. Neural Computing and Applications  23(7):2085–2092  2013.

[19] X. He. Laplacian regularized d-optimal design for active learning and its application to image retrieval.

IEEE Trans. Image Processing  19(1):254–263  2010.

[20] T. Horel  S. Ioannidis  and S. Muthukrishnan. Budget Feasible Mechanisms for Experimental Design 

pages 719–730. Springer Berlin Heidelberg  2014.

[21] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press  1985.
[22] D. A. Jackson and Y. Chen. Robust principal component analysis and outlier detection with ecological

data. Environmetrics  15(2):129–139  2004.

[23] T. Jain. Derivatives for antisymmetric tensor powers and perturbation bounds. Linear Algebra and its

Applications  435(5):1111 – 1121  2011.

[24] R. Jozsa and G. Mitchison. Symmetric polynomials in information theory: Entropy and subentropy.

Journal of Mathematical Physics  56(6)  2015.

[25] A. I. Khuri  B. Mukherjee  B. K. Sinha  and M. Ghosh. Design issues for generalized linear models: A

review. Statist. Sci.  21(3):376–399  2006.

[26] J. Kiefer. Optimal design: Variation in structure and performance under change of criterion. Biometrika 

62:277–288  1975.

9

[27] A. Krause  A. Singh  and C. Guestrin. Near-optimal sensor placements in gaussian processes: Theory 
efﬁcient algorithms and empirical studies. Journal of Machine Learning Research  9(Feb):235–284  2008.
[28] F. S. Lasheras  J. V. Vilán  P. G. Nieto  and J. del Coz Díaz. The use of design of experiments to improve a
neural network model in order to predict the thickness of the chromium layer in a hard chromium plating
process. Mathematical and Computer Modelling  52(78):1169 – 1176  2010.

[29] A. S. Lewis. Derivatives of spectral functions. Math. Oper. Res.  21(3):576–588  1996.
[30] I. G. Macdonald. Symmetric functions and Hall polynomials. Oxford university press  1998.
[31] A. J. Miller and N.-K. Nguyen. A fedorov exchange algorithm for d-optimal design. Applied Statistics 

43:669–677  1994.

[32] W. W. Muir. Inequalities concerning the inverses of positive deﬁnite matrices. Proceedings of the Edin-

burgh Mathematical Society  19(2):109113  1974.

[33] F. Pukelsheim. Optimal Design of Experiments. Society for Industrial and Applied Mathematics  2006.
[34] P. J. Rousseeuw and A. M. Leroy. Robust Regression and Outlier Detection. John Wiley & Sons  Inc. 

1987.

[35] G. Sagnol. Optimal design of experiments with application to the inference of trafﬁc matrices in large
networks: second order cone programming and submodularity. Theses  École Nationale Supérieure des
Mines de Paris  2010.

[36] A. Schein and L. Ungar. A-Optimality for Active Learning of Logistic Regression Classiﬁers  2004.
[37] A. Shpilka and A. Wigderson. Depth-3 arithmetic circuits over ﬁelds of characteristic zero. computational

complexity  10(1):1–27  2001.

[38] S. Silvey  D. Titterington  and B. Torsney. An algorithm for optimal designs on a design space. Commu-

nications in Statistics - Theory and Methods  7(14):1379–1389  1978.

[39] J. D. Smith and M. T. Thai. Breaking the bonds of submodularity: Empirical estimation of approximation

ratios for monotone non-submodular greedy maximization. CoRR  abs/1702.07002  2017.

[40] S. Sra and R. Hosseini. Conic Geometric Optimization on the Manifold of Positive Deﬁnite Matrices.

SIAM J. Optimization (SIOPT)  25(1):713–739  2015.

[41] P. Sun and R. M. Freund. Computation of minimum-volume covering ellipsoids. Operations Research 

52(5):690–706  2004.

[42] M. Todd. Minimum-Volume Ellipsoids. Society for Industrial and Applied Mathematics  2016.
[43] L. Vandenberghe  S. Boyd  and S.-P. Wu. Determinant maximization with linear matrix inequality con-

straints. SIAM J. Matrix Anal. Appl.  19(2):499–533  1998.

[44] Y. Wang  A. W. Yu  and A. Singh. On computationally tractable selection of experiments in regression

models  2016.

[45] Z. Wang  B. Moran  X. Wang  and Q. Pan. Approximation for maximizing monotone non-decreasing set

functions with a greedy method. J. Comb. Optim.  31(1):29–43  2016.

[46] T. C. Xygkis  G. N. Korres  and N. M. Manousakis. Fisher information based meter placement in distri-

bution grids via the d-optimal experimental design. IEEE Transactions on Smart Grid  PP(99)  2016.

[47] I.-C. Yeh. Modeling of strength of high-performance concrete using artiﬁcial neural networks. Cement

and Concrete Research  28(12):1797 – 1808  1998.

[48] Y. Yu. Monotonic convergence of a general algorithm for computing optimal designs. Ann. Statist.  38

(3):1593–1606  2010.

10

,Antoine Bordes
Nicolas Usunier
Alberto Garcia-Duran
Jason Weston
Oksana Yakhnenko
Zelda Mariet
Suvrit Sra