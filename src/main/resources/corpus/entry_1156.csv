2019,Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation,Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data  while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately  e.g.  learning different attributes' representations or using multiple attribute-specific decoders. However  it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems  we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically  we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text  then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore  we also show that our model can not only control the degree of transfer freely but also allow to transfer over multiple aspects at the same time.,Controllable Unsupervised Text Attribute Transfer

via Editing Entangled Latent Representation

Ke Wang

Hang Hua

Xiaojun Wan

Wangxuan Institute of Computer Technology  Peking University

The MOE Key Laboratory of Computational Linguistics  Peking University

{wangke17  huahang  wanxiaojun}@pku.edu.cn

Abstract

Unsupervised text attribute transfer automatically transforms a text to alter a spe-
ciﬁc attribute (e.g. sentiment) without using any parallel data  while simultaneously
preserving its attribute-independent content. The dominant approaches are trying
to model the content-independent attribute separately  e.g.  learning different at-
tributes’ representations or using multiple attribute-speciﬁc decoders. However  it
may lead to inﬂexibility from the perspective of controlling the degree of transfer
or transferring over multiple aspects at the same time. To address the above prob-
lems  we propose a more ﬂexible unsupervised text attribute transfer framework
which replaces the process of modeling attribute with minimal editing of latent
representations based on an attribute classiﬁer. Speciﬁcally  we ﬁrst propose a
Transformer-based autoencoder to learn an entangled latent representation for a dis-
crete text  then we transform the attribute transfer task to an optimization problem
and propose the Fast-Gradient-Iterative-Modiﬁcation algorithm to edit the latent
representation until conforming to the target attribute. Extensive experimental
results demonstrate that our model achieves very competitive performance on three
public data sets. Furthermore  we also show that our model can not only control
the degree of transfer freely but also allow transferring over multiple aspects at the
same time.1

1

Introduction

Text attribute transfer is a task of editing a text to alter speciﬁc attributes  such as sentiment  style
and tense [17]. It has drawn much attention in the natural language generation ﬁeld and it is a
requirement of a controllable natural language generation system. Given a source text with an
attribute (e.g.  positive sentiment)  the goal of the task is to generate a new text with a different
attribute (e.g.  negative sentiment). The generated text should meet the requirements: (i) maintaining
the attribute-independent content as the source text  (ii) conforming to the target attribute and (iii)
still maintaining the linguistic ﬂuency. However  due to the lack of parallel corpora exemplifying the
desired transformations between source and target attributes  most approaches [5  10  28  26  29  20 
40  41  38  17  16] are unsupervised and can only access non-parallel or monolingual data.
The dominant methods of unsupervised text attribute transfer are to separately model attribute and
content representations  such as using multiple attribute-speciﬁc decoders [5] or combining the
content representations with different attribute representations to decode texts with target attribute in
an adversarial [10  28  26  29  20  40  41  38  17] or non-adversarial [16] way. Nevertheless  such
practices have shortcomings. First  because they try to disentangle attribute and attribute-independent
content  this may undermine the integrity (i.e.  naturality) and result in poor readability of the

1Our codes are available at https://github.com/Nrgeup/controllable-text-attribute-transfer

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

generated sentences. Second  they require modeling each new attribute separately and thus lack
ﬂexibility and controllability.
To address the above problems  we propose a controllable unsupervised text attribute transfer frame-
work  which not only can ﬂexibly control the degree of transfer  but also can control transfer over
multiple aspects (e.g.  modiﬁcation of sentiments towards multiple aspects in a text) at the same
time. We achieve this goal by modifying the source text’s latent representation. Different from the
mainstream methods [10  28  26  29  20  40  41  38]  which learn the attribute and content represen-
tations separately and then decode the text with the target attribute  our latent representation is an
entangled representation of both attribute and content. Our model consists of a Transformer-based
autoencoder and an attribute classiﬁer. We ﬁrst train the autoencoder and the classiﬁer separately
and use the encoder to get the latent representation of the source text  and then we use our proposed
Fast-Gradient-Iterative-Modiﬁcation (FGIM) algorithm to iteratively edit the latent representation 
until the latent representation can be identiﬁed as target attribute by the classiﬁer. So that the target
text can be decoded from the modiﬁed latent representation by the decoder.
Our contributions are summarized as follows: (1) We build a Transformer-based autoencoder with
low reconstruction bias to learn an entangled latent representation for both attribute and content 
rather than treating them separately  so the integrity of the language expressions will not be damaged.
And the decoded target text will keep natural and ﬂuent (requirement iii). (2) We design a Fast-
Gradient-Iterative-Modiﬁcation (FGIM) algorithm by using a well-trained attribute classiﬁer to
provide an appropriate modiﬁcation direction for the latent representation  so we will modify the
latent representation as little as possible (requirement i) until conforming to the target attribute
(requirement ii). (3) Our method is capable of controlling text attribute in a more ﬂexible way  e.g. 
controlling the degree of attribute transfer and allowing to transfer over multiple aspects. (4) Our
method achieves very competitive performance on three datasets  especially in terms of text ﬂuency
and transfer success rate.

2 Related Work

Text Attribute Transfer: Text attribute transfer [39  22  11] is a type of conditional text generation
[14  31  1  4  10  37  20  36]  inspired by visual style transfer [12  7  44  18]. Recently  various
approaches have been proposed for handling textual data  mainly aiming at con the writing style of
sentences. However  it is hard to ﬁnd large scale datasets of parallel sentences written in different
styles [22]. Li et al. [17] released three small crowd-sourced text style transfer datasets for evaluation
purposes  where the sentiment had been swapped (between positive and negative) while preserving
the content. Controlled text attribute transfer from unsupervised data is thus the focus of recent
researches.
In general  related researches on unsupervised text attribute transfer are divided into two main
categories  phrase based and latent representation based. The phrase-based approaches [17  38]
explicitly separate attribute phrases from attribute-independent content phrases and replace them
with phrases of target attribute. But it may damage the overall consistency of the sentence and make
the generated text unnatural [33]. Another is to learn latent representations  and most solutions use
adversarial methods [10  28  26  5  29  20  40  41] to learn the latent representation of attribute and
content  and then pass through different attribute-speciﬁc decoders or combining attribute and content
latent representations to generate a variation of the input sentence with different attribute.
As mentioned above most approaches learn the attribute and content representations separately.
However  the generated text of such methods may have poor readability. Besides  Lample et al. [16]’s
experiment also shows there is no need to disentangle the attribute and content. Lample et al. [16]’s
work is most related to ours  but in Lample et al. [16]’s approach  it still needs an extra attribute
embedding to control the attribute of generated text. In this study  we try to explore an unsupervised
attribute transfer method that only needs to iteratively edit the entangled latent representation of
attribute and content.
Adversarial Samples Generation: Our work is also related to adversarial samples generation [8  42] 
which also uses the adversarial gradient to edit continuous samples to change classiﬁer’s predictions.
However  different from them  we edit on the latent space and then decode samples  rather than
directly editing the samples. In addition  we want to generate meaningful samples that match the
goals  rather than producing a small perturbation to fool the classiﬁer.

2

Figure 1: Model architecture.

Activation Maximization: Our work is also related to the activation maximization methods [3  24 
23  43]  which synthesize an input (e.g. an image) that highly activates a neuron. Although these
methods have been successfully applied to producing generative models  they are generally limited to
continuous spaces (e.g.  images  etc.) because the gradients are difﬁcult to pass in discrete spaces.
Different from these studies [24  23  43]  our method is the ﬁrst to apply activation maximization
to text generation  which consists of two aspects: 1) encoding discrete texts into contiguous latent
spaces with an autoencoder. 2) modifying the latent representations based on the direction that highly
actives the classiﬁer.

3 Model

3.1 Problem Formalization

We consider a dataset X  which has n sentences  and each sentence x is paired with an attribute
vector y. For example  we can use y = (ytense  ysentiment) to represent both “tense” and “sentiment”
attributes of a sentence  or use y = (yappearance  yaroma  ypalate  ytaste  yoverall) to represent the
sentiment types or values on ﬁve aspects of a beer review. In most cases  y actually contains only
one attribute  e.g.  the overall sentiment. In general  given source text x and target attribute y(cid:48) 
text attribute transfer seeks to generate ﬂuent target text ˆx(cid:48)  which preserves the original attribute-
independent content but conforms to the target attribute y(cid:48).

3.2 Model Overview

The architecture of our proposed model is depicted in Figure 1. The whole framework can be divided
into three sub-models: an encoder Eθe which encodes the text x into a latent representation z  a
decoder Dθd which decodes text ˆx from z  and an attribute classiﬁer Cθc that classiﬁes attribute of
the latent representation z. That is:

z = Eθe (x); y = Cθc (z); ˆx = Dθd (z).

(1)

Formally  in this work  we formulate the text attribute transfer task as an optimization problem.
More speciﬁcally  we ﬁrst propose a Transformer-based autoencoder to learn a latent representation
z = Eθ(x) of a discrete text  which is entangled with content and attribute. Then  the task of ﬁnding
the target text ˆx(cid:48) with target attribute y(cid:48) can be formulated as the following optimization problem:
(2)

ˆx(cid:48) = Dθd (z(cid:48)) where z(cid:48) = argminz∗||z∗ − Eθe (x)|| s.t. Cθc(z∗) = y(cid:48).

3

Add&NormFeed ForwardMulti-HeadAttentionEmbeddingxInputText GRUOutput Text(Shifted right)LinearSoftmaxReluReluSigmoidEncoderDecoderClassifierLatent SpaceOutputTextAttributeEncoderDecoderClassifierˆxzy'yˆ'x'zI feel extremely uncomfortable there   and the buffet is not worth its price.PositionLatentSpaceSumI enjoy eating there  the buffet is great  value for money!I feel uncomfortable there   and the buffet is not worth its price.I feel uncomfortable there   but the buffet is worth its price.I feel comfortable there   and the buffet is worth its price.I feel very comfortable there   and the buffet's price is also great.To solve this problem  we propose the Fast-Gradient-Iterative-Modiﬁcation algorithm (FGIM)  which
modiﬁes z based on the gradient of back-propagation by linearizing the attribute classiﬁer’s loss
function on z.
In brief  we transform the original problem to ﬁnd an optimal representation z(cid:48) that conforms to the
target attribute y(cid:48) (requirement ii) and is “closest” to z (requirement i)  then we decode the target
text ˆx(cid:48) from z(cid:48) (requirement iii).
Transformer-based Autoencoder: One of the key points of our model is to build an autoencoder
with low reconstruction bias. Inspired by the superiority of Transformer [34] on many text generation
tasks [34  27  2]  we propose a Transformer-based autoencoder with low reconstruction bias to
learn the latent representation of source text. We ﬁrst pass source text x through the original
Transformer’s encoder (Etransf ormer) [34] and get the intermediate representations U. Because
the Transformer architecture is suboptimal for language modelling itself  neither self-attention nor
positional embedding in the Transformer is able to effectively incorporate the word-level sequential
context [35]. So we add extra positional embeddings H [34] to U. Next we pass U through
a GRU layer with self-attention to further utilize the sequence information. Then we apply a
sigmoid activation function on the GRU hidden representations and sum them to get the ﬁnal latent
representation z (Figure 1):

|x|(cid:88)

z = Eθe (x) = Sum(Sigmoid(GRU (U + H)))  where U = Etransf ormer(x).

(3)
Finally the target text ˆx can be decoded from z. During the autoencoder optimization process  we
adopt the label smoothing regularization [32] to improve the performance of the model. Hence  our
autoencoder reconstruction loss is:
Lae(Dθd (Eθe(x))  x) = Lae(Dθd (z)  x) = −
(cid:80)v

where v denotes the vocabulary size  and ε denotes the smoothing parameter. The last item
( ε
i=1 log(pi)) is the introduction of noise to relax our conﬁdence in the label. For each time step 
v
p and ¯p are the predicted probability distribution and the ground truth probability distribution over
the vocabulary  respectively.
Attribute Classiﬁer for Latent Representation: In our framework  we use an attribute classiﬁer to
provide the direction (gradient) for editing the latent representation so that it conforms to the target
attribute. Our classiﬁer is two stacks of linear layer with sigmoid activation function  and the attribute
classiﬁcation loss is:

((1 − ε)

v(cid:88)

i=1

v(cid:88)

i=1

¯pi log(pi) +

log(pi)) 

(4)

ε
v

|q|(cid:88)

Lc(Cθc(z)  y) = −

¯qi log qi 

(5)

where q represents the predicted attribute probability distribution and ¯q is the true attribute probability
distribution. Additionally  in practice we ﬁnd it beneﬁts the results to optimize the above two loss
functions separately  rather than training them jointly.

i=1

3.3 Fast Gradient Iterative Modiﬁcation Algorithm

The goal of editing the latent representation is to transfer from the source attribute to the target
attribute. That is to ﬁnd an optimal representation z(cid:48)  which is “closest” to z in the latent space
and conforms to the target attribute y(cid:48). Inspired by Goodfellow et al. [8]  we ascertain the fastest
modiﬁcation direction with the gradient back-propagation of attribute classiﬁcation loss calculation.
More speciﬁcally  to get an optimal z(cid:48)  we ﬁrst use z as the input of Cθc and use y(cid:48) as the label
to calculate the gradient to z. Then we modify z in this direction iteratively until we get a z(cid:48) that
can be identiﬁed as the target attribute y(cid:48) by the classiﬁer Cθc. Note that the gradient is computed
with respect to the input z  instead of the model parameters θc. In other words  we use the gradient
to change z rather than change model parameters θc. In each iteration  the newly modiﬁed latent
representation z∗ can be formulated as:

(6)
where wi is the modiﬁcation weight used for controlling the degree of transfer. Contrary to Goodfellow
et al. [8]  we want a modiﬁcation to make the latent representation more different in attribute  but not

z∗ = z − wi∇zLc(Cθc (z)  y(cid:48)) 

4

a tiny adversarial perturbation to fool the classiﬁer. Thus we propose a Dynamic-weight-initialization
method to allocate the initial modiﬁcation weight wi in each trial process. More speciﬁcally  we give
a set of weights w = {wi}  and our algorithm will dynamically try each weight in w from small to
large until we get our target latent representation z(cid:48). This will prevent the modiﬁcation of z from
falling into local optimum. In each trial process  the initial weight wi ∈ w will iteratively decay by
multiplying a ﬁxed decay coefﬁcient λ. Our algorithm is shown in Alg 1.

z∗ = z − wi∇zLc(Cθc (z)  y(cid:48));
for s-steps do

Algorithm 1 Fast Gradient Iterative Modiﬁcation Algorithm.
Input: Original latent representation z; Well-trained attribute classiﬁer Cθc; A set of weights w = {wi};
Decay coefﬁcient λ; Target attribute y(cid:48); Threshold t;
Output: An optimal modiﬁed latent representation z(cid:48);
1: for each wi ∈ w do
2:
3:
4:
5:
6:
7:
end for
8:
9: end for
10: return z(cid:48);

if |y(cid:48) − Cθc (z∗)| < t then z(cid:48) = z∗ ; Break;
end if
wi = λwi;
z∗ = z∗ − wi∇z∗Lc(Cθc (z∗)  y(cid:48));

Our Fast Gradient Iterative Modiﬁcation algorithm has the following advantages:
Attribute Transfer over Multiple Aspects: Compared to the methods which use extra attribute
embedding [28  10] or multi-decoder [5]  our proposed framework transfers the source text’s attribute
into any target attribute by using only the classiﬁer Cθc and the target attribute y. One of the
advantages of our model is the ﬂexibility to design the goals of the attribute classiﬁer to achieve the
attribute transfer over multiple aspects  which no other models have attempted.
Transfer Degree Control: Our model can use different modiﬁcation weight in w to control the
degree of modiﬁcation  thus achieving the control of the degree of attribute transfer  which is never
considered by other models.

4 Experiment

4.1

Implementation

In our Transformer-based autoencoder  the embedding size  the latent size and the dimension size
of self-attention are all set to 256. The hidden size of GRU and batch-size are set to 128. The inner
dimension of Feed-Forward Networks (FFN) in Transformer is set to 1024. Besides  each of the
encoder and decoder is stacked by two layers of Transformer. The smoothing parameter ε is set to
0.1. For the classiﬁer  the dimensions of the two linear layers are 100 and 50. For our FGIM  the
weight set w  the threshold t and the decay coefﬁcient λ are set to {1.0  2.0  3.0  4.0  5.0  6.0}  0.001
and 0.9  respectively. The optimizer we use is Adam [15] and the initial learning rate is 0.001. We
implement our model based on Pytorch 0.4.

4.2 Datasets

We use datasets provided in Li et al. [17] for sentiment and style transfer experiments  where the test
sets contain human-written references.
Yelp: This dataset consists of Yelp reviews for ﬂipping sentiment. We consider reviews with a rating
above three as positive samples and those below three as negative ones;
Amazon: This dataset consists of product reviews from Amazon [9] for ﬂipping sentiment. Similar
to Yelp  we label the reviews with a rating higher than three as positive and less than three as negative;
Captions: This dataset consists of image captions [6] for changing between romantic and humorous
styles. Each caption is labeled as either romantic or humorous.

5

It is worth noting that there are only manual reference answers on the test set. The statistics of the
above three datasets are shown in Table 1.

Table 1: Statistics for Yelp  Amazon  Captions datasets.

Yelp

Dataset

Styles
Negative
Positive
Negative
Amazon
Positive
Captions Humorous
Romantic

#Train
180 000
270 000
277 000
278 000
6 000
6 000

#Dev
2 000
2 000
1 015
985
300
300

#Test
500
500
500
500
300
300

#Vocab Max-Length Mean-Length

9 640

58 991

8 693

15

34

20

8.89

14.84

14.04

4.3 Sentiment and Style Transfer Results

We compare our model with eight state-of-the-art models  including CrossAlign [28]  MultiDec [5] 
StyleEmb [5]  CycleRL [38]  BackTrans [26]  RuleBase [17]  DelRetrGen [17] and UnsupMT [41].
Automatic Evaluation: Following previous works [28  17  41]  we evaluate models’ performance
from three aspects: 1) Acc: we measure the attribute transfer accuracy of the generated texts with a
fastText classiﬁer [13] trained on the training data; 2) BLEU [25]: we use the multi-BLEU2 metric to
calculate the similarity between the generated sentences and the references written by human; 3) PPL:
we measure the ﬂuency of the generated sentences by the perplexity calculated with the language
model trained on the respective corpus. The language model is borrowed from the language modeling
toolkit - SRILM [30]. The results are shown in Table 2.
From the results  we can see that: 1) Phrase-based methods (e.g.  RuleBase [17]) are not good at
keeping ﬂuency  even if they have achieved high BLEU scores. 2) The attribute accuracy and BLEU
scores of the sentences generated by our model are promisingly high  indicating that our model
can effectively modify attributes without changing too much attribute-independent content. 3) The
sentences generated by our model are more ﬂuent than that of baseline models. Overall  our model
performs better than all baseline models over all metrics on the Yelp and Amazon datasets  and
outperforms most of the baseline models on the Captions datasets.
Human Evaluation: Further  we conduct a human evaluation to evaluate the quality of generated
sentences more accurately. For each dataset  we randomly extract 200 samples (i.e.  100 sentences
generated for each target attribute  e.g.  positive → negative and negative → positive  or humorous
→ romantic and romantic → humorous.) and then hire three workers on Amazon Mechanical Turk
(AMT) to score each of the items from three aspects: the attribute accuracy (Att)  the retainment of
content (Con) and the ﬂuency of sentences (Gra). Moreover  we divided the test samples of each
model on each task into the same number of small sets  and the same person annotated the same task
for all the models. The scores range from 1 to 5  and 5 is the best. The ﬁnal average scores are shown
in Table 3.

2https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl

Table 2: Automatic evaluation results. ↓ means the smaller the better. We underline the results of our
model and bold the best results.

Methods

CrossAlign [28]

MultiDec [5]
StyleEmb [5]
CycleRL [38]
BackTrans [26]
RuleBase [17]
DelRetrGen [17]
UnsupMT [41]

Ours

Acc
72.3%
50.2%
10.2%
53.6%
93.4%
80.3%
88.8%
95.2%
95.4%

Yelp
BLEU PPL ↓
9.1
50.8
84.5
14.5
47.9
21.1
98.2
18.8
49.5
2.5
22.6
66.6
49.6
16.0
53.9
22.8
24.6
46.2

Amazon
BLEU PPL ↓
Acc
1.9
66.2
70.3%
60.3
9.1
67.3%
60.1
15.1
43.6%
183.2
14.4
52.3%
48.3
1.5
84.6%
33.6
52.1
67.8%
55.4
29.3
51.2%
57.9
84.2%
33.9
85.3% 34.1
47.4

Captions
BLEU PPL ↓
Acc
1.8
69.8
78.3%
60.2
6.6
68.3%
57.1
8.8
56.2%
50.3
5.8
45.2%
68.3
1.6
78.3%
19.2
35.6
85.3%
33.4
90.4%
12.0
95.5% 12.7
31.2
23.7
92.3%
17.6

6

Table 3: Human evaluation results. The kappa coefﬁcient of the three workers is 0.56 ∈ (0.41  0.60) 
which means that the consistency is moderate.

Methods

CrossAlign [28]

MultiDec [5]
StyleEmb [5]
CycleRL [38]
BackTrans [26]
RuleBase [17]
DelRetrGen [17]
UnsupMT [41]

Ours

2.8
3.1
3.0
3.0
2.4
3.2
2.9
3.3
3.5

2.7
2.6
3.1
3.2
2.6
3.6
3.7
3.7
4.0

Yelp

Amazon

Captions

Att Con Gra Att Con Gra Att Con Gra
2.5
3.0
2.9
2.3
3.0
2.6
2.8
2.9
2.8
2.0
3.4
3.0
3.2
3.2
3.3
3.2
3.6
3.5

2.7
2.9
2.8
3.1
2.8
3.7
3.6
4.0
4.2

2.5
2.6
3.1
2.9
2.8
3.1
2.9
2.8
3.4

3.1
2.9
3.2
3.2
3.4
3.8
3.4
3.7
4.1

3.3
2.7
2.9
3.2
2.9
3.4
3.0
3.5
3.8

2.1
2.5
2.3
2.5
2.4
2.6
2.5
2.8
3.5

As can be seen from the results  our model outperforms baselines by a wide margin on all metrics 
which demonstrates the effectiveness of our proposed Transformer-based autoencoder and FGIM
algorithm. Moreover  texts generated by our model have better ﬂuency and attribute accuracy.
Interestingly  in our experiments  the results of manual and automatic evaluations are not always
consistent with each other  which deserves further study. But even in the eyes of humans  our model
excels in the preservation of content  indicating that our model loses less information while achieving
the goal of attribute transfer. We show some examples generated by the models in Supplementary
Material.

4.4 Multi-Aspect Sentiment Transfer

In order to evaluate the capability of multi-aspect sentiment transfer of our model  we use a Beer-
Advocate dataset  which was scraped from Beer Advocate [19]. Beer-Advocate is a large online
review community boasting 1 586 614 reviews of 66 051 distinct items composed by 33 387 users.
Each review is accompanied by ﬁve numerical ratings over ﬁve aspects of "appearance"  "aroma" 
"palate"  "taste" and "overall" (here we simply treat "overall" as a special aspect)  and each rating is
normalized into [0  1].
As far as we know  there are no previous works investigating aspect-based sentiment transfer  because
it is difﬁcult to disentangle sentiment attributes from multiple different aspects or learn so many
different combinations of aspect-based sentiments. However  our model can achieve this goal
by training the corresponding aspect-based sentiment classiﬁer. We train our Transformer-based
autoencoder on this dataset using Lae  and then we train our aspect-based sentiment classiﬁer by
the new ﬁve-dimension attribute vector y = {yappearance  yaroma  ypalate  ytaste  yoverall}  which
means ﬁve sentiment values of a beer review towards ﬁve aspects. For evaluation  we randomly
sample 300 items to perform the multi-aspect sentiment transfer. For the sake of simplicity  we aim to
transform 150 texts into texts with all negative sentiments over ﬁve aspects y = (0.0  0.0  0.0  0.0  0.0)
and transform the other 150 texts into texts with all positive sentiments y = (1.0  1.0  1.0  1.0  1.0).
We evaluate the sentiment accuracy (Acc) of generated texts towards different aspects by a FastText
classiﬁer [13] trained on the training data. Moreover  we employ three workers on AMT to score
each of them according to sentiment accuracy (Att)  preservation of content (Con) and ﬂuency (Gra) 
the same as before.
The results are shown in Table 4  and some cases are shown in Supplementary Material. We see
that the achieved sentiment accuracy is high  which means that our model can perform sentiment
transfer over multiple aspects at the same time. Considering the results of human evaluation  our
model has good ﬂuency and preservation of content when performing sentiment transferring over
multiple aspects. To the best of our knowledge  this is the ﬁrst work investigating the aspect-based
attribute transfer task.

4.5 Transfer Degree Control

As is mentioned before  our model can use modiﬁcation weight in w to control the degree of attribute
transfer. Further  We want to have an insight into the impact of different w on the modiﬁcation results.

7

Table 4: Results for multi-aspect attribute transfer. The kappa coefﬁcient of the three workers is 0.67
∈ (0.61  0.80)  which means that the consistency is substantial.

Aspects
Appearance
Aroma
Palate
Taste
Overall

Acc
90.2% 3.2
89.3% 3.4
91.2% 3.1
88.2% 3.4
87.3% 3.6

Att Con Gra
3.8
3.7
3.7
3.6
3.8

3.5
3.9
3.8
3.7
4.0

Figure 2: Inﬂuence of the modiﬁcation weight w.

We let w contain only one value and then let the value change from small to large  the visualization
results are shown in Figure 2. Some conclusions can be concluded from the results: 1) As the value
in w increases  the attribute of the generated sentence becomes more and more accurate. 2) However 
the BLUE score ﬁrst increases and then decreases  we argue that this is because the attribute of
some human-written references is not obvious. 3) PPL has not changed so much  which proves the
effectiveness of our autoencoder with low reconstruction bias  and our latent representation editing
method does not damage the ﬂuency and naturalness of the sentence.
We also show two examples in Yelp test dataset in Table 5 (more cases are shown in Supplementary
Material). From the table  we can see that as the value in w increases  the target attribute of the
generated sentence becomes more obvious. To the best of our knowledge  our model is the ﬁrst one
that can control the degree of attribute transfer.

4.6 Latent Representation Modiﬁcation Study

In order to illustrate the latent representation editing result more clearly  we use T-SNE [21] to
visualize the latent representation in the modiﬁcation process. More speciﬁcally  we present latent
representations of Yelp’s test dataset (source)  and the modiﬁed latent representations with different
transfer degree weight in w  as shown in Figure 3.

Table 5: Examples of generation with different modiﬁcation weight w.

Source:
Human:
w = {1}
w = {2}
w = {3}
w = {4}
w = {5}
w = {6}

Positive ->Negative
really good service and food .
the service was bad
really good service and food .
very good service and food .
very good food but service is terrible !
not good food and service is terrible !
bad service and food !
very terrible service and food !

Negative ->Positive
it is n’t terrible   but it is n’t very good either .
it is n’t perfect   but it is very good .
it is n’t terrible   but it is n’t very good either .
it is n’t terrible   but it is n’t very good delicious either .
it is n’t terrible   but it is very good delicious either .
it is n’t terrible   but it is very good and delicious .
it is n’t terrible   but it is very good and delicious
appetizer .
it is excellent   and it is very good and delicious well .

8

1234567w0.700.750.800.850.900.95AccYelpAmazonCaptions1234567w1520253035BLEU1234567w253035404550PPLFrom Figure 3  we can see that the original representations of positive texts and negative texts are
mixed together in the latent space. However  as the value in w increases  the distinction between the
modiﬁed latent representations of positive texts and negative texts becomes more and more obvious 
which can also prove the effectiveness of using w to control the degree of attribute transfer.

Figure 3: Visualization of representations with different modiﬁcation weight w.

5 Conclusion and Discussion

In this work  we present a controllable unsupervised text attribute transfer framework  which can
edit the entangled latent representation instead of modeling attribute and content separately. To the
best of our knowledge  this is the ﬁrst one that can not only control the degree of transfer freely
but also perform sentiment transfer over multiple aspects at the same time. Nevertheless  we ﬁnd
that there may be some failure cases  such as learning some attribute-independent data bias or just
adding phrases that match the target attribute but are useless (some cases are shown in Supplementary
Material). Therefore we will try to further improve the performance in the future.

Acknowledgments

This work was supported by National Natural Science Foundation of China (61772036) and Key
Laboratory of Science  Technology and Standard in Press Industry (Key Laboratory of Intelligent
Press Media Technology). We appreciate the anonymous reviewers for their helpful comments.
Xiaojun Wan is the corresponding author.

References
[1] Samuel R. Bowman  Luke Vilnis  Oriol Vinyals  Andrew M. Dai  Rafal Józefowicz  and Samy Bengio.

Generating sentences from a continuous space. In CoNLL  pages 10–21  2016.

[2] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. BERT: pre-training of deep

bidirectional transformers for language understanding. CoRR  abs/1810.04805  2018.

[3] Dumitru Erhan  Yoshua Bengio  Aaron Courville  and Pascal Vincent. Visualizing higher-layer features of

a deep network. University of Montreal  1341(3):1  2009.

[4] Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. CoRR 

abs/1707.02633  2017.

[5] Zhenxin Fu  Xiaoye Tan  Nanyun Peng  Dongyan Zhao  and Rui Yan. Style transfer in text: Exploration

and evaluation. In AAAI  pages 663–670  2018.

[6] Chuang Gan  Zhe Gan  Xiaodong He  Jianfeng Gao  and Li Deng. Stylenet: Generating attractive visual

captions with styles. In CVPR  pages 955–964  2017.

[7] Leon A. Gatys  Alexander S. Ecker  and Matthias Bethge. Image style transfer using convolutional neural

networks. In CVPR  pages 2414–2423  2016.

9

50050Source1001020positivenegative50050w={2}201510505101550050w={4}302010010203025025w={6}302010010203040[8] Ian J. Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial

examples. In ICLR  2015.

[9] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with

one-class collaborative ﬁltering. In WWW  pages 507–517  2016.

[10] Zhiting Hu  Zichao Yang  Xiaodan Liang  Ruslan Salakhutdinov  and Eric P. Xing. Toward controlled

generation of text. In ICML  pages 1587–1596  2017.

[11] Harsh Jhamtani  Varun Gangal  Eduard H. Hovy  and Eric Nyberg. Shakespearizing modern language

using copy-enriched sequence-to-sequence models. CoRR  abs/1707.01161  2017.

[12] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer and

super-resolution. In ECCV  pages 694–711  2016.

[13] Armand Joulin  Edouard Grave  Piotr Bojanowski  and Tomas Mikolov. Bag of tricks for efﬁcient text

classiﬁcation. In ACL  pages 427–431  April 2017.

[14] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models.

1700–1709  2013.

In EMNLP  pages

[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR  2015.

[16] Guillaume Lample  Sandeep Subramanian  Eric Smith  Ludovic Denoyer  Marc’Aurelio Ranzato  and

Y-Lan Boureau. Multiple-attribute text rewriting. In ICLR  2019.

[17] Juncen Li  Robin Jia  He He  and Percy Liang. Delete  retrieve  generate: a simple approach to sentiment

and style transfer. In NAACL-HLT  pages 1865–1874  2018.

[18] Jing Liao  Yuan Yao  Lu Yuan  Gang Hua  and Sing Bing Kang. Visual attribute transfer through deep

image analogy. ACM Trans. Graph.  36(4):120:1–120:15  2017.

[19] Zachary C Lipton  Sharad Vikram  and Julian McAuley. Generative concatenative nets jointly learn to

write and classify reviews. CoRR  abs/1511.03683  2015.

[20] Lajanugen Logeswaran  Honglak Lee  and Samy Bengio. Content preserving text generation with attribute

controls. In NeurIPS  pages 5108–5118  2018.

[21] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning

research  9(Nov):2579–2605  2008.

[22] Igor Melnyk  Cícero Nogueira dos Santos  Kahini Wadhawan  Inkit Padhi  and Abhishek Kumar. Improved

neural text attribute transfer with non-parallel data. CoRR  abs/1711.09395  2017.

[23] Anh Nguyen  Jeff Clune  Yoshua Bengio  Alexey Dosovitskiy  and Jason Yosinski. Plug & play generative
networks: Conditional iterative generation of images in latent space. In CVPR  pages 3510–3520  2017.

[24] Anh Mai Nguyen  Alexey Dosovitskiy  Jason Yosinski  Thomas Brox  and Jeff Clune. Synthesizing the
preferred inputs for neurons in neural networks via deep generator networks. In NeurIPS  pages 3387–3395 
2016.

[25] Kishore Papineni  Salim Roukos  Todd Ward  and Wei-Jing Zhu. Bleu: a method for automatic evaluation

of machine translation. In ACL  pages 311–318  2002.

[26] Shrimai Prabhumoye  Yulia Tsvetkov  Ruslan Salakhutdinov  and Alan W. Black. Style transfer through

back-translation. In ACL  pages 866–876  2018.

[27] Alec Radford  Karthik Narasimhan  Tim Salimans  and Ilya Sutskever. Improving language understanding

by generative pre-training. 2018.

[28] Tianxiao Shen  Tao Lei  Regina Barzilay  and Tommi S. Jaakkola. Style transfer from non-parallel text by

cross-alignment. In NeurIPS  pages 6833–6844  2017.

[29] Ayush Singh and Ritu Palod. Sentiment transfer using seq2seq adversarial autoencoders. CoRR 

abs/1804.04003  2018.

[30] Andreas Stolcke. SRILM - an extensible language modeling toolkit. In INTERSPEECH  2002.

[31] Ilya Sutskever  Oriol Vinyals  and Quoc V. Le. Sequence to sequence learning with neural networks. In

NeurIPS  pages 3104–3112  2014.

10

[32] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jonathon Shlens  and Zbigniew Wojna. Rethinking

the inception architecture for computer vision. In CVPR  pages 2818–2826  2016.

[33] Alexey Tikhonov and Ivan P. Yamshchikov. What is wrong with style transfer for texts? CoRR 

abs/1808.04365  2018.

[34] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N. Gomez  Lukasz

Kaiser  and Illia Polosukhin. Attention is all you need. In NeurIPS  pages 6000–6010  2017.

[35] Chenguang Wang  Mu Li  and Alexander J. Smola. Language models with transformers. CoRR 

abs/1904.09408  2019.

[36] K. Wang and X. Wan. Automatic generation of sentimental texts via mixture adversarial networks. Artif.

Intell.  275:540–558  2019.

[37] Ke Wang and Xiaojun Wan. Sentigan: Generating sentimental texts via mixture adversarial networks. In

IJCAI  pages 4446–4452  2018.

[38] Jingjing Xu  Xu Sun  Qi Zeng  Xiaodong Zhang  Xuancheng Ren  Houfeng Wang  and Wenjie Li. Unpaired
sentiment-to-sentiment translation: A cycled reinforcement learning approach. In ACL  pages 979–988 
2018.

[39] Wei Xu  Alan Ritter  Bill Dolan  Ralph Grishman  and Colin Cherry. Paraphrasing for style. In COLING 

pages 2899–2914  2012.

[40] Zichao Yang  Zhiting Hu  Chris Dyer  Eric P. Xing  and Taylor Berg-Kirkpatrick. Unsupervised text style

transfer using language models as discriminators. In NeurIPS  pages 7298–7309  2018.

[41] Zhirui Zhang  Shuo Ren  Shujie Liu  Jianyong Wang  Peng Chen  Mu Li  Ming Zhou  and Enhong Chen.

Style transfer as unsupervised machine translation. CoRR  abs/1808.07894  2018.

[42] Zhengli Zhao  Dheeru Dua  and Sameer Singh. Generating natural adversarial examples. In ICLR  2018.

[43] Zhiming Zhou  Han Cai  Shu Rong  Yuxuan Song  Kan Ren  Weinan Zhang  Jun Wang  and Yong Yu.

Activation maximization generative adversarial nets. In ICLR  2018.

[44] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A. Efros. Unpaired image-to-image translation using

cycle-consistent adversarial networks. In ICCV  pages 2242–2251  2017.

11

,Ke Wang
Hang Hua
Xiaojun Wan