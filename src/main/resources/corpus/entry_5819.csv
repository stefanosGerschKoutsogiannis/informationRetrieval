2018,Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise,The growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling for large datasets  non-expert labeling  and label corruption by data poisoning adversaries. In the latter case  corruptions may be arbitrarily bad  even so bad that a classifier predicts the wrong labels with high confidence. To protect against such sources of noise  we leverage the fact that a small set of clean labels is often easy to procure. We demonstrate that robustness to label noise up to severe strengths can be achieved by using a set of trusted data with clean labels  and propose a loss correction that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks  we experiment with various label noises at several strengths  and show that our method significantly outperforms existing methods.,Using Trusted Data to Train Deep Networks on

Labels Corrupted by Severe Noise

Dan Hendrycks∗

University of California  Berkeley
hendrycks@berkeley.edu

Mantas Mazeika∗
University of Chicago
mantas@ttic.edu

Duncan Wilson

Foundational Research Institute
duncanw@nevada.unr.edu

Kevin Gimpel

Toyota Technological Institute at Chicago

kgimpel@ttic.edu

Abstract

The growing importance of massive datasets used for deep learning makes robust-
ness to label noise a critical property for classiﬁers to have. Sources of label noise
include automatic labeling  non-expert labeling  and label corruption by data poi-
soning adversaries. Numerous previous works assume that no source of labels can
be trusted. We relax this assumption and assume that a small subset of the training
data is trusted. This enables substantial label corruption robustness performance
gains. In addition  particularly severe label noise can be combated by using a set of
trusted data with clean labels. We utilize trusted data by proposing a loss correction
technique that utilizes trusted examples in a data-efﬁcient manner to mitigate the
effects of label noise on deep neural network classiﬁers. Across vision and natural
language processing tasks  we experiment with various label noises at several
strengths  and show that our method signiﬁcantly outperforms existing methods.

1

Introduction

Robustness to label noise is set to become an increasingly important property of supervised learning
models. With the advent of deep learning  the need for more labeled data makes it inevitable that
not all examples will have high-quality labels. This is especially true of data sources that admit
automatic label extraction  such as web crawling for images  and tasks for which high-quality labels
are expensive to produce  such as semantic segmentation or parsing. Additionally  label corruption
may arise in data poisoning [10  24]. Both natural and malicious label corruptions tend to sharply
degrade the performance of classiﬁcation systems [30].
Most prior work on label corruption robustness assumes that all training data are potentially corrupted.
However  it is usually the case that a number of trusted examples are available. Trusted data are
gathered to create validation and test sets. When it is possible to curate trusted data  a small set of
trusted data could be created for training. We depart from the assumption that all training data are
potentially corrupted by assuming that a subset of the training is trusted. In turn we demonstrate
that having some amount of trusted training data enables signiﬁcant robustness gains.
To leverage the additional information from trusted labels  we propose a new loss correction and
empirically verify it on a number of vision and natural language datasets with label corruption.
Speciﬁcally  we demonstrate recovery from extremely high levels of label noise  including the dire
case when the untrusted data has a majority of its labels corrupted. Such severe corruption can occur
in adversarial situations like data poisoning  or when the number of classes is large. In comparison to

∗Equal contribution.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

loss corrections that do not employ trusted data [18]  our method is signiﬁcantly more accurate in
problem settings with moderate to severe label noise. Relative to a recent method which also uses
trusted data [11]  our method is far more data-efﬁcient and generally more accurate. These results
demonstrate that systems can weather label corruption with access only to a small number of gold
standard labels. Experiment code is available at https://github.com/mmazeika/glc.

2 Related Work

The performance of machine learning systems reliant on labeled data has been shown to degrade
noticeably in the presence of label noise [17  19].
In the case of adversarial label noise  this
degradation can be even worse [20]. Accordingly  modeling  correcting  and learning with noisy
labels has been well studied [16  1  3].
The methods of [15  9  18  25] allow for label noise robustness by modifying the model’s architecture
or by implementing a loss correction. Unlike Mnih and Hinton [15] who focus on binary classiﬁcation
of aerial images and Larsen et al. [9] who assume symmetric label noise  [18  25] consider label noise
in the multi-class problem setting with asymmetric noise.
Sukhbaatar et al. [25] introduce a stochastic matrix measuring label corruption  note its inability
to be calculated without access to the true labels  and propose a method of forward loss correction.
Forward loss correction adds a linear layer to the end of the model and the loss is adjusted accordingly
to incorporate learning about the label noise. Patrini et al. [18] also make use of the forward loss
correction mechanism  and propose an estimate of the label corruption estimation matrix which relies
on strong assumptions  and does not make use of clean labels.
Contra [25  18]  we make the assumption that during training the model has access to a small set
of clean labels. See Charikar  Steinhardt  and Valiant [2] for a general analysis of this assumption.
This assumption has been leveraged by others for the purpose of label noise robustness  most notably
[26  11  27  21]. Veit et al. [26] use human-veriﬁed labels to train a label cleaning network by
estimating the residuals between the noisy and clean labels in a multi-label classiﬁcation setting. In
the multi-class setting that we focus on in this work  Li et al. [11] propose distilling the predictions of
a model trained on clean labels into a second network trained on these predictions and the noisy labels.
Our work differs from these two in that we do not train neural networks on the clean labels alone.

3 Gold Loss Correction

this estimate is obtained  we use it to train a modiﬁed
classiﬁer from which we recover an estimate of the desired
conditional distribution p(y | x). We call this method the
Gold Loss Correction (GLC)  so named because we make use of trusted or gold standard labels.

Figure 1: A label corruption matrix (top
left) and three matrix estimates for a
corrupted CIFAR-10 dataset. Entry Cij
is the probability that a label of class i
is corrupted to class j  or symbolically

Cij = p((cid:101)y = j|y = i).

We are given an untrusted dataset (cid:101)D of u examples (x (cid:101)y) 
distribution p((cid:101)y | y  x). We are also given a trusted dataset

and we assume that these examples are potentially cor-
rupted examples from the true data distribution p(x  y)
with K classes. Corruption is speciﬁed by a label noise
D of t examples drawn from p(x  y)  where t/u (cid:28) 1.
We refer to t/u as the trusted fraction. Concretely  a web
scraper labeling images from metadata may produce an un-
trusted set  while expert-annotated examples would form
a trusted dataset and be a gold standard.
We explore two avenues of utilizing D to improve this
approach. The ﬁrst directly uses the trusted data while
training the ﬁnal classiﬁer. As this could be applied to ex-
isting methods  we run ablations to demonstrate its effect.
The second avenue uses the additional information con-
ferred by the clean labels to better model the label noise
for use in a corrected classiﬁer.
Our method makes use of D to estimate the K × K matrix

of corruption probabilities Cij = p((cid:101)y = j | y = i). Once

2

TrueGLC (Ours)ForwardConfusion Matrix0.00.20.40.60.81.0(cid:90)

p(x |(cid:101)y  y) dx = p((cid:101)y | y).

Estimating The Corruption Matrix. First  we train a classiﬁer(cid:98)p((cid:101)y | x) on (cid:101)D. Let(cid:101)y and y be in
the set of possible labels. To estimate the probability p((cid:101)y | y)  we use the identity p((cid:101)y | y  x)p(x |
y) = p((cid:101)y | y)p(x |(cid:101)y  y). Integrating over all x gives us
(cid:90)
p((cid:101)y | y  x)p(x | y) dx = p((cid:101)y | y)
We can approximate the integral on the left with the expectation of p((cid:101)y | y  x) over the empirical
distribution of x given y. Assuming conditional independence of(cid:101)y and y given x  we have p((cid:101)y |
y  x) = p((cid:101)y | x)  which is directly approximated by(cid:98)p((cid:101)y | x)  the classiﬁer trained on (cid:101)D. More
explicitly  let Ai be the subset of x in D with label i. Denote our estimate of C by (cid:98)C. We have
(cid:98)p((cid:101)y = j | y = i  x) ≈ p((cid:101)y = j | y = i).
This is how we estimate our corruption matrix for the GLC. The approximation relies on(cid:98)p((cid:101)y | x)
being a good estimate of p((cid:101)y | x)  on the number of trusted examples of each class  and on the

(cid:98)p((cid:101)y = j | x) =

extent to which the conditional independence assumption is satisﬁed. The conditional independence
assumption is reasonable  as it is usually the case that noisy labeling processes do not have access to
the true label. Moreover  when the data are separable (i.e. y is deterministic given x)  the assumption
follows. A proof of this is provided in the Supplementary Material. We investigate these factors in
experiments.
Training a Corrected Classiﬁer.

(cid:98)Cij =

(cid:88)

(cid:88)

1
|Ai|

1
|Ai|

x∈Ai

x∈Ai

Now with (cid:98)C  we follow the method of [25 
we deﬁne the new output as(cid:101)s := (cid:98)C Ts. We
then train(cid:98)p((cid:101)s | x) on the noisy labels with

18] to train a corrected classiﬁer  which
we now brieﬂy describe. Given the K × 1
softmax output s of an untrained classiﬁer 

cross-entropy loss. We can further improve
on this method by using trusted data to train
the corrected classiﬁer. Thus  we apply no
correction on examples from the trusted
set encountered during training. This has
the effect of allowing the GLC to handle a
degree of instance-dependency in the label
noise [14]  though our experiments suggest
that most of the GLC’s performance gains

derive from our (cid:98)C estimate. A concrete

algorithm of our method is provided here.

Algorithm GOLD LOSS CORRECTION (GLC)

1: Input: Trusted data D  untrusted data (cid:101)D  loss (cid:96)
2: Train network f (x) =(cid:98)p((cid:101)y|x; θ) ∈ RK on (cid:101)D
3: Fill (cid:98)C ∈ RK×K with zeros

num_examples += 1

num_examples = 0
for (xi  yi) ∈ D such that yi = k do

4: for k = 1  . . .   K do
5:
6:
7:
8:
9:
10:
11: end for

(cid:98)Ck• += f (xi) {add f (xi) to kth row}
(cid:98)Ck• /= num_examples
12: Initialize new model g(x) =(cid:98)p(y|x; θ)
13: Train with (cid:96)(g(x)  y) on D  (cid:96)((cid:98)C Tg(x) (cid:101)y) on (cid:101)D
14: Output: Model(cid:98)p(y|x; θ)

end for

4 Experiments

Generating Corrupted Labels. Suppose our dataset has t + u examples. We sample a set of t

trusted datapoints D  and the remaining u untrusted examples form (cid:101)D  which we probabilistically
To generate the untrusted labels from the true labels in (cid:101)D  we ﬁrst obtain a corruption matrix C. Then 

corrupt according to a true corruption matrix C. Note that we do not have knowledge of which of our
u untrusted examples are corrupted. We only know that they are potentially corrupted.

for an example with true label i  we sample the corrupted label from the categorical distribution
parameterized by the ith row of C. Note that this does not satisfy the conditional independence
assumption required for our estimate of C. However  we ﬁnd that the GLC still works well in practice 
perhaps because this assumption is also satisﬁed when the data are separable  in the sense that each x
has a single true y  which is approximately true in many of our experiments.
Comparing Loss Correction Methods. The GLC differs from previous loss corrections for label
noise in that it reasonably assumes access to a high-quality annotation source. Therefore  to compare
to other loss correction methods  we ask how each method performs when starting from the same
dataset with the same label noise. In other words  the only additional information our method uses is
knowledge of which examples are trusted  and which are potentially corrupted.

3

Figure 2: Error curves for numerous label correction methods using a full range of label corruption
strengths on several different vision and natural language processing datasets.

4.1 Datasets and Architectures
MNIST. The MNIST dataset contains 28 × 28 grayscale images of the digits 0-9. The training set
has 60 000 images and the test set has 10 000 images. For preprocessing  we rescale the pixels to the
interval [0  1].We train a 2-layer fully connected network with 256 hidden dimensions. We train with
Adam for 10 epochs using batches of size 32 and a learning rate of 0.001. For regularization  we use
(cid:96)2 weight decay on all layers with λ = 1 × 10−6.
CIFAR. The two CIFAR datasets contain 32× 32× 3 color images. CIFAR-10 has ten classes  and
CIFAR-100 has 100 classes. CIFAR-100 has 20 “superclasses” which partition its 100 classes into 20
semantically similar sets. We use these superclasses for hierarchical noise. Both datasets have 50 000
training images and 10 000 testing images. For both datasets  we train a Wide Residual Network [29]
of depth 40 and a widening factor of 2. We train for 75 epochs using SGD with Nesterov momentum
and a cosine learning rate schedule [12].
IMDB. The IMDB Large Movie Reviews dataset [13] contains 50 000 highly polarized movie
reviews from the Internet Movie Database  split evenly into train and test sets. We pad and clip reviews
to a length of 200 tokens  and learn 50-dimensional word vectors from scratch for a vocab size of
5 000.We train an LSTM with 64 hidden dimensions on this data. We train using the Adam optimizer
[8] for 3 epochs with batch size 64 and the suggested learning rate of 0.001. For regularization  we
use dropout [23] on the linear output layer with a dropping probability of 0.2.
Twitter. The Twitter Part of Speech dataset [4] contains 1 827 tweets annotated with 25 POS tags.
This is split into a training set of 1 000 tweets  a development set of 327 tweets  and a test set of 500
tweets. We use the development set to augment the training set. We use pretrained 50-dimensional
word vectors  and for each token  we concatenate word vectors in a ﬁxed window centered on the
token. These form our training and test set. We use a window size of 3  and train a 2-layer fully
connected network with hidden size 256  and use the GELU nonlinearity [7]. We train with Adam for
15 epochs with batch size 64 and learning rate of 0.001. For regularization  we use (cid:96)2 weight decay
with λ = 5 × 10−5 on all but the linear output layer.
SST. The Stanford Sentiment Treebank dataset consists of single sentence movie reviews [22]. We
use the 2-class version (i.e. SST2)  which has 6 911 reviews in the training set  872 in the development
set  and 1 821 in the test set. We use the development set to augment the training set. We pad and clip
reviews to a length of 30 tokens and learn 100-dimensional word vectors from scratch for a vocab
size of 10 000. Our classiﬁer is a word-averaging model with an afﬁne output layer. We use the
Adam optimizer for 5 epochs with batch size 50 and learning rate 0.001. For regularization  we use
(cid:96)2 weight decay with λ = 1 × 10−4 on the output layer.

4

0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorCIFAR-10  Flip  10% trustedGLC (Ours)DistillationForward GoldForwardNo Correction0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorMNIST  Flip  1% trusted0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorSST  Flip  1% trusted0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorCIFAR-100  Flip  10% trusted0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorCIFAR-10  Uniform  10% trusted0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorIMDB  Flip  5% trusted0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorCIFAR-100  Hier.  10% trusted0.00.20.40.60.81.0Corruption Strength0.00.20.40.60.81.0Test ErrorCIFAR-100  Uniform  10% trustedT
S
I
N
M

0
1
-
R
A
F
I
C

0
0
1
-
R
A
F
I
C

Corruption
Type
Uniform
Uniform
Uniform
Flip
Flip
Flip

Mean

Uniform
Uniform
Uniform
Flip
Flip
Flip

Percent
Trusted
5
10
25
5
10
25

5
10
25
5
10
25

Mean

5
Uniform
10
Uniform
25
Uniform
5
Flip
10
Flip
Flip
25
Hierarchical 5
Hierarchical 10
Hierarchical 25

Mean

Trusted
Only
37.6
12.9
6.6
37.6
12.9
6.6
19.0
39.6
31.3
17.4
39.6
31.3
17.4
29.4
82.4
67.3
52.2
82.4
67.3
52.2
82.4
67.3
52.2
67.3

12.9
12.3
9.3
50.1
51.1
47.7
30.6
31.9
31.9
32.7
53.3
53.2
52.7
42.6
48.8
48.4
45.4
62.1
61.9
59.6
50.9
51.9
54.3
53.7

14.5
13.9
11.8
51.7
48.8
50.2
31.8
9.1
8.6
7.7
38.6
36.5
37.6
23.0
47.7
47.2
43.6
61.6
61.0
57.5
51.0
50.5
47.0
51.9

Gold
13.5
12.3
9.2
41.4
36.4
37.1
25.0
27.8
20.6
27.1
47.8
51.0
49.5
37.3
49.6
48.9
46.0
62.6
62.2
61.4
52.4
52.1
51.1
54.0

No Corr. Forward Forward

Distill. Confusion

Matrix
21.8
15.1
11.0
11.7
5.6
3.8
11.5
22.4
22.7
16.7
8.1
8.2
7.1
14.2
53.6
49.7
39.6
28.6
26.9
25.1
45.8
38.8
29.7
37.5

GLC
(Ours)
10.3
6.3
4.7
3.4
2.9
2.6
5.0
9.0
6.9
6.4
6.6
6.2
6.1
6.9
42.4
33.9
27.3
27.1
25.8
24.7
34.8
30.2
25.4
30.2

42.1
9.2
5.8
46.5
32.4
28.2
27.4
29.7
18.3
11.6
29.7
18.1
11.8
19.9
87.5
61.2
39.8
87.1
61.8
40.0
87.1
61.7
39.7
62.9

Table 1: Vision dataset results. Percent trusted is the trusted fraction multiplied by 100. Unless
otherwise indicated  all values are percentages representing the area under the error curve computed
at 11 test points. The best mean result is bolded.

4.2 Label Noise Corrections

Forward Loss Correction. The forward correction method from Patrini et al. [18] also obtains (cid:98)C

by training a classiﬁer on the noisy labels  and using the resulting softmax probabilities. However 
this method does not make use of a trusted fraction of the training data. Instead  it uses the argmax
at the 97th percentile of softmax probabilities for a given class as a heuristic for detecting an example
that is truly a member of said class. As in the original paper  we replace this with the argmax over
all softmax probabilities for a given class on CIFAR-100 experiments. The estimate of C is then used
to train a corrected classiﬁer in the same way as the GLC.
Forward Gold. To examine the effect of training on trusted labels as done by the GLC  we augment
the Forward method by replacing its estimate of C with the identity on trusted examples. We call this
method Forward Gold. It can also be seen as the GLC with the Forward method’s estimate of C.
Distillation. The distillation method of Li et al. [11] involves training a neural network on a large
trusted dataset and using this network to provide soft targets for the untrusted data. In this way  labels
are “distilled” from a neural network. If the classiﬁer’s decisions for untrusted inputs are less reliable
than the original noisy labels  then the network’s utility is limited. Thus  to obtain a reliable neural
network  a large trusted dataset is necessary. A new classiﬁer is trained using labels that are a convex
combination of the soft targets and the original untrusted labels.
Confusion Matrices. An intuitive alternative to the GLC is to estimate C by a confusion matrix.
To do this  we train a classiﬁer on the untrusted examples  obtain its confusion matrix on the trusted
examples  row-normalize the matrix  and then train a corrected classiﬁer as in the GLC.

4.3 Uniform  Flip  and Hierarchical Corruption

Corruption-Generating Matrices. We consider three types of corruption matrices: corrupting
uniformly to all classes  i.e. Cij = 1/K  ﬂipping a label to a different class  and corrupting uniformly
to classes which are semantically similar. To create a uniform corruption at different strengths  we

5

No Corr. Forward Forward

Distill. Confusion

Corruption
Type
Uniform
Uniform
Uniform
Flip
Flip
Flip

Mean

Uniform
Uniform
Uniform
Flip
Flip
Flip

Uniform
Uniform
Uniform
Flip
Flip
Flip

Mean

Mean

Percent
Trusted
5
10
25
5
10
25

5
10
25
5
10
25

5
10
25
5
10
25

Trusted
Only
45.4
35.2
26.1
45.4
35.2
26.1
35.6
36.9
26.2
22.2
36.9
26.2
22.2
28.5
35.9
23.6
16.3
35.9
23.6
16.3
25.3

T
S
S

B
D
M

I

r
e
t
t
i

w
T

27.5
27.2
26.5
50.2
49.9
48.7
38.3
26.7
25.8
21.4
49.2
47.8
39.4
35.0
37.1
33.5
25.5
56.2
53.8
43.0
41.5

26.5
26.2
25.3
50.3
50.1
49.0
37.9
27.9
27.2
23.0
49.2
48.3
39.6
35.9
51.7
49.5
40.6
61.6
59.0
52.5
52.5

Gold
26.6
25.9
24.6
50.3
49.9
47.3
37.4
27.6
26.1
20.1
49.2
47.5
36.6
34.5
44.1
40.2
26.4
54.8
48.9
36.7
41.9

43.4
33.3
25.0
48.8
42.1
31.8
37.4
35.5
24.9
21.0
41.8
28.0
23.5
29.1
32.0
22.2
16.6
36.4
26.1
20.5
25.7

Matrix
26.1
25.0
22.4
26.0
24.6
22.4
24.4
25.4
23.3
18.9
25.8
22.1
19.2
22.5
41.5
33.6
20.0
23.4
15.9
13.3
24.6

GLC
(Ours)
24.2
23.5
21.7
24.9
23.5
21.7
23.3
25.0
22.3
18.7
25.2
22.0
18.5
22.0
31.0
22.3
15.5
15.8
12.9
12.8
18.4

Table 2: NLP dataset results. Percent trusted is the trusted fraction multiplied by 100. Unless
otherwise indicated  all values are percentages representing the area under the error curve computed
at 11 test points. The best mean result is bolded.

take a convex combination of an identity matrix and the matrix 11T/K. We refer to the coefﬁcient
of 11T/K as the corruption strength for a “uniform” corruption. A “ﬂip” corruption at strength m
involves  for each row  giving an off-diagonal column probability mass m and the entries along the
diagonal probability mass 1 − m. Finally  a more realistic corruption is hierarchical corruption. For
this corruption  we apply uniform corruption only to semantically similar classes; for example  “bed”
may be corrupted to “couch” but not “beaver” in CIFAR-100. For CIFAR-100  examples are deemed
semantically similar if they share the same “superclass” label speciﬁed by the dataset creators.
Experiments and Analysis of Results. We train the models described in Section 4.1 under uniform 
label-ﬂipping  and hierarchical label corruptions at various fractions of trusted data. To assess the
performance of the GLC  we compare it to other loss correction methods and two baselines: one where
we train a network only on trusted data without any label corrections  and one where the network trains
on all data without any label corrections. We record errors on the test sets at the corruption strengths
{0  0.1  . . .   1.0}. Since we compute the model’s accuracy at numerous corruption strengths  CIFAR
experiments involve training over 500 Wide Residual Networks. In Tables 1 and 2  we report the area
under the error curves across corruption strengths {0  0.1  . . .   1.0} for all baselines and corrections.
A sample of error curves are displayed in Figure 2. These curves are the linear interpolation of the
errors at the eleven corruption strengths.
Across all experiments  the GLC obtains better area under the error curve than the baselines and the
Forward and Distillation methods. The rankings of the other methods and baselines are mixed. On
MNIST  training on the trusted data alone outperforms all methods save for the GLC and Confusion
Matrix  but performs signiﬁcantly worse on CIFAR-100  even with large trusted fractions.
The Confusion Matrix correction performs second to the GLC  which indicates that normalized
confusion matrices are not as accurate as our method of estimating C. We veriﬁed this by inspecting
the estimates directly  and found that normalized confusion matrices give a highly biased estimate
due to taking an argmax over class scores rather than using random sampling. Figure 1 shows an
example of this bias in the case of label ﬂipping corruption at a strength of 7/10.
Interestingly  Forward Gold performs worse than Forward on several datasets. We did not observe the
same behavior when turning off the corresponding component of the GLC  and believe it may be due
to variance introduced during training by the difference in signal provided by the Forward method’s

6

C estimate and the clean labels. The GLC provides a superior C estimate  and thus may be better
able to leverage training on the clean labels. Additional results on SVHN are in the Supplementary
Material.
We also compare the GLC to the recent work of Ren et al. [21]  which proposes a loss correction that
uses a trusted set and meta-learning. We ﬁnd that the GLC consistently outperforms this method. To
conserve space  results are in the Supplementary Material.

Percent
Trusted
1
5
10

5
10
25

Trusted
Only
62.9
39.6
31.3
44.6
82.4
67.3
52.2
67.3

No
Corr.
28.3
27.1
25.9
27.1
71.1
66
56.9
64.7

CIFAR-10

Mean

CIFAR-100

Mean

28.1
26.6
25.1
26.6
73.9
68.2
56.9
66.3

Forward Forward

Gold
30.9
25.5
22.9
26.4
73.6
66.1
51.4
63.7

Distill.

60.4
28.1
17.8
35.44
88.3
62.5
39.7
63.5

Confusion
Matrix
31.9
27
24.2
27.7
74.1
63.8
50.8
62.9

GLC
(Ours)
26.9
21.9
19.2
22.7
68.7
56.6
40.8
55.4

Table 3: Results when obtaining noisy labels by sampling from the softmax distribution of a weak
classiﬁer. Percent trusted is the trusted fraction multiplied by 100. Unless otherwise indicated  all
values are the percent error. The best average result for each dataset is shown in bold.
4.4 Weak Classiﬁer Labels

Our next benchmark for the GLC is to use noisy labels obtained from a weak classiﬁer. This models
the scenario of label noise arising from a classiﬁcation system weaker than one’s own  but with access
to information about the true labels that one wishes to transfer to one’s own system. For example 
scraping image labels from surrounding text on web pages provides a valuable signal  but these
labels would train a sub-par classiﬁer without correcting the label noise. This setting exactly satisﬁes

the conditional independence assumption on label noise used for our (cid:98)C estimate  because the weak

classiﬁer does not take the true label as input when outputting noisy labels.
Weak Classiﬁer Label Generation. To obtain the labels  we train 40-layer Wide Residual Net-
works on CIFAR-10 and CIFAR-100 with clean labels for ten epochs each. Then  we sample from
their softmax distributions with a temperature of 5  and ﬁx the resulting labels. This results in
noisy labels which we use in place of the labels obtained through the uniform  ﬂip  and hierarchical
corruption methods. The labelings produced by the weak classiﬁers have accuracies of 40% on
CIFAR-10 and 7% on CIFAR-100. Despite the presence of highly corrupted labels  we are able
to signiﬁcantly recover performance with the use of a trusted set. Note that unlike the previous
corruption methods  weak classiﬁer labels have only one corruption strength. Thus  performance is
measured in percent error rather than area under the error curve. Results are displayed in Table 3.
Analysis of Results. On average  the GLC outperforms all other methods in the weak classiﬁer label
experiments. The Distillation method performs better than the GLC by a small margin at the highest
trusted fraction  but performs worse at lower trusted fractions  indicating that the GLC enjoys superior
data efﬁciency. This is highlighted by the GLC attaining a 26.94% error rate on CIFAR-10 with a
trusted fraction of only 1%  down from the original error rate of 60%. It should be noted  however  that
training with no correction attains 28.32% error on this experiment  suggesting that the weak classiﬁer
labels have low bias. The improvement conferred by the GLC is greater with larger trusted fractions.

5 Discussion

Data Efﬁciency. We have seen that the GLC works for small trusted fractions. We further corrobo-
rate its data efﬁciency by turning to the Clothing1M dataset [27]. Clothing1M is a massive dataset
with both human-annotated and noisy labels  which we use to compare the data efﬁciency of the GLC
to that of Distillation when very few trusted labels are present. It consists in 1 million noisily labeled
clothing images obtained by crawling online marketplaces. 50 000 images have human-annotated
examples  from which we take subsamples as our trusted set.
For both the GLC and Distillation  we ﬁrst ﬁne-tune a ResNet-34 on untrusted training examples for
four epochs  and use this to estimate our corruption matrix. Thereafter  we ﬁne-tune the network for

7

four more epochs on the combined trusted and untrusted sets using the respective method. During ﬁne
tuning  we freeze the ﬁrst seven layers  and train using gradient descent with Nesterov momentum
and a cosine learning rate schedule. For preprocessing  we randomly crop and use mirroring. We also
upsample the trusted dataset  ﬁnding this to give better performance for both methods.
As shown in Figure 3  the GLC outperforms Distillation by a
large margin  especially with fewer trusted examples. This is be-
cause Distillation requires ﬁne-tuning a classiﬁer on the trusted
data alone  which generalizes poorly with very few examples.
By contrast  estimating the C matrix can be done with very
few examples. Correspondingly  we ﬁnd that our advantage
decreases as the number of trusted examples increases.
With more trusted labels  performance on Clothing1M saturates 
as evident in Figure 3. We consider the extreme and train on the
entire trusted set for Clothing1M. We ﬁne-tune a pre-trained 50-
layer ResNeXt [28] on untrusted training examples to estimate
our corruption matrix. Then  we ﬁne-tune the ResNeXt on all
training examples. During ﬁne-tuning  we use gradient descent
with Nesterov momentum. During the ﬁrst two epochs  we tune
only the output layer with a learning rate of 10−2. Thereafter  we tune the whole network at a learning
rate of 10−3 for two epochs  and for another two epochs at 10−4. Then we apply our loss correction.
Now  we ﬁne-tune the entire network at a learning rate of 10−3 for two epochs  continue training
at 10−4  and early-stop based upon the validation set. In a previous work  Xiao et al. [27] obtain
78.24% in this setting. However  our method obtains a state-of-the-art accuracy of 80.67%  while
with this procedure the Forward method only obtains 79.03% accuracy.

Figure 3: Data efﬁciency of our
method compared to Distillation on
Clothing1M.

ran several variants of the GLC experiment on CIFAR-100 under the label ﬂipping corruption at a
trusted fraction of 5/100 which we now describe. For all variants  we averaged the area under the
error curve over ﬁve random initializations.

Improving (cid:98)C Estimation. For some datasets  the classiﬁer(cid:98)p((cid:101)y | x) may be a poor estimate of
p((cid:101)y | x)  presenting a bottleneck in the estimation of (cid:98)C for the GLC. To see the extent to which
this could impact performance  and whether simple methods for improving(cid:98)p((cid:101)y | x) could help  we
1. In the ﬁrst variant  we replaced the GLC estimate of (cid:98)C with C  the true corruption matrix.
(cid:98)p((cid:101)y | x) estimate  despite the higher entropy of the noisy labels  so we used the temperature scaling
conﬁdence calibration method proposed in the paper to calibrate(cid:98)p((cid:101)y | x).
3. Suppose we know the base rates of corrupted labels(cid:101)b  where(cid:101)bi = p((cid:101)y = i)  and the base rate
of true labels b of the trusted set. If we posit that (cid:98)C0 corrupted the labels  then we should have
bT(cid:98)C0 =(cid:101)bT. Thus  we may obtain a superior estimate of the corruption matrix by computing a new
estimate (cid:98)C = argmin(cid:98)C (cid:107)bT(cid:98)C0 −(cid:101)bT(cid:107)2
2 subject to (cid:98)C1 = 1.
We found that using the true corruption matrix as our (cid:98)C provides a beneﬁt of 0.96 percentage points
difﬁcult without directly improving the performance of the neural network used to estimate(cid:98)p(y | x).

in area under the error curve  but neither the conﬁdence calibration nor the base rate incorporation
was able to change the performance from the original GLC. This indicates that the GLC is robust
to the use of uncalibrated networks for estimating C  and that improving its performance may be

2. As demonstrated by Hendrycks and Gimpel [6] and Guo et al. [5]  modern deep neural network
classiﬁers tend to have overconﬁdent softmax distributions. We found this to be the case with our

2 + λ(cid:107)(cid:98)C − (cid:98)C0(cid:107)2

6 Conclusion

In this work  we have shown the impact of having a small set of trusted examples on label noise
robustness in neural network classiﬁers. We proposed the Gold Loss Correction (GLC)  a method
for coping with label noise. This method leverages the assumption that the model has access to a
small set of correct labels in order to yield accurate estimates of the noise distribution. Throughout
our experiments  the GLC surpasses previous label noise robustness methods across various natural
language processing and vision domains which we showed by considering several corruptions and
numerous strengths  including severe strengths. These results demonstrate that the GLC is a powerful 
data-efﬁcient method for improving robustness to label noise.

8

1005001000Number of Trusted Examples3040506070Percent AccuracyDistillationGLC (Ours)Acknowledgments

We thank NVIDIA for donating GPUs used in this research.

References

[1] B Biggio  B Nelson  and P Laskov. “Support Vector Machines Under Adversarial Label Noise”.

In: ACML (2011).

[2] Moses Charikar  Jacob Steinhardt  and Gregory Valiant. “Learning from Untrusted Data”. In:

STOC (2017).

[3] Benoît Frénay and Michel Verleysen. “Classiﬁcation in the presence of label noise: a survey”.

In: IEEE Trans Neural Netw Learn Syst (2014).

[4] Kevin Gimpel et al. “Part-of-speech Tagging for Twitter: Annotation  Features  and Experi-

ments”. In: ACL (2011).

[5] Chuan Guo et al. “On Calibration of Modern Neural Networks”. In: ICML (2017).
[6] Dan Hendrycks and Kevin Gimpel. “A Baseline for Detecting Misclassiﬁed and Out-of-

Distribution Examples in Neural Networks”. In: ICLR (2017).

[7] Dan Hendrycks and Kevin Gimpel. “Gaussian Error Linear Units (GELUs)”. In: arXiv

1606.08415 (2016).

[8] Diederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In: ICLR

(2014).
J Larsen et al. “Design of robust neural network classiﬁers”. In: Acoustics  Speech and Signal
Processing  1998. Proceedings of the 1998 IEEE International Conference on. 1998.

[9]

[10] Bo Li et al. “Data Poisoning Attacks on Factorization-Based Collaborative Filtering”. In: NIPS

(2016).

[11] Yuncheng Li et al. “Learning from Noisy Labels with Distillation”. In: ICCV (2017).
[12]

Ilya Loshchilov and Frank Hutter. “SGDR: Stochastic Gradient Descent with Warm Restarts”.
In: ICLR (2016).

[13] Andrew L. Maas et al. “Learning Word Vectors for Sentiment Analysis”. In: Proceedings of
the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies. 2011.

[14] Aditya Krishna Menon  Brendan van Rooyen  and Nagarajan Natarajan. “Learning from Binary

Labels with Instance-Dependent Corruption”. In: CoRR (2016).

[15] Volodymyr Mnih and Geoffrey E Hinton. “Learning to label aerial images from noisy data”.

In: ICML (2012).

[16] Nagarajan Natarajan et al. “Learning with Noisy Labels”. In: Advances in Neural Information

Processing Systems 26. 2013.

[17] David F Nettleton  Albert Orriols-Puig  and Albert Fornells. “A study of the effect of different
types of noise on the precision of supervised learning techniques”. In: Artif Intell Rev (2010).
[18] Giorgio Patrini et al. “Making Deep Neural Networks Robust to Label Noise: a Loss Correction

Approach”. In: CVPR (2016).

[19] M Pechenizkiy et al. “Class Noise and Supervised Learning in Medical Domains: The Effect
of Feature Extraction”. In: 19th IEEE Symposium on Computer-Based Medical Systems
(CBMS’06). 2006.

[20] Scott Reed et al. “Training Deep Neural Networks on Noisy Labels with Bootstrapping”. In:

ICLR Workshop (2014).

[21] Thu Mengye Ren et al. “Learning to Reweight Examples for Robust Deep Learning”. In: ICML

(2018).

[22] Richard Socher et al. “Recursive Deep Models for Semantic Compositionality Over a Sentiment

Treebank”. In: Conference on Empirical Methods in Natural Language Processing. 2013.

[23] Nitish Srivastava et al. “Dropout: A simple way to prevent neural networks from overﬁtting”.

In: The Journal of Machine Learning Research (2014).
Jacob Steinhardt  Pang Wei Koh  and Percy Liang. “Certiﬁed Defenses for Data Poisoning
Attacks”. In: NIPS. 2017.

[24]

9

[25] Sainbayar Sukhbaatar et al. “Training Convolutional Networks with Noisy Labels”. In: ICLR

Workshop (2014).

[26] Andreas Veit et al. “Learning From Noisy Large-Scale Datasets With Minimal Supervision”.

In: CVPR (2017).

[27] Tong Xiao et al. “Learning from massive noisy labeled data for image classiﬁcation”. In: CVPR

(2015).

[28] Saining Xie et al. “Aggregated Residual Transformations for Deep Neural Networks”. In:

CVPR (2016).

[29] Sergey Zagoruyko and Nikos Komodakis. “Wide Residual Networks”. In: BMVC (2016).
[30] Xingquan Zhu and Xindong Wu. “Class Noise vs. Attribute Noise: A Quantitative Study”. In:

Artiﬁcial Intelligence Review (2004).

10

,Dan Hendrycks
Mantas Mazeika
Duncan Wilson