2012,Bayesian nonparametric models for bipartite graphs,We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization  an Indian Buffet-like generative process for network growth  and a simple and efficient Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks.,Bayesian nonparametric models for bipartite graphs

Franc¸ois Caron

INRIA

IMB - University of Bordeaux

Talence  France

Francois.Caron@inria.fr

Abstract

We develop a novel Bayesian nonparametric model for random bipartite graphs.
The model is based on the theory of completely random measures and is able
to handle a potentially inﬁnite number of nodes. We show that the model has
appealing properties and in particular it may exhibit a power-law behavior. We
derive a posterior characterization  a generative process for network growth  and
a simple Gibbs sampler for posterior simulation. Our model is shown to be well
ﬁtted to several real-world social networks.

1

Introduction

The last few years have seen a tremendous interest in the study  understanding and statistical mod-
eling of complex networks [14  6]. A network is a set if items  called vertices  with connections
between them  called edges. In this article  we shall focus on bipartite networks  also known as two-
mode  afﬁliation or collaboration networks [16  17]. In bipartite networks  items are divided into two
different types A and B  and only connections between items of different types are allowed. Exam-
ples of this kind can be found in movie actors co-starring the same movie  scientists co-authoring a
scientiﬁc paper  internet users posting a message on the same forum  people reading the same book
or listening to the same song  members of the boards of company directors sitting on the same board 
etc. Following the readers-books example  we will refer to items of type A as readers and items of
type B as books. An example of bipartite graph is shown on Figure 1(b). An important summariz-
ing quantity of a bipartite graph is the degree distribution of readers (resp. books) [14]. The degree
of a vertex in a network is the number of edges connected to that vertex. Degree distributions of
real-world networks are often strongly non-Poissonian and exhibit a power-law behavior [15].
A bipartite graph can be represented by a set of binary variables (zij) where zij = 1 if reader i has
read book j  0 otherwise. In many situations  the number of available books may be very large and
potentially unknown. In this case  a Bayesian nonparametric (BNP) approach can be sensible  by
assuming that the pool of books is inﬁnite. To formalize this framework  it will then be convenient
to represent the bipartite graph by a collection of atomic measures Zi  i = 1  . . .   n with

Zi =

zijδθj

(1)

j=1

where {θj} is the set of books and typically Zi only has a ﬁnite set of non-zero zij corresponding to
books reader i has read. Grifﬁths and Ghahramani [8  9] have proposed a BNP model for such binary
random measures. The so-called Indian Buffet Process (IBP) is a simple generative process for the
conditional distribution of Zi given Z1  . . .   Zi−1. Such process can be constructed by considering
that the binary measures Zi are i.i.d. from some random measure drawn from a beta process [19  10].
It has found several applications for inferring hidden causes [20]  choices [7] or features [5]. Teh
and Gor¨ur [18] proposed a three-parameter extension of the IBP  named stable IBP  that enables to

1

∞(cid:88)

model a power-law behavior for the degree distribution of books. Although more ﬂexible  the stable
IBP still induces a Poissonian distribution for the degree of readers.
In this paper  we propose a novel Bayesian nonparametric model for bipartite graphs that addresses
some of the limitations of the stable IBP  while retaining computational tractability. We assume
that each book j is assigned a positive popularity parameter wj > 0. This parameter measures the
popularity of the book  larger weights indicating larger probability to be read. Similarly  each reader
i is assigned a positive parameter γi which represents its ability to read books. The higher γi  the
more books the reader i is willing to read. Given the weights wj and γi  reader i reads book j with
probability 1 − exp(−γiwj). We will consider that the weights wj and/or γi are the points of a
Poisson process with a given L´evy measure. We show that depending on the choice of the L´evy
measure  a power-law behavior can be obtained for the degree distribution of books and/or readers.
Moreover  using a set of suitably chosen latent variables  we can derive a generative process for
network growth  and an efﬁcient Gibbs sampler for approximate inference. We provide illustrations
of the ﬁt of the proposed model on several real-world bipartite social networks. Finally  we discuss
some potentially useful extensions of our work  in particular to latent factor models.

2 Statistical Model

2.1 Completely Random Measures

We ﬁrst provide a brief overview of completely random measures (CRM) [12  13] before describing
the BNP model for bipartite graphs in Section 2.2. Let Θ be a measurable space. A CRM is a
random measure G such that for any collection of disjoint measurable subsets A1  . . .   An of Θ 
the random masses of the subsets G(A1)  . . .   G(An) are independent. CRM can be decomposed
into a sum of three independent parts: a non-random measure  a countable collection of atoms with
ﬁxed locations  and a countable collection of atoms with randoms masses at random locations. In this
paper  we will be concerned with models deﬁned by CRMs with random masses at random locations 
j=1 wjδθj . The law of G can be characterized in terms of a Poisson process over the point
set {(wj  θj)  j = 1  . . .  ∞} ⊂ R+ × Θ. The mean measure Λ of this Poisson process is known as
the L´evy measure. We will assume in the following that the L´evy measure decomposes as a product
of two non-atomic densities  i.e. that G is a homogeneous CRM Λ(dw  dθ) = λ(w)h(θ)dwdθ with
Θ h(θ)dθ = 1. It implies that the locations of the atoms in G are independent
of the masses  and are i.i.d. from h  while the masses are distributed according to a Poisson process
j=1 wj is
positive and ﬁnite with probability one  which is guaranteed if the following conditions are satisﬁed

i.e. G =(cid:80)∞
h : Θ → [0  +∞) and(cid:82)
over R+ with mean intensity λ. We will further assume that the total mass G(Θ) = (cid:80)∞
(cid:90) ∞

(cid:90) ∞

λ(w)dw = ∞ and

(1 − exp(−w))λ(w)dw < ∞

0

0

and note g(x) its probability density function evaluated at x. We will refer to λ as the L´evy intensity
in the following  and to h as the base density of G  and write G ∼ CRM(λ  h). We will also note

(2)

(3)

(4)

(5)

(cid:90) ∞

ψλ(t) = − log E [exp(−tG(Θ))] =

(1 − exp(−tw))λ(w)dw

(cid:101)ψλ(t  b) =

κ(n  z) =

(cid:90) ∞
(cid:90) ∞

0

0

(1 − exp(−tw))λ(w) exp(−bw)dw

0

λ(w)wne−zwdw

As a notable particular example of CRM  we can mention the generalized gamma process (GGP) [1] 
whose L´evy intensity is given by

λ(w) =

α

Γ(1 − σ)

w−σ−1e−wτ

GGP encompasses the gamma process (σ = 0)  the inverse Gaussian process (σ = 0.5) and the
stable process (τ = 0) as special cases. Table ?? in supplementary material provides the expressions
of λ  ψ and κ for these processes.

2

2.2 A Bayesian nonparametric model for bipartite graphs
Let G ∼ CRM(λ  h) where λ satisﬁes conditions (2). A draw G takes the form

∞(cid:88)

j=1

(cid:26) zij = 1 if vij < 1

zij = 0 otherwise

∞(cid:88)

∞(cid:88)

∞(cid:88)

(6)
where {θj} is the set of books and {wj} the set of popularity parameters of books. For i = 1  . . .   n 
let consider the latent exponential process

wjδθj

G =

j=1

(7)
deﬁned for j = 1  . . .  ∞ by vij|wj ∼ Exp(wjγi) where Exp(a) denotes the exponential distri-
bution of rate a. The higher wj and/or γi  the lower vij. We then deﬁne the binary process Zi
conditionally on Vi by

vijδθj

Vi =

j=1

Zi =

zijδθj with

(8)

By integrating out the latent variables vij we clearly have p(zij = 1|wj  γi) = 1 − exp(−γiwj).
Proposition 1 Zi is marginally characterized by a Poisson process over the point set {(θ∗

1  . . .  ∞} ⊂ Θ  of intensity measure ψλ(γi)h(θ∗). Hence  the total mass Zi(Θ) = (cid:80)∞

j )  j =
j=1 zij 
which corresponds to the total number of books read by reader i is ﬁnite with probability one and
admits a Poisson(ψλ(γi)) distribution  where ψλ(z) is deﬁned in Equation (3)  while the locations
θ∗
j are i.i.d. from h.
The proof  which makes use of Campbell’s theorem for point processes [13] is given in supplemen-

tary material. As an example  for the gamma process we have Zi(Θ) ∼ Poisson(cid:0)α log(cid:0)1 + γi

(cid:1)(cid:1).

τ

It will be useful in the following to introduce a censored version of the latent process Vi  deﬁned by

(9)
where uij = min(vij  1)  for i = 1  . . .   n and j = 1  . . .  ∞. Note that Zi can be obtained
deterministically from Ui.

uijδθj

Ui =

j=1

2.3 Characterization of the conditional distributions

The conditional distribution of G given Z1  . . .   Zn cannot be obtained in closed form1. We will
make use of the latent process Ui. In this section  we derive the formula for the conditional laws
P (U1  . . .   Un|G)  P (U1  . . .   Un) and P (G|U1  . . .   Un) . Based on these results  we derive in Sec-
tion 2.4 a generative process and in Section 2.5 a Gibbs sampler for our model  that both rely on the
introduction of these latent variables.

j=1 zij the degree
i=1 zij the degree of
book j (number of people having read book j). The conditional likelihood of U1  . . . Un given G is
given by

Assume that K books {θ1  . . .   θK} have appeared. We write Ki = Zi(Θ) =(cid:80)∞
i=1 Zi({θj}) =(cid:80)n
of reader i (number of books read by reader i) and mj =(cid:80)n
 exp (−γiG(Θ\{θ1  . . .   θK}))
(cid:33) exp
(cid:33)
(cid:32)

n(cid:89)
(cid:33) K(cid:89)

 K(cid:89)
(cid:32)

P (U1  . . . Un|G) =

exp (−γiwjuij)

(cid:32) n(cid:88)

(cid:32) n(cid:89)

γi(uij − 1)





n(cid:88)

γzij
i wzij

−wj

(cid:33)

G(Θ)

i=1

j=1

wmj

j

exp

γKi
i

(10)

=

−

γi

j

i=1

j=1

i=1

i=1

1In the case where γi = γ  it is possible to derive P (Z1  . . .   Zn) and P (Zn+1|Z1  . . .   Zn) where the
random measure G and the latent variables U are marginalized out. This particular case is described in supple-
mentary material.

3

Proposition 2 The marginal distribution P (U1  . . . Un) is given by

(cid:32) n(cid:89)

(cid:33)

(cid:34)

γKi
i

exp

−ψλ

(cid:32) n(cid:88)

(cid:33)(cid:35) K(cid:89)

i=1

i=1

j=1

(cid:32)

(cid:33)

γiuij

(11)

n(cid:88)

i=1

P (U1  . . . Un) =

γi

h(θj)κ

mj 

where ψλ and κ are resp. deﬁned by Eq. (3) and (5).
Proof. The proof  detailed in supplementary material  is obtained by an application of the Palm
formula for CRMs [3  11]  and is the same as that of Theorem 1 in [2].

Proposition 3 The conditional distribution of G given the latent processes U1  . . . Un can be ex-
pressed as

G = G∗ +
where G∗ and (wj) are mutually independent with

K(cid:88)

j=1

wjδθj

G∗ ∼ CRM(λ∗  h)

and the masses are

P (wj|rest) =

λ∗(w) = λ(w) exp

(cid:80)n

exp (−wj

κ(mj (cid:80)n

j

i=1 γiuij)

i=1 γiUij)

λ(wj)wmj

(cid:32)

−w

(cid:33)

n(cid:88)

i=1

γi

(12)

(13)

(14)

Proof. The proof  based on the application of the Palm formula and detailed in supplementary
material  is the same as that of Theorem 2 in [2].

In the case of the GGP  G∗ is still a GGP of parameters (α∗ = α  σ∗ = σ  τ∗ = τ +(cid:80)n

i=1 γi)  while

the wj’s are conditionally gamma distributed  i.e.

wj|rest ∼ Gamma

mj − σ  τ +

γiuij

(cid:33)

n(cid:88)

i=1

(cid:32)

K(cid:88)

Corollary 4 The predictive distribution of Zn+1 given the latent processes U1  . . .   Un is given by

Zn+1 = Z∗

n+1 +

zn+1 jδθj

j=1

(cid:18)

n+1 with

1 − κ(mj  τ + γn+1 +(cid:80)n
κ(mj  τ +(cid:80)n

i=1 γiuij)

i=1 γiuij)

(cid:19)

where the zn+1 j are independent of Z∗

zn+1 j|U ∼ Ber

where Ber is the Bernoulli distribution and Z∗
intensity measure ψλ∗ (γn+1) h(θ).

For the GGP  we have

n+1(Θ) ∼
Z∗

and

zn+1 j|U ∼ Ber

1 −

1 +

n+1 is a homogeneous Poisson process over Θ of

 Poisson
(cid:16) α
(cid:16)
(cid:32)
(cid:18)

Poisson

σ

(cid:17)σ − (τ +(cid:80)n
(cid:17)(cid:17)
(cid:19)−mj +σ(cid:33)

i=1 γi

(cid:104)(cid:16)
τ +(cid:80)n+1
(cid:16)
τ +(cid:80)n
τ +(cid:80)n

i=1 γi
1 + γn+1

γn+1

i=1 γiuij

α log

i=1 γi)σ(cid:105)(cid:17)

if σ (cid:54)= 0
if σ = 0

.

n(cid:88)

Finally  we consider the distribution of un+1 j|zn+1 j = 1  u1:n j. This is given by

p(un+1 j|zn+1 j = 1  u1:n j) ∝ κ(mj + 1  un+1 jγn+1 +

γiuij)1un+1 j∈[0 1]

(15)

In supplementary material  we show how to sample from this distribution by the inverse cdf method
for the GGP.

i=1

4

Reader 1

Reader 2

Reader 3

18
4
0
12
16 10

14
8
0

Books

13
0

4
14

9

6

(a)

...
...
...

A1

A2

A3

B1

B2

B3

B4

B5

B6

B7

(b)

Figure 1: Illustration of the generative process described in Section 2.4.

2.4 A generative process

In this section we describe the generative process for Zi given (U1  . . .   Ui−1)  G being integrated
out. This reinforcement process  where popular books will be more likely to be picked  is remi-
niscent of the generative process for the beta-Bernoulli process  popularized under the name of the
Indian buffet process [8]. Let xij = − log(uij) ≥ 0 be latent positive scores.
Consider a set of n readers who successively enter into a library with an inﬁnite number of books.
Each reader i = 1  . . . n  has some interest in reading quantiﬁed by a positive parameter γi > 0.
The ﬁrst reader picks a number K1 ∼ Poisson(ψλ(γ1)) books. Then he assigns a positive score
x1j = − log(u1j) > 0 to each of these books  where u1j is drawn from distribution (15).
Now consider that reader i enters into the library  and knows about the books read by previous
readers and their scores. Let K be the total number of books chosen by the previous i − 1 readers 
and mj the number of times each of the K books has been read. Then for each book j = 1  . . .   K 
reader i will choose this book with probability

and then will choose an additional number of K +

1 − κ(mj  τ + γi +(cid:80)i−1
κ(mj  τ +(cid:80)i−1
(cid:32)(cid:101)ψλ
(cid:32)
i−1(cid:88)

i ∼ Poisson
K +

k=1 γkukj)
i books where

γi 

γk

k=1 γkukj)

(cid:33)(cid:33)

Reader i will then assign a score xij = − log uij > 0 to each book j he has read  where uij is drawn
from (15). Otherwise he will set the default score xij = 0. This generative process is illustrated in
Figure 1 together with the underlying bipartite graph . In Figure 2 are represented draws from this
generative process with a GGP with parameters γi = 2 for all i  τ = 1  and different values for α
and σ.

k=1

2.5 Gibbs sampling

From the results derived in Proposition 3  a Gibbs sampler can be easily derived to approximate
the posterior distribution P (G  U|Z). The sampler successively updates U given (w  G∗(Θ)) then
(w  G∗(Θ)) given U. We present here the conditional distributions in the GGP case. For i =
1  . . .   n  j = 1  . . .   K  set uij = 1 if zij = 0  otherwise sample
uij|zij  wj  γi ∼ rExp(γiwj  1)

where rExp(λ  a) is the right-truncated exponential distribution of pdf λ exp(−λx)/(1 −
exp(−λa))1x∈[0 a] from which we can sample exactly. For j = 1  . . .   K  sample

(cid:33)
and the total mass G∗(Θ) follows a distribution g∗(w) ∝ g(w) exp (−w(cid:80)n
update G∗(Θ) ∼ Gamma (α  τ +(cid:80)n

i=1 γi) where g(w) is
the distribution of G(Θ). In the case of the GGP  g∗(w) is an exponentially tilted stable distribution
for which exact samplers exist [4]. In the particular case of the gamma process  we have the simple

wj|U  γi ∼ Gamma

mj − σ  τ +

(cid:32)

n(cid:88)

i=1

γiuij

i=1 γi) .

5

(a) α = 1  σ = 0

(b) α = 5  σ = 0

(c) α = 10  σ = 0

(d) α = 2  σ = 0.1

(e) α = 2  σ = 0.5

(f) α = 2  σ = 0.9

Figure 2: Realisations from the generative process of Section 2.4 with a GGP of parameters γ = 2 
τ = 1 and various values of α and σ.

3 Update of γi and other hyperparameters

We may also consider the weight parameters γi to be unknown and estimate them from the graph.
We can assign a gamma prior γi ∼ Gamma(aγ  bγ) with parameters (aγ > 0  bγ > 0) and update
it conditionally on other variables with

aγ +

K(cid:88)

K(cid:88)

γi|G  U ∼ Gamma

zij  bγ +

wjuij + G∗(Θ)

j=1

j=1



i=1 γiδ(cid:101)θi

i=1 γiδ(cid:101)θi

γi|G  U ∼ Gamma

In this case  the marginal distribution of Zi(Θ)  hence the degree distribution of books  follows a
continuous mixture of Poisson distributions  which offers more ﬂexibility in the modelling.
We may also go a step further and consider that there is an inﬁnite number of readers with weights γi

. This provides a lot of ﬂexibility in the modelling of the distribution of the degree
of readers  allowing in particular to obtain a power-law behavior  as shown in Section 5. We focus
here on the case where Γ is drawn from a generalized gamma process of parameters (αγ  σγ  τγ) for
where for i = 1  . . .   n 

associated to a given CRM Γ ∼ CRM(λγ  hγ) and a measurable space of readers(cid:101)Θ. We then have
Γ =(cid:80)∞
simplicity. Conditionally on (w  G∗(Θ)  U )  we have Γ = Γ∗ +(cid:80)n

K(cid:88)
(cid:17)(cid:17)
(cid:16)(cid:80)K
update for (w  G∗) conditional on (U  γ  Γ((cid:101)Θ)) is now for j = 1  . . .   K
j=1 wj + G∗(Θ)
(cid:33)
n(cid:88)
γiuij + Γ∗((cid:101)Θ)
(cid:16)(cid:80)n
(cid:16)−w
i=1 γi + Γ∗((cid:101)Θ)

 K(cid:88)
(cid:32)

wj|U  Γ ∼ Gamma

and G∗ ∼ CRM(λ∗  h) with λ∗(w) = λ(w) exp
. Note that
there is now symmetry in the treatment of books/readers.
For the scale parameter α of
the GGP  we can assign a gamma prior α ∼ Gamma(aα  bα) and update it with α|γ ∼
. Other parameters of the GGP can be updated
Gamma
using a Metropolis-Hastings step.

(cid:16)(cid:80)n
(cid:17)(cid:17)
i=1 γi + Γ∗((cid:101)Θ)

aα + K  bα + ψλ

(cid:16)

and Γ∗ ∼ CRM(λ∗

γ  hγ) with λ∗

zij − σγ  τ +

wjuij + G∗(Θ)

mj − σ  τ +

γ(γ) = λγ(γ) exp

(cid:16)−γ

i=1

. In this case  the

j=1

j=1

(cid:17)(cid:17)

6

BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders2040608051015202530BooksReaders20406080510152025304 Discussion

Power-law behavior. We now discuss some of the properties of the model  in the case of the
GGP. The total number of books read by n readers is O(nσ). Moreover  for σ > 0  the degree
distribution follows a power-law distribution: asymptotically  the proportion of books read by m
readers is O(m−1−σ) (details in supplementary material). These results are similar to those of the
stable IBP [18]. However  in our case  a similar behavior can be obtained for the degree distribution
of readers when assigning a GGP to it  while it will always be Poisson for the stable IBP.
Connection to IBP. The stable beta process [18] is a particular case of our construction  obtained
by setting weights γi = γ and L´evy measure

λ(w) = α

Γ(1 + c)

Γ(1 − σ)Γ(c + σ)

γ(1 − e−γw)−σ−1e−γw(c+σ)

(16)

The proof is obtained by a change of variable from the L´evy measure of the stable beta process.
Extensions to latent factor models. So far  we have assumed that the binary matrix Z was observed.
The proposed model can also be used as a prior for latent factor models  similarly to the IBP. As
an example of the potential usefulness of our model compared to IBP  consider the extraction of
features from time series of different lengths. Longer time series are more likely to exhibit more
features than shorter ones  and it is sensible in this case to assume different weights γi. In a more
general setting  we may want γi to depend on a set of metadata associated to reader i. Inference for
latent factor models is described in supplementary material.

5

Illustrations on real-world social networks

We now consider estimating the parameters of our model and evaluating its predictive performance
on six bipartite social networks of various sizes. We ﬁrst provide a short description of these net-
works. The dataset ‘Boards’ contains information about members of the boards of Norwegian com-
panies sitting at the same board in August 20112. ‘Forum’ is a forum network about web users
contributing to the same forums3. ‘Books’ concerns data collected from the Book-Crossing com-
munity about users providing ratings on books4 where we extracted the bipartite network from the
ratings. ‘Citations’ is the co-authorship network based on preprints posted to Condensed Matter
section of ArXiv between 1995 and 1999 [15]. ‘Movielens100k’ contains information about users
rating particular movies5 from which we extracted the bipartite network. Finally  ‘IMDB’ contains
information about actors co-starring a movie6. The sizes of the different networks are given in
Table 1.

Dataset
Board

n
355

899
5064
16726
943
28088

K
5766

552
36275
22016
1682
178074

GGP
-68.6
(31.9)
-5.6e3
4.4e4
-3.4e4
-5.5e4
-1.1e5
Table 1: Size of the different datasets and test log-likelihood of four different models.

Forum
Books
Citations
Movielens100k
IMDB

IG
-145.1
(81.9)
-5.5e3
4.6e4
-3.1e4
-5.5e4
-1.1e5

Edges
1746

7089
49997
58595
100000
341313

S-IBP
9.82
(29.8)
-6.7e3
83.1
-3.7e4
-6.7e4
-1.5e5

SG
8.3
(30.8)
-6.7e3
214
-3.7e4
-6.7e4
-1.5e5

We evaluate the ﬁt of four different models on these datasets. First  the stable IBP [18] with param-
eters (αIBP   τIBP   σIBP ) (S-IBP). Second  our model where the parameter γ is the same over dif-
ferent readers  and is assigned a ﬂat prior (SG). Third our model where each γi ∼ Gamma(aγ  bγ)
where (aγ  bγ) are unknown parameters with ﬂat improper prior (IG). Finally  our model with a
GGP model for γi  with parameters (αγ  σγ  τγ) (GGP). We divide each dataset between a training

2Data can be downloaded from http://www.boardsandgender.com/data.php
3Data for the forum and citation datasets can be downloaded from http://toreopsahl.com/datasets/
4http://www.informatik.uni-freiburg.de/ cziegler/BX/
5The dataset can be downloaded from http://www.grouplens.org
6The dataset can be downloaded from http://www.cise.uﬂ.edu/research/sparse/matrices/Pajek/IMDB.html

7

(a) S-IBP

(b) GS

(c) IG

(d) GGP

(e) S-IBP

(f) GS

(g) IG

(h) GGP

Figure 3: Degree distributions for movies (a-d) and actors (e-h) for the IMDB movie-actor dataset
with four different models. Data are represented by red plus and samples from the model by blue
crosses.

(a) S-IBP

(b) GS

(c) IG

(d) GGP

(e) S-IBP

(f) GS

(g) IG

(h) GGP

Figure 4: Degree distributions for readers (a-d) and books (e-h) for the BX books dataset with four
different models. Data are represented by red plus and samples from the model by blue crosses.

γ = (cid:98)αγ/3 to take into account the different sample

set containing 3/4 of the readers and a test set with the remaining. For each model  we approximate
the posterior mean of the unknown parameters (respectively (αIBP   τIBP   σIBP )  γ  (aγ  bγ) and
(αγ  σγ  τγ) for S-IBP  SG  IG and GGP) given the training network with a Gibbs sampler with
10000 burn-in iterations then 10000 samples; then we evaluate the log-likelihood of the estimated
model on the test data. For GGP  we use αtest
sizes. For ‘Boards’  we do 10 replications with random permutations given the small sample size
and report standard deviation together with mean value. Table 1 shows the results over the different
networks for the different models. Typically  S-IBP and SG give very similar results. This is not
surprising  as they share the same properties  i.e. Poissonian degree distribution for readers and
power-law degree distribution for books. Both methods perform better solely on the Board dataset 
where the Poisson assumption on the number of people sitting on the same board makes sense. On
all the other datasets  IG and GGP perform better and similarly  with slightly better performances for
IG. These two models are better able to capture the power-law distribution of the degrees of readers.
These properties are shown on Figures 3 and 4 which resp. give the empirical degree distributions
of the test network and a draw from the estimated models  for the IMDB dataset and the Books
dataset. It is clearly seen that the four models are able to capture the power-law behavior of the
degree distribution of actors (Figure 3(e-h)) or books (Figure 4(e-h)). However  only IG and GGP
are able to capture the power-law behavior of the degree distribution of movies (Figure 3(a-d)) or
readers (Figure 4(a-d)).

8

100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103104Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100102100101102103Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelData100100101102103104105Degree  ModelDataReferences
[1] A. Brix. Generalized gamma measures and shot-noise Cox processes. Advances in Applied

Probability  31(4):929–953  1999.

[2] F. Caron and Y. W. Teh. Bayesian nonparametric models for ranked data. In Neural Information

Processing Systems (NIPS)  2012.

[3] D.J. Daley and D. Vere-Jones. An introduction to the theory of point processes. Springer

Verlag  2008.

[4] L. Devroye. Random variate generation for exponentially and polynomially tilted stable dis-
tributions. ACM Transactions on Modeling and Computer Simulation (TOMACS)  19(4):18 
2009.

[5] E.B. Fox  E.B. Sudderth  M.I. Jordan  and A.S. Willsky. Sharing features among dynamical
In Advances in Neural Information Processing Systems  vol-

systems with beta processes.
ume 22  pages 549–557  2009.

[6] A. Goldenberg  A.X. Zheng  S.E. Fienberg  and E.M. Airoldi. A survey of statistical network

models. Foundations and Trends in Machine Learning  2(2):129–233  2010.

[7] D. G¨or¨ur  F. J¨akel  and C.E. Rasmussen. A choice model with inﬁnitely many latent features. In
Proceedings of the 23rd international conference on Machine learning  pages 361–368. ACM 
2006.

[8] T Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In

NIPS  2005.

[9] T. Grifﬁths and Z. Ghahramani. The Indian buffet process: an introduction and review. Journal

of Machine Learning Research  12(April):1185–1224  2011.

[10] N.L. Hjort. Nonparametric bayes estimators based on beta processes in models for life history

data. The Annals of Statistics  18(3):1259–1294  1990.

[11] L.F. James  A. Lijoi  and I. Pr¨unster. Posterior analysis for normalized random measures with

independent increments. Scandinavian Journal of Statistics  36(1):76–97  2009.

[12] J.F.C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics  21(1):59–78 

1967.

[13] J.F.C. Kingman. Poisson processes  volume 3. Oxford University Press  USA  1993.
[14] M.E.J. Newman. The structure and function of complex networks. SIAM review  pages 167–

256  2003.

[15] M.E.J. Newman  S.H. Strogatz  and D.J. Watts. Random graphs with arbitrary degree distribu-

tions and their applications. Physical Review E  64(2):26118  2001.

[16] M.E.J. Newman  D.J. Watts  and S.H. Strogatz. Random graph models of social networks.

Proceedings of the National Academy of Sciences  99:2566  2002.

[17] J.J. Ramasco  S.N. Dorogovtsev  and R. Pastor-Satorras. Self-organization of collaboration

networks. Physical review E  70(3):036106  2004.

[18] Y.W. Teh and D. G¨or¨ur. Indian buffet processes with power-law behavior. In NIPS  2009.
[19] R. Thibaux and M. Jordan. Hierarchical beta processes and the Indian buffet process.

In
International Conference on Artiﬁcial Intelligence and Statistics  volume 11  pages 564–571 
2007.

[20] F. Wood  T.L. Grifﬁths  and Z. Ghahramani. A non-parametric Bayesian method for inferring
In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence 

hidden causes.
volume 22  2006.

9

,Jinwoo Shin
Andrew Gelfand
Misha Chertkov
Isabel Valera
Francisco Ruiz
Lennart Svensson
Muhammad Yousefnezhad
Daoqiang Zhang