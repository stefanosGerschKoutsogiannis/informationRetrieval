2010,Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework,Regularization technique has become a principle tool for statistics and machine learning research and practice. However  in most situations  these regularization terms are not well interpreted  especially on how they are related to the loss function and data. In this paper  we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression  lasso  elastic net  fused lasso  group lasso  local coordinate coding  multiple kernel learning  etc.  as special cases. Within this minimax framework  we further gave mathematically exact definition for a novel representation called sparse grouping representation (SGR)  and proved sufficient conditions for generating such group level sparsity. Under these sufficient conditions  a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information  and it outperforms group lasso when there appears group label noise. We also gave out some generalization bounds in a classification setting.,Sufﬁcient Conditions for Generating Group Level

Sparsity in a Robust Minimax Framework

Hongbo Zhou and Qiang Cheng

Computer Science department 

Southern Illinois University Carbondale  IL  62901
hongboz@siu.edu  qcheng@cs.siu.edu

Abstract

Regularization technique has become a principled tool for statistics and machine
learning research and practice. However  in most situations  these regularization
terms are not well interpreted  especially on how they are related to the loss func-
tion and data. In this paper  we propose a robust minimax framework to interpret
the relationship between data and regularization terms for a large class of loss
functions. We show that various regularization terms are essentially correspond-
ing to different distortions to the original data matrix. This minimax framework
includes ridge regression  lasso  elastic net  fused lasso  group lasso  local coordi-
nate coding  multiple kernel learning  etc.  as special cases. Within this minimax
framework  we further give mathematically exact deﬁnition for a novel represen-
tation called sparse grouping representation (SGR)  and prove a set of sufﬁcient
conditions for generating such group level sparsity. Under these sufﬁcient con-
ditions  a large set of consistent regularization terms can be designed. This SGR
is essentially different from group lasso in the way of using class or group infor-
mation  and it outperforms group lasso when there appears group label noise. We
also provide some generalization bounds in a classiﬁcation setting.

1

Introduction

A general form of estimating a quantity w ∈ Rn from an empirical measurement set X by minimiz-
ing a regularized or penalized functional is

ˆw = argmin

w

{L(Iw(X )) + λJ (w)} 

(1)

where Iw(X ) ∈ Rm expresses the relationship between w and data X   L(.) := Rm → R+ is a
loss function  J (.) := Rn → R+ is a regularization term and λ ∈ R is a weight. Positive integers
n  m represent the dimensions of the associated Euclidean spaces. Varying in speciﬁc applications 
the loss function L has lots of forms  and the most often used are these induced (A is induced
by B  means B is the core part of A) by squared Euclidean norm or squared Hilbertian norms.
Empirically  the functional J is often interpreted as smoothing function  model bias or uncertainty.
Although Equation (1) has been widely used  it is difﬁcult to establish a general mathematically
exact relationship between L and J . This directly encumbers the interpretability of parameters in
the model selection. It would be desirable if we can represent Equation (1) by a simpler form

ˆw = argmin

′

L

w

′

(I

w(X )).

(2)

Obviously  Equation (2) provides a better interpretability for the regularization term in Equation (1)
by explicitly expressing the model bias or uncertainty as a variable of the relationship functional. In
this paper  we introduce a minimax framework and show that for a large family of Euclidean norm
induced loss functions  an equivalence relationship between Equation (1) and Equation (2) can be

1

established. Moreover  the model bias or uncertainty will be expressed as distortions associated
with certain functional spaces. We will give a series of corollaries to show that well-studied lasso 
group lasso  local coordinate coding  multiple kernel learning  etc.  are all special cases of this novel
framework. As a result  we shall see that various regularization terms associated with lasso  group
lasso  etc.  can be interpreted as distortions that belong to different distortion sets.

Within this framework  we further investigate a large family of distortion sets which can generate
a special type of group level sparsity which we call sparse grouping representation (SGR). Instead
of merely designing one speciﬁc regularization term  we give sufﬁcient conditions for the distortion
sets to generate the SGR. Under these sufﬁcient conditions  a large set of consistent regularization
terms can be designed. Compared with the well-known group lasso which uses group distribution
information in a supervised learning setting  the SGR is an unsupervised one and thus essentially
different from the group lasso.
In a novel fault-tolerance classiﬁcation application  where there
appears class or group label noise  we show that the SGR outperforms the group lasso. This is not
surprising because the class or group label information is used as a core part of the group lasso while
the group sparsity produced by the SGR is intrinsic  in that the SGR does not need the class label
information as priors. Finally  we also note that the group level sparsity is of great interests due to
its wide applications in various supervised learning settings.

In this paper  we will state our results in a classiﬁcation setting. In Section 2 we will review some
closely related work  and we will introduce the robust minimax framework in Section 3. In Section
4  we will deﬁne the sparse grouping representation and prove a set of sufﬁcient conditions for
generating group level sparsity. An experimental veriﬁcation on a low resolution face recognition
task will be reported in Section 5.

2 Related Work

In this paper  we will mainly work with the penalized linear regression problem and we shall review
some closely related work here. For penalized linear regression  several well-studied regularization
procedures are ridge regression or Tikhonov regularization [15]  bridge regression [10]  lasso [19]
and subset selection [5]  fused lasso [20]  elastic net [27]  group lasso [25]  multiple kernel learning
[3  2]  local coordinate coding [24]  etc. The lasso has at least three prominent features to make itself
a principled tool among all of these procedures: continuous shrinkage and automatic variable selec-
tion at the same time  computational tractability (can be solved by linear programming methods) as
well as inducing sparsity. Recent results show that lasso can recover the solution of l0 regularization
under certain regularity conditions [8  6  7]. Recent advances such as fused lasso [20]  elastic net
[27]  group lasso [25] and local coordinate coding [24] are motivated by lasso [19].

Two concepts closely related to our work are the elastic net or grouping effect observed by [27] and
the group lasso [25]. The elastic net model hybridizes lasso and ridge regression to preserve some
redundancy for the variable selection  and it can be viewed as a stabilized version of lasso [27] and
hence it is still biased. The group lasso can produce group level sparsity [25  2] but it requires the
group label information as prior. We shall see that in a novel classiﬁcation application when there
appears class label noise [22  18  17  26]  the group lasso fails. We will discuss the differences of
various regularization procedures in a classiﬁcation setting. We will use the basic schema for the
sparse representation classiﬁcation (SRC) algorithm proposed in [21]  and different regularization
procedures will be used to replace the lasso in the SRC.

The proposed framework reveals a fundamental connection between robust linear regression and
various regularized techniques using regularization terms of l0  l1  l2  etc. Although [11] ﬁrst intro-
duced a robust model for least square problem with uncertain data and [23] discussed a robust model
for lasso  our results allow for using any positive regularization functions and a large family of loss
functions.

3 Minimax Framework for Robust Linear Regression

In this section  we will start with taking the loss function L as squared Euclidean norm  and we will
generalize the results to other loss functions in section 3.4.

2

3.1 Notations and Problem Statement

In a general M (M > 1)-classes classiﬁcation setting  we are given a training dataset T =
{(xi  gi)}n
i=1  where xi ∈ Rp is the feature vector and gi ∈ {1  · · ·   M } is the class label for
the ith observation. A data (observation) matrix is formed as A = [x1  · · ·   xn] of size p × n. Given
a test example y  the goal is to determine its class label.

3.2 Distortion Models

Assume that the jth class Cj has nj observations x(j)
x ∈ span{x(j)

1   · · ·   x(j)

nj }. We approximate y by a linear combination of the training examples:

1   · · ·   x(j)

nj . If x belongs to the jth class  then

(3)
where w = [w1  w2  · · ·   wn]T is a vector of combining coefﬁcients; and η ∈ Rp represents a vector
of additive zero-mean noise. We assume a Gaussian model v ∼ N (0  σ2I) for this additive noise 
so a least squares estimator can be used to compute the combining coefﬁcients.

y = Aw + η 

The observed training dataset T may have undergone various noise or distortions. We deﬁne the
following two classes of distortion models.

A random matrix ∆A is called bounded example-wise (or attribute) distortion
Deﬁnition 1:
(BED) with a bound λ  denoted as BED(λ)  if ∆A := [d1  · · ·   dn]  dk ∈ Rp  ||dk||2 ≤ λ  k =
1  · · ·   n. where λ is a positive parameter.

This distortion model assumes that each observation (signal) is distorted independently from the
other observations  and the distortion has a uniformly upper bounded energy (“uniformity” refers to
the fact that all the examples have the same bound). BED includes attribute noise deﬁned in [22 
26]  and some examples of BED include Gaussian noise and sampling noise in face recognition.

Deﬁnition 2: A random matrix ∆A is called bounded coefﬁcient distortion (BCD) with bound f  
denoted as BCD(f )  if ||∆Aw||2 ≤ f (w)  ∀w ∈ Rp  where f (w) ∈ R+ .

The above deﬁnition allows for any distortion with or without inter-observation dependency. For
example  we can take f (w) = λ||w||2  and Deﬁnition 2 with this f (w) means that the maximum
eigenvalue of ∆A is upper limited by λ. This can be easily seen as follows. Denote the maximum
eigenvalue of ∆A by σmax(∆A). Then we have

σmax(∆A) = sup
u v6=0

uT ∆Av
||u||2||v||2

= sup
u6=0

||∆Au||2

||u||2

 

which is a standard result from the singular value decomposition (SVD) [12]. That is  the condition
of ||∆Aw||2 ≤ λ||w||2 is equivalent to the condition that the maximum eigenvalue of ∆A is upper
bounded by λ. In fact  BED is a subset of BCD by using triangular inequality and taking special
forms of f (w). We will use D := BCD to represent the distortion model.

Besides the additive residue η generated from ﬁtting models  to account for the above distortion
models  we shall consider multiplicative noise by extending Equation (3) as follows:

y = (A + ∆A)w + η 

(4)

where ∆A ∈ D represents a possible distortion imposed to the observations.

3.3 Fundamental Theorem of Distortion

Now with the above reﬁned linear model that incorporates a distortion model  we estimate the model
parameters w by minimizing the variance of Gaussian residues for the worst distortions within a
permissible distortion set D. Thus our robust model is

min
w∈Rp

max
∆A∈D

||y − (A + ∆A)w||2.

(5)

The above minimax estimation will be used in our robust framework.

An advantage of this model is that it considers additive noise as well as multiplicative one within
a class of allowable noise models. As the optimal estimation of the model parameter in Equation

3

(5)  w∗  is derived for the worst distortion in D  w∗ will be insensitive to any deviation from the
underlying (unknown) noise-free examples  provided the deviation is limited to the tolerance level
given by D. The estimate w∗ thus is applicable to any A + ∆A with ∆A ∈ D.
In brief  the
robustness of our framework is offered by modeling possible multiplicative noise as well as the
consequent insensitivity of the estimated parameter to any deviations (within D) from the noise-free
underlying (unknown) data. Moreover  this model can seamlessly incorporate either example-wise
noise or class noise  or both.

Equation (5) provides a clear interpretation of the robust model. In the following  we will give a
theorem to show an equivalence relationship between the robust minimax model of Equation (5)
and a general form of regularized linear regression procedure.

Theorem 1. Equation (5) with distortion set D(f ) is equivalent to the following generalized regu-
larized minimization problem:

min
w∈Rp

||y − Aw||2 + f (w).

(6)

Sketch of the proof: Fix w = w∗ and establish equality between upper bound and lower bound.

||y − (A + ∆A)w∗||2 ≤ ||y − Aw∗||2 + ||∆Aw∗||2
≤ ||y − Aw∗||2 + f (w∗).

In the above we have used the triangle inequality of norms.
(y − Aw∗)/||y − Aw∗||2. Since max
∆A∈D
where t(w∗
i 6= 0  t(w∗
i (note
that w∗ is ﬁxed so we can deﬁne t(w∗))  we can actually attain the upper bound. It is easily veriﬁed
that the expression is also valid if y − Aw∗ = 0.

If y − Aw∗ 6= 0  we deﬁne u =
f (∆A) ≥ f (∆A∗)  by taking ∆A∗ = −uf (w∗)t(w∗)T /k 
i ) = 0 for w∗

i = 0 and k is the number of non-zero w∗

i ) = 1/w∗

i for w∗

Theorem 1 gives an equivalence relationship between general regularized least squares problems
and the robust regression under certain distortions. It should be noted that Equation (6) involves
min ||.||2  and the standard form for least squares problem uses min ||.||2
It
is known that these two coincide up to a change of the regularization coefﬁcient so the following
conclusions are valid for both of them. Several corollaries related to l0  l1  l2  elastic net  group
lasso  local coordinate coding  etc.  can be derived based on Theorem 1.

2 as a loss function.

l0 regularized regression is equivalent to taking a distortion set D(f l0 ) where

Corollary 1:
f l0(w) = t(w)wT   t(wi) = 1/wi for wi 6= 0  t(wi) = 0 for wi = 0.
Corollary 2: l1 regularized regression (lasso) is equivalent to taking a distortion set D(f l1 ) where
f l1(w) = λ||w||1.
Corollary 3: Ridge regression (l2) is equivalent to taking a distortion set D(f l2 ) where f l2(w) =
λ||w||2.
Corollary 4: Elastic net regression [27] (l2 + l1) is equivalent to taking a distortion set D(f e)
where f e(w) = λ1||w||1 + λ2||w||2
Corollary 5: Group lasso [25] (grouped l1 of l2) is equivalent to taking a distortion set D(f gl1 )
where f gl1(w) = Pm
Corollary 6: Local coordinate coding [24] is equivalent to taking a distortion set D(f lcc) where
f lcc(w) = Pn

j=1 dj||wj||2  dj is the weight for jth group and m is the number of group.

2  xi is ith basis  n is the number of basis  y is the test example.

2  with λ1 > 0  λ2 > 0.

i=1 |wi|||xi − y||2

Similar results can be derived for multiple kernel learning [3  2]  overlapped group lasso [16]  etc.

3.4 Generalization to Other Loss Functions

From the proof of Theorem 1  we can see the Euclidean norm used in Theorem 1 can be generalized
to other loss functions too. We only require the loss function is a proper norm in a normed vector
space. Thus  we have the following Theorem for a general form of Equation (1).

Theorem 2. Given the relationship function Iw(X ) = y − Aw and J ∈ R+ in a normed vector
space  if the loss functional L is a norm  then Equation (1) is equivalent to the following minimax
estimation with a distortion set D(J ):

min
w∈Rp

max

∆A∈D(J )

L(y − (A + ∆A)w).

(7)

4

4 Sparse Grouping Representation

4.1 Deﬁnition of SGR

We consider a classiﬁcation application where class noise is present. The class noise can be viewed
as inter-example distortions. The following novel representation is proposed to deal with such dis-
tortions.

Deﬁnition 3. Assume all examples are standardized with zero mean and unit variance. Let ρij =
i xj be the correlation for any two examples xi  xj ∈ T. Given a test example y  w ∈ Rn is deﬁned
xT
as a sparse grouping representation for y  if both of the following two conditions are satisﬁed 
(a) If wi ≥ ǫ and ρij > δ  then |wi − wj| → 0 (when δ → 1) for all i and j.
(b) If wi < ǫ and ρij > δ  then wj → 0 (when δ → 1) for all i and j.
Especially  ǫ is the sparsity threshold  and δ is the grouping threshold.

This deﬁnition requires that if two examples are highly correlated  then the resulted coefﬁcients
tend to be identical. Condition (b) produces sparsity by requiring that these small coefﬁcients will
be automatically thresholded to zero. Condition (a) preserves grouping effects [27] by selecting
all these coefﬁcients which are larger than a certain threshold. In the following we will provide
sufﬁcient conditions for the distortion set D(J ) to produce this group level sparsity.

4.2 Group Level Sparsity

As known  D(l1) or lasso can only select arbitrarily one example from many identical candidates
[27]. This leads to the sensitivity to the class noise as the example lasso chooses may be mislabeled.
As a consequence  the sparse representation classiﬁcation (SRC)  a lasso based classiﬁcation schema
[21]  is not suitable for applications in the presence of class noise. The group lasso can produce
group level sparsity  but it uses group label information to restrict the distribution of the coefﬁcients.
When there exists group label noise or class noise  group lasso will fail because it cannot correctly
determine the group. Deﬁnition 3 says that the SGR is deﬁned by example correlations and thus it
will not be affected by class noise.

In the general situation where the examples are not identical but have high within-class correlations 
we give the following theorem to show that the grouping is robust in terms of data correlation. From
now on  for distortion set D(f (w))  we require that f (w) = 0 for w = 0 and we use a special form
of f (w)  which is a sum of components fj(w) 

f (w) = µ

n

X

j=1

fj(wj).

Theorem 3. Assume all examples are standardized. Let ρij = xT
i xj be the correlation for any two
examples. For a given test example y  if both fi 6= 0 and fj 6= 0 have ﬁrst order derivatives  we
have

′

′

|f

i − f

j| ≤

2||y||2

µ q2(1 − ρij).
2 + P fj with respect to wi and wj respectively 
j = 0. The difference of these two

j {y − Aw} + µf

(8)

′

Sketch of the proof: By differentiating ||y − Aw||2
we have −2xT

i {y − Aw} + µf
i −xT

2(xT

i = 0 and −2xT
j )r

′

′

′

j =

i − f

where r = y − Aw is the residual vector. Since all examples are
equations is f
standardized  we have ||xT
2 = 2(1 − ρij) where ρ = xT
i xj. For a particular value w = 0  we
have ||r||2 = ||y||2  and thus we can get ||r||2 ≤ ||y||2 for the optimal value of w. Combining r and
||xT

j ||2  we proved the Theorem 3.

i − xT

i − xT

j ||2

µ

This theorem is different from the Theorem 1 in [27] in the following aspects: a) we have no re-
strictions on the sign of the wi or wj ; b) we use a family of functions which give us more choices to
bound the coefﬁcients. As aforementioned  it is not necessary for fi to be the same with fj and we
even can use different growth rates for different components; and c) f
i (wi) does not have to be wi
and a monotonous function with very small growth rate would be enough.

′

5

As an illustrative example  we can choose fi(wi) or fj(wj) to be a second order function with
respect to wi or wj . Then the resulted |f
j| will be the difference of the coefﬁcients λ|wi − wj|
with a constant λ. If the two examples are highly correlated and µ is sufﬁciently large  then we can
conclude that the difference of the coefﬁcients will be close to zero.

i − f

′

′

The sparsity implies an automatic thresholding ability with which all small estimated coefﬁcients
will be shrunk to zero  that is  f (w) has to be singular at the point w = 0 [9]. Incorporating this
requirement with Theorem 3  we can achieve group level sparsity: if some of the group coefﬁcients
are small and automatically thresholded to zero  all other coefﬁcients within this group will be reset
to zero too. This correlation based group level sparsity does not require any prior information on the
distribution of group labels.

To make a good estimator  there are still two properties we have to consider: continuity and un-
biasedness [9]. In short  to avoid instability  we always require the resulted estimator for w be a
continuous function; and a sufﬁcient condition for unbiasedness is that f
(|w|) = 0 when |w| is
large. Generally  the requirement of stability is not consistent with that of sparsity. Smoothness
determines the stability and singularity at zero measures the degree of sparsity. As an extreme ex-
ample  l1 can produce sparsity while l2 does not because l1 is singular while l2 is smooth at zero;
at the same time  l2 is more stable than l1. More details regarding these conditions can be found in
[1  9].

′

4.3 Sufﬁcient Condition for SGR

Based on the above discussion  we can readily construct a sparse grouping representation based
on Equation (5) where we only need to specify a distortion set D(f ∗(w)) satisfying the following
sufﬁcient conditions:

′′

′

′

j 6= 0.

∈ R+ for all f

(|wj|) = 0 for large |wj| for all j.

Lemma 1: Sufﬁcient condition for SGR.
(a). f ∗
j
(b). f ∗
j is continuous and singular at zero with respect to wj for all j.
(c). f ∗
j
Proof: Together with Theorem 3  it is easy to be veriﬁed.
As we can see  the regularization term λl1 + (1 − λ)l2
2 proposed by [27] satisﬁes the above condition
(a) and (b)  but it fails to comply with (c). So  it may become biased for large |w|. Based on
these conditions  we can easily construct regularization terms f ∗ to generate the sparse grouping
representation. We will call these f ∗ as core functions for producing the SGR. As some concrete
examples  we can construct a large family of clipped µ1Lq + µ2l2
2 where 0 < q ≤ 1 by restricting
f ′
i = wiI(|wi| < ǫ) + c for some constant ǫ and c. Also  SCAD [9] satisﬁes all three conditions
so it belongs to f ∗. This gives more theoretic justiﬁcations for previous empirical success of using
SCAD.

4.4 Generalization Bounds for Presence of Class Noise

We will follow the algorithm given in [21] and merely replace the lasso with the SGR or group lasso.
After estimating the (minimax) optimal combining coefﬁcient vector w∗ by the SGR or group lasso 
we may calculate the distance from the new test data y to the projected point in the subspace spanned
by class Ci:

di(A  w∗|Ci ) = di(A|Ci   w∗) = ||y − Aw∗|Ci ||2

(9)
j 1(xj ∈ Ci)  where

where w∗|Ci represents restricting w∗ to the ith class Ci; that is  (w∗|Ci)j = w∗
1(·) is an indicator function; and similarly A|Ci represents restricting A to the ith class Ci.

A decision rule may be obtained by choosing the class with the minimum distance:

ˆi = argmini∈{1 ···  M }{di}.

(10)

Based on these notations  we now have the following generalization bounds for the SGR in the
presence of class noise in the training data.

Theorem 4. All examples are standardized to be zero mean and unit variance. For an arbitrary
class Ci of N examples  we have p (p < 0.5) percent (fault level) of labels mis-classiﬁed into class

6

Ck 6= Ci. We assume w is a sparse grouping representation for any test example y and ρij > δ (δ
is in Deﬁnition 3) for any two examples. Under the distance function d(A|Ci   w) = d(A  w|Ci ) =
j = w for all j  we have conﬁdence threshold τ to give correct estimation ˆi for
||y − Aw|Ci ||2 and f
y  where

′

(1 − p) × N × (w0)2

 

τ ≤

d

where w0 is a constant and the conﬁdence threshold is deﬁned as τ = di(A|Ci ) − di(A|Ck ).
Sketch of the proof: Assume y is in class Ci. The correctly labeled (mislabeled  respectively) subset
for Ci is C 1
i . We use A1w to
denote Aw|C 1

i   respectively) and the size of set C 1
and A2w to denote Aw|C 2

. By triangular inequality  we have

is larger than that of C 2

i (C 2

i

i

i

τ = ||y − Aw|C 1

i

||2
||2 − ||y − Aw|C 2
≤ ||A1w − A2w||2.

i

i . Finally we subtract the summation of C 2

For each k ∈ C 1
i   we differentiate with respect to wk and do the same procedure as in proof of
Theorem 3. Then summarizing all equalities for C 1
i and repeating the same procedure for each
i ∈ C 2
i . Use the conditions
that w is a sparse grouping representation and ρij > δ  combing Deﬁnition 3  so all wk in class Ci
should be the same as a constant w0 while others → 0. By taking the l2-norm for both sides  we
have ||A1w − A2w||2 ≤ (1−p)N (w0)2
This theorem gives an upper bound for the fault-tolerance against class noise. By this theorem  we
can see that the class noise must be smaller than a certain value to guarantee a given fault correction
conﬁdence level τ .

i from the summation of C 1

d

.

5 Experimental Veriﬁcation

In this section  we compare several methods on a challenging low-resolution face recognition task
(multi-class classiﬁcation) in the presence of class noise. We use the Yale database [4] which consists
of 165 gray scale images of 15 individuals (each person is a class). There are 11 images per subject 
one per different facial expression or conﬁguration: center-light  w/glasses  happy  left-light  w/no
glasses  normal  right-light  sad  sleepy  surprised  and wink. Starting from the orignal 64 × 64
images  all images are down-sampled to have a dimension of 49. A training/test data set is generated
by uniformly selecting 8 images per individual to form the training set  and the rest of the database
is used as the test set; repeating this procedure to generate ﬁve random split copies of training/test
data sets. Five class noise levels are tested. Class noise level=p means there are p percent of labels
(uniformly drawn from all labels of each class) mislabeled for each class.

For SVM  we use the standard implementation of multiple-class (one-vs-all) LibSVM in Mat-
labArsenal1. For lasso based SRC  we use the CVX software [13  14] to solve the corresponding
convex optimization problems. The group lasso based classiﬁer is implemented in the same way as
the SRC. We use a clipped λl1 + (1 − λ)l2 as an illustrative example of the SGR  and the corre-
sponding classiﬁer is denoted as SGRC. For lasso  group Lasso and the SGR based classiﬁer  we
run through λ ∈ {0.001  0.005  0.01  0.05  0.1  0.2} and report the best results for each classiﬁer.
Figure 1 (b) shows the parameter range of λ that is appropriate for lasso  group lasso and the SGR
based classiﬁer. Figure 1 (a) shows that the SGR based classiﬁer is more robust than lasso or group
lasso based classiﬁer in terms of class noise. These results verify that in a novel application when
there exists class noise in the training data  the SGR is more suitable than group lasso for generating
group level sparsity.

6 Conclusion

Towards a better understanding of various regularized procedures in robust linear regression  we
introduce a robust minimax framework which considers both additive and multiplicative noise or
distortions. Within this uniﬁed framework  various regularization terms correspond to different

1A matlab

be
http://www.informedia.cs.cmu.edu/yanrong/MATLABArsenal/MATLABArsenal.htm.

algorithms which

classiﬁcation

package

can

for

downloaded

from

7

 

SVM
SRC
SGRC
Group lasso

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

t

e
a
r
 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

 

SRC
SGRC
Group lasso

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

t

e
a
r
 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

0.2

 

0.1

0.15

0.2

0.3

0.25
0.4
Class noise level

0.35

(a)

0.45

0.5

0.55

0.25

 
0

0.02

0.04

0.06

0.08

0.12

0.14

0.16

0.18

0.2

0.1
λ

(b)

Figure 1: (a) Comparison of SVM  SRC (lasso)  SGRC and Group lasso based classiﬁers on the low
resolution Yale face database. At each level of class noise  the error rate is averaged over ﬁve copies
of training/test datasets for each classiﬁer. For each classiﬁer  the variance bars for each class noise
level are plotted. (b) Illustration of the paths for SRC (lasso)  SGRC and group lasso. λ is the weight
for regularization term. All data points are averaged over ﬁve copies with the same class noise level
of 0.2.

distortions to the original data matrix. We further investigate a novel sparse grouping representation
(SGR) and prove sufﬁcient conditions for generating such group level sparsity. We also provide a
generalization bound for the SGR. In a novel classiﬁcation application when there exists class noise
in the training example  we show that the SGR is more robust than group lasso. The SCAD and
clipped elastic net are special instances of the SGR.

References

[1] A. Antoniadis and J. Fan. Regularitation of wavelets approximations. J. the American Statis-

tical Association  96:939–967  2001.

[2] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine

Learning Research  9:1179–1225  2008.

[3] F. Bach  G. R. G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and
the smo algorithm. In Proceedings of the Twenty-ﬁrst International Conference on Machine
Learning  2004.

[4] P. N. Bellhumer  J. Hespanha  and D. Kriegman. Eigenfaces vs. ﬁsherfaces: Recognition using
class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach. Intelligence  17(7):711–720 
1997.

[5] L. Breiman. Heuristics of instability and stabilization in model selection. Ann. Statist. 

24:2350–2383  1996.

[6] E. Cand´es  J. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate

measurements. Comm. on Pure and Applied Math  59(8):1207–1233  2006.

[7] E. Cand´es and T. Tao. Near-optimal signal recovery from random projections: Universal en-

coding strategies? IEEE Trans. Information Theory  52(12):5406–5425  2006.

[8] D. Donoho. For most large underdetermined systems of linear equations the minimum l1 nom
solution is also the sparsest solution. Comm. on Pure and Applied Math  59(6):797–829  2006.

[9] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle proper-

ties. J. Am. Statist. Ass.  96:1348–1360  2001.

[10] I. Frank and J. Friedman. A statistical view of some chemometrics regression tools. Techno-

metrics  35:109–148  1993.

[11] L. El Ghaoui and H. Lebret. Robust solutions to least-squares problems with uncertain data.

SIAM Journal Matrix Analysis and Applications  18:1035–1064  1997.

[12] G.H. Golub and C.F. Van Loan. Matrix computations. Johns Hopkins Univ Pr  1996.

8

[13] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs  recent ad-
vances in learning and control. Lecture Notes in Control and Information Sciences  pages
95–110  2008.

[14] M. Grant and S. Boyd. UCI machine learning repositorycvx: Matlab software for disciplined

convex programming  2009.

[15] A. Hoerl and R. Kennard. Ridge regression. Encyclpedia of Statistical Science  8:129–136 

1988.

[16] L. Jacob  G. Obozinski  and J.-P. Vert. Group lasso with overlap and graph lasso.

In Pro-
ceedings of the Twenty-six International Conference on Machine Learning  pages 433–440 
2009.

[17] J. Maletic and A. Marcus. Data cleansing: Beyond integrity analysis. In Proceedings of the

Conference on Information Quality  2000.

[18] K. Orr. Data quality and systems theory. Communications of the ACM  41(2):66–71  1998.

[19] R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B  58:267–

288  1996.

[20] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via the

fused lasso. J.R.Statist.Soc.B  67:91–108  2005.

[21] J. Wright  A.Y. Yang  A. Ganesh  S.S. Sastry  and Y. Ma. Robust face recognition via sparse
representation. IEEE Transactions on Pattern Analysis and Machine Intelligence  pages 210–
227  2009.

[22] X. Wu. Knowledge Acquisition from Databases. Ablex Pulishing Corp  Greenwich  CT  USA 

1995.

[23] H. Xu  C. Caramanis  and S. Mannor. Robust regression and lasso. In NIPS  2008.

[24] K. Yu  T. Zhang  and Y. Gong. Nonlinear learning using local coordinate coding. In Advances

in Neural Information Processing Systems  volume 22  2009.

[25] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of The Royal Statistical Society Series B  68(1):49–67  2006.

[26] X. Zhu  X. Wu  and S. Chen. Eliminating class noise in large datasets. In Proceedings of the
20th ICML International Conference on Machine Learning  Washington D.C.  USA  March
2003.

[27] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Statist.

Soc. B  67(2):301–320  2005.

9

,Yanbo Fan
Siwei Lyu
Yiming Ying
Baogang Hu