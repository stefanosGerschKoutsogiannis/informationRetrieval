2019,Dynamic Incentive-Aware Learning: Robust Pricing in Contextual Auctions,Motivated by pricing in ad exchange markets  we consider the problem  of  robust learning of reserve  prices against strategic  buyers in repeated contextual second-price auctions. Buyers' valuations \new{for} an item depend on the context  that describes the item.  However  the seller is not aware of  the relationship between the context  and buyers' valuations  i.e.  buyers' preferences. The seller's goal is to design a learning policy to set reserve prices via observing the past sales data  and her objective is  to minimize her regret for revenue  where the regret  is computed against   a clairvoyant policy that knows buyers' heterogeneous  preferences. Given the seller's goal   utility-maximizing buyers  have the incentive to bid untruthfully in order to manipulate the seller's learning policy.  We propose two learning policies that are robust to such strategic behavior. These policies use  the outcomes of the auctions  rather than the submitted bids  to estimate the preferences  while controlling the long-term effect of the outcome of each auction on the future reserve prices. The first policy called Contextual Robust Pricing (CORP) is designed for the setting where the market noise distribution is known to the seller and achieves a T-period regret  of  $O(d\log(Td) \log (T))$  where $d$ is the dimension of {the} contextual information.  The second policy  which is a variant of the first policy  is called Stable CORP (SCORP). This policy is tailored to the setting where  the market noise distribution is unknown to the seller and belongs to an ambiguity set.  We show that the SCORP policy has  a T-period regret  of  $O(\sqrt{d\log(Td)}\;T^{2/3})$.,Dynamic Incentive-aware Learning: Robust Pricing

in Contextual Auctions

Negin Golrezaei

Sloan School of Management

Massachusetts Institute of Technology

Cambridge  MA

golrezae@mit.edu

Adel Javanmard

Data Sciences and Operations Department

University of Southern California

Los Angeles  CA

ajavanma@usc.edu

Vahab Mirrokni
Google Research
New York  NY

mirrokni@google.com

Abstract

Motivated by pricing in ad exchange markets  we consider the problem of robust
learning of reserve prices against strategic buyers in repeated contextual second-
price auctions. Buyers’ valuations for an item depend on the context that describes
the item. However  the seller is not aware of the relationship between the context
and buyers’ valuations  i.e.  buyers’ preferences. The seller’s goal is to design
a learning policy to set reserve prices via observing the past sales data  and her
objective is to minimize her regret for revenue  where the regret is computed
against a clairvoyant policy that knows buyers’ heterogeneous preferences. Given
the seller’s goal  utility-maximizing buyers have the incentive to bid untruthfully
in order to manipulate the seller’s learning policy. We propose two learning
policies that are robust to such strategic behavior. These policies use the outcomes
of the auctions  rather than the submitted bids  to estimate the preferences while
controlling the long-term effect of the outcome of each auction on the future reserve
prices. The ﬁrst policy called Contextual Robust Pricing (CORP) is designed for
the setting where the market noise distribution is known to the seller and achieves a
T-period regret of O(d log(T d) log(T ))  where d is the dimension of the contextual
information. The second policy  which is a variant of the ﬁrst policy  is called
Stable CORP (SCORP). This policy is tailored to the setting where the market
noise distribution is unknown to the seller and belongs to an ambiguity set. We

show that the SCORP policy has a T-period regret of O((cid:112)d log(T d) T 2/3).

1

Introduction

In many online marketplaces  both sides of the market have access to rich dynamic contextual
information about the products being sold over time. On the buy side  such information can inﬂuence
the willingness-to-pay of the buyers for the products  potentially in a heterogeneous way. On the sell
side  the information can help the seller differentiate the products and set contextual and possibly
personalized prices. To do so  the seller needs to learn the impact of this information on buyers’
willingness-to-pay. Such contextual learning can be challenging for the seller when there are repeated
interactions between the buy and the sell sides. With repeated interactions  the utility-maximizing
buyers may have the incentive to act strategically and trick the learning policy of the seller into
lowering the prices. Motivated by this  our key research question is as follows: How can the seller

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

dynamically optimize (personalized) prices in a robust manner  taking into account the strategic
behavior of the buyers?
One of the online marketplaces that faces this problem is online advertising market. In this market 
a prevalent approach to sell online ads is via running real-time second-price auctions in which the
advertisers can use an abundance of detailed contextual information before deciding what to bid. In
this practice  advertisers can target Internet users based on their (heterogeneous) preferences and
targeting criteria. Targeting can create a thin and uncompetitive market in which few advertisers
show an interest in each auction. In such a thin market  it is crucial for the ad exchanges to effectively
optimize the reserve prices in order to boost their revenue. However  learning the optimal reserve
prices is rather difﬁcult due to frequent interactions between advertisers and ad exchanges.
Inspired by this environment  we study a model in which a seller runs repeated second-price auctions
with reserve over time. In each auction  an item is being sold to at most one of the buyers. The
valuation (willingness-to-pay) of each buyer for the item in period t  which is his private information 
depends on an observable d-dimensional contextual information in that period and his preference
vector. We focus on an important special case of this contextual-based valuation model in which
the buyer’s value is a linear function of his preference vector and contextual information plus some
random noise term  where the noise models the impact of contexts that are not measured/observed by
the seller. The preference vector  which is unknown to the seller  varies across the buyers. Thus  the
preference vectors capture heterogeneity in buyers’ valuation.
The seller’s goal is to design a policy that dynamically learns/optimizes personalized reserve prices.
The buyers are fully aware of the learning policy used by the seller and act strategically in order
to maximize their (time-discounted) cumulative utility. Dealing with such a strategic population of
buyers  the seller aims at extracting as much revenue as the clairvoyant policy that is cognizant of the
preference vectors a priori. These vectors determine the relationship between the valuation of the
buyers and contextual information. Put differently  the seller would like to minimize her regret where
the regret is deﬁned as the difference between the seller’s revenue and that under the clairvoyant
policy. Note that the clairvoyant policy provides a strong benchmark because the policy posts the
optimal personalized reserve prices based on the observed contexts.
As stated earlier  one of the main hurdles in designing a low-regret learning policy in this setting is
the frequent interactions between the seller and the buyers. Due to such interactions  the strategic
buyers might have the incentive to bid untruthfully. This way  they may sacriﬁce their short-term
utility in order to deceive the seller to post them lower future reserve prices. Thus  while a single shot
second-price auction is a truthful mechanism  repeated second-price auctions in which the seller aims
at dynamically learning optimal reserve prices of strategic and utility-maximizing buyers may not be
truthful. The untruthful bidding behavior of the buyers makes it hard for the seller to learn the optimal
reserve prices  and this  in turn  can lead to her revenue loss. This highlights the necessity to design a
robust learning policy that reduces buyers’ incentive to follow untruthful strategy. Beside this hurdle 
the availability of the dynamic contextual information requires the seller to change the reserve prices
dynamically over time based on the contextual information. To do so  the seller needs to learn how
buyers react to such information and based on the reactions  posts (dynamic) personalized reserve
prices.
We consider setting where the seller (ﬁrm) is more patient than the buyer. We formalize it by
considering time-discounted utility for the buyers. This is motivated by various applications. For
example  in online advertisement markets  the advertisers (buyers) who retarget Internet users prefer
showing their ads to the users who visited their website sooner rather than later. In this paper  we
propose two learning policies. The ﬁrst policy  that we call Contextual Robust Pricing (CORP)  is
tailored to a setting where the distribution of the noise term in buyers’ valuation is known to the
seller. We will refer to this noise as market or valuation noise. Our CORP policy gets the cumulative
T-period regret of order O(d log(T d) log(T ))  where the regret is computed against the clairvoyant
policy that knows the preference vectors as well as the market noise distribution. The second policy 
called Stable CORP (SCORP)  is a variant of the ﬁrst policy. This policy lends itself to a setting
where the distribution of the market noise is unknown and belongs to an ambiguity set. In this setting 
the seller does not have the intention of learning the market noise distribution. She instead would like
to design a learning policy that is robust to the uncertainty in the noise distribution. SCORP achieves

the T-period regret of order O((cid:112)d log(T d) T 2/3). Here  we highlight two important aspects of these

policies. First  they have an episodic structure and update the estimate of preference vectors only at

2

the beginning of each episode. Such design make the policy robust by restricting the future effect
of the submitted bids. Speciﬁcally  bids in an episode are not used in choosing the reserve prices
until the beginning of the next episode. Therefore  there is always a delay until a buyer observes
the effect of a bid on reserves. Then  considering the fact that buyers are impatient and discount the
future  they are less incentivized to bid untruthfully. The second important aspect of the policies that
ensure robustness is their estimation method of the buyer’s preference vectors. Rather than using the
submitted bids to estimate the preference vectors  the policy simply uses the outcome of the auctions.
Because of this feature of the policy  bidding untruthfully does not always result in lower reserve
prices; Instead  it can impact the future reserve prices of a buyer only when it leads to changing the
outcome of an auction  i.e.  when a buyer loses an auction due to underbidding or a buyer wins an
auction due to overbidding.
2 Related Work
There is a growing body of research on dynamic pricing with learning. Of necessity  we do not
provide a complete set of references  and instead refer the reader to [12] for an in-depth survey on
this area. In the following we discuss the literature that is closely related to our setting. Recently 
several works considered the problem of dynamic pricing in a contextual setting  with non-strategic
buyers. [10] studied this problem when the demand function follows the logit model and proposed
an ML-based learning algorithm. [24  11]  and [25] proposed a learning algorithm based on the
binary search method when the demand function is linear and deterministic. In their models  buyers
have homogenous preference vectors and are non-strategic. Hence  the problem reduces to a single
buyer setting  where the buyer acts myopically  i.e.  the buyer does not consider the impact of the
current actions on the future prices. There is also a new line of literature that studied dynamic pricing
with demand learning when the contextual information is high dimensional (but sparse); see [19  6].
Similar problems have been investigated in [18] (assuming varying coefﬁcient valuation models)
and [20] (considering a setting where multiple products are offered at each round).
As mentioned earlier  in our setting  the seller repeatedly interacts with a small number of strategic and
heterogeneous buyers. We note that [13] presented empirical evidence that showed buyers in online
advertising markets act strategically. The work [1  29  22] examined the problem of dynamic pricing
with strategic buyers in a non-contextual environment. In [1  29]  the seller repeatedly interacts with a
single strategic buyer via a posted-price mechanism. Similar to our setting  the seller is more patient
than the buyer in a sense that the buyer discounts his future utility. [1] showed that no learning
algorithm can obtain a sub-linear regret when the buyer is as patient as the seller. In addition  via
designing learning policies  they demonstrated that the seller can get a sub-linear regret bound when
the buyer is less patient. [22] studied dynamic pricing when a group of strategic buyers competes with
each other in repeated non-contextual second-price auctions. Further  it is assumed that products to
be sold are ex-ante identical  and that buyers are homogenous and their valuations are all drawn from
a single distribution  which is unknown to the seller. With respect to the homogeneity assumption 
we point out that there exists empirical evidence that buyers are indeed heterogeneous [16  21  15].
It is not surprising that the heterogeneity in the markets makes the design of selling mechanisms
more difﬁcult. In addition  such difﬁculties get more severe when the seller needs to design dynamic
selling mechanisms for a group of strategic buyers that compete with each other repeatedly.
Recently  [26] studied a similar problem in a static non-contextual setting with strategic buyers.
Assuming that the market power of each buyer is negligible  they design a mechanism that incentivizes
the buyers to be truthful in the ﬁrst place  by using techniques from differential privacy [28]. Closer
√
to the spirit of this paper  [2] studies the problem of pricing inventory in a repeated posted-price
auction. The authors propose a pricing algorithm whose regret is in the order of O(
log T T 2/3) in a
contextual setting  against a strategic buyer. 1 We point out that our regret result improves upon [2] in
the following directions: (i) We allow for market noise in our model  whereas [2] considers noiseless
setting which posits that buyer’s valuation is given as a linear function of features. By adding the
noise component  we make the model richer. When the noise distribution is known  our CORP
policy obtains a T-period regret of O(d log(T d) log(T )). In addition  when the noise distribution is
unknown  our SCORP policy  which is doubly robust against strategic buyers and the uncertainty in

the noise distribution  obtains a T-period regret of O((cid:112)d log(T d) T 2/3). (ii) We consider a market

of strategic buyers who participate in a second-price auction at each round  while [2]  motivated by
targeting in online advertising  considers a single buyer case. Note that in case of a single buyer  there

1Dependency on d is hidden in the big-O notation.

3

is no notion of bid  as the buyer only needs to decide if he is willing to get the item at the posted price.
By contrast  in a market of buyers  each submitted bid of a buyer can potentially affect the utility of
that buyer (instant and long-term utility)  other buyers’ utilities and the seller’s revenue.2
3 Model
Before we describe the model  we adopt some notation that will be used throughout the paper. For an
integer a  we write [a] = {1  2  . . .   a}. In addition  for a vector v ∈ Rd  we denote its jth coordinates
j=1 ujvj

by vj  for j ∈ [d]  and indicate its (cid:96)2 norm by (cid:107)v(cid:107). For two vectors v  u ∈ Rd  (cid:104)u  v(cid:105) =(cid:80)d

represents their inner product.
We consider a ﬁrm who runs repeated second-price auctions with personalized reserve over a ﬁnite
time horizon with length T . In each period t ≥ 1  the ﬁrm would like to sell an item to one of N
buyers. The item in period t is represented by an observable feature (context) vector denoted by
xt ∈ Rd. We assume that the features are drawn independently from a ﬁxed distribution D  with a
bounded support X ⊆ Rd. Note that the length of the time horizon T and distribution D are unknown
to the ﬁrm. For the sake of normalization and without loss of generality  we assume that (cid:107)xt(cid:107) ≤ 1 
and hence take X = {x ∈ Rd : (cid:107)x(cid:107) ≤ 1}. We let Σx = E[xtxT
t ] be the second moment matrix of
distribution D  and assume that Σx is a positive deﬁnite matrix  where Σx is unknown to the ﬁrm.
For buyers’ valuations  we consider a feature-based model that captures heterogeneity among the
buyers. In the following  we discuss the speciﬁcs of the valuation model. Valuation of buyer i ∈ [N ]
for an item in period t ≥ 1 depends on the feature vector xt and period t and is denoted by vit(xt).
We assume that vit(xt) is a linear function of a preference vector βi and the feature vector xt:

(1)
Whenever it is clear from the context  we may remove the dependency of valuation vit(xt) on the
feature vector xt and denote it by vit. Here  βi ∈ Rd represents the buyer i’s preference vector 
and for the sake of normalization  we assume that (cid:107)βi(cid:107) ≤ Bp  i ∈ [N ]  where Bp is a constant.
The terms zit’s  i ∈ [N ]  t ≥ 1  which are independent of the feature vector xt  are idiosyncratic
shocks and are referred to as noise. The noise terms are drawn independently and identically from a
mean zero distribution F : [−Bn  Bn] → [0  1] with density f : [−Bn  Bn] → R+  where Bn is a
constant.3 We assume that the ﬁrm knows the distribution of the noise F . We relax this assumption
later in Section 5. Note that the valuation of buyer i  vit  is not known to the ﬁrm  as the preference
vector βi and realization of the noise zit are not observable to her. In addition  by our normalization 
vit(xt) ≤ B  with B = Bp + Bn.
We make the following assumption on distribution of the noise F .
Assumption 3.1 (Log-concavity). F (z) and 1 − F (z) are log-concave in z ∈ [−Bn  Bn].
Assumption 3.1  which is prevalent in the economics literature [5]  holds by several common proba-
bility distributions including uniform  and (truncated) Laplace  exponential  and logistic distributions.
A few remarks are in order regarding Assumption 3.1. If distribution F is log-concave and its density
f is symmetric  i.e.  f (z) = f (−z)  then 1 − F (z) = F (−z) is also log-concave. Moreover  if
density f is log-concave  the distribution F is also log-concave [8]. This implies that Assumption 3.1
is satisﬁed when density f is symmetric and log-concave. We also point out that if a distribution has
a monotone hazard rate (MHR)  i.e.  1−F (z)
is decreasing in z  then 1 − F (z) is log-concave. This
point  in turn  shows that all MHR and symmetric distributions satisfy Assumption 3.1.
We next describe the repeated second-price auctions and discuss the ﬁrm’s problem. The goal of the
ﬁrm is to maximize the cumulative expected revenue in repeated second-price auctions. The ﬁrm
tries to achieve this by choosing reserves in a dynamic and personalized manner.

f (z)

vit(xt) = (cid:104)xt  βi(cid:105) + zit

i ∈ [N ]  t ≥ 1 .

3.1 Second-price Auctions with Dynamic Personalized Reserves
Before deﬁning a second-price auction  we need to establish some notation. For buyer i ∈ [N ] and
period t ≥ 1  we let pit be the payment from buyer i in period t. Further  let qit be the allocation
2Section 5 in [2] considers an extension to the multiple buyers case but assumes that the highest valuation in
each period t can be written as (cid:104)xt  β(cid:105) for a ﬁxed parameter vector β  and product feature (context) xt  which
we ﬁnd to be a strong assumption.

3The noise aims at capturing features that are not observed/measured by the ﬁrm.

4

variable: qit = 1 if the item in period t is allocated to buyer i and is zero otherwise. We also let bit
be the bid submitted by buyer i and rit be the reserve price posted by the ﬁrm for buyer i in period t.
We deﬁne bt = (b1t  . . .   bN t) and r = (r1t  . . .   rN t) as the vectors of bids and reserves in period t 
respectively. Moreover  we denote by Hτ the history set observed by the ﬁrm up to period τ. This set
includes buyers’ bids and reserve prices for all t < τ:

Hτ = {(r1  b1)  . . .   (rτ−1  bτ−1)} .

(2)

vit  deﬁned in Eq. (1).

Below  we explain the details of the second-price auction with reserve. In period t ≥ 1 
• The ﬁrm observes the feature vector xt ∼ D. In addition  each buyer i ∈ [N ] learns his valuation
• For each buyer  the ﬁrm computes reserve price rit  as a function of history set Ht.
• Each buyer i ∈ [N ] submits a bid of bit.
• Let i(cid:63) = arg maxi∈[N ]{bit}. If bi(cid:63)t ≥ ri(cid:63)t  then the item is allocated to buyer i(cid:63)  and we have
qi(cid:63)t = 1. In case of tie  the item is allocated uniformly at random to one of the buyers among those
with the highest bid. For all buyers who do not get the item  we have qit = 0.
• For each buyer i  if he gets the item (qit = 1)  then he pays pit = max{rit  maxj(cid:54)=i{bjt}}.

Otherwise  pit = 0.

t   b−

t   ri(cid:63)t = r+

t respectively denote the highest and second highest bids. Likewise  we deﬁne v+

t } if the item gets allocated and zero otherwise. We assume that for all periods t  b+

To lighten the notation  we henceforth use the following shorthands. For each period t  we let b+
t
and b−
t and v−
t
as the highest and second highest valuations in period t. We also let r+
t be the reserve price of the
buyer with the highest bid. Therefore  bi(cid:63)t = b+
t   and the ﬁrm receives a payment of
max{r+
t ≤ M
for some constant M > 0. In words  buyers submit bounded bids.
The ﬁrm’s decision in any period t ≥ 1 is to ﬁnd optimal reserve price rit  i ∈ [N ]  and her objective
is to maximize her (cumulative) expected revenue. Note that revenue of the ﬁrm is the total payment
she collects from the buyers over the length of the time horizon. Let
t }I(b+

t )(cid:3)
policy used by the ﬁrm. Then  the total revenue of the ﬁrm is given by(cid:80)T

be the expected revenue of the ﬁrm in period t ≥ 1  where the expectation is w.r.t. to the noise distri-
bution F   feature distribution D  and any randomness in the bidding strategy of buyers and learning
t=1 revt. Maximizing the
ﬁrm’s revenue is equivalent to minimizing her regret where the regret is deﬁned as the difference
between the ﬁrms’ revenue and the maximum expected revenue that the ﬁrm could earn if she knew
the preference vectors {βi}i∈[N ]. In the next section  we will formally deﬁne the ﬁrm’s regret.

revt = E(cid:104)(cid:88)

= E(cid:2)max{b−

t ≥ r+

t   r+

i∈[N ]

(3)

(cid:105)

pitqit

3.2 Benchmark and Firm’s Regret

When the preference vectors and noise distribution F are known  to set the optimal reserves rit  the
benchmark policy does not need any knowledge from the history set Ht. Thus  with the knowledge of
the preference vectors  all buyers are incentivized to bid truthfully against the benchmark policy. This
is the case because single-shot second-price auctions are strategy proof [30]. We next characterize
the benchmark policy. Let r(cid:63)
it be the reserve of buyer i in period t posted by the benchmark policy
and following our convention  we denote by r(cid:63)+
the reserve price of the buyer with the highest bid.
Proposition 3.2 (Benchmark). If the ﬁrm knows the preference vectors {βi}i∈[N ]  then the optimal
reserve price of buyer i ∈ [N ] for a feature vector x ∈ X is given by

(cid:8)y(cid:0)1 − F (y − (cid:104)x  βi(cid:105))(cid:1)(cid:9) i ∈ [N ]  x ∈ X  

(4)
r(cid:63)
i (x) = arg max
i (xt). In addition  in any period t ≥ 1  the benchmark expected revenue is given

y

it = r(cid:63)

t

and hence r(cid:63)
by

t = E(cid:2) max{v−

rev(cid:63)

t }I(v+

t ≥ r(cid:63)+

t   r(cid:63)+

t

)(cid:3)  

(5)

where expectation is w.r.t. to the noise distribution F and the feature distribution D.

5

We refer to Appendix E for the proof of Proposition 3.2. We remark that the benchmark revenue rev(cid:63)
t
is measured against truthful buyers  while the ﬁrm’s revenue under our policy is measured against
strategic buyers who may not necessarily follow the truthful strategy. Observe that the optimal reserve
price of buyer i in period t  denoted by r(cid:63)

it  solves the following optimization problem

(cid:8)y · P ((cid:104)xt  βi(cid:105) + zit ≥ y)(cid:9) .

r(cid:63)
it = arg max

y

{y · P (vit(xt) ≥ y)} = arg max

y

This shows that the optimal reserve price of buyer i does not depend on the number of buyers
participating in the auction or their preference vectors. In other words  in (lazy) second-price auctions 
when the preference vectors are known to the ﬁrm  the problem of optimizing reserve prices can be
decoupled. Because of this  the benchmark  deﬁned in Proposition 3.2  has a simple structure: For
any feature vector x ∈ X   the optimal reserve price of buyer i  r(cid:63)
i (x)  only depends on βi and feature
x  and is independent of βj  j (cid:54)= i.
Having deﬁned the benchmark  we are now ready to formally deﬁne the regret of a ﬁrm’s policy π.
Consider a policy π that posts a vector of reserve prices rπ
N t)  as a function of history
set Ht observed by the ﬁrm. Suppose that the buyers submit bids of bt = (b1t  . . .   bN t)  t ≥ 1 
where bt may not be equal to the vector of valuations vt = (v1t  . . .   vN t). The submitted bid of
buyer i  bit  can depend on the learning policy used by the ﬁrm  context xt  his valuation vit  and
history Hit  where

1t  . . .   rπ

t = (rπ

Hit = {(vi1  bi1  qi1  pi1)  . . .   (vi(t−1)  bi(t−1)  qi(t−1)  pi(t−1))}.

Recalling our notation  we write rπ+
to denote the reserve price  set by policy π  of the buyer with
the highest bid in period t. Then  the expected revenue of the ﬁrm under policy π in period t reads as
(6)
where expectation is w.r.t. to the noise distribution F   feature distribution D  and any randomness in
bidding strategy of the buyers. Then  the worst-case cumulative regret of policy π is deﬁned by

t = E(cid:2) max{b−

t ≥ rπ+

t }I(b+

)(cid:3)  

t   rπ+

revπ

t

t

t ) : (cid:107)βi(cid:107) ≤ Bp  for i ∈ [N ]  supp(D) ⊆ X(cid:111)

t − revπ

(rev(cid:63)

Regπ(T ) = max

(7)
Note that the regret of the policy π is not a function of the feature distribution D and the feature
vectors {βi}i∈[N ]. That is  we compute the regret of the policy π against the worst feature distribution
D and preference vectors {βi}i∈[N ]. In the next section  we discuss buyers’ bidding behavior.

t=1

.

(cid:110) T(cid:88)

Ui = (cid:80)∞

3.3 Utility-maximizing Buyers
We assume that each buyer i ∈ [N ] is risk neutral and aims at maximizing his (time-discounted)
cumulative expected utility. The utility of buyer i in period t ≥ 1 with valuation vit is given by
uit = vitqit−pit. Note that through the allocation variables qit  utility uit  depends on the submitted
bids of all the buyers  bt  and their reserve price rt used by the ﬁrm.
Each buyer i would like to maximize his time-discounted cumulative utility  which is deﬁned as
t=1 γtE[uit]  where γ ∈ (0  1) is a discount factor. The discount factor highlights the fact
that the ﬁrm is more patient than the buyers. For instance  in online advertising markets  advertisers
are willing to show their ads to the users who just visited their websites.4 As another example 
in cloud computing markets  the consumers would like to access enough capacity whenever they
need it [7]. We note that [1] showed that it is impossible to get a sub-linear regret when buyers are
utility-maximizer and do not discount their future utilities.
All buyers fully know the learning policy that the ﬁrm is using to set the reserves.5 Armed with this
knowledge  buyers can potentially increase their future utility they earn via bidding untruthfully.
Particularly  a buyer can underbid (shade) his bid by submitting bid bit < vit  or he can overbid by
submitting bid bit > vit. Both shading and overbidding can potentially impact the ﬁrms’ estimate
of preference vectors of the buyers and this  in turn  can hurt the ﬁrms’ revenue. However  shading
can lead to a utility loss in the current period  as by shading  the buyer may lose an auction that he
would have won by bidding truthfully. Similarly  overbidding can result in a utility loss in the current
period  as by overbidding the buyer might end up paying more than his valuation.

4Such a practice is known as retargeting [2  15].
5This assumption is inspired by the literature on the behavior-based pricing where it is shown that the ﬁrm

can earn more revenue by committing to a pricing strategy [17  31]. See also [3  4] for a similar insight.

6

Figure 1: Schematic representation of the CORP policy. The dark blue rectangles show the random exploration periods.

4 CORP: A Contextual Robust Pricing Policy
In this section  we present our learning policy. The description of the policy is provided in Table
1. For reader’s convenience  we also provide a schematic representation of CORP in Figure 1. The
policy works in an episodic manner. It tries to learn the preference vectors by using Maximum
Likelihood Estimation (MLE) and meanwhile sets the reserve prices based on its current estimates
of the preference vectors. Episodes are indexed by k = 1  2  . . .  where the length of each episode 
denoted by (cid:96)k  is given by 2k−1. Thus  episode k starts in period (cid:96)k = 2k−1 and ends in period
(cid:96)k+1 − 1 = 2k − 1. Note that the length of episodes increases exponentially with k. Throughout  we
use notation Ek to refer to periods in episode k  i.e.  Ek ≡ {(cid:96)k  . . .   (cid:96)k+1 − 1}.
At the beginning of each episode k  we estimate the preference vectors of the buyers using the
outcome of the auctions (qit’s) in the pervious episode  i.e.  episode k − 1  and we do not change our

estimates during episode k. Let(cid:98)βik be the estimated preference vector of buyer i at the beginning of
episode k. Then (cid:98)βik solves the following optimization problem:

(cid:98)βik = arg min
(cid:88)

(cid:107)β(cid:107)≤Bp

where

Lik(β) = − 1
(cid:96)k−1

t∈Ek−1

Lik(β)  i ∈ [N ]  

(cid:8)qit log(cid:0)(1 − F (max{b+−it  rit} − (cid:104)xt  β(cid:105)))(cid:1)
+ (1 − qit) log(cid:0)F (max{b+−it  rit} − (cid:104)xt  β(cid:105))(cid:1)(cid:9)

(8)

(9)

is the negative of the log-likelihood function. Here  b+−it refers to the maximum bids of buyers
other than buyer i  in period t; that is  b+−it = maxj(cid:54)=i bjt. Then  buyer i wins the auction in
period t if and only if bit > max{b+−it  rit}. Similarly  we deﬁne v+−it = maxj(cid:54)=i vjt.
Note
that F (max{b+−it  rit} − (cid:104)xt  β(cid:105)) is the probability of event (cid:104)xt  β(cid:105) + zit ≤ max{b+−it  rit}  which
is the probability that buyer i does not win the item at time t  upon bidding truthfully. The log-
likelihood function Lik(β) is computed after running the auctions in all the periods of episode Ek−1.
Therefore  the ﬁrm has access to the required knowledge to compute the log-likelihood function
Lik(β). Speciﬁcally  by the time the ﬁrm computes Lik(β)  she has access to the submitted bids of
the buyers in periods t ∈ Ek−1 as well as the reserve prices used in these periods. After estimating
the preference vectors at the beginning of each episode k  the policy proceeds to use its estimation to
set reserve prices. In particular  inspired by Proposition 3.2  the reserve price in period t ∈ Ek  rit 
solves (11).
We now discuss some of the important features of our policy.
(i) In each episode k  every period t is assigned to exploitation with probability 1 − 1/(cid:96)k  and is
assigned to exploration with probability 1/(cid:96)k. In the exploration periods  the ﬁrm chooses one
of the buyers at random and allocates the item to him if his submitted bid is above a reserve
price r ∼ uniform(0  B) where uniform(0  B) is the uniform distribution in the range [0  B]. In
exploitation periods  the ﬁrm exploits her current estimate of the preference vectors to set the reserve
prices where the estimates are obtained by applying the MLE method to the outcomes of auctions
in episode k − 1. The main purpose of setting reserve prices randomly in the exploration periods is
to motivate the buyers to be truthful. Note that the buyer does not know if in a given period t  the
prices are set randomly. Thus  if he underbids in such a period  with a positive probability  he loses
the opportunity to obtain a positive utility.

7

……Estimate"#’sEstimate"#’sEstimate"#’sEpisode k(ℓ%=2%())Episode k-1(ℓ%()=2%(*)Episode k-2(ℓ%(*=2%(+)Outcome of Auctions CORP: A Contextual Robust Pricing
Initialization: For any k ∈ Z+  let (cid:96)k = 2k−1 and Ek = {(cid:96)k  . . .   (cid:96)k+1 − 1}. Moreover  we let ri1 = 0 and

(cid:98)βi1 = 0 for any i ∈ [N ].
estimate the preference vectors  denoted by {(cid:98)βik}i∈[N ]  as follows

Updating Preference Vectors: At the start of each episode k = 1  2  . . .  i.e  at the beginning of period t = (cid:96)k 

Lik(β)  i ∈ [N ]  

(10)

(cid:98)βik = arg min

(cid:107)β(cid:107)≤Bp

where Lik(β) is deﬁned in Eq. (9).
Setting Reserves: In each episode k = 1  2  . . .  and for any period t in this episode  i.e.  t ∈ Ek 

- Exploration Phase: With probability 1
(cid:96)k

  choose one of the N buyers uniformly at random and offer him
the item at price of r ∼ uniform(0  B)  where uniform(0  B) is the uniform distribution in the range
[0  B]. For other buyers  set their reserve prices to ∞.

- Exploitation Phase: With probability 1 − 1

  observe the feature vector xt and set the reserve of each

buyer i ∈ [N ] to

(cid:96)k

(cid:8)y(cid:0)1 − F (y − (cid:104)xt (cid:98)βik(cid:105))(cid:1)(cid:9) .

rit = arg max

y

(11)

Table 1: CORP Policy

(ii) We highlight that CORP policy does not use the submitted bids in estimating the preference
vectors: It only uses the outcomes of the auctions  i.e.  qit’s  to estimate these vectors; see the
deﬁnition of the log-likelihood function in Equation (9). This makes the estimation procedure of the
policy robust to untruthful bidding behavior of the buyers  as untruthful bidding may not necessarily
lead to a different outcome. In addition  due to this feature of the learning policy  the buyers are
incentivized to bid truthfully unless they are interested in changing the outcome of the auction at the
expense of losing their current utility.
(iii) Other important factors that makes the CORP policy robust is its episodic structure and impa-
tience of buyers. In the CORP policy  submitted bids in episode k are not used in setting reserve
prices until the beginning of episode (k + 1). Therefore  there is always a delay until buyers observe
the effect of a bid on their reserves. Then  since buyers are impatient and maximize their discounted
cumulative utility  they have less incentive to bid untruthfully. This is a salient property of the
CORP policy that bounds the perpetual effect of each bid and  as we will see in the analysis  leads to
robustness of the learning policy to the strategic behavior of buyers.
Theorem 4.1 (Regret Bound: Known Market Noise Distribution). Suppose that Assumption 3.1
holds and the ﬁrm knows the market noise distribution F . Then  the T-period worst-case regret of the
CORP policy is at most O(d log(T d) log(T ))  where the regret is computed against the benchmark 
deﬁned in Proposition 3.2.

In Appendix A we give a proof sketch of Theorem 4.1 and refer to Appendix C for a detailed proof.

5 SCORP: Stable CORP Policy
The CORP policy is assumed to know the market noise distribution F . Nevertheless  in practice  it
may very well be the case that distribution F is unknown or cannot be well approximated (e.g.  it
changes over time). To address this problem  we propose a variant of the CORP policy  called Stable
Contextual Robust Pricing (SCORP)  which is robust against the lack of a precise knowledge of F .
Speciﬁcally  we consider an ambiguity set F of possible probability distributions for the market noise
and propose a policy that works well for every distribution in the ambiguity set.
Due to space constraint  we brieﬂy explain SCORP here and refer to Appendix B for more details
and a formal description of the policy. Similar to the COPR policy  SCORP has an episodic theme 
with the length of episodes growing exponentially. As before  we denote the set of periods in episode
k by Ek  i.e.  Ek = {(cid:96)k  . . .   (cid:96)k+1 − 1}  with (cid:96)k = 2k−1. However  instead of having randomized

8

exploration  each episode k starts with a pure exploration phase of length (cid:100)(cid:96)2/3
k (cid:101). We use notation
Ik to refer to periods in the pure exploration phase of episode k  i.e.  Ik ≡ {(cid:96)k  . . .   (cid:96)k + (cid:100)(cid:96)2/3
k (cid:101)}.
During each period in Ik  we choose one of the N buyers uniformly at random and offer him the item
at price of r ∼ uniform(0  B). For other buyers  we set their reserve prices to ∞. In the remaining
periods of the episode (i.e.  Ek\Ik)  we offer the reserve prices based on the current estimates of
the preference vectors which are obtained by applying the least-square estimator to the outcomes of
auctions in the pure exploration phase  Ik. This is the exploitation phase as we set reserves based
on our best guess of the preference vectors. In the least-square estimator  SCORP uses the outcome
of the auctions  not the submitted bids  which makes SCORP robust to the strategic buyers. In
addition  in the exploitation phase  SCORP chooses reserve prices in a way to maximize the worst
case revenue  over the ambiguity set F  based on the current estimate of the preference vectors. In
this sense  SCORP is robust also against the uncertainty in the noise distribution. Thus  SCORP is
indeed doubly robust. In Theorem B.3 in Appendix B  we show that SCORP achieves the T-period

worst-case regret of O((cid:112)d log(T d) T 2/3).

6 Extension to nonlinear models

Although the paper focuses on linear valuation models  it is straightforward to generalize our analysis
to some of the nonlinear valuation models. Speciﬁcally  consider model

vit(xt) = ψ((cid:104)φ(xt)  βi(cid:105) + zit)

i ∈ [N ] 

t ≥ 1  

where φ : Rd (cid:55)→ Rd is a mapping and ψ : R (cid:55)→ R is an increasing function. Then by the change of
variable ˜vit = ψ−1(vit)  ˜xt = φ(xt)   we arrive at the relation ˜vit = (cid:104)˜xt  βi(cid:105) + zit. By modifying
the CORP policy for this relation  we can get a policy that also achieve logarithmic regret for these
nonlinear settings. Some examples include: log-log model  semi-log model and logistic model. We
refer to the long version of the paper [14] for further discussion on this matter.
While these models have been popular for some applications (the ﬁrst two in hedonic pricing and
the last in click-through-rate prediction)  it is still an interesting direction to consider nonparametric
models (similar to [27] and [9] as examples) but it is beyond the scope of current paper.
7 Conclusion

Motivated by online marketplaces with highly differentiated products  we formulated a dynamic
pricing problem in the contextual setting. In this problem  a ﬁrm runs repeated second-price auctions
with reserve and the item to be sold in each period is described by a context (feature) vector. In our
model  contextual information of an item inﬂuences buyers’ valuations of that item in a heterogeneous
way  via buyers’ preference vectors. Due to the repeated interaction of buyers with the ﬁrm  buyers
have the incentive to game the ﬁrm’s policy by bidding untruthfully. We proposed two pricing policies
to set the reserve prices of buyers. These policies aim at learning the preference vectors of buyers in
a robust way against strategic buyers and meanwhile maximize the ﬁrm’s collected revenue.
The main insight behind the robustness property of our approach is that by an episodic design  we
limit the long-term effect of each bid on the ﬁrm’s estimates of the preference vectors. Further  instead
of using the bids (data) we use only the outcomes of auctions (censored data) in estimating preference
vectors. Interestingly  we show that using this censored data does not hamper the learning rate while
bringing in robustness property. As the granularity of real-time data increases at an unprecedented
rate  we believe the ideas of this work can serve as a starting point for other complex dynamic
contextual learning and decision making problems.
Acknowledgement

A. Javanmard was supported in part by an Outlier Research in Business (iORB) grant from the USC
Marshall School of Business  a Google Faculty Research Award and the NSF CAREER Award
DMS-1844481. A. Javanmard would also like to acknowledge the ﬁnancial support of the Ofﬁce of
the Provost at the University of Southern California through the Zumberge Fund Individual Grant
Program.

9

References
[1] K. Amin  A. Rostamizadeh  and U. Syed. Learning prices for repeated auctions with strategic buyers. In

Advances in Neural Information Processing Systems  pages 1169–1177  2013.

[2] K. Amin  A. Rostamizadeh  and U. Syed. Repeated contextual auctions with strategic buyers. In Advances

in Neural Information Processing Systems  pages 622–630  2014.

[3] Y. Aviv and A. Pazgal. Optimal pricing of seasonal products in the presence of forward-looking consumers.

Manufacturing & Service Operations Management  10(3):339–359  2008.

[4] Y. Aviv  M. M. Wei  and F. Zhang. Responsive pricing of fashion products: The effects of demand learning

and strategic consumer behavior. Technical report  Working Paper  Washington University  2015.

[5] M. Bagnoli and T. Bergstrom. Log-concave probability and its applications. Economic theory  26(2):445–

469  2005.

[6] G.-Y. Ban and N. B. Keskin. Personalized dynamic pricing with machine learning. 2017.

[7] C. Borgs  O. Candogan  J. Chayes  I. Lobel  and H. Nazerzadeh. Optimal multiperiod pricing with service

guarantees. Management Science  60(7):1792–1811  2014.

[8] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.

[9] N. Chen and G. Gallego. Nonparametric learning and optimization with covariates. arXiv preprint

arXiv:1805.01136  2018.

[10] X. Chen  Z. Owen  C. Pixton  and D. Simchi-Levi. A statistical learning approach to personalization in

revenue management. 2015.

[11] M. Cohen  I. Lobel  and R. Paes Leme. Feature-based dynamic pricing. 2016.

[12] A. V. den Boer. Dynamic pricing and learning: historical origins  current research  and new directions.

Surveys in operations research and management science  20(1):1–18  2015.

[13] B. Edelman and M. Ostrovsky. Strategic bidder behavior in sponsored search auctions. Decision support

systems  43(1):192–198  2007.

[14] N. Golrezaei  A. Javanmard  and V. Mirrokni. Dynamic incentive-aware learning: Robust pricing in

contextual auctions. Available at SSRN 3144034  2018.

[15] N. Golrezaei  M. Lin  V. Mirrokni  and H. Nazerzadeh. Boosted second-price auctions for heterogeneous

bidders. 2017.

[16] B. Guimaraes and K. D. Sheedy. Sales and monetary policy. American Economic Review  101(2):844–76 

2011.

[17] O. D. Hart and J. Tirole. Contract renegotiation and coasian dynamics. The Review of Economic Studies 

55(4):509–540  1988.

[18] A. Javanmard. Perishability of data: Dynamic pricing under varying-coefﬁcient models. Journal of

Machine Learning Research  18(53):1–31  2017.

[19] A. Javanmard and H. Nazerzadeh. Dynamic pricing in high-dimensions. The Journal of Machine Learning

Research  20(1):315–363  2019.

[20] A. Javanmard  H. Nazerzadeh  and S. Shao. Multi-product dynamic pricing in high-dimensions with

heterogenous price sensitivity. arXiv preprint arXiv:1901.01030  2019.

[21] J. P. Johnson and D. P. Myatt. Multiproduct quality competition: Fighting brands and product line pruning.

American Economic Review  93(3):748–774  2003.

[22] Y. Kanoria and H. Nazerzadeh. Dynamic reserve prices for repeated auctions: Learning from bids. 2017.

[23] C. Koufogiannakis and N. E. Young. A nearly linear-time ptas for explicit fractional packing and covering

linear programs. Algorithmica  70(4):648–674  2014.

[24] R. P. Leme and J. Schneider. Contextual search via intrinsic volumes.

In 2018 IEEE 59th Annual

Symposium on Foundations of Computer Science (FOCS)  pages 268–282. IEEE  2018.

10

[25] I. Lobel  R. P. Leme  and A. Vladu. Multidimensional binary search for contextual decision-making. arXiv

preprint arXiv:1611.00829  2016.

[26] M. Mahdian  V. Mirrokni  and S. Zuo. Incentive-aware learning for large markets. In Proceedings of the
26th International Conference on World Wide Web. International World Wide Web Conferences Steering
Committee  2017.

[27] J. Mao  R. Leme  and J. Schneider. Contextual pricing for lipschitz buyers. In Advances in Neural

Information Processing Systems  pages 5643–5651  2018.

[28] F. McSherry and K. Talwar. Mechanism design via differential privacy. In 48th Annual IEEE Symposium on
Foundations of Computer Science (FOCS 2007)  October 20-23  2007  Providence  RI  USA  Proceedings 
pages 94–103  2007.

[29] A. M. Medina and M. Mohri. Learning theory and algorithms for revenue optimization in second price
auctions with reserve. In Proceedings of the 31st International Conference on Machine Learning (ICML-
14)  pages 262–270  2014.

[30] R. B. Myerson. Optimal auction design. Mathematics of operations research  6(1):58–73  1981.

[31] S. W. Salant. When is inducing self-selection suboptimal for a monopolist? The Quarterly Journal of

Economics  104(2):391–397  1989.

[32] J. Tropp. Freedman’s inequality for matrix martingales. Electronic Communications in Probability 

16:262–270  2011.

[33] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed sensing 

pages 210–268. Cambridge Univ. Press  Cambridge  2012.

11

,Han Zhao
Shanghang Zhang
Guanhang Wu
José M. F. Moura
Joao Costeira
Geoffrey Gordon
Negin Golrezaei
Adel Javanmard
Vahab Mirrokni