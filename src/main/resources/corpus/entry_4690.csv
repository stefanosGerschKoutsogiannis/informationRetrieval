2010,Repeated Games against Budgeted Adversaries,We study repeated zero-sum games against an adversary on a budget. Given that an adversary has some constraint on the sequence of actions that he plays  we consider what ought to be the player's best mixed strategy with knowledge of this budget. We show that  for a general class of normal-form games  the minimax strategy is indeed efficiently computable and relies on a random playout" technique. We give three diverse applications of this algorithmic template: a cost-sensitive "Hedge" setting  a particular problem in Metrical Task Systems  and the design of combinatorial prediction markets.",Repeated Games against Budgeted Adversaries

Jacob Abernethy∗

Division of Computer Science

UC Berkeley

jake@cs.berkeley.edu

manfred@cse.ucsc.edu

Manfred K. Warmuth†

Department of Computer Science

UC Santa Cruz

Abstract

We study repeated zero-sum games against an adversary on a budget. Given that
an adversary has some constraint on the sequence of actions that he plays  we
consider what ought to be the player’s best mixed strategy with knowledge of
this budget. We show that  for a general class of normal-form games  the min-
imax strategy is indeed efﬁciently computable and relies on a “random playout”
technique. We give three diverse applications of this new algorithmic template:
a cost-sensitive “Hedge” setting  a particular problem in Metrical Task Systems 
and the design of combinatorial prediction markets.

1

Introduction

How can we reasonably expect to learn given possibly adversarial data? Overcoming this obstacle
has been one of the major successes of the Online Learning framework or  more generally  the
so-called competitive analysis of algorithms: rather than measure an algorithm only by the cost it
incurs  consider this cost relative to an optimal “comparator algorithm” which has knowledge of
the data in advance. A classic example is the so-called “experts setting”: assume we must predict a
sequence of binary outcomes and we are given access to a set of experts  each of which reveals their
own prediction for each outcome. After each round we learn the true outcome and  hence  which
experts predicted correctly or incorrectly. The expert setting is based around a simple assumption 
that while some experts’ predictions may be adversarial  we have an a priori belief that there is at
least one good expert whose predictions will be reasonably accurate. Under this relatively weak
good-expert assumption  one can construct algorithms that have quite strong loss guarantees.
Another way to interpret this sequential prediction model is to treat it as a repeated two-player
zero-sum game against an adversary on a budget; that is  the adversary’s sequence of actions is
restricted in that play ceases once the adversary exceeds the budget.
In the experts setting  the
assumption “there is a good expert” can be reinterpreted as a “nature shall not let the best expert err
too frequently”  perhaps more than some ﬁxed number of times.
In the present paper  we develop a general framework for repeated game-playing against an adver-
sary on a budget  and we provide a simple randomized strategy for the learner/player for a particular
class of these games. The proposed algorithms are based on a technique  which we refer to as a
“random playout”  that has become a very popular heuristic for solving games with massively-large
state spaces. Roughly speaking  a random playout in an extensive-form game is a way to measure
the likely outcome at a given state by ﬁnishing the game randomly from this state. Random play-
outs  often known simply as Monte Carlo methods  have become particularly popular for solving
the game of Go [5]  which has led to much follow-up work for general games [12  11]. The Bud-
geted Adversary game we consider also involves exponentially large state spaces  yet we achieve
efﬁciency using these random playouts. The key result of this paper is that the proposed random
playout is not simply a good heuristic  it is indeed minimax optimal for the games we consider.

∗Supported by a Yahoo! PhD Fellowship and NSF grant 0830410.
†Supported by NSF grant IIS-0917397.

1

Abernethy et al [1] was the ﬁrst to use a random playout strategy to optimally solve an adversarial
learning problem  namely for the case of the so-called Hedge Setting introduced by Freund and
Schapire [10]. Indeed  their model can be interpreted as a particular special case of a Budgeted
Adversary problem. The generalized framework that we give in the ﬁrst half of the paper  however 
has a much larger range of applications. We give three such examples  described brieﬂy below.
More details are given in the second half of the paper.
Cost-sensitive Hedge Setting. In the standard Hedge setting  it is assumed that each expert suffers
a cost in [0  1] on each round. But a surprisingly-overlooked case is when the cost ranges differ 
where expert i may suffer per-round cost in [0  ci] for some ﬁxed ci > 0. The vanilla approach  to
use a generic bound of maxi ci  is extremely loose  and we know of no better bounds for this case.
Our results provide the optimal strategy for this cost-sensitive Hedge setting.
Metrical Task Systems (MTS). The MTS problem is decision/learning problem similar to the
Hedge Setting above but with an added difﬁculty: the learner is required to pay the cost of moving
through a given metric space. Finding even a near-optimal generic algorithm has remained elusive
for some time  with recent encouraging progress made in one special case [2]  for the so-called
“weighted-star” metric. Our results provide a simple minimax optimal algorithm for this problem.
Combinatorial Prediction Market Design: There has been a great deal of work in designing so-
called prediction markets  where bettors may purchase contracts that pay off when the outcome of a
future event is correctly guessed. One important goal of such markets is to minimize the potential
risk of the “market maker” who sells the contracts and pays the winning bettors. Another goal is
to design “combinatorial” markets  that is where the outcome space might be complex. The latter
has proven quite challenging  and there are few positive results within this area. We show how
to translate the market-design problem into a Budgeted Adversary problem  and from here how to
incorporate certain kinds of combinatorial outcomes.

2 Preliminaries
Notation: We shall write [n] for the set {1  2  . . .   n}  and [n]∗ to be the set of all ﬁnite-length
sequences of elements of [n]. We will use the greek symbols ρ and σ to denote such sequences
i1i2 . . . iT   where it ∈ [n]. We let ∅ denote the empty sequence. When we have deﬁned some
T -length sequence ρ = i1i2 . . . iT   we may write ρt to refer to the t-length preﬁx of ρ  namely
ρt = i1i2 . . . it  and clearly t ≤ T . We will generally use w to refer to a distribution in ∆n  the
n-simplex  where wi denotes the ith coordinate of w. We use the symbol ei to denote the ith basis
vector in n dimensions  namely a vector with a 1 in the ith coordinate  and 0’s elsewhere. We shall
use 1[·] to denote the “indicator function”  where 1[predicate] is 1 if predicate is true  and
0 if it is false. It may be that predicate is a random variable  in which case 1[predicate] is a
random variable as well.

2.1 The Setting: Budgeted Adversary Games

We will now describe the generic sequential decision problem  where a problem instance is char-
acterized by the following triple: an n × n loss matrix M  a monotonic “cost function” cost :
[n]∗ → R+  and a cost budget k. A cost function is monotonic as long as it satisﬁes the relation
cost(ρσ) ≤ cost(ρiσ) for all ρ  σ ∈ [n]∗ and all i ∈ [n]. Play proceeds as follows:

1. On each round t  the player chooses a distribution wt ∈ ∆n over his action space.
2. An outcome it ∈ [n] is chosen by Nature (potentially an adversary).
3. The player suffers w>
4. The game proceeds until the ﬁrst round in which the budget is spent  i.e. the round T when

t Meit.

cost(i1i2 . . . iT−1) ≤ k < cost(i1i2 . . . iT−1iT ).

The goal of the Player is to choose each wt in order to minimize the total cost of this repeated game
on all sequences of outcomes. Note  importantly  that the player can learn from the past  and hence
would like an efﬁciently computable function w : [n]∗ → ∆n  where on round t the player is given
ρt−1 = (i1 . . . it−1) and sets wt ← w(ρt−1). We can deﬁne the worst-case cost of an algorithm

2

w : [n]∗ → ∆n by its performance against a worst-case sequence  that is

WorstCaseLoss(w; M  cost  k) :=

max

ρ = i1i2 . . . ∈ [n]∗

cost(ρT −1) ≤ k < cost(ρT )

w(ρt−1)>Meit.

TX

t=1

TX

t=1

Note that above T is a parameter chosen according to ρ and the budget. We can also deﬁne the min-
imax loss  which is deﬁned by choosing the w(·) which minimizes WorstCaseLoss(·). Speciﬁcally 

MinimaxLoss(M  cost  k) :=

min

w:[n]∗→∆n

max

ρ = i1i2 . . . ∈ [n]∗

cost(ρT −1) ≤ k < cost(ρT )

w(ρt−1)>Meit.

In the next section  we describe the optimal algorithm for a restricted class of M. That is  we obtain
the mapping w which optimizes WorstCaseLoss(w; M  cost  k).

We will start by assuming that M is a nonnegative diagonal matrix  that is M = diag(c1  c2  . . .   cn) 

3 The Algorithm
and ci > 0 for all i. With these values ci  deﬁne the distribution q ∈ ∆n with qi := 1/ciP
Given a current state ρ  the algorithm will rely heavily on our ability to compute the following
function Φ(·). For any ρ ∈ [n]∗ such that cost(ρ) > k  deﬁne Φ(ρ) := 0. Otherwise  let

j 1/cj

.

1P

Φ(ρ) :=

i 1/ci

E∀t:it∼q

" ∞X

t=0

#

1[cost(ρi1 . . . it) ≤ k]

Notice  this is the expected length of a random process. Of course  we must impose the natural con-
dition that the length of this process has a ﬁnite expectation. Also  since we assume that the cost in-
creases  it is reasonable to require that the distribution over the length  i.e. min{t : cost(ρi1 . . . it) >
k}  has an exponentially decaying tail. Under these weak conditions  the following m-trial Monte
Carlo method will provide a high probability estimate to error within O(m−1/2).

Algorithm 1 Efﬁcient Estimation of Φ(ρ)

Sample: inﬁnite random sequence σ := i1i2 . . . where Pr(it = i) = qi
Let: Ti = max{t : cost(ρσt−1) ≤ k}

for i=1. . . m do

end for
Return

Pm

i=1 Ti
m

Notice that the inﬁnite sequence σ does not have to be fully generated. Instead  we can continue to
sample the sequence and simply stop when the condition cost(ρσt−1) ≥ k is reached. We can now
deﬁne our algorithm in terms of Φ(·).

Algorithm 2 Player’s optimal strategy

Input: state ρ
Compute: Φ(ρ)  Φ(ρ  1)  Φ(ρ  2)  . . .   Φ(ρ  n)
Let: set w(ρ) with values wi(ρ) = Φ(ρ)−Φ(ρ i)

ci

4 Minimax Optimality

Now we prove that Algorithm 2 is both “legal” and minimax optimal.
Lemma 4.1. The vector w(ρ) computed in Algorithm 2 is always a valid distribution.

3

Proof. It must ﬁrst be established that wi(ρ) ≥ 0 for all i and ρ. This  however  follows because we
assume that the function cost() is monotonic  which implies that cost(ρσ) ≤ cost(ρiσ) and hence
cost(ρiσ) ≤ k =⇒ cost(ρσ) ≤ k  and hence 1[cost(ρiσ) ≤ k] ≤ 1[cost(ρσ) ≤ k]. Taking the
expected difference of the inﬁnite sum of these two indicators leads to Φ(ρ) − Φ(ρi) ≥ 0  which
implies wi(ρ) ≥ 0 as desired.

i wi(ρ) = 1. We claim that the following recurrence relation holds for

We must also show thatP

Φ(ρ) =

1P
| {z }

i 1/ci
ﬁrst step

the function Φ(ρ) whenever cost(ρ) ≤ k:

+X
|
deﬁned by q  and scaled by the constant (P
! 

wi(ρ) = X
 X

Φ(ρ) − Φ(ρi)

X

ci

i

i

i

remaining steps

qiΦ(ρi)

{z
!
i 1/ci)−1. Hence 

}
 X
+X

1/ci

=

i

  for any ρ s.t. cost(ρ) < k.

Φ(ρ) −X
!
−X

i

Φ(ρi)

ci

This is clear from noticing that Φ is an expected random walk length  with transition probabilities

=

1P
i 1/ci
where the last equality holds because qi = 1/ciP
Theorem 4.1. For M = diag(c1  . . .   cn)  Algorithm 2 is minimax optimal for the Budgeted Adver-
sary problem. Furthermore  Φ(∅) = MinimaxLoss(M  cost  k).

qiΦ(ρi)

= 1

Φ(ρi)

1/ci

j 1/cj

ci

.

i

i

i

TX

t=1

TX

t=1

TX

t=1

Proof. First we prove an upper bound. Notice that  for an sequence ρ = i1i2i3 . . . iT   the total cost
of Algorithm 2 will be

w(ρt−1)>Meit =

wit(ρt−1)cit =

Φ(ρt−1) − Φ(ρt)

cit

cit = Φ(∅) − Φ(ρT ) ≤ Φ(∅)

and hence the total cost of the algorithm is always bounded by Φ(∅).
On the other hand  we claim that Φ(∅) can always be achieved by an adversary for any algorithm
w0(·). Construct a sequence ρ as follows. Given that ρt−1 has been constructed so far  select any
coordinate it ∈ [n] for which wit(ρt−1) ≤ w0
(ρt−1)  that is  where the the algorithm w0 places at
least as much weight on it as the proposed algorithm w we deﬁned in Algorithm 2. This must always
be possible because both w(ρt−1) and w0(ρt−1) are distributions and neither can fully dominate the
other. Set ρt ← ρt−1i. Continue constructing ρ until the budget is reached  i.e. cost(ρ) > k. Now 
let us check the loss of w0 on this sequence ρ:

it

TX

t=1

TX

t=1

(ρt−1)cit ≥ TX

w0

it

t=1

w0(ρt−1)>Meit =

wit(ρt−1)cit = Φ(∅) − Φ(ρ) = Φ(∅)

Hence  an adversary can achieve at least Φ(∅) loss for any algorithm w0.

4.1 Extensions

For simplicity of exposition  we proved Theorem 4.1 under a somewhat limited scope: only for
diagonal matrices M  known budget k and cost(). But with some work  these restrictions can be
lifted. We sketch a few extensions of the result  although we omit the details due to lack of space.
First  the concept of a cost() function and a budget k is not entirely necessary. Indeed  we can
redeﬁne the Budgeted Adversary game in terms of an arbitrary stopping criterion δ : [n]∗ → {0  1} 
where δ(ρ) = 0 is equivalent to “the budget has been exceeded”. The only requirement is that δ()
is monotonic  which is naturally deﬁned as δ(ρiσ) = 1 =⇒ δ(ρσ) = 1 for all ρ  σ ∈ [n]∗ and
all i ∈ [n]. This alternative budget interpretation lets us consider the sequence ρ as a path through

4

a game tree. At a given node ρt of the tree  the adversary’s action it+1 determines which branch to
follow. As soon as δ(ρt) = 0 we have reached a terminal node of this tree.
Second  we need not assume that the budget k  or even the generalized stopping criterion δ()  is
known in advance. Instead  we can work with the following generalization: the stopping criterion δ
is drawn from a known prior λ and given to the adversary before the start of the game. The resulting
optimal algorithm depends simply on estimating a new version of Φ(ρ). Φ(ρ) is now redeﬁned as
both an expectation over a random σ and a random δ drawn from the posterior of λ  that is where
we condition on the event δ(ρ) = 1.
Third  Theorem 4.1 can be extended to a more general class of M  namely inverse-nonnegative
matrices  where M is invertible and M−1 has all nonnegative entries. (In all the examples we give
we need only diagonal M  but we sketch this generalization for completeness). If we let 1n be
the vector of n ones  then deﬁne D = diag−1(M−11n)  which is a nonnegative diagonal matrix.
Also let N = DM−1 and notice that the rows of N are the normalized rows of M−1. We can
use Algorithm 2 with the diagonal matrix D  and attain distribution w0(ρ) for any ρ. To obtain an
algorithm for the matrix M (not D)  we simply let w(ρ) = (w0(ρ)>N)>  which is guaranteed to
be a distribution. The loss of w is identical to w0 since w(ρ)>M = w0(ρ)>D by construction.
Fourth  we have only discussed minimizing loss against a budgeted adversary. But all the results
can be extended easily to the case where the player is instead maximizing gain (and the adversary
is minimizing). A particularly surprising result is that the minimax strategy is identical in either
case; that is  the the recursive deﬁnition of wi(ρ) is the same whether the player is maximizing
or minimizing. However  the termination condition might change depending on whether we are
minimizing or maximizing. For example in the expert setting  the game stops when all experts have
cost larger than k versus at least one expert has gain at least k. Therefore for the same budget size
k  the minimax value of the gain version is typically smaller than the value of the loss version.

Simpliﬁed Notation. For many examples  including two that we consider below  recording the
entire sequence ρ is unnecessary—the only relevant information is the number of times each i occurs
in ρ and not where it occurs. This is the case precisely when the function cost(ρ) is unchanged up
to permutations of ρ. In such situations  we can consider a smaller state space  which records the
“counts” of each i in the sequence ρ. We will use the notation s ∈ Nn  where st = ei1 + . . . + eit
for the sequence ρt = i1i2 . . . it.

5 The Cost-Sensitive Hedge Setting

receive a sequence of loss vectors ‘t ∈ {0  1}n. The total loss to the learner isP

A straightforward application of Budgeted Adversary games is the “Hedge setting” introduced by
Freund and Schapire [10]  a version of the aforementioned experts setting. The minimax algorithm
for this special case was already thoroughly developed by Abernethy et al [1]. We describe an
interesting extension that can be achieved using our techniques which has not yet been solved.
The Hedge game goes as follows. A learner must predict a sequence of distributions wt ∈ ∆n  and
t wt · ‘t  and the
game ceases only once the best expert has more than k errors  i.e. mini
t ‘t i > k. The learner
wants to minimize his total loss.
The natural way to transform the Hedge game into a Budgeted Adversary problem is as follows.
We’ll let s be the state  deﬁned as the vector of cumulative losses of all the experts.

P

" 1

#

M =

...

1

cost(s) = min

i

si

X

wt · ‘t =X

t

t

w>
t Meit

The proposed reduction almost works  except for one key issue: this only allows cost vectors of the
form ‘t = Meit = eit  since by deﬁnition Nature chooses columns of M. However  as shown in
Abernethy et al  this is not a problem.
Lemma 5.1 (Lemma 11 and Theorem 12 of [1]). In the Hedge game  the worst case adversary
always chooses ‘t ∈ {e1  . . .   en}.
The standard and more well-known  although non-minimax  algorithm for the Hedge setting [10]
uses a simple modiﬁcation of the Weighted Majority Algorithm [14]  and is described simply by

5

P

√

setting wi(s) = exp(−ηsi)
j exp(−ηsj ). With the appropriate tuning of η  it is possible to bound the total
loss of this algorithm by k +
2k ln n + ln n  which is known to be roughly optimal in the limit.
Abernethy et al [1] provide the minimax optimal algorithm  but state the bound in terms of an
expected length of a random walk. This is essentially equivalent to our description of the minimax
cost in terms of Φ(∅).
A signiﬁcant drawback of the Hedge result  however  is that it requires the losses to be uniformly
bounded in [0  1]  that is ‘t ∈ [0  1]n. Ideally  we would like an algorithm and a bound that can handle
non-uniform cost ranges  i.e. where expert i suffers loss in some range [0  ci]. The ‘t i ∈ [0  1]
assumption is fundamental to the Hedge analysis  and we see no simple way of modifying it to
achieve a tight bound. The simplest trick  which is just to take cmax := maxi ci  leads to a bound of
the form k +
2cmaxk ln n + cmax ln n which we know to be very loose. Intuitively  this is because
only a single “risky” expert  with a large ci  should not affect the bound signiﬁcantly.
letting M =
In our Budgeted Adversary framework 
diag(c1  . . .   cn) and cost(s) = mini sici gives us immediately an optimal algorithm that  by Theo-
rem 4.1  we know to be minimax optimal. According to the same theorem  the minimax loss bound
is simply Φ(∅) which  unfortunately  is in terms of a random walk length. We do not know how to
obtain a closed form estimate of this expression  and we leave this as an intriguing open question.

this case can be dealt with trivially:

√

6 Metrical Task Systems

A classic problem from the Online Algorithms community is known as Metrical Task Systems
(MTS)  which we now describe. A player (decision-maker  algorithm  etc.) is presented with a
ﬁnite metric space and on each of a sequence of rounds will occupy a single state (or point) within
this metric space. At the beginning of each round the player is presented with a cost vector  describ-
ing the cost of occupying each point in the metric space. The player has the option to remain at the
his present state and pay this states associated cost  or he can decide to switch to another point in
the metric and pay the cost of the new state. In the latter case  however  the player must also pay the
switching cost which is exactly the metric distance between the two points.
The MTS problem is a useful abstraction for a number of problems; among these is job-scheduling.
An algorithm would like to determine on which machine  across a large network  it should process a
job. At any given time point  the algorithm observes the number of available cycles on each machine 
and can choose to migrate the job to another machine. Of course  if the subsequent machine is a
great distance  then the algorithm also pays the travel time of the job migration through the network.
Notice that  were we given a sequence of cost vectors in advance  we could compute the optimal path
of the algorithm that minimized total cost. Indeed  this is efﬁciently solved by dynamic program-
ming  and we will refer to this as the optimal ofﬂine cost  or just the ofﬂine cost. What we would
like is an algorithm that performs well relative to the ofﬂine cost without knowledge of the sequence
of cost vectors. The standard measure of performance for an online algorithm is the competitive
ratio  which is the ratio of cost of the online algorithm to the optimal ofﬂine cost. For all the results
discussed below  we assume that the online algorithm can maintain a randomized state—a distri-
bution over the metric—and pays the expected cost according to this random choice (Randomized
algorithms tend to exhibit much better competitive ratios than deterministic algorithms).
When the metric is uniform  i.e. where all pairs of points are at unit distance  it is known that
the competitive ratio is O(log n)  where n is the number of points in the metric; this was shown
by Borodin  Linial and Saks who introduced the problem [4]. For general metric spaces  Bartal et
al achieved a competitive ratio of O(log6 n) [3]  and this was improved to O(log2 n) by Fiat and
Mendel [9]. The latter two techniques  however  rely on a scheme of randomly approximating the
metric space with a hierarchical tree metric  adding a (likely-unnecessary) multiplicative cost factor
of log n. It is widely believed that the minimax competitive ratio is O(log n) in general  but this gap
has remained elusive for at least 10 years.
The most signiﬁcant progress towards O(log n) is the 2007 work of Bansal et al [2] who achieved
such a ratio for the case of “weighted-star metrics”. A weighted star is a metric such that each point
i has a ﬁxed distance di from some “center state”  and traveling between any state i and j requires

6

going through the center  hence incurring a switching cost of di + dj. For weighted-star metrics 
Bansal et al managed to justify two simpliﬁcations which are quite useful:

1. We can assume that the cost vector is of the form h0  . . .  ∞  . . .   0i; that is  all state receive

0 cost  except some state i which receives an inﬁnite cost.

2. When the online algorithm is currently maintaining a distribution w over the metric  and an
inﬁnite cost occurs at state i  we can assume1 that algorithm incurs exactly 2diwi  exactly
the cost of having wi probability weight enter and leave i from the center.

Bansal et al provide an efﬁcient algorithm for this setting using primal-dual techniques developed
for solving linear programs. With the methods developed in the present paper  however  we can give
the minimax optimal online algorithm under the above simpliﬁcations. Notice that the adversary is
now choosing a sequence of states i1  i2  i3 . . . ∈ [n] at which to assign an inﬁnite cost. If we let
ρ = i1i2i3 . . .  then the online algorithm’s job is to choose a sequence of distributions w(ρt)  and
pays 2dit+1wit+1(ρt) at each step. In the end  the online algorithm’s cost is compared to the ofﬂine
MTS cost of ρ  which we will call cost(ρ). Assume2 we know the cost of the ofﬂine in advance  say
it’s k  and let us deﬁne M = diag(2d1  . . .   2dn). Then the player’s job is to select an algorithm w
which minimizes

TX

max

ρ = (i1  . . .   iT )

cost(ρ) ≤ k

w(ρt−1)>Meit.

algorithm is precisely lim supk→∞(cid:0) 1

t=1

k MinimaxLoss(M  cost  k)(cid:1). Notice the convenient trick here:

As we have shown  Algorithm 2 is minimax optimal for this setting. The competitive ratio of this

by bounding a priori the cost of the ofﬂine at k  we can simply imagine playing this repeated game
until the budget k is achieved. Then the competitive ratio is just the worst-case loss over the ofﬂine
cost  k. On the downside  we don’t know of any easy way to bound the worst-case loss Φ(∅).

7 Combinatorial Information Markets

We now consider the design of so-called cost-function-based information markets  a popular type
of prediction market. This work is well-developed by Chen and Pennock [7]  with much useful
discussion by Chen and Vaughn [8]. We refer the reader to the latter work  which provides a very
clear picture of the nice relationship between online learning and the design of information markets.
In the simplest setting  a prediction market is a mechanism for selling n types of contract  where
a contract of type i corresponds to some potential future outcome  say “event i will occur”. The
standard assumption is that the set of possible outcomes are mutually exclusive  so only one of the
n events will occur—for example  a pending election with n competing candidates and one eventual
winner. When a bettor purchases a contract of type i  the manager of the market  or “market maker” 
promises to pay out $1 if the outcome is i and $0 otherwise.
A popular research question in recent years is how to design such prediction markets when the out-
come has a combinatorial structure. An election might produce a complex outcome like a group
of candidates winning  and a bettor may desire to bet on a complex predicate  such as “none of
the winning candidates will be from my state”. This question is explored in Hanson [13]  although
without much discussion of the relevant computational issues. The computational aspects of com-
binatorial information markets are addressed in Chen et al [6]  who provide a particular hardness
result regarding computation of certain price functions  as well as a positive result for an alternative
type of combinatorial market. In the present section  we propose a new technique for designing
combinatorial markets using the techniques laid out in the present work.
In this type of information market  the task of a market maker is to choose a price for each of
the n contracts  but where the prices may be set adaptively according to the present demand. Let
s ∈ Nn denote the current volume  where si is the number of contracts sold of type i. In a cost-
function-based market  these prices are set according to a given convex “cost function” C(s) which

1Precisely  they claim that it should be upper-bounded by 4di. We omit the details regarding this issue  but

it only contributes a multiplicative factor of 2 to the competitive ratio.

2Even when we do not know the ofﬂine cost in advance  standard “doubling tricks” allow you to guess this

value and increase the guess as the game proceeds. For space  we omit these details.

7

b logPn

i1i2 . . . is purchased  the market maker’s total earning isP

represents a potential on the demand. It is assumed that C(·) satisﬁes the relation C(s + α1) =
C(s) + α for all s  and α > 0 and ∂2C
> 0. A typical example of such a cost function is C(s) =
∂s2
i
i=1 exp(si/b) where b is a parameter (see Chen and Pennock for further discussion [7]); it’s
easy to check this function satisﬁes the desired properties.
Given the current volume s  the price of contract i is set at C(s + ei) − C(s). This pricing scheme
has the advantage that the total money earned in this market is easy to compute: it’s exactly C(s)
regardless of the order in which the contracts were purchased. A disadvantage of this market  how-
ever  is that the posted prices (typically) sum to greater than $1! A primary goal of an information
market is to incentivize bettors to reveal their private knowledge of the outcome of an event. If a
given bettor believes the true distribution of the outcome to be q ∈ ∆n  he will have an incentive to
purchase any contract i for which the current price pi is smaller than qi  thus providing positive ex-
pected reward (relative to his predicted distribution). Using this cost-function scheme  it is possible
that qi < C(s + ei) − C(s) for all i and hence a bettor will have no incentive to bet.
We propose instead an alternative market mechanism that avoids this difﬁculty: for every given
volume state s  the market maker will advertise a price vector w(s) ∈ ∆n. If a contract of type i is
purchased  the state proceeds to s + ei  and the market maker earns wi(s). If a sequence of contracts
t w(ei1 + . . . + eit−1) · eit. On the
other hand  if the ﬁnal demand is s  in the worst case the market maker may have to payout a total of
maxi si dollars. If we assume the market maker has a ﬁxed budget k on the max number of contracts
he is willing to sell  and wants to maximize the total earned money from selling contracts subject to
this constraint  then we have3 exactly a Budgeted Adversary problem: let M be the identity and let
cost(s) := maxi si.
This looks quite similar to the Budgeted Adversary reduction in the Hedge Setting described above 
which is perhaps not too surprising given the strong connections discovered in Chen and Vaughn [8]
between learning with experts and market design. But this reduction gives us additional power: we
now have a natural way to design combinatorial prediction markets. We sketch one such example 
but we note that many more can be worked out also.
Assume we are in a setting where we have n election candidates  but some subset of size m < n will
become the “winners”  and any such subset is possible. In this case  we can imagine a market maker
selling a contract of type i with the following promise: if candidate i is in the winning subset  the
payout is 1/m and 0 otherwise. For similar reasons as above  the market maker should sell contracts
i pi = 1. If we assume that market maker has a budget constraint of k for
the ﬁnal payout  then we can handle this new setting within the Budgeted Adversary framework by
simply modifying the cost function appropriately:

at prices pi whereP

cost(s) =

max

U⊂[n] |U|=m

X

i∈U

si
m

.

This solution looks quite simple  so what did we gain? The beneﬁt of our Budgeted Adversary
framework is that we can handle arbitrary monotonic budget constraints  and the combinatorial
nature of this problem can be encoded within the budget. We showed this for the case of “subset
betting”  but it can be applied to a wide range of settings with combinatorial outcomes.

8 Open problem

We have provided a very general framework for solving repeated zero-sum games against a budgeted
adversary. Unfortunately  the generality of these results only go as far as games with payoff matrices
that are inverse-nonnegative. For one-shot games  of course  Von Neumann’s minimax theorem leads
us to an efﬁcient algorithm  i.e. linear programming  which can handle any payoff matrix  and we
would hope this is achievable here. We thus pose the following open question: Is there an efﬁcient
algorithm for solving Budgeted Adversary games for arbitrary matrices M?

3The careful reader may notice that this modiﬁed model may lead to a problem not present in the cost-
function based markets: an arbitrage opportunity for the bettors. This issue can be dealt with by including a
sufﬁcient transaction fee per contract  but we omit these details due to space constraints.

8

References
[1] J. Abernethy  M. K. Warmuth  and J. Yellin. Optimal strategies from random walks. In Pro-
ceedings of the 21st Annual Conference on Learning Theory (COLT 08)  pages 437–445  July
2008.

[2] Nikhil Bansal  Niv Buchbinder  and Joseph (Sefﬁ) Naor. A Primal-Dual randomized algorithm
for weighted paging. In Proceedings of the 48th Annual IEEE Symposium on Foundations of
Computer Science  pages 507–517. IEEE Computer Society  2007.

[3] Y. Bartal  A. Blum  C. Burch  and A. Tomkins. A polylog (n)-competitive algorithm for met-
rical task systems. In Proceedings of the twenty-ninth annual ACM symposium on Theory of
computing  page 711719  1997.

[4] A. Borodin  N. Linial  and M. E Saks. An optimal on-line algorithm for metrical task system.

Journal of the ACM (JACM)  39(4):745763  1992.

[5] B. Br¨ugmann. Monte carlo go. Master’s Thesis  Unpublished  1993.

[6] Y. Chen  L. Fortnow  N. Lambert  D. M Pennock  and J. Wortman. Complexity of combina-
torial market makers. In Proceedings of the ACM Conference on Electronic Commerce (EC) 
2008.

[7] Y. Chen and D. M Pennock. A utility framework for bounded-loss market makers. In Proceed-

ings of the 23rd Conference on Uncertainty in Artiﬁcial Intelligence  page 4956  2007.

[8] Y. Chen and J. W Vaughan. A new understanding of prediction markets via No-Regret learning.

Arxiv preprint arXiv:1003.0034  2010.

[9] A. Fiat and M. Mendel. Better algorithms for unfair metrical task systems and applications.
In Proceedings of the thirty-second annual ACM symposium on Theory of computing  page
725734  2000.

[10] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to Boosting. J. Comput. Syst. Sci.  55(1):119–139  1997. Special Issue for
EuroCOLT ’95.

[11] S. Gelly and D. Silver. Combining online and ofﬂine knowledge in UCT. In Proceedings of

the 24th international conference on Machine learning  page 280  2007.

[12] S. Gelly  Y. Wang  R. Munos  and O. Teytaud. Modiﬁcation of UCT with patterns in Monte-

Carlo go. 2006.

[13] R. Hanson. Combinatorial information market design.

5(1):107119  2003.

Information Systems Frontiers 

[14] N. Littlestone and M. K. Warmuth. The Weighted Majority algorithm.

108(2):212–261  1994. Preliminary version in FOCS 89.

Inform. Comput. 

9

,Dylan Festa
Guillaume Hennequin
Mate Lengyel
Laetitia Papaxanthos
Felipe Llinares-López
Dean Bodenham
Karsten Borgwardt
Wei-Ning Hsu
Yu Zhang
James Glass