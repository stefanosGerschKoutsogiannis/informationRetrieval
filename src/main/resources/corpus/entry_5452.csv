2011,The Fast Convergence of Boosting,This manuscript considers the convergence rate of boosting under a large class of losses  including the exponential and logistic losses  where the best previous rate of convergence was O(exp(1/ε²)).  First  it is established that the setting of weak learnability aids the entire class  granting a rate O(ln(1/ε)).  Next  the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class  and a new proof is given for the known rate O(ln(1/ε)).  Finally  it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases  yielding a rate O(1/ε)  with a matching lower bound for the logistic loss.  The principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.,The Fast Convergence of Boosting

Department of Computer Science and Engineering

Matus Telgarsky

University of California  San Diego

9500 Gilman Drive  La Jolla  CA 92093-0404

mtelgars@cs.ucsd.edu

Abstract

This manuscript considers the convergence rate of boosting under a large class of
losses  including the exponential and logistic losses  where the best previous rate
of convergence was O(exp(1/✏2)). First  it is established that the setting of weak
learnability aids the entire class  granting a rate O(ln(1/✏)). Next  the (disjoint)
conditions under which the inﬁmal empirical risk is attainable are characterized
in terms of the sample and weak learning class  and a new proof is given for the
known rate O(ln(1/✏)). Finally  it is established that any instance can be de-
composed into two smaller instances resembling the two preceding special cases 
yielding a rate O(1/✏)  with a matching lower bound for the logistic loss. The
principal technical hurdle throughout this work is the potential unattainability of
the inﬁmal empirical risk; the technique for overcoming this barrier may be of
general interest.

1

Introduction

Boosting is the task of converting inaccurate weak learners into a single accurate predictor. The
existence of any such method was unknown until the breakthrough result of Schapire [1]: under
a weak learning assumption  it is possible to combine many carefully chosen weak learners into a
majority of majorities with arbitrarily low training error. Soon after  Freund [2] noted that a single
majority is enough  and that O(ln(1/✏)) iterations are both necessary and sufﬁcient to attain accu-
racy ✏. Finally  their combined effort produced AdaBoost  which attains the optimal convergence
rate (under the weak learning assumption)  and has an astonishingly simple implementation [3].
It was eventually revealed that AdaBoost was minimizing a risk functional  speciﬁcally the expo-
nential loss [4]. Aiming to alleviate perceived deﬁciencies in the algorithm  other loss functions
were proposed  foremost amongst these being the logistic loss [5]. Given the wide practical suc-
cess of boosting with the logistic loss  it is perhaps surprising that no convergence rate better than
O(exp(1/✏2)) was known  even under the weak learning assumption [6]. The reason for this de-
ﬁciency is simple: unlike SVM  least squares  and basically any other optimization problem con-
sidered in machine learning  there might not exist a choice which attains the minimal risk! This
reliance is carried over from convex optimization  where the assumption of attainability is generally
made  either directly  or through stronger conditions like compact level sets or strong convexity [7].
Convergence rate analysis provides a valuable mechanism to compare and improve of minimiza-
tion algorithms. But there is a deeper signiﬁcance with boosting: a convergence rate of O(ln(1/✏))
means that  with a combination of just O(ln(1/✏)) predictors  one can construct an ✏-optimal clas-
siﬁer  which is crucial to both the computational efﬁciency and statistical stability of this predictor.
The contribution of this manuscript is to provide a tight convergence theory for a large class of
losses  including the exponential and logistic losses  which has heretofore resisted analysis. The goal
is a general analysis without any assumptions (attainability of the minimum  or weak learnability) 

1

however this manuscript also demonstrates how the classically understood scenarios of attainability
and weak learnability can be understood directly from the sample and the weak learning class.
The organization is as follows. Section 2 provides a few pieces of background: how to encode the
weak learning class and sample as a matrix  boosting as coordinate descent  and the primal objective
function. Section 3 then gives the dual problem  max entropy. Given these tools  section 4 shows
how to adjust the weak learning rate to a quantity which is useful without any assumptions. The ﬁrst
step towards convergence rates is then taken in section 5  which demonstrates that the weak learning
rate is in fact a mechanism to convert between the primal and dual problems.
The convergence rates then follow: section 6 and section 7 discuss  respectively  the conditions
under which classical weak learnability and (disjointly) attainability hold  both yielding the rate
O(ln(1/✏))  and ﬁnally section 8 shows how the general case may be decomposed into these two 
and the conﬂicting optimization behavior leads to a degraded rate of O(1/✏). The last section will
also exhibit an ⌦(1/✏) lower bound for the logistic loss.

1.1 Related Work

The development of general convergence rates has a number of important milestones in the past
decade. The ﬁrst convergence result  albeit without any rates  is due to Collins et al. [8]; the work
considered the improvement due to a single step  and as its update rule was less aggressive than the
line search of boosting  it appears to imply general convergence. Next  Bickel et al. [6] showed a
rate of O(exp(1/✏2))  where the assumptions of bounded second derivatives on compact sets are
also necessary here.
Many extremely important cases have also been handled. The ﬁrst is the original rate of O(ln(1/✏))
for the exponential loss under the weak learning assumption [3]. Next  R¨atsch et al. [9] showed  for
a class of losses similar to those considered here  a rate of O(ln(1/✏)) when the loss minimizer is
attainable. The current manuscript provides another mechanism to analyze this case (with the same
rate)  which is crucial to being able to produce a general analysis. And  very recently  parallel to this
work  Mukherjee et al. [10] established the general convergence under the exponential loss  with a
rate of ⇥(1/✏). The same matrix  due to Schapire [11]  was used to show the lower bound there as
for the logistic loss here; their upper bound proof also utilized a decomposition theorem.
It is interesting to mention that  for many variants of boosting  general convergence rates were
known. Speciﬁcally  once it was revealed that boosting is trying to be not only correct but also
have large margins [12]  much work was invested into methods which explicitly maximized the
margin [13]  or penalized variants focused on the inseparable case [14  15]. These methods generally
impose some form of regularization [15]  which grants attainability of the risk minimizer  and allows
standard techniques to grant general convergence rates. Interestingly  the guarantees in those works
cited in this paragraph are O(1/✏2).

2 Setup

A view of boosting  which pervades this manuscript  is that the action of the weak learning class
upon the sample can be encoded as a matrix [9  15]. Let a sample S := {(xi  yi)}m
1 ✓ (X⇥Y )m
and a weak learning class H be given. For every h 2H   let S|h denote the projection onto S
induced by h; that is  S|h is a vector of length m  with coordinates (S|h)i = yih(xi). If the set
of all such columns {S|h : h 2H} is ﬁnite  collect them into the matrix A 2 Rm⇥n. Let ai
denote the ith row of A  corresponding to the example (xi  yi)  and let {hj}n
1 index the set of weak
learners corresponding to columns of A. It is assumed  for convenience  that entries of A are within
[1  +1]; relaxing this assumption merely scales the presented rates by a constant.
The setting considered in this manuscript is that this ﬁnite matrix can be constructed. Note that this
can encode inﬁnite classes  so long as they map to only k < 1 values (in which case A has at most
km columns). As another example  if the weak learners are binary  and H has VC dimension d 
then Sauer’s lemma grants that A has at most (m + 1)d columns. This matrix view of boosting is
thus similar to the interpretation of boosting performing descent on functional space  but the class
complexity and ﬁnite sample have been used to reduce the function class to a ﬁnite object [16  5].

2

Routine BOOST.
Input Convex function f  A.
Output Approximate primal optimum .
1. Initialize 0 := 0n.
2. For t = 1  2  . . .  while r(f  A)(t1) 6= 0n:

(a) Choose column jt := argmaxj |r(f  A)(t1)>ej|.
(b) Line search: ↵t apx. minimizes ↵ 7! (f  A)(t1 + ↵ejt).
(c) Update t := t1 + ↵tejt.

3. Return t1.

Figure 1: l1 steepest descent [17  Algorithm 9.4] applied to f  A.

To make the connection to boosting  the missing ingredient is the loss function. Let G0 denote the
set of loss functions g satisfying: g is twice continuously differentiable  g00 > 0 (which implies
strict convexity)  and limx!1 g(x) = 0. (A few more conditions will be added in section 5 to prove
convergence rates  but these properties sufﬁce for the current exposition.) Crucially  the exponential
loss exp(x) from AdaBoost and the logistic loss ln(1 + exp(x)) are in G0 (and the eventual G).
Boosting determines some weighting  2 Rn of the columns of A  which correspond to weak
learners in H. The (unnormalized) margin of example i is thus hai i = e>i A  where ei is an
indicator vector. Since the prediction on xi is 1[hai i  0]  it follows that A > 0m (where 0m is
the zero vector) implies a training error of zero. As such  boosting solves the minimization problem

mXi=1

mXi=1

inf
2Rn

g(hai i) = inf
2Rn

g(e>i A) = inf
2Rn

f (A) = inf
2Rn

(f  A)() =: ¯fA 

(2.1)

where f : Rm ! R is the convenience function f (x) = Pi g((x)i)  and in the present problem

denotes the (unnormalized) empirical risk. ¯fA will denote the optimal objective value.
The inﬁmum in eq. (2.1) may well not be attainable. Suppose there exists 0 such that A0 > 0m
(theorem 6.1 will show that this is equivalent to the weak learning assumption). Then
f (c(A0)) = 0.

f (A)  inf {f (A) :  = c0  c > 0} = inf

c>0

0  inf
2Rn

On the other hand  for any  2 Rn  f (A) > 0. Thus the inﬁmum is never attainable when weak
learnability holds.
The template boosting algorithm appears in ﬁg. 1  formulated in terms of f  A to make the connec-
tion to coordinate descent as clear as possible. To interpret the gradient terms  note that

(r(f  A)())j = (A>rf (A))j =

g0(hai i)hj(xi)yi 

mXi=1

which is the expected correlation of hj with the target labels according to an unnormalized dis-
tribution with weights g0(hai i). The stopping condition r(f  A)() = 0m means: either the
distribution is degenerate (it is exactly zero)  or every weak learner is uncorrelated with the target.
As such  eq. (2.1) represents an equivalent formulation of boosting  with one minor modiﬁcation:
the column (weak learner) selection has an absolute value. But note that this is the same as closing
H under complementation (i.e.  for any h 2H   there exists h0 with h(x) = h0(x))  which is
assumed in many theoretical treatments of boosting.
In the case of the exponential loss with binary weak learners  the line search step has a conve-
nient closed form; but for other losses  or even for the exponential loss but with conﬁdence-rated
predictors  there may not be a closed form. Moreover  this univariate search problem may lack a
minimizer. To produce the eventual convergence rates  this manuscript utilizes a step size minimiz-
ing an upper bounding quadratic (which is guaranteed to exist); if instead a standard iterative line
search guarantee were used  rates would only degrade by a constant factor [17  section 9.3.1].

3

1 of A as a collection of m points in Rn. Due to the form
As a ﬁnal remark  consider the rows {ai}m
of g  BOOST is therefore searching for a halfspace  parameterized by a vector   which contains
all of the points. Sometimes such a halfspace may not exist  and g applies a smoothly increasing
penalty to points that are farther and farther outside it.

3 Dual Problem

This section provides the convex dual to eq. (2.1). The relevance of the dual to convergence rates is
as follows. First  although the primal optimum may not be attainable  the dual optimum is always
attainable—this suggests a strategy of mapping the convergence strategy to the dual  where there
exists a clear notion of progress to the optimum. Second  this section determines the dual feasible
set—the space of dual variables or what the boosting literature typically calls unnormalized weights.
Understanding this set is key to relating weak learnability  attainability  and general instances.
Before proceeding  note that the dual formulation will make use of the Fenchel conjugate h⇤() =
supx2dom(h) hx  i  h(x)  a concept taking a central place in convex analysis [18  19]. Interest-
ingly  the Fenchel conjugates to the exponential and logistic losses are respectively the Boltzmann-
Shannon and Fermi-Dirac entropies [19  Commentary  section 3.3]  and thus the dual is explicitly
performing entropy maximization (cf. lemma C.2). As a ﬁnal piece of notation  denote the kernel of
a matrix B 2 Rm⇥n by Ker(B) = { 2 Rn : B = 0m}.
Theorem 3.1. For any A 2 Rm⇥n and g 2 G0 with f (x) =Pi g((x)i) 
inf {f (A) :  2 Rn} = sup{f⇤() :  2 A}  
where A := Ker(A>)\Rm
Lastly  f⇤() =Pm
+ has a strong interpretation. Suppose 2 A; then 
The dual feasible set A = Ker(A>) \ Rm
+ )  and  for any j  0 = (>A)j = Pm
is a nonnegative vector (since 2 Rm
i=1 iyihj(xi). That
is to say  every nonzero feasible dual vector provides a (an unnormalized) distribution upon which
every weak learner is uncorrelated! Furthermore  recall that the weak learning assumption states that
under any weighting of the input  there exists a correlated weak learner; as such  weak learnability
necessitates that the dual feasible set contains only the zero vector.
There is also a geometric interpretation. Ignoring the constraint  f⇤ attains its maximum at some
rescaling of the uniform distribution (for details  please see lemma C.2). As such  the constrained
dual problem is aiming to write the origin as a high entropy convex combination of the points {ai}m
1 .

+ is the dual feasible set. The dual optimum A is unique and attainable.

i=1 g⇤(()i).

(3.2)

4 A Generalized Weak Learning Rate

The weak learning rate was critical to the original convergence analysis of AdaBoost  providing a
handle on the progress of the algorithm. Recall that the quantity appeared in the denominator of the
convergence rate  and a weak learning assumption critically provided that this quantity is nonzero.
This section will generalize the weak learning rate to a quantity which is always positive  without
any assumptions.
Note brieﬂy that this manuscript will differ slightly from the norm in that weak learning will be a
purely sample-speciﬁc concept. That is  the concern here is convergence  and all that matters is the
1   as encoded in A; it doesn’t matter if there are wild points outside this
sample S = {(xi  yi)}m
sample  because the algorithm has no access to them.
This distinction has the following implication. The usual weak learning assumption states that there
exists no uncorrelating distribution over the input space. This of course implies that any training
sample S used by the algorithm will also have this property; however  it sufﬁces that there is no
distribution over the input sample S which uncorrelates the weak learners from the target.
Returning to task  the weak learning assumption posits the existence of a constant  the weak learning
rate   which lower bounds the correlation of the best weak learner with the target for any distribu-

4

tion. Stated in terms of the matrix A 

0 < = inf
2Rm
kk=1

+

mXi=1

max

j2[n]

()iyihj(xi)

=

inf
2Rm
+ \{0m}

kA>k1
kk1

=

inf
2Rm
+ \{0m}

kA>k1
k  0mk1

.

(4.1)
The only way this quantity can be positive is if  62 Ker(A>) \ Rm
+ = A  meaning the dual
feasible set is exactly {0m}. As such  one candidate adjustment is to simply replace {0m} with the
dual feasible set:

0 := inf
2Rm

+ \A

kA>k1

inf 2A k  k1

.

Indeed  by the forthcoming proposition 4.3  0 > 0 as desired. Due to technical considerations
which will be postponed until the various convergence rates  it is necessary to tighten this deﬁnition
with another set.
Deﬁnition 4.2. For a given matrix A 2 Rm⇥n and set S ✓ Rm  deﬁne

(A  S) := inf⇢

kA>k1

inf 2S\Ker(A>) k  k1

:  2 S \ Ker(A>) .

⌃

Crucially  for the choices of S pertinent here  this quantity is always positive.
Proposition 4.3. Let A 6= 0m⇥n and polyhedron S be given. If S \ Ker(A>) 6= ; and S has
nonempty interior  (A  S) 2 (0 1).
To simplify discussion  the following projection and distance notation will be used in the sequel:

Pp
C(x) 2 Argmin

y2C ky  xkp 

Dp
C(x) = kx  Pp

C(x)kp 

with some arbitrary choice made when the minimizer is not unique.

5 Prelude to Convergence Rates: Three Alternatives

The pieces are in place to ﬁnally sketch how the convergence rates may be proved. This section
identiﬁes how the weak learning rate (A  S) can be used to convert the standard gradient guarantees
into something which can be used in the presence of no attainable minimum. To close  three basic
optimization scenarios are identiﬁed  which lead to the following three sections on convergence
rates. But ﬁrst  it is a good time to deﬁne the ﬁnal loss function class.
Deﬁnition 5.1. Every g 2 G satisﬁes the following properties. First  g 2 G0. Next  for any x 2 Rm
satisfying f (x)  f (A0)  and for any coordinate (x)i  there exist constants ⌘> 0 and > 0 such
⌃
that g00((x)i)  ⌘g((x)i) and g((x)i)  g0((x)i).
The exponential loss is in this class with ⌘ =  = 1 since exp(·) is a ﬁxed point with respect to
the differentiation operator. Furthermore  as is veriﬁed in remark F.1 of the full version  the logistic
loss is also in this class  with ⌘ = 2m/(m ln(2)) and   1 + 2m. Intuitively  ⌘ and  encode
how similar some g 2 G is to the exponential loss  and thus these parameters can degrade radically.
However  outside the weak learnability case  the other terms in the bounds here will also incur a
penalty of the form em for the exponential loss  and there is some evidence that this is unavoidable
(see the lower bounds in Mukherjee et al. [10] or the upper bounds in R¨atsch et al. [9]).
Next  note how the standard guarantee for coordinate descent methods can lead to guarantees on the
progress of the algorithm in terms of dual distances  thanks to (A  S).
Proposition 5.2. For any t  A 6= 0m⇥n  S ◆ {rf (At)} with (A  S) > 0  and g 2 G 

f (At+1)  ¯fA  f (At)  ¯fA 

(A  S)2D1

S\Ker(A>)(rf (At))2
2⌘f (At)

.

Proof. The stopping condition grants rf (At) 62 Ker(A>). Thus  by deﬁnition of (A  S) 

(A  S) =

inf

2S\Ker(A>)

kA>k1
S\Ker(A>)() 

D1

kA>rf (At)k1
D1
S\Ker(A>)(rf (At))

.

5

(a) Weak learnability.

(b) Attainability.

(c) General case.

Figure 2: Viewing the rows {ai}m
1 of A as points in Rn  boosting seeks a homogeneous halfspace 
parameterized by a normal  2 Rn  which contains all m points. The dual  on the other hand  aims
to express the origin as a high entropy convex combination of the rows. The convergence rate and
dynamics of this process are controlled by A  which dictates one of the three above scenarios.

Combined with a standard guarantee of coordinate descent progress (cf. lemma F.2) 

f (At)  f (At+1)  kA>rf (At)k2

2⌘f (At)

1

(A  S)2D1

S\Ker(A>)(rf (At))2
2⌘f (At)

.



Subtracting ¯fA from both sides and rearranging yields the statement.

Recall the interpretation of boosting closing section 2: boosting seeks a halfspace  parameterized by
1 . Progress onward from proposition 5.2 will be divided
 2 Rn  which contains the points {ai}m
into three cases  each distinguished by the kind of halfspace which boosting can reach.
These cases appear in ﬁg. 2. The ﬁrst case is weak learnability: positive margins can be attained
on each example  meaning a halfspace exists which strictly contains all points. Boosting races to
push all these margins unboundedly large  and has a convergence rate O(ln(1/✏)). Next is the case
that no halfspace contains the points within its interior: either any such halfspace has the points on
its boundary  or no such halfspace exists at all (the degenerate choice  = 0n). This is the case of
attainability: boosting races towards ﬁnite margins at the rate O(ln(1/✏)).
The ﬁnal situation is a mix of the two: there exists a halfspace with some points on the boundary 
some within its interior. Boosting will try to push some margins to inﬁnity  and keep others ﬁnite.
These two desires are at odds  and the rate degrades to O(1/✏). Less metaphorically  the analysis
will proceed by decomposing this case into the previous two  applying the above analysis in parallel 
and then stitching the result back together. It is precisely while stitching up that an incompatibility
arises  and the rate degrades. This is no artifact: a lower bound will be shown for the logistic loss.

6 Convergence Rate under Weak Learnability

To start this section  the following result characterizes weak learnability  including the earlier rela-
tionship to the dual feasible set (speciﬁcally  that it is precisely the origin)  and  as analyzed by many
authors  the relationship to separability [1  9  15].
Theorem 6.1. For any A 2 Rm⇥n and g 2 G the following conditions are equivalent:

f (A) = 0 

9 2 Rn ⇧ A 2 Rm
++ 
inf
2Rn
 A = 0m 
A = {0m}.

(6.2)
(6.3)

(6.4)
(6.5)

The equivalence means the presence of any of these properties sufﬁces to indicate weak learnability.
The last two statements encode the usual distributional version of the weak learning assumption.

6

The ﬁrst encodes the fact that there exists a homogeneous halfspace containing all points within
its interior; this encodes separability  since removing the factor yi from the deﬁnition of ai will
place all negative points outside the halfspace. Lastly  the second statement encodes the fact that the
empirical risk approaches zero.
Theorem 6.6. Suppose A > 0m and g 2 G; then (A  Rm
f (At)  ¯fA  f (A0)✓1 
+ \ Ker(A>) = A = {0m}  which combined with g  g0 gives

+ ) > 0  and for all t 
(A  Rm

22⌘ ◆t

Proof. By theorem 6.1  Rm

+ )2

.

D1
A(rf (At)) = inf

 2A k  rf (At)  k1 = krf (At)k1  f (At)/.

Plugging this and ¯fA = 0 (again by theorem 6.1) along with polyhedron Rm
(A  Rm

+ ) > 0 by proposition 4.3 since A 2 Rm

+ ) into proposition 5.2 gives

f (At+1)  f (At) 

(A  Rm

+ )2f (At)
22⌘

and recursively applying this inequality yields the result.

+ ◆ rf (Rm) (whereby
22⌘ ◆  
(A  Rm

+ )2

= f (At)✓1 

+ grants
+ ) is exactly the original weak learning rate. When specialized for the exponential loss
+ )2/2)t  which exactly recovers the bound of

Since the present setting is weak learnability  note by (4.1) that the choice of polyhedron Rm
that (A  Rm
(where ⌘ =  = 1)  the bound becomes (1  (A  Rm
Schapire and Singer [20]  although via different analysis.
In general  solving for t in the expression ✏ = f (At) ¯fA
reveals that t  22⌘
(A S)2 ln(1/✏) iterations sufﬁce to reach error ✏. Recall that  and ⌘  in the case of
the logistic loss  have only been bounded by quantities like 2m. While it is unclear if this analysis
of  and ⌘ was tight  note that it is plausible that the logistic loss is slower than the exponential loss
in this scenario  as it works less in initial phases to correct minor margin violations.

f (A0) ¯fA ⇣1  (f A)2
22⌘ ⌘t

 exp⇣ (f A)2t
22⌘ ⌘

7 Convergence Rate under Attainability
Theorem 7.1. For any A 2 Rm⇥n and g 2 G  the following conditions are equivalent:

+ \ {0m} 

8 2 Rn ⇧ A 62 Rm
f  A has minimizers 
 A 2 Rm
++ 
A \ Rm
++ 6= ;.

(7.2)
(7.3)
(7.4)
(7.5)

Interestingly  as revealed in (7.4) and (7.5)  attainability entails that the dual has fully interior points 
and furthermore that the dual optimum is interior. On the other hand  under weak learnability 
eq. (6.4) provided that the dual optimum has zeros at every coordinate. As will be made clear in
section 8  the primal and dual weights have the following dichotomy: either the margin hai i goes
to inﬁnity and ( A)i goes to zero  or the margin stays ﬁnite and ( A)i goes to some positive value.
Theorem 7.6. Suppose A 6= 0m⇥n  g 2 G  and the inﬁmum of eq. (2.1) is attainable. Then there
exists a (compact) tightest axis-aligned retangle C containing the initial level set  and f is strongly
convex with modulus c > 0 over C. Finally  (A rf (C)) > 0  and for all t 
◆t
c(A rf (C))2

⌘f (A0)

.

f (At)  ¯fA  (f (0m)  ¯fA)✓1 
c(A rf (C))2 ln( 1

⌘f (A0)

✏ ) iterations sufﬁce to reach error ✏. The appearance of a
In other words  t 
modulus of strong convexity c (i.e.  a lower bound on the eigenvalues of the Hessian of f) may seem
surprising  and sketching the proof illuminates its appearance and subsequent function.

7

When the inﬁmum is attainable  every margin hai i converges to some ﬁnite value. In fact  they all
remain bounded: (7.2) provides that no halfspace contains all points  so if one margin becomes pos-
itive and large  another becomes negative and large  giving a terrible objective value. But objective
values never increase with coordinate descent. To ﬁnish the proof  strong convexity (i.e.  quadratic
lower bounds in the primal) grants quadratic upper bounds in the dual  which can be used to bound
the dual distance in proposition 5.2  and yield the desired convergence rate. This approach fails
under weak learnability—some primal weights grow unboundedly  all dual weights shrink to zero 
and no compact set contains all margins.

8 General Convergence Rate

The ﬁnal characterization encodes two principles: the rows of A may be partitioned into two matri-
ces A0  A+ which respectively satisfy theorem 6.1 and theorem 7.1  and that these two subproblems
affect the optimization problem essentially independently.
Theorem 8.1. Let A0 2 Rz⇥n  A+ 2 Rp⇥n  and g 2 G be given. Set m := z + p  and A 2 Rm⇥n
to be the matrix obtained by stacking A0 on top of A+. The following conditions are equivalent:
(8.2)
(8.3)

++ ^ A+ = 0p) ^ (8 2 Rn ⇧ A+ 62 Rp
f (A+)) ^ ( inf
2Rn

+ \ {0p}) 

f (A0) = 0) ^ f  A+ has minimizers 
++ 

(9 2 Rn ⇧ A0 2 Rz
f (A) = inf
( inf
2Rn
2Rn
 A =h A0
(A0 = {0z}) ^ (A+ \ Rp

 A+i with A0 = 0z ^ A+ 2 Rp

(8.4)
(8.5)

++ 6= ;) ^ (A = A0 ⇥ A+).

To see that any matrix A falls into one of the three scenarios here  ﬁx a loss function g  and recall
from theorem 3.1 that A is unique. In particular  the set of zero entries in A exactly speciﬁes
which of the three scenarios hold  the current scenario allowing for simultaneous positive and zero
entries. Although this reasoning made use of A  note that it is A which dictates the behavior: in
fact  as is shown in remark I.1 of the full version  the decomposition is unique.
Returning to theorem 8.1  the geometry of ﬁg. 2c is provided by (8.2) and (8.5). The analysis
will start from (8.3)  which allows the primal problem to be split into two pieces  which are then
individually handled precisely as in the preceding sections. To ﬁnish  (8.5) will allow these pieces
to be stitched together.
Theorem 8.6. Suppose A 6= 0m⇥n  g 2 G  A 2 Rm
++ \ {0m}  and the notation from
(rf (A+t))k1. Then w < 1  and there exists
theorem 8.1. Set w := supt krf (A+t) + P1
a tightest cube C+ so that C+ ◆{ x 2 Rp : f (x)  f (A0)}  and let c > 0 be the modulus of
+ ⇥ rf (C+)) > 0  and for all t  f (At)  ¯fA 
strong convexity of f over C+. Then (A  Rz
2f (A0)/(t + 1) min1  (A  Rz
(In the case of the logistic loss  w  supx2Rm krf (x)k1  m.)
As discussed previously  the bounds deteriorate to O(1/✏) because the ﬁnite and inﬁnite margins
sought by the two pieces A0  A+ are in conﬂict. For a beautifully simple  concrete case of this 
consider the following matrix  due to Schapire [11]:

+ ⇥ rf (C+))2/(( + w/(2c))2⌘)  .

+ \ Rm

A+

S :="1 +1
+1 +1# .

+1 1

The optimal solution here is to push both coordinates of  unboundedly positive  with margins
approaching (0  0 1). But pushing any coordinate i too quickly will increase the objective value 
rather than decreasing it. In fact  this instance will provide a lower bound  and the mechanism of the
proof shows that the primal weights grow extremely slowly  as O(ln(t)).
Theorem 8.7. Using the logistic loss and exact line search  for any t  1  f (St)  ¯fS  1/(8t).
Acknowledgement

The author thanks Sanjoy Dasgupta  Daniel Hsu  Indraneel Mukherjee  and Robert Schapire for
valuable conversations. The NSF supported this work under grants IIS-0713540 and IIS-0812598.

8

References
[1] Robert E. Schapire. The strength of weak learnability. Machine Learning  5:197–227  July

1990.

[2] Yoav Freund. Boosting a weak learning algorithm by majority. Information and Computation 

121(2):256–285  1995.

[3] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning

and an application to boosting. J. Comput. Syst. Sci.  55(1):119–139  1997.

[4] Leo Breiman. Prediction games and arcing algorithms. Neural Computation  11:1493–1517 

October 1999.

[5] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Additive logistic regression: a statis-

tical view of boosting. Annals of Statistics  28  1998.

[6] Peter J. Bickel  Yaacov Ritov  and Alon Zakai. Some theory for generalized boosting algo-

rithms. Journal of Machine Learning Research  7:705–732  2006.

[7] Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex
differentiable minimization. Journal of Optimization Theory and Applications  72:7–35  1992.
[8] Michael Collins  Robert E. Schapire  and Yoram Singer. Logistic regression  AdaBoost and

Bregman distances. Machine Learning  48(1-3):253–285  2002.

[9] Gunnar R¨atsch  Sebastian Mika  and Manfred K. Warmuth. On the convergence of leveraging.

In NIPS  pages 487–494  2001.

[10] Indraneel Mukherjee  Cynthia Rudin  and Robert Schapire. The convergence rate of AdaBoost.

In COLT  2011.

[11] Robert E. Schapire. The convergence rate of AdaBoost. In COLT  2010.
[12] Robert E. Schapire  Yoav Freund  Peter Barlett  and Wee Sun Lee. Boosting the margin: A

new explanation for the effectiveness of voting methods. In ICML  pages 322–330  1997.

[13] Gunnar R¨atsch and Manfred K. Warmuth. Maximizing the margin with boosting. In COLT 

pages 334–350  2002.

[14] Manfred K. Warmuth  Karen A. Glocer  and Gunnar R¨atsch. Boosting algorithms for maxi-

mizing the soft margin. In NIPS  2007.

[15] Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear
In COLT  pages 311–322 

separability: New relaxations and efﬁcient boosting algorithms.
2008.

[16] Llew Mason  Jonathan Baxter  Peter L. Bartlett  and Marcus R. Frean. Functional gradient
techniques for combining hypotheses. In A.J. Smola  P.L. Bartlett  B. Sch¨olkopf  and D. Schu-
urmans  editors  Advances in Large Margin Classiﬁers  pages 221–246  Cambridge  MA  2000.
MIT Press.

[17] Stephen P. Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University

Press  2004.

[18] Jean-Baptiste Hiriart-Urruty and Claude Lemar´echal. Fundamentals of Convex Analysis.

Springer Publishing Company  Incorporated  2001.

[19] Jonathan Borwein and Adrian Lewis. Convex Analysis and Nonlinear Optimization. Springer

Publishing Company  Incorporated  2000.

[20] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using conﬁdence-rated

predictions. Machine Learning  37(3):297–336  1999.

[21] George B. Dantzig and Mukund N. Thapa. Linear Programming 2: Theory and Extensions.

Springer  2003.

[22] Adi Ben-Israel. Motzkin’s transposition theorem  and the related theorems of Farkas  Gordan
and Stiemke. In M. Hazewinkel  editor  Encyclopaedia of Mathematics  Supplement III. 2002.

9

,Francesca Petralia
Joshua Vogelstein
David Dunson
Emmanuel Abbe
Sanjeev Kulkarni
Eun Jee Lee
Mikhail Yurochkin
Sebastian Claici
Edward Chien
Farzaneh Mirzazadeh
Justin Solomon