2016,Stochastic Structured Prediction under Bandit Feedback,Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations  the learner receives an input  predicts an output structure  and receives partial feedback in form of a task loss evaluation of the predicted structure. We present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods. We present an experimental evaluation on problems of natural language processing over exponential output spaces  and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm. Best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.,Stochastic Structured Prediction

under Bandit Feedback

Artem Sokolov(cid:5) ∗  Julia Kreutzer∗  Christopher Lo† ∗  Stefan Riezler‡ ∗

∗Computational Linguistics & ‡IWR  Heidelberg University  Germany

{sokolov kreutzer riezler}@cl.uni-heidelberg.de

†Department of Mathematics  Tufts University  Boston  MA  USA

chris.aa.lo@gmail.com

(cid:5)Amazon Development Center  Berlin  Germany

Abstract

Stochastic structured prediction under bandit feedback follows a learning protocol
where on each of a sequence of iterations  the learner receives an input  predicts
an output structure  and receives partial feedback in form of a task loss evaluation
of the predicted structure. We present applications of this learning scenario to
convex and non-convex objectives for structured prediction and analyze them as
stochastic ﬁrst-order methods. We present an experimental evaluation on problems
of natural language processing over exponential output spaces  and compare con-
vergence speed across different objectives under the practical criterion of optimal
task performance on development data and the optimization-theoretic criterion of
minimal squared gradient norm. Best results under both criteria are obtained for a
non-convex objective for pairwise preference learning under bandit feedback.

1

Introduction

We present algorithms for stochastic structured prediction under bandit feedback that obey the
following learning protocol: On each of a sequence of iterations  the learner receives an input 
predicts an output structure  and receives partial feedback in form of a task loss evaluation of the
predicted structure. In contrast to the full-information batch learning scenario  the gradient cannot
be averaged over the complete input set. Furthermore  in contrast to standard stochastic learning 
the correct output structure is not revealed to the learner. We present algorithms that use this
feedback to “banditize” expected loss minimization approaches to structured prediction [18  25]. The
algorithms follow the structure of performing simultaneous exploration/exploitation by sampling
output structures from a log-linear probability model  receiving feedback to the sampled structure  and
conducting an update in the negative direction of an unbiased estimate of the gradient of the respective
full information objective. The algorithms apply to situations where learning proceeds online on a
sequence of inputs for which gold standard structures are not available  but feedback to predicted
structures can be elicited from users. A practical example is interactive machine translation where
instead of human generated reference translations only translation quality judgments on predicted
translations are used for learning [20]. The example of machine translation showcases the complexity
of the problem: For each input sentence  we receive feedback for only a single predicted translation
out of a space that is exponential in sentence length  while the goal is to learn to predict the translation
with the smallest loss under a complex evaluation metric.
[19] showed that partial feedback is indeed sufﬁcient for optimization of feature-rich linear structured
prediction over large output spaces in various natural language processing (NLP) tasks. Their
experiments follow the standard online-to-batch conversion practice in NLP applications where the

∗ The work for this paper was done while the authors were at Heidelberg University.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

model with optimal task performance on development data is selected for ﬁnal evaluation on a test set.
The contribution of our paper is to analyze these algorithms as stochastic ﬁrst-order (SFO) methods
in the framework of [7] and investigate the connection of optimization for task performance with
optimization-theoretic concepts of convergence.
Our analysis starts with revisiting the approach to stochastic optimization of a non-convex expected
loss criterion presented by [20]. The iteration complexity of stochastic optimization of a non-convex
objective J(wt) can be analyzed in the framework of [7] as O(1/2) in terms of the number of
iterations needed to reach an accuracy of  for the criterion E[(cid:107)∇J(wt)(cid:107)2] ≤ . [19] attempt to
improve convergence speed by introducing a cross-entropy objective that can be seen as a (strong)
convexiﬁcation of the expected loss objective. The known best iteration complexity for strongly
convex stochastic optimization is O(1/) for the suboptimality criterion E[J(wt)] − J(w∗) ≤ .
Lastly  we analyze the pairwise preference learning algorithm introduced by [19]. This algorithm can
also be analyzed as an SFO method for non-convex optimization. To our knowledge  this is the ﬁrst
SFO approach to stochastic learning form pairwise comparison feedback  while related approaches
fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24  1  7  4]. Convergence
rate for SZO methods depends on the dimensionality d of the function to be evaluated  for example 
the non-convex SZO algorithm of [7] has an iteration complexity of O(d/2). SFO algorithms do
not depend on d which is crucial if the dimensionality of the feature space is large as is common in
structured prediction.
Furthermore  we present a comparison of empirical and theoretical convergence criteria for the NLP
tasks of machine translation and noun-phrase chunking. We compare the empirical convergence
criterion of optimal task performance on development data with the theoretically motivated criterion
of minimal squared gradient norm. We ﬁnd a correspondence of fastest convergence of pairwise
preference learning on both tasks. Given the standard analysis of asymptotic complexity bounds 
this result is surprising. An explanation can be given by measuring variance and Lipschitz constant
of the stochastic gradient  which is smallest for pairwise preference learning and largest for cross-
entropy minimization by several orders of magnitude. This offsets the possible gains in asymptotic
convergence rates for strongly convex stochastic optimization  and makes pairwise preference learning
an attractive method for fast optimization in practical interactive scenarios.

2 Related Work

The methods presented in this paper are related to various other machine learning problems where
predictions over large output spaces have to be learned from partial information.
Reinforcement learning has the goal of maximizing the expected reward for choosing an action at
a given state in a Markov Decision Process (MDP) model  where unknown rewards are received at
each state  or once at the ﬁnal state. The algorithms in this paper can be seen as one-state MDPs with
context where choosing an action corresponds to predicting a structured output. Most closely related
are recent applications of policy gradient methods to exponential output spaces in NLP problems
[22  3  15]. Similar to our expected loss minimization approaches  these approaches are based on
non-convex models  however  convergence rates are rarely a focus in the reinforcement learning
literature. One focus of our paper is to present an analysis of asymptotic convergence and convergence
rates of non-convex stochastic ﬁrst-order methods.
Contextual one-state MDPs are also known as contextual bandits [11  13] which operate in a scenario
of maximizing the expected reward for selecting an arm of a multi-armed slot machine. Similar to
our case  the feedback is partial  and the models consist of a single state. While bandit learning
is mostly formalized as online regret minimization with respect to the best ﬁxed arm in hindsight 
we characterize our approach in an asymptotic convergence framework. Furthermore  our high-
dimensional models predict structures over exponential output spaces. Since we aim to train these
models in interaction with real users  we focus on the ease of elicitability of the feedback and on
speed of convergence. In the spectrum of stochastic versus adversarial bandits  our approach is
semi-adversarial in making stochastic assumptions on inputs  but not on rewards [12].
Pairwise preference learning has been studied in the full information supervised setting [8  10  6]
where given preference pairs are assumed. Work on stochastic pairwise learning has been formalized
as derivative-free stochastic zeroth-order optimization [24  1  7  4]. To our knowledge  our approach

2

Algorithm 1 Bandit Structured Prediction
1: Input: sequence of learning rates γt
2: Initialize w0
3: for t = 0  . . .   T do
4:
5:
6:
7:
8: Choose a solution ˆw from the list {w0  . . .   wT}

Observe xt
Sample ˜yt ∼ pwt(y|xt)
Obtain feedback ∆(˜yt)
wt+1 = wt − γt st

to pairwise preference learning from partial feedback is the ﬁrst SFO approach to learning from
pairwise preferences in form of relative task loss evaluations.

3 Expected Loss Minimization for Structured Prediction

[18  25] introduce the expected loss criterion for structured prediction as the minimization of the
expectation of a given task loss function with respect to the conditional distribution over structured
outputs. Let X be a structured input space  let Y(x) be the set of possible output structures for input
x  and let ∆y : Y → [0  1] quantify the loss ∆y(y(cid:48)) suffered for predicting y(cid:48) instead of the gold
standard structure y. In the full information setting  for a given (empirical) data distribution p(x  y) 
the learning problem is deﬁned as

(cid:88)

x y

(cid:88)

p(x  y)

y(cid:48)∈Y(x)

min
w∈Rd

Ep(x y)pw(y(cid:48)|x) [∆y(y(cid:48))] = min
w∈Rd

∆y(y(cid:48))pw(y(cid:48)|x) 

(1)

where

pw(y|x) = exp(w(cid:62)φ(x  y))/Zw(x)

(2)
is a Gibbs distribution with joint feature representation φ : X × Y → Rd  weight vector w ∈ Rd  and
normalization constant Zw(x). Despite being a highly non-convex optimization problem  positive
results have been obtained by gradient-based optimization with respect to

(cid:104)

∆y(y(cid:48))(cid:0)φ(x  y(cid:48)) − Epw(y(cid:48)|x)[φ(x  y(cid:48))](cid:1)(cid:105)

.

(3)

∇Ep(x y)pw(y(cid:48)|x) [∆y(y(cid:48))] = Ep(x y)pw(y(cid:48)|x)

Unlike in the full information scenario  in structured learning under bandit feedback the gold standard
output structure y with respect to which the objective function is evaluated is not revealed to the
learner. Thus we can neither evaluate the task loss ∆ nor calculate the gradient (3) as in the full
information case. A solution to this problem is to pass the evaluation of the loss function to the user 
i.e  we access the loss directly through user feedback without assuming existence of a ﬁxed reference
y. In the following  we will drop the subscript referring to the gold standard structure in the deﬁnition
of ∆ to indicate that the feedback is in general independent of gold standard outputs. In particular 
we allow ∆ to be equal to 0 for several outputs.

4 Stochastic Structured Prediction under Partial Feedback

Algorithm Structure. Algorithm 1 shows the structure of the methods analyzed in this paper. It
assumes a sequence of input structures xt  t = 0  . . .   T that are generated by a ﬁxed  unknown
distribution p(x) (line 4). For each randomly chosen input  an output ˜yt is sampled from a Gibbs
model to perform simultaneous exploitation (use the current best estimate) / exploration (get new
information) on output structures (line 5). Then  feedback ∆(˜yt) to the predicted structure is obtained
(line 6). An update is performed by taking a step in the negative direction of the stochastic gradient
st  at a rate γt (line 7). As a post-optimization step  a solution ˆw is chosen from the list of vectors
wt ∈ {w0  . . .   wT} (line 8).
Given Algorithm 1  we can formalize the notion of “banditization” of objective functions by presenting
different instantiations of the vector st  and showing them to be unbiased estimates of the gradients
of corresponding full information objectives.

3

Expected Loss Minimization.
loss objective. It is non-convex for the speciﬁc instantiations in this paper:
∆(y)pw(y|x).

Ep(x)pw(y|x) [∆(y)] =

(cid:88)

(cid:88)

p(x)

[20] presented an algorithm that minimizes the following expected

(4)

x

y∈Y(x)

The vector st used in their algorithm can be seen as a stochastic gradient of this objective  i.e.  an
evaluation of the full gradient at a randomly chosen input xt and output ˜yt:

st = ∆(˜yt)(cid:0)φ(xt  ˜yt) − Epwt (y|xt)[φ(xt  y)](cid:1).

(5)

Instantiating st in Algorithm 1 to the stochastic gradient in equation (5) yields an update that compares
the sampled feature vector to the average feature vector  and performs a step into the opposite direction
of this difference  the more so the higher the loss of the sampled structure is. In the following  we
refer to the algorithm for expected loss minimization deﬁned by the update (5) as Algorithm EL.

Pairwise Preference Learning. Decomposing complex problems into a series of pairwise com-
parisons has been shown to be advantageous for human decision making [23]. For the example
of machine translation  this means that instead of requiring numerical assessments of translation
quality from human users  only a relative preference judgement on a pair of translations needs to be
elicited. This idea is formalized in [19] as an expected loss objective with respect to a conditional
distribution of pairs of structured outputs. Let P(x) = {(cid:104)yi  yj(cid:105)|yi  yj ∈ Y(x)} denote the set of
output pairs for an input x  and let ∆((cid:104)yi  yj(cid:105)) : P(x) → [0  1] denote a task loss function that
speciﬁes a dispreference of yi compared to yj. In the experiments reported in this paper  we simulate
two types of pairwise feedback. Firstly  continuous pairwise feedback is computed as

(cid:26)∆(yi) − ∆(yj)

0

otherwise.

∆((cid:104)yi  yj(cid:105)) =

if ∆(yi) > ∆(yj) 

(6)

(7)

A binary feedback function is computed as

∆((cid:104)yi  yj(cid:105)) =

(cid:26)1

0

if ∆(yi) > ∆(yj) 
otherwise.

Furthermore  we assume a feature representation φ(x (cid:104)yi  yj(cid:105)) = φ(x  yi) − φ(x  yj) and a Gibbs
model on pairs of output structures

pw((cid:104)yi  yj(cid:105)|x) =

(cid:80)

ew(cid:62)(φ(x yi)−φ(x yj ))

ew(cid:62)(φ(x yi)−φ(x yj ))

(cid:104)yi yj(cid:105)∈P(x)

= pw(yi|x)p−w(yj|x).

(8)

The factorization of this model into the product pw(yi|x)p−w(yj|x) allows efﬁcient sampling and
calculation of expectations. Instantiating objective (4) to the case of pairs of output structures deﬁnes
the following objective that is again non-convex in the use cases in this paper:

Ep(x)pw((cid:104)yi yj(cid:105)|x) [∆((cid:104)yi  yj(cid:105))] =

p(x)

∆((cid:104)yi  yj(cid:105)) pw((cid:104)yi  yj(cid:105)|x).

(9)

(cid:88)

x

(cid:88)

(cid:104)yi yj(cid:105)∈P(x)

Learning from partial feedback on pairwise preferences will ensure that the model ﬁnds a ranking
function that assigns low probabilities to discordant pairs with respect the the observed preference
pairs. Stronger assumptions on the learned ranking can be made if asymmetry and transitivity of the
observed ordering of pairs is required.2 An algorithm for pairwise preference learning can be deﬁned
by instantiating Algorithm 1 to sampling output pairs (cid:104)˜yi  ˜yj(cid:105)t  receiving feedback ∆((cid:104)˜yi  ˜yj(cid:105)t)  and
performing a stochastic gradient update using

st = ∆((cid:104)˜yi  ˜yj(cid:105)t)(cid:0)φ(xt (cid:104)˜yi  ˜yj(cid:105)t) − Epwt ((cid:104)yi yj(cid:105)|xt)[φ(xt (cid:104)yi  yj(cid:105))](cid:1).

(10)

The algorithms for pairwise preference ranking deﬁned by update (10) are referred to as Algorithms
PR(bin) and PR(cont)  depending on the use of binary or continuous feedback.

2See [2] for an overview of bandit learning from consistent and inconsistent pairwise comparisons.

4

Cross-Entropy Minimization. The standard theory of stochastic optimization predicts consider-
able improvements in convergence speed depending on the functional form of the objective. This
motivated the formalization of a convex upper bounds on expected normalized loss in [19]. If a
y∈Y(x) g(y)  and g = 1 − ∆  the
normalized gain function ¯g(y) = g(y)
objective can be seen as the cross-entropy of model pw(y|x) with respect to ¯g(y):
¯g(y) log pw(y|x).

Zg(x) is used where Zg(x) =(cid:80)
(cid:88)

Ep(x)¯g(y) [− log pw(y|x)] = −(cid:88)

(11)

p(x)

x

y∈Y(x)

For a proper probability distribution ¯g(y)  an application of Jensen’s inequality to the convex negative
logarithm function shows that objective (11) is a convex upper bound on objective (4). However 
normalizing the gain function is prohibitive in a partial feedback setting since it would require to
elicit user feedback for each structure in the output space. [19] thus work with an unnormalized gain
function g(y) that preserves convexity. This can be seen by rewriting the objective as the sum of a
linear and a convex function in w:

Ep(x)g(y) [− log pw(y|x)] = −(cid:88)
(cid:88)

x

(cid:88)

p(x)

y∈Y(x)

(cid:88)

g(y)w(cid:62)φ(x  y)

(12)

+

p(x)(log

x

y∈Y(x)

exp(w(cid:62)φ(x  y)))α(x) 

where α(x) =(cid:80)

y∈Y(x) g(y) is a constant factor not depending on w. Instantiating Algorithm 1 to the
following stochastic gradient st of this objective yields an algorithm for cross-entropy minimization:

(cid:0) − φ(xt  ˜yt) + Epwt

[φ(xt  yt)](cid:1).

g(˜yt)

st =

pwt(˜yt|xt)

(13)
Note that the ability to sample structures from pwt(˜yt|xt) comes at the price of having to normalize
st by 1/pwt(˜yt|xt). While minimization of this objective will assign high probabilities to structures
with high gain  as desired  each update is affected by a probability that changes over time and is
unreliable when training is started. This further increases the variance already present in stochastic
optimization. We deal with this problem by clipping too small sampling probabilities to ˆpwt(˜yt|xt) =
max{pwt(˜yt|xt)  k} for a constant k [9]. The algorithm for cross-entropy minimization based on the
stochastic gradient (13) is referred to as Algorithm CE in the following.

5 Convergence Analysis

To analyze convergence  we describe Algorithms EL  PR  and CE as stochastic ﬁrst-order (SFO)
methods in the framework of [7]. We assume lower bounded  differentiable objective functions J(w)
with Lipschitz continuous gradient ∇J(w) satisfying

(cid:107)∇J(w + w(cid:48)) − ∇J(w)(cid:107) ≤ L(cid:107)w(cid:48)(cid:107) ∀w  w(cid:48) ∃L ≥ 0.

(14)
For an iterative process of the form wt+1 = wt− γt st  the conditions to be met concern unbiasedness
of the gradient estimate

E[st] = ∇J(wt)  ∀t ≥ 0 

(15)

and boundedness of the variance of the stochastic gradient

E[||st − ∇J(wt)||2] ≤ σ2  ∀t ≥ 0.

(16)
Condition (15) is met for all three Algorithms by taking expectations over all sources of randomness 
i.e.  over random inputs and output structures. Assuming (cid:107)φ(x  y)(cid:107) ≤ R  ∆(y) ∈ [0  1] and
g(y) ∈ [0  1] for all x  y  and since the ratio
ˆpwt (˜yt|xt) is bounded  the variance in condition (16) is
bounded. Note that the analysis of [7] justiﬁes the use of constant learning rates γt = γ  t = 0  . . .   T .
Convergence speed can be quantiﬁed in terms of the number of iterations needed to reach an accuracy
of  for a gradient-based criterion E[(cid:107)∇J(wt)(cid:107)2] ≤ . For stochastic optimization of non-convex
objectives  the iteration complexity with respect to this criterion is analyzed as O(1/2) in [7]. This
complexity result applies to our Algorithms EL and PR.

g(˜yt)

5

T wt).

The iteration complexity of stochastic optimization of (strongly) convex objectives has been analyzed
as at best O(1/) for the suboptimality criterion E[J(wt)] − J(w∗) ≤  for decreasing learning rates
2(cid:107)w(cid:107)2
[14].3 Strong convexity of objective (12) can be achieved easily by adding an (cid:96)2 regularizer λ
with constant λ > 0. Algorithm CE is then modiﬁed to use the following regularized update rule
wt+1 = wt − γt (st + λ
This standard analysis shows two interesting points: First  Algorithms EL and PR can be analyzed as
SFO methods where the latter only requires relative preference feedback for learning  while enjoying
an iteration complexity that does not depend on the dimensionality of the function as in gradient-free
stochastic zeroth-order (SZO) approaches. Second  the standard asymptotic complexity bound of
O(1/2) for non-convex stochastic optimization hides the constants L and σ2 in which iteration
complexity increases linearly. As we will show  these constants have a substantial inﬂuence  possibly
offsetting the advantages in asymptotic convergence speed of Algorithm CE.

6 Experiments

Measuring Numerical Convergence and Task Loss Performance.
In the following  we will
present an experimental evaluation for two complex structured prediction tasks from the area of
NLP  namely statistical machine translation and noun phrase chunking. Both tasks involve dynamic
programming over exponential output spaces  large sparse feature spaces  and non-linear non-
decomposable task loss metrics. Training for both tasks was done by simulating bandit feedback by
evaluating ∆ against gold standard structures which are never revealed to the learner. We compare
the empirical convergence criterion of optimal task performance on development data with numerical
results on theoretically motivated convergence criteria.
For the purpose of measuring convergence with respect to optimal task performance  we report an
evaluation of convergence speed on a ﬁxed set of unseen data as performed in [19]. This instantiates
the selection criterion in line (8) in Algorithm 1 to an evaluation of the respective task loss function
∆(ˆywt(x)) under MAP prediction ˆyw(x) = arg maxy∈Y(x) pw(y|x) on the development data. This
corresponds to the standard practice of online-to-batch conversion where the model selected on the
development data is used for ﬁnal evaluation on a further unseen test set. For bandit structured
prediction algorithms  ﬁnal results are averaged over three runs with different random seeds.
For the purpose of obtaining numerical results on convergence speed  we compute estimates of the
expected squared gradient norm E[(cid:107)∇J(wt)(cid:107)2]  the Lipschitz constant L and the variance σ2 in
which the asymptotic bounds on iteration complexity grow linearly.4 We estimate the squared gradient
norm by the squared norm of the stochastic gradient (cid:107)sT(cid:107)2 at a ﬁxed time horizon T . The Lipschitz
(cid:107)si−sj(cid:107)
(cid:107)wi−wj(cid:107) for 500 pairs wi and wj randomly drawn
constant L in equation (14) is estimated by maxi j
from the weights produced during training. The variance σ2 in equation (16) is computed as the
empirical variance of the stochastic gradient  taken at regular intervals after each epoch of size D 
D(cid:99). All estimates include
yielding the estimate 1
K
multiplication of the stochastic gradient with the learning rate. For comparability of results across
different algorithms  we use the same T and the same constant learning rates for all algorithms.5

(cid:80)K
k=1 skD(cid:107)2 where K = (cid:98) T

(cid:80)K
k=1 (cid:107)skD − 1

K

Statistical Machine Translation.
In this experiment  an interactive machine translation scenario
is simulated where a given machine translation system is adapted to user style and domain based on
feedback to predicted translations. Domain adaptation from Europarl to NewsCommentary domains
using the data provided at the WMT 2007 shared task is performed for French-to-English translation.6
The MT experiments are based on the synchronous context-free grammar decoder cdec [5]. The
models use a standard set of dense and lexicalized sparse features  including an out-of and an in-

stochastic optimization.

3For constant learning rates  [21] show even faster convergence in the search phase of strongly-convex
4For example  these constants appear as O( L
2 ) in the complexity bound for non-convex stochastic
optimization of [7].
5Note that the squared gradient norm upper bounds the suboptimality criterion s.t. (cid:107)∇J(wt)(cid:107)2 ≥
2λJ(wt)] − J(w∗) for strongly convex functions. Together with the use of constant learning rates this means
that we measure convergence to a point near an optimum for strongly convex objectives.

 + Lσ2

6http://www.statmt.org/wmt07/shared-task.html

6

Task

SMT

Chunking

Algorithm Iterations
CE
EL
PR(bin)
CE
EL
PR(cont)

281k
370k
115k
5.9M
7.5M
4.7M

Score
0.271±0.001
0.267±8e−6
0.273±0.0005
0.891±0.005
0.923±0.002
0.914±0.002

γ
1e-6
1e-5
1e-4
1e-6
1e-4
1e-4

λ
1e-6

k
5e-3

1e-6

1e-2

Table 1: Test set evaluation for stochastic learning under bandit feedback from [19]  for chunking
under F1-score  and for machine translation under BLEU. Higher is better for both scores. Results
for stochastic learners are averaged over three runs of each algorithm  with standard deviation shown
in subscripts. The meta-parameter settings were determined on dev sets for constant learning rate γ 
clipping constant k  (cid:96)2 regularization constant λ.

domain language model. The out-of-domain baseline model has around 200k active features. The
pre-processing  data splits  feature sets and tuning strategies are described in detail in [19]. The
difference in the task loss evaluation between out-of-domain (BLEU: 0.2651) and in-domain (BLEU:
0.2831) models gives the range of possible improvements (1.8 BLEU points) for bandit learning.
Learning under bandit feedback starts at the learned weights of the out-of-domain median models.
It uses parallel in-domain data (news-commentary  40 444 sentences) to simulate bandit feedback 
by evaluating the sampled translation against the reference using as loss function ∆ a smoothed
per-sentence 1 − BLEU (zero n-gram counts being replaced with 0.01). After each update  the
hypergraph is re-decoded and all hypotheses are re-ranked. Training is distributed across 38 shards
using a multitask-based feature selection algorithm [17].

Noun-phrase Chunking. The experimental setting for chunking is the same as in [19]. Following
[16]  conditional random ﬁelds (CRF) are applied to the noun phrase chunking task on the CoNLL-
2000 dataset7. The implemented set of feature templates is a simpliﬁed version of [16] and leads to
around 2M active features. Training under full information with a log-likelihood objective yields
0.935 F1. In difference to machine translation  training with bandit feedback starts from w0 = 0  not
from a pre-trained model.

Task Loss Evaluation. Table 1 lists the results of the task loss evaluation for machine translation
and chunking as performed in [19]  together with the optimal meta-parameters and the number of
iterations needed to ﬁnd an optimal result on the development set. Note that the pairwise feedback
type (cont or bin) is treated as a meta-parameter for Algorithm PR in our simulation experiment.
We found that bin is preferable for machine translation and cont for chunking in order to obtain the
highest task scores.
For machine translation  all bandit learning runs show signiﬁcant improvements in BLEU score
over the out-of-domain baseline. Early stopping by task performance on the development led to the
selection of algorithm PR(bin) at a number of iterations that is by a factor of 2-4 smaller compared to
Algorithms EL and CE.
For the chunking experiment  the F1-score results obtained for bandit learning are close to the full-
information baseline. The number of iterations needed to ﬁnd an optimal result on the development
set is smallest for Algorithm PR(cont)  compared to Algorithms EL and CE. However  the best
F1-score is obtained by Algorithm EL.
Numerical Convergence Results. Estimates of E[(cid:107)∇J(wt)(cid:107)2]  L and σ2 for three runs of each
algorithm and task with different random seeds are listed in Table 2.
For machine translation  at time horizon T   the estimated squared gradient norm for Algorithm PR
is several orders of magnitude smaller than for Algorithms EL and CE. Furthermore  the estimated
Lipschitz constant L and the estimated variance σ2 are smallest for Algorithm PR. Since the iteration
complexity increases linearly with respect to these terms  smaller constants L and σ2 and a smaller

7http://www.cnts.ua.ac.be/conll2000/chunking/

7

Task

SMT

Chunking

Algorithm T
CE
EL
PR(bin)
PR(cont)
CE
EL
PR(bin)
PR(cont)

767 000
767 000
767 000
767 000
3 174 400
3 174 400
3 174 400
3 174 400

(cid:107)sT(cid:107)2
3.04±0.02
0.02±0.03
2.88e-4±3.40e−6
1.03e-8±2.91e−10
4.20±0.71
1.21e-3±1.1e−4
7.71e-4±2.53e−4
5.99e-3±7.24e−4

L
0.54±0.3
1.63±0.67
0.08±0.01
0.10±5.70e−3
1.60±0.11
1.16±0.31
1.33±0.24
1.11±0.30

σ2
35 ±6
3.13e-4±3.60e−6
3.79e-5±9.50e−8
1.78e-7±1.45e−10
4.88±0.07
0.01±9.51e−5
4.44e-3±2.66e−5
0.03±4.68e−4

Table 2: Estimates of squared gradient norm (cid:107)sT(cid:107)2  Lipschitz constant L and variance σ2 of stochastic
gradient (including multiplication with learning rate) for ﬁxed time horizon T and constant learning
rates γ = 1e − 6 for SMT and for chunking. The clipping and regularization parameters for CE are
set as in Table 1 for machine translation  except for chunking CE λ = 1e − 5. Results are averaged
over three runs of each algorithm  with standard deviation shown in subscripts.

value of the estimate E[(cid:107)∇J(wt)(cid:107)2] at the same number of iterations indicates fastest convergence
for Algorithm PR. This theoretically motivated result is consistent with the practical convergence
criterion of early stopping on development data: Algorithm PR which yields the smallest squared
gradient norm at time horizon T also needs the smallest number of iterations to achieve optimal
performance on the development set. In the case of machine translation  Algorithm PR even achieves
the nominally best BLEU score on test data.
For the chunking experiment  after T iterations  the estimated squared gradient norm and either of
the constants L and σ2 for Algorithm PR are several orders of magnitude smaller than for Algorithm
CE  but similar to the results for Algorithm EL. The corresponding iteration counts determined by
early stopping on development data show an improvement of Algorithm PR over Algorithms CE and
EL  however  by a smaller factor than in the machine translation experiment.
Note that for comparability across algorithms  the same constant learning rates were used in all runs.
However  we obtained similar relations between algorithms by using the meta-parameter settings
chosen on development data as shown in Table 1. Furthermore  the above tendendencies hold for
both settings of the meta-parameter bin or cont of Algorithm PR.

7 Conclusion

We presented learning objectives and algorithms for stochastic structured prediction under bandit
feedback. The presented methods “banditize” well-known approaches to probabilistic structured
prediction such as expected loss minimization  pairwise preference ranking  and cross-entropy mini-
mization. We presented a comparison of practical convergence criteria based on early stopping with
theoretically motivated convergence criteria based on the squared gradient norm. Our experimental
results showed fastest convergence speed under both criteria for pairwise preference learning. Our
numerical evaluation showed smallest variance for pairwise preference learning  which possibly
explains fastest convergence despite the underlying non-convex objective. Furthermore  since this
algorithm requires only easily obtainable relative preference feedback for learning  it is an attractive
choice for practical interactive learning scenarios.

Acknowledgments.

This research was supported in part by the German research foundation (DFG)  and in part by a
research cooperation grant with the Amazon Development Center Germany.

8

References
[1] Agarwal  A.  Dekel  O.  and Xiao  L. (2010). Optimal algorithms for online convex optimization with

multi-point bandit feedback. In COLT.

[2] Busa-Fekete  R. and Hüllermeier  E. (2014). A survey of preference-based online learning with bandit

algorithms. In ALT.

[3] Chang  K.-W.  Krishnamurthy  A.  Agarwal  A.  Daume  H.  and Langford  J. (2015). Learning to search

better than your teacher. In ICML.

[4] Duchi  J. C.  Jordan  M. I.  Wainwright  M. J.  and Wibisono  A. (2015). Optimal rates for zero-order
convex optimization: The power of two function evaluations. IEEE Translactions on Information Theory 
61(5):2788–2806.

[5] Dyer  C.  Lopez  A.  Ganitkevitch  J.  Weese  J.  Ture  F.  Blunsom  P.  Setiawan  H.  Eidelman  V.  and
Resnik  P. (2010). cdec: A decoder  alignment  and learning framework for ﬁnite-state and context-free
translation models. In ACL Demo.

[6] Freund  Y.  Iyer  R.  Schapire  R. E.  and Singer  Y. (2003). An efﬁcient boosting algorithm for combining

preferences. JMLR  4:933–969.

[7] Ghadimi  S. and Lan  G. (2012). Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic

programming. SIAM J. on Optimization  4(23):2342–2368.

[8] Herbrich  R.  Graepel  T.  and Obermayer  K. (2000). Large margin rank boundaries for ordinal regression.

In Advances in Large Margin Classiﬁers  pages 115–132.

[9] Ionides  E. L. (2008). Truncated importance sampling. J. of Comp. and Graph. Stat.  17(2):295–311.

[10] Joachims  T. (2002). Optimizing search engines using clickthrough data. In KDD.

[11] Langford  J. and Zhang  T. (2007). The epoch-greedy algorithm for contextual multi-armed bandits. In

NIPS.

[12] Lazaric  A. and Munos  R. (2012). Learning with stochastic inputs and adversarial outputs. Journal of

Computer and System Sciences  (78):1516–1537.

[13] Li  L.  Chu  W.  Langford  J.  and Schapire  R. E. (2010). A contextual-bandit approach to personalized

news article recommendation. In WWW.

[14] Polyak  B. T. (1987). Introduction to Optimization. Optimization Software  Inc.  New York.

[15] Ranzato  M.  Chopra  S.  Auli  M.  and Zaremba  W. (2016). Sequence level training with recurrent neural

networks. In ICLR.

[16] Sha  F. and Pereira  F. (2003). Shallow parsing with conditional random ﬁelds. In NAACL.

[17] Simianer  P.  Riezler  S.  and Dyer  C. (2012). Joint feature selection in distributed stochastic learning for

large-scale discriminative training in SMT. In ACL.

[18] Smith  N. A. (2011). Linguistic Structure Prediction. Morgan and Claypool.

[19] Sokolov  A.  Kreutzer  J.  Lo  C.  and Riezler  S. (2016). Learning structured predictors from bandit

feedback for interactive NLP. In ACL.

[20] Sokolov  A.  Riezler  S.  and Urvoy  T. (2015). Bandit structured prediction for learning from user feedback

in statistical machine translation. In MT Summit XV.

[21] Solodov  M. V. (1998). Incremental gradient algorithms with stepsizes bounded away from zero. Computa-

tional Optimization and Applications  11:23–35.

[22] Sutton  R. S.  McAllester  D.  Singh  S.  and Mansour  Y. (2000). Policy gradient methods for reinforcement

learning with function approximation. In NIPS.

[23] Thurstone  L. L. (1927). A law of comparative judgement. Psychological Review  34:278–286.

[24] Yue  Y. and Joachims  T. (2009). Interactively optimizing information retrieval systems as a dueling bandits

problem. In ICML.

[25] Yuille  A. and He  X. (2012). Probabilistic models of vision and max-margin methods. Frontiers of

Electrical and Electronic Engineering  7(1):94–106.

9

,Elias Bareinboim
Andrew Forney
Judea Pearl
Artem Sokolov
Julia Kreutzer
Stefan Riezler