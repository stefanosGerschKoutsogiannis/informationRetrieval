2008,Robust Regression and Lasso,We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems  and we exhibit a particular uncertainty set for which the robust problem is equivalent to $\ell_1$ regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets  which all lead to tractable convex optimization problems. Therefore  we provide a new methodology for designing regression algorithms  which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations  we use it directly to show sparsity properties of Lasso  as well as to prove a general consistency result for robust regression problems  including Lasso  from a unified robustness perspective.,Robust Regression and Lasso

Department of Electrical and Computer Engineering

Huan Xu

McGill University

Montreal  QC Canada

xuhuan@cim.mcgill.ca

Constantine Caramanis

Department of Electrical and Computer Engineering

The University of Texas at Austin

Austin  Texas

cmcaram@ece.utexas.edu

Department of Electrical and Computer Engineering

Shie Mannor

McGill University

Montreal  QC Canada

shie.mannor@mcgill.ca

Abstract

We consider robust least-squares regression with feature-wise disturbance. We
show that this formulation leads to tractable convex optimization problems  and
we exhibit a particular uncertainty set for which the robust problem is equivalent
to `1 regularized regression (Lasso). This provides an interpretation of Lasso from
a robust optimization perspective. We generalize this robust formulation to con-
sider more general uncertainty sets  which all lead to tractable convex optimization
problems. Therefore  we provide a new methodology for designing regression al-
gorithms  which generalize known formulations. The advantage is that robustness
to disturbance is a physical property that can be exploited: in addition to obtaining
new formulations  we use it directly to show sparsity properties of Lasso  as well
as to prove a general consistency result for robust regression problems  including
Lasso  from a uniﬁed robustness perspective.

1 Introduction

In this paper we consider linear regression problems with least-square error. The problem is to ﬁnd
a vector x so that the `2 norm of the residual b − Ax is minimized  for a given matrix A ∈ Rn×m
and vector b ∈ Rn. From a learning/regression perspective  each row of A can be regarded as a
training sample  and the corresponding element of b as the target value of this observed sample.
Each column of A corresponds to a feature  and the objective is to ﬁnd a set of weights so that the
weighted sum of the feature values approximates the target value.

It is well known that minimizing the least squared error can lead to sensitive solutions [1  2]. Many
regularization methods have been proposed to decrease this sensitivity. Among them  Tikhonov
regularization [3] and Lasso [4  5] are two widely known and cited algorithms. These methods
minimize a weighted sum of the residual norm and a certain regularization term  kxk2 for Tikhonov
In addition to providing regularity  Lasso is also known for
regularization and kxk1 for Lasso.

1

the tendency to select sparse solutions. Recently this has attracted much attention for its ability
to reconstruct sparse solutions when sampling occurs far below the Nyquist rate  and also for its
ability to recover the sparsity pattern exactly with probability one  asymptotically as the number of
observations increases (there is an extensive literature on this subject  and we refer the reader to
[6  7  8  9  10] and references therein). In many of these approaches  the choice of regularization
parameters often has no fundamental connection to an underlying noise model [2].

In [11]  the authors propose an alternative approach to reducing sensitivity of linear regression  by
considering a robust version of the regression problem: they minimize the worst-case residual for
the observations under some unknown but bounded disturbances. They show that their robust least
squares formulation is equivalent to `2-regularized least squares  and they explore computational
aspects of the problem. In that paper  and in most of the subsequent research in this area and the
more general area of Robust Optimization (see [12  13] and references therein) the disturbance is
taken to be either row-wise and uncorrelated [14]  or given by bounding the Frobenius norm of the
disturbance matrix [11].

In this paper we investigate the robust regression problem under more general uncertainty sets 
focusing in particular on the case where the uncertainty set is deﬁned by feature-wise constraints 
and also the case where features are meaningfully correlated. This is of interest when values of
features are obtained with some noisy pre-processing steps  and the magnitudes of such noises are
known or bounded. We prove that all our formulations are computationally tractable. Unlike much
of the previous literature  we provide a focus on structural properties of the robust solution. In
addition to giving new formulations  and new properties of the solutions to these robust problems 
we focus on the inherent importance of robustness  and its ability to prove from scratch important
properties such as sparseness  and asymptotic consistency of Lasso in the statistical learning context.
In particular  our main contributions in this paper are as follows.

• We formulate the robust regression problem with feature-wise independent disturbances 
and show that this formulation is equivalent to a least-square problem with a weighted `1
norm regularization term. Hence  we provide an interpretation for Lasso from a robustness
perspective. This can be helpful in choosing the regularization parameter. We generalize
the robust regression formulation to loss functions given by an arbitrary norm  and uncer-
tainty sets that allow correlation between disturbances of different features.

• We investigate the sparsity properties for the robust regression problem with feature-wise
independent disturbances  showing that such formulations encourage sparsity. We thus eas-
ily recover standard sparsity results for Lasso using a robustness argument. This also im-
plies a fundamental connection between the feature-wise independence of the disturbance
and the sparsity.

• Next  we relate Lasso to kernel density estimation. This allows us to re-prove consistency
in a statistical learning setup  using the new robustness tools and formulation we introduce.

Notation. We use capital letters to represent matrices  and boldface letters to represent column
vectors. For a vector z  we let zi denote the ith element. Throughout the paper  ai and r>
j denote
the ith column and the jth row of the observation matrix A  respectively; aij is the ij element of A 
hence it is the jth element of ri  and ith element of aj. For a convex function f (·)  ∂f (z) represents
any of its sub-gradients evaluated at z.

2 Robust Regression with Feature-wise Disturbance

We show that our robust regression formulation recovers Lasso as a special case. The regression
formulation we consider differs from the standard Lasso formulation  as we minimize the norm of
the error  rather than the squared norm. It is known that these two coincide up to a change of the reg-
ularization coefﬁcient. Yet our results amount to more than a representation or equivalence theorem.
In addition to more ﬂexible and potentially powerful robust formulations  we prove new results  and
give new insight into known results. In Section 3  we show the robust formulation gives rise to new
sparsity results. Some of our results there (e.g. Theorem 4) fundamentally depend on (and follow
from) the robustness argument  which is not found elsewhere in the literature. Then in Section 4 
we establish consistency of Lasso directly from the robustness properties of our formulation  thus
explaining consistency from a more physically motivated and perhaps more general perspective.

2

2.1 Formulation

Robust linear regression considers the case that the observed matrix A is corrupted by some distur-
bance. We seek the optimal weight for the uncorrupted (yet unknown) sample matrix. We consider
the following min-max formulation:

Robust Linear Regression: min

x∈Rm(cid:26) max

∆A∈U kb − (A + ∆A)xk2(cid:27) .

(1)

Here  U is the set of admissible disturbances of the matrix A. In this section  we consider the speciﬁc
setup where the disturbance is feature-wise uncorrelated  and norm-bounded for each feature:

U  n(δ1 ···   δm)(cid:12)(cid:12)(cid:12)kδik2 ≤ ci  i = 1 ···   mo 

for given ci ≥ 0. This formulation recovers the well-known Lasso:
Theorem 1. The robust regression problem (1) with the uncertainty set (2) is equivalent to the
following `1 regularized regression problem:

(2)

min

x∈Rmnkb − Axk2 +

m

Xi=1

ci|xi|o.

(3)

Proof. We defer the full details to [15]  and give only an outline of the proof here. Showing that the
robust regression is a lower bound for the regularized regression follows from the standard triangle
inequality. Conversely  one can take the worst-case noise to be δ∗
i )u  where u is given
i
by

  −cisgn(x∗

u  (cid:26) b−Ax∗

if Ax∗ 6= b 
kb−Ax∗k2
any vector with unit `2 norm otherwise;

from which the result follows after some algebra.

 

If we take ci = c and normalized ai for all i  Problem (3) is the well-known Lasso [4  5].

2.2 Arbitrary norm and correlated disturbance

min

It is possible to generalize this result to the case where the `2-norm is replaced by an arbitrary norm 
and where the uncertainty is correlated from feature to feature. For space considerations  we refer
to the full version ([15])  and simply state the main results here.
Theorem 2. Let k · ka denote an arbitrary norm. Then the robust regression problem
x∈Rm(cid:26) max
is equivalent to the regularized regression problem minx∈Rmnkb − Axka +Pm

∆A∈Ua kb − (A + ∆A)xka(cid:27) ; Ua  n(δ1 ···   δm)(cid:12)(cid:12)(cid:12)kδika ≤ ci  i = 1 ···   mo;
i=1 ci|xi|o.

Using feature-wise uncorrelated disturbance may lead to overly conservative results. We relax this 
allowing the disturbances of different features to be correlated. Consider the following uncertainty
set:

U 0  (cid:8)(δ1 ···   δm)(cid:12)(cid:12)fj(kδ1ka ···  kδmka) ≤ 0; j = 1 ···   k(cid:9)  

where fj(·) are convex functions. Notice that both k and fj can be arbitrary  hence this is a very
general formulation and provides us with signiﬁcant ﬂexibility in designing uncertainty sets and
equivalently new regression algorithms. The following theorem converts this formulation to a con-
vex and tractable optimization problem.
Theorem 3. Assume that the set Z   {z ∈ Rm|fj(z) ≤ 0  j = 1 ···   k; z ≥ 0} has non-empty
relative interior. The robust regression problem

min

x∈Rm(cid:26) max

∆A∈U 0 kb − (A + ∆A)xka(cid:27)  

3

is equivalent to the following regularized regression problem

min
+ κ∈Rm

λ∈Rk

+  x∈Rmnkb − Axka + v(λ  κ  x)o;
Xj=1

c∈Rmh(κ + |x|)>c −

k

where: v(λ  κ  x)   max

(4)

λjfj(c)i.

Example 1. Suppose U 0 = n(δ1 ···   δm)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)kδ1ka ···  kδmka(cid:13)(cid:13)s ≤ l;o for a symmetric norm

k · ks  then the resulting regularized regression problem is
so; where k · k∗

x∈Rmnkb − Axka + lkxk∗

s is the dual norm of k · ks.

min

The robust regression formulation (1) considers disturbances that are bounded in a set  while in
practice  often the disturbance is a random variable with unbounded support. In such cases  it is not
possible to simply use an uncertainty set that includes all admissible disturbances  and we need to
construct a meaningful U based on probabilistic information. In the full version [15] we consider
computationally efﬁcient ways to use chance constraints to construct uncertainty sets.

3 Sparsity

In this section  we investigate the sparsity properties of robust regression (1)  and equivalently Lasso.
Lasso’s ability to recover sparse solutions has been extensively discussed (cf [6  7  8  9])  and takes
one of two approaches. The ﬁrst approach investigates the problem from a statistical perspective.
That is  it assumes that the observations are generated by a (sparse) linear combination of the fea-
tures  and investigates the asymptotic or probabilistic conditions required for Lasso to correctly
recover the generative model. The second approach treats the problem from an optimization per-
spective  and studies under what conditions a pair (A  b) deﬁnes a problem with sparse solutions
(e.g.  [16]).

We follow the second approach and do not assume a generative model. Instead  we consider the
conditions that lead to a feature receiving zero weight. In particular  we show that (i) as a direct
result of feature-wise independence of the uncertainty set  a slight change of a feature that was
originally assigned zero weight still gets zero weight (Theorem 4); (ii) using Theorem 4  we show
that “nearly” orthogonal features get zero weight (Corollary 1); and (iii) “nearly” linearly dependent
features get zero weight (Theorem 5). Substantial research regarding sparsity properties of Lasso
can be found in the literature (cf [6  7  8  9  17  18  19  20] and many others). In particular  similar
results as in point (ii)  that rely on an incoherence property  have been established in  e.g.  [16]  and
are used as standard tools in investigating sparsity of Lasso from a statistical perspective. However 
a proof exploiting robustness and properties of the uncertainty is novel. Indeed  such a proof shows
a fundamental connection between robustness and sparsity  and implies that robustifying w.r.t. a
feature-wise independent uncertainty set might be a plausible way to achieve sparsity for other
problems.
Theorem 4. Given ( ˜A  b)  let x∗ be an optimal solution of the robust regression problem:

min

x∈Rm(cid:26) max

∆A∈U kb − ( ˜A + ∆A)xk2(cid:27) .

Let I ⊆ {1 ···   m} be such that for all i ∈ I  x∗

i = 0. Now let

Then  x∗ is an optimal solution of

˜U  n(δ1 ···   δm)(cid:12)(cid:12)(cid:12)kδjk2 ≤ cj  j 6∈ I; kδik2 ≤ ci + `i  i ∈ Io.

∆A∈ ˜U kb − (A + ∆A)xk2(cid:27)  
for any A that satisﬁes kai − ˜aik ≤ `i for i ∈ I  and aj = ˜aj for j 6∈ I.

x∈Rm(cid:26) max

min

4

Proof. Notice that for i ∈ I  x∗
residual. We have

i = 0  hence the ith column of both A and ∆A has no effect on the

.

max

= max

= max

By deﬁnition of x∗ 

b − ( ˜A + ∆A)x∗(cid:13)(cid:13)(cid:13)2
∆A∈ ˜U(cid:13)(cid:13)(cid:13)
For i ∈ I  kai−˜aik ≤ li  and aj = ˜aj for j 6∈ I. Thus(cid:8) ˜A+∆A(cid:12)(cid:12)∆A ∈ U(cid:9) ⊆(cid:8)A+∆A(cid:12)(cid:12)∆A ∈ ˜U(cid:9).

Therefore  for any ﬁxed x0  the following holds:

∆A∈U(cid:13)(cid:13)(cid:13)

b − (A + ∆A)x∗(cid:13)(cid:13)(cid:13)2
∆A∈U(cid:13)(cid:13)(cid:13)
∆A∈U(cid:13)(cid:13)(cid:13)
∆A∈ ˜U(cid:13)(cid:13)(cid:13)

b − (A + ∆A)x∗(cid:13)(cid:13)(cid:13)2
b − ( ˜A + ∆A)x0(cid:13)(cid:13)(cid:13)2 ≤ max
∆A∈ ˜U(cid:13)(cid:13)(cid:13)
b − ( ˜A + ∆A)x∗(cid:13)(cid:13)(cid:13)2 ≤ max
∆A∈U(cid:13)(cid:13)(cid:13)
b − (A + ∆A)x∗(cid:13)(cid:13)(cid:13)2 ≤ max
∆A∈ ˜U(cid:13)(cid:13)(cid:13)

∆A∈U(cid:13)(cid:13)(cid:13)
b − (A + ∆A)x0(cid:13)(cid:13)(cid:13)2
b − ( ˜A + ∆A)x0(cid:13)(cid:13)(cid:13)2
b − (A + ∆A)x0(cid:13)(cid:13)(cid:13)2

Since this holds for arbitrary x0  we establish the theorem.

Therefore we have

.

.

.

max

max

max

Theorem 4 is established using the robustness argument  and is a direct result of the feature-wise
independence of the uncertainty set.
It explains why Lasso tends to assign zero weight to non-

relative features. Consider a generative model1 b =Pi∈I wiai + ˜ξ where I ⊆ {1···   m} and ˜ξ is
a random variable  i.e.  b is generated by features belonging to I. In this case  for a feature i0 6∈ I 
Lasso would assign zero weight as long as there exists a perturbed value of this feature  such that
the optimal regression assigned it zero weight. This is also shown in the next corollary  in which
we apply Theorem 4 to show that the problem has a sparse solution as long as an incoherence-type
property is satisﬁed (this result is more in line with the traditional sparsity results).
Corollary 1. Suppose that for all i  ci = c.

If there exists I ⊂ {1 ···   m} such that for all
v ∈ span(cid:0){ai  i ∈ I}S{b}(cid:1)  kvk = 1  we have v>aj ≤ c ∀j 6∈ I  then any optimal solution x∗
j = 0  ∀j 6∈ I.
Proof. For j 6∈ I  let a=
j denote the projection of aj onto the span of {ai  i ∈ I}S{b}  and let
a+
j

j . Thus  we have ka=

  aj − a=

satisﬁes x∗

j k ≤ c. Let ˆA be such that
i ∈ I;
i 6∈ I.

ˆai =(cid:26) ai

a+
i

Now let

ˆU   {(δ1 ···   δm)|kδik2 ≤ c  i ∈ I;kδjk2 = 0  j 6∈ I}.

Consider the robust regression problem minˆxn max∆A∈ ˆU(cid:13)(cid:13)
to minˆxn(cid:13)(cid:13)

b−( ˆA+∆A)ˆx(cid:13)(cid:13)2o  which is equivalent
b − ˆAˆx(cid:13)(cid:13)2 +Pi∈I c|ˆxi|(cid:9). Now we show that there exists an optimal solution ˆx∗ such
that ˆx∗
j = 0 for all j 6∈ I. This is because ˆaj are orthogonal to the span of of {ˆai  i ∈ I}S{b}.
Hence for any given ˆx  by changing ˆxj to zero for all j 6∈ I  the minimizing objective does not
increase.
j k ≤ c ∀j 6∈ I  (and recall that U = {(δ1 ···   δm)|kδik2 ≤ c ∀i}) applying
Since kˆa − ˆajk = ka=
Theorem 4 we establish the corollary.

The next corollary follows easily from Corollary 1.
Corollary 2. Suppose there exists I ⊆ {1 ···   m}  such that for all i ∈ I  kaik < ci. Then any
optimal solution x∗ satisﬁes x∗

i = 0  for i ∈ I.

1While we are not assuming generative models to establish the results  it is still interesting to see how these

results can help in a generative model setup.

5

The next theorem shows that sparsity is achieved when a set of features are “almost” linearly depen-
dent. Again we refer to [15] for the proof.
Theorem 5. Given I ⊆ {1 ···   m} such that there exists a non-zero vector (wi)i∈I satisfying

i = 0.

σiciwi| 

kXi∈I

wiaik2 ≤ min

0  which leads to the following corollary.

σi∈{−1 +1}|Xi∈I
then there exists an optimal solution x∗ such that ∃i ∈ I : x∗
Notice that for linearly dependent features  there exists non-zero (wi)i∈I such that kPi∈I wiaik2 =
Corollary 3. Given I ⊆ {1 ···   m}  let AI   (cid:16)ai(cid:17)i∈I
Setting I = {1 ···   m}  we immediately get the following corollary.
Corollary 4. If n < m  then there exists an optimal solution with no more than n non-zero coefﬁ-
cients.

i∈I has at most t non-zero coefﬁcients.

  and t   rank(AI ). There exists an

optimal solution x∗ such that x∗
I

  (xi)>

4 Density Estimation and Consistency

In this section  we investigate the robust linear regression formulation from a statistical perspective
and rederive using only robustness properties that Lasso is asymptotically consistent. We note that
our result applies to a considerably more general framework than Lasso. In the full version ([15])
we use some intermediate results used to prove consistency  to show that regularization can be
identiﬁed with the so-called maxmin expected utility (MMEU) framework  thus tying regularization
to a fundamental tenet of decision-theory.

n

i

i

x)2 + cnkxk1o;

√n

(bi − r>

x(P)   arg min

x(cn Sn)   arg min

We restrict our discussion to the case where the magnitude of the allowable uncertainty for all
features equals c  (i.e.  the standard Lasso) and establish the statistical consistency of Lasso from
a distributional robustness argument. Generalization to the non-uniform case is straightforward.
Throughout  we use cn to represent c where there are n samples (we take cn to zero).
Recall the standard generative model in statistical learning: let P be a probability measure with
bounded support that generates i.i.d. samples (bi  ri)  and has a density f ∗(·). Denote the set of the
ﬁrst n samples by Sn. Deﬁne
x nvuut
1
Xi=1
n
x nsZb r
(b − r>x)2dP(b  r)o.

x)2 + cnkxk1o = arg min
x n

In words  x(cn Sn) is the solution to Lasso with the tradeoff parameter set to cn√n  and x(P)

is the “true” optimal solution. We have the following consistency result. The theorem itself is a
well-known result. However  the proof technique is novel. This technique is of interest because
the standard techniques to establish consistency in statistical learning including VC dimension and
algorithm stability often work for a limited range of algorithms  e.g.  SVMs are known to have
inﬁnite VC dimension  and we show in the full version ([15]) that Lasso is not stable. In contrast 
a much wider range of algorithms have robustness interpretations  allowing a uniﬁed approach to
prove their consistency.
Theorem 6. Let {cn} be such that cn ↓ 0 and limn→∞ n(cn)m+1 = ∞. Suppose there exists a
constant H such that kx(cn Sn)k2 ≤ H almost surely. Then 
(b − r>x(cn Sn))2dP(b  r) =sZb r

(b − r>x(P))2dP(b  r) 

n→∞sZb r

n vuut

(bi − r>

n

Xi=1

lim

almost surely.

6

The full proof and results we develop along the way are deferred to [15]  but we provide the main
ideas and outline here. The key to the proof is establishing a connection between robustness and
kernel density estimation.
Step 1: For a given x  we show that the robust regression loss over the training data is equal to the
worst-case expected generalization error. To show this we establish a more general result:
Proposition 1. Given a function g : Rm+1 → R and Borel sets Z1 ···  Zn ⊆ Rm+1  let

The following holds

Pn   {µ ∈ P|∀S ⊆ {1 ···   n} : µ([i∈S
µ∈PnZRm+1

h(ri  bi) = sup

(ri bi)∈Zi

sup

1
n

n

Xi=1

Zi) ≥ |S|/n}.

h(r  b)dµ(r  b).

Step 2: Next we show that robust regression has a form like that in the left hand side above. Also 
the set of distributions we supremize over  in the right hand side above  includes a kernel density
estimator for the true (unknown) distribution. Indeed  consider the following kernel estimator: given
samples (bi  ri)n

i=1 

hn(b  r)   (ncm+1)−1

n

Xi=1

K(cid:18) b − bi  r − ri

c

(cid:19)  

(5)

where: K(x)   I[−1 +1]m+1(x)/2m+1.

Observe that the estimated distribution given by Equation (5) belongs to the set of distributions

Pn(A  ∆  b  c)   {µ ∈ P|Zi = [bi − c  bi + c] ×

m

Yj=1
[aij − δij  aij + δij];

ij =nc2

∀S ⊆ {1 ···   n} : µ([i∈S

of distributions used in the representation from Proposition 1.

Zi) ≥ |S|/n} 
and hence belongs to ˆP(n) = ˆP(n)   S∆|∀j Pi δ2
j Pn(A  ∆  b  c)  which is precisely the set
Step 3: Combining the last two steps  and using the fact thatRb r |hn(b  r) − h(b  r)|d(b  r) goes to
↑ ∞ since hn(·) is a kernel density estimation of f (·)
zero almost surely when cn ↓ 0 and ncm+1
(see e.g. Theorem 3.1 of [21])  we prove consistency of robust regression.
We can remove the assumption that kx(cn Sn)k2 ≤ H  and as in Theorem 6  the proof technique
rather than the result itself is of interest. We postpone the proof to [15].
Theorem 7. Let {cn} converge to zero sufﬁciently slowly. Then
(b − r>x(cn Sn))2dP(b  r) =sZb r

(b − r>x(P))2dP(b  r) 

n→∞sZb r

lim

n

almost surely.

5 Conclusion

In this paper  we consider robust regression with a least-square-error loss  and extend the results of
[11] (i.e.  Tikhonov regularization is equivalent to a robust formulation for Frobenius norm-bounded
disturbance set) to a broader range of disturbance sets and hence regularization schemes. A special
case of our formulation recovers the well-known Lasso algorithm  and we obtain an interpretation
of Lasso from a robustness perspective. We consider more general robust regression formulations 
allowing correlation between the feature-wise noise  and we show that this too leads to tractable
convex optimization problems.

We exploit the new robustness formulation to give direct proofs of sparseness and consistency for
Lasso. As our results follow from robustness properties  it suggests that they may be far more
general than Lasso  and that in particular  consistency and sparseness may be properties one can
obtain more generally from robustiﬁed algorithms.

7

References

[1] L. Elden. Perturbation theory for the least-square problem with linear equality constraints. BIT  24:472–

476  1985.

[2] G. Golub and C. Van Loan. Matrix Computation. John Hopkins University Press  Baltimore  1989.
[3] A. Tikhonov and V. Arsenin. Solution for Ill-Posed Problems. Wiley  New York  1977.
[4] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[5] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics 

32(2):407–499  2004.

[6] S. Chen  D. Donoho  and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on

Scientiﬁc Computing  20(1):33–61  1998.

[7] A. Feuer and A. Nemirovski. On sparse representation in pairs of bases. IEEE Transactions on Informa-

tion Theory  49(6):1579–1581  2003.

[8] E. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly

incomplete frequency information. IEEE Transactions on Information Theory  52(2):489–509  2006.

[9] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information

Theory  50(10):2231–2242  2004.

[10] M. Wainwright.

using

Sharp thresholds

for noisy and high-dimensional

spar-
sity
from:
http://www.stat.berkeley.edu/tech-reports/709.pdf  Department of Statistics 
UC Berkeley  2006.

recovery of
Technical Report Available

`1-constrained

quadratic

programming.

[11] L. El Ghaoui and H. Lebret. Robust solutions to least-squares problems with uncertain data. SIAM Journal

on Matrix Analysis and Applications  18:1035–1064  1997.

[12] A. Ben-Tal and A. Nemirovski. Robust solutions of uncertain linear programs. Operations Research

Letters  25(1):1–13  August 1999.

[13] D. Bertsimas and M. Sim. The price of robustness. Operations Research  52(1):35–53  January 2004.
[14] P. Shivaswamy  C. Bhattacharyya  and A. Smola. Second order cone programming approaches for han-

dling missing and uncertain data. Journal of Machine Learning Research  7:1283–1314  July 2006.

[15] H. Xu  C. Caramanis  and S. Mannor. Robust regression and Lasso. Submitted  available from

http://arxiv.org/abs/0811.1790v1  2008.

[16] J. Tropp. Just relax: Convex programming methods for identifying sparse signals. IEEE Transactions on

Information Theory  51(3):1030–1051  2006.

[17] F. Girosi. An equivalence between sparse approximation and support vector machines. Neural Computa-

tion  10(6):1445–1480  1998.

[18] R. R. Coifman and M. V. Wickerhauser. Entropy-based algorithms for best-basis selection. IEEE Trans-

actions on Information Theory  38(2):713–718  1992.

[19] S. Mallat and Z. Zhang. Matching Pursuits with time-frequence dictionaries. IEEE Transactions on Signal

Processing  41(12):3397–3415  1993.

[20] D. Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):1289–1306  2006.
[21] L. Devroye and L. Gy¨orﬁ. Nonparametric Density Estimation: the l1 View. John Wiley & Sons  1985.

8

,Kewei Tu
Maria Pavlovskaia
Song-Chun Zhu
Anselm Rothe
Brenden Lake
Todd Gureckis