2009,On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation,Motivated from real world problems  like object categorization  we study a particular mixed-norm regularization for Multiple Kernel Learning (MKL). It is assumed that the given set of kernels are grouped into distinct components where each component is crucial for the learning task at hand. The formulation hence employs $l_\infty$ regularization for promoting combinations at the component level and $l_1$ regularization for promoting sparsity among kernels in each component. While previous attempts have formulated this as a non-convex problem  the formulation given here is an instance of non-smooth convex optimization problem which admits an efficient Mirror-Descent (MD) based procedure. The MD procedure optimizes over product of simplexes  which is not a well-studied case in literature. Results on real-world datasets show that the new MKL formulation is well-suited for object categorization tasks and that the MD based algorithm outperforms state-of-the-art MKL solvers like \texttt{simpleMKL} in terms of computational effort.,On the Algorithmics and Applications of a

Mixed-norm based Kernel Learning Formulation

J. Saketha Nath

Dept. of Computer Science & Engg. 

Indian Institute of Technology  Bombay.

saketh@cse.iitb.ac.in

G. Dinesh

Dept. of Computer Science & Automation 

Indian Institute of Science  Bangalore.
dinesh@csa.iisc.ernet.in

S. Raman

Dept. of Computer Science & Automation 

Indian Institute of Science  Bangalore.
sraman@csa.iisc.ernet.in

Chiranjib Bhattacharyya

Dept. of Computer Science & Automation 

Indian Institute of Science  Bangalore.

chiru@csa.iisc.ernet.in

Aharon Ben-Tal

Technion  Haifa.

Faculty of Industrial Engg. & Management 

abental@ie.technion.ac.il

K. R. Ramakrishnan

Dept. of Electrical Engg. 

Indian Institute of Science  Bangalore.

krr@ee.iisc.ernet.in

Abstract

Motivated from real world problems  like object categorization  we study a par-
ticular mixed-norm regularization for Multiple Kernel Learning (MKL). It is as-
sumed that the given set of kernels are grouped into distinct components where
each component is crucial for the learning task at hand. The formulation hence
employs l∞ regularization for promoting combinations at the component level and
l1 regularization for promoting sparsity among kernels in each component. While
previous attempts have formulated this as a non-convex problem  the formula-
tion given here is an instance of non-smooth convex optimization problem which
admits an efﬁcient Mirror-Descent (MD) based procedure. The MD procedure
optimizes over product of simplexes  which is not a well-studied case in literature.
Results on real-world datasets show that the new MKL formulation is well-suited
for object categorization tasks and that the MD based algorithm outperforms state-
of-the-art MKL solvers like simpleMKL in terms of computational effort.

1 Introduction

In this paper the problem of Multiple Kernel Learning (MKL) is studied where the given kernels are
assumed to be grouped into distinct components and each component is crucial for the learning task
in hand. The focus of this paper is to study the formalism  algorithmics of a speciﬁc mixed-norm
regularization based MKL formulation suited for such tasks.
Majority of existing MKL literature have considered employing a block l1 norm regularization lead-
ing to selection of few of the given kernels [8  1  16  14  20] . Such formulations tend to select
the “best” among the given kernels and consequently the decision functions tend to depend only on
the selected kernel. Recently [17] extended the framework of MKL to the case where kernels are
partitioned into groups and introduces a generic mixed-norm regularization based MKL formulation
in order to handle groups of kernels. Again the idea is to promote sparsity leading to low number of
kernels. This paper differs from [17] by assuming that every component (group of kernels) is highly

1

crucial for success of the learning task. It is well known in optimization literature that l∞ regulariza-
tions often promote combinations with equal preferences and l1 regularizations lead to selections.
The proposed MKL formulation hence employs l∞ regularization and promotes combinations of
kernels at the component level. Moreover it employs l1 regularization for promoting sparsity among
kernels in each component.
The formulation studied here is motivated by real-world learning applications like object categoriza-
tion where multiple feature representations need to be employed simultaneously for achieving good
generalization. Combining feature descriptors using the framework of Multiple Kernel Learning
(MKL) [8] for object categorization has been a topic of interest for many recent studies [19  13].
For e.g.  in the case of ﬂower classiﬁcation feature descriptors for shape  color and texture need
to be employed in order to achieve good visual discrimination as well as signiﬁcant within-class
variation [12]. A key ﬁnding of [12] is the following: in object categorization tasks  employing few
of the feature descriptors or employing a canonical combination of them often leads to sub-optimal
solutions. Hence  in the framework of MKL  employing a l1 regularization  which is equivalent to
selecting one of the given kernels  as well as employing a l2 regularization  which is equivalent to
working with a canonical combination of the given kernels  may lead to sub-optimality. This im-
portant ﬁnding clearly motivates the use of l∞ norm regularization for combining kernels generated
from different feature descriptors and l1 norm regularization for selecting kernels generated from
the same feature descriptor. Hence  by grouping kernels generated from the same feature descriptor
together and employing the new MKL formulation  classiﬁers which are potentially well-suited for
object categorization tasks can be built.
Apart from the novel MKL formulation the main contribution of the paper is a highly efﬁcient
algorithm for solving it. Since the formulation is an instance of a Second Order Cone Program
(SOCP)  it can be solved using generic interior point algorithms. However it is impractical to work
with such solvers even for moderately large number of data points and kernels. Also the generic
wrapper approach proposed in [17] cannot be employed as it solves a non-convex variant of the
proposed (convex) formulation. The proposed algorithm employs mirror-descent [3  2  9] leading to
extremely scalable solutions.
The feasibility set for the minimization problem tackled by Mirror-Descent (MD) turns out to be
direct product of simplexes  which is not a standard set-up discussed in optimization literature. We
employ a weighted version of the entropy function as the prox-function in the auxiliary problem
solved by MD at each iteration and justify its suitability for the case of direct product of simplexes.
The mirror-descent based algorithm presented here is also of independent interest to the MKL com-
munity as it can solve the traditional MKL problem; namely the case when the number of groups is
unity. Empirically we show that the mirror-descent based algorithm proposed here scales better than
the state-of-the-art steepest descent based algorithms [14].
The remainder of this paper is organized as follows: in section 2  details of the new MKL formulation
and its dual are presented. The mirror-descent based algorithm which efﬁciently solves the dual is
presented in section 3. This is followed by a summary of the numerical experiments carried for
verifying the major claims of the paper. In particular  the empirical ﬁndings are a) the new MKL
formulation is well-suited for object categorization tasks b) the MD based algorithm scales better
than state-of-the-art gradient descent methods (e.g. simpleMKL) in solving the special case where
number of components (groups) of kernels is unity.

2 Mixed-norm based MKL Formulation

This section presents the novel mixed-norm regularization based MKL formulation and its dual.
In the following text we concentrate on the case of binary classiﬁcation. However many of the
ideas presented here apply to other learning problems too. Let the training dataset be denoted by
D = {(xi  yi)  i = 1  . . .   m | xi ∈ X   yi ∈ {−1  1}}. Here  xi represents the ith training data
point with label yi. Let Y denote the diagonal matrix with entries as yi. Suppose the given ker-
nels are divided into n groups (components) and the jth component has nj number of kernels. Let
the feature-space mapping generated from the kth kernel of the jth component be φjk(·) and the
1. We are in search of a hyperplane clas-
corresponding gram-matrix of training data points be Kjk

1The gram-matrices are unit-trace normalized.

2

siﬁer of the form(cid:80)n

(cid:80)nj
k=1 w(cid:62)

(1)

(2)

j=1

jkφjk(xi) − b = 0. As discussed above  we wish to perform a
block l∞ regularization over the model parameters wjk associated with distinct components and l1
regularization for those associated with the same component. Intuitively  such a regularization pro-
motes combinations of kernels belonging to different components and selections among kernels of
the same component. Following the framework of MKL and the mixed norm regularization detailed
here  the following formulation is immediate:

min
wjk b ξi

s.t.

yi

1
2

(cid:16)(cid:80)n

j=1

(cid:104)
(cid:80)nj
maxj
k=1 w(cid:62)

(cid:0)(cid:80)nj
k=1 (cid:107)wjk(cid:107)2
jkφjk(xi) − b

(cid:1)2(cid:105)
+ C(cid:80)
(cid:17) ≥ 1 − ξi  ξi ≥ 0 ∀ i

i ξi

j=1

i ξi

k=1

λjk

1
2

maxj

s.t.

yi

(cid:107)wjk(cid:107)2

2

(cid:17)(cid:105)

min
wjk b ξi

Here  ξi variables measure the slack in correctly classifying the ith training data point and C is the
regularization parameter controlling weightage given to the mixed-norm regularization term and the
total slack. MKL formulation in (1) is convex and moreover an instance of SOCP. This formulation
can also be realized as a limiting case of the generic CAP formulation presented in [17] (with γ =
1  γ0 → ∞). However since the motivation of that work was to perform feature selection  this
limiting case was neither theoretically studied nor empirically evaluated. Moreover  the generic
wrapper approach of [17] is inappropriate for solving this limiting case as that approach would solve
a non-convex variant of this (convex) formulation. In the following text  a dual of (1) is derived.
Let a simplex of dimensionality d be represented by ∆d. Following the strategy of [14]  one can

introduce variables λj ≡(cid:2)λj1 . . . λjnj
(cid:3)(cid:62) ∈ ∆nj and re-write (1) as follows:
(cid:104)
(cid:16)
+ C(cid:80)
(cid:80)nj
(cid:16)(cid:80)n
(cid:17) ≥ 1 − ξi  ξi ≥ 0 ∀ i
(cid:80)nj
minλj∈∆nj
jkφjk(xi) − b
k=1 w(cid:62)
This is because for any vector [a1 . . . an] ≥ 0  the following holds: minxi≥0 P
((cid:80)
=
(cid:80)nj
i ai)2. Notice that the max over j and min over λj can be interchanged. To see that rewrite
≤ t  where t is a new decision variable.
maxj as mint t with constraints minλj∈∆nj
constraints to obtain an equivalent problem: minλj∈∆nj ∀j t t subject to(cid:80)nj
This problem is feasible in both λjs and t and hence we can drop the minimization over individual
≤ t. One can
now eliminate t by reintroducing the maxj and interchange the minλj∈∆nj ∀j with other variables
+ C(cid:80)
(cid:80)nj
to obtain:
(cid:17) ≥ 1 − ξi  ξi ≥ 0 ∀ i
(cid:80)nj
2 maxj
jkφjk(xi) − b
k=1 w(cid:62)
(cid:19) α
(cid:18)(cid:80)nj
where α  γ are Lagrange multipliers  Sm(C) ≡ {x ∈ Rm | 0 ≤ x ≤ C1  (cid:80)m

Now one can derive the standard dual of (3) wrt. to the variables wjk  b  ξi alone  leading to:

 n(cid:88)

(cid:16)(cid:80)n

i=1 xiyi = 0} and

Qjk ≡ YKjkY. The following points regarding (4) must to be noted:

α∈Sm(C)  γ∈∆n

1(cid:62)α − 1

k=1 λjkQjk

min
wjk b ξi

λj∈∆nj ∀j

λj∈∆nj ∀j

2 α(cid:62)

(cid:80)

(cid:107)wjk(cid:107)2

2

(cid:107)wjk(cid:107)2

2

(cid:107)wjk(cid:107)2

2

k=1

λjk

s.t.

yi

k=1

λjk

k=1

λjk

(3)

(4)

i xi=1

a2
i
xi

i

min

max

min

j=1

γj

i ξi

1

j=1

(cid:18)Pnj

(cid:80)n

• (4) is equivalent
k=1 λ∗
γ∗

j=1

j

and λ∗

(cid:19)

to the well-known SVM [18] formulation with kernel Kef f ≡
is the weight given to the jth component

In other words  1
γ∗

2.

jkKjk

j

jk is weight given to the kth kernel of the jth component.

• It can be shown that none of γj  j = 1  . . .   n can be zero provided the given gram-matrices

Kjk are positive deﬁnite3.

2Superscript ‘*’ represents the optimal value as per (4)
3Add a small ridge if positive semi-deﬁnite.

3

• By construction  most of the weights λjk are zero and at-least for one kernel in every

component the weight is non-zero (see also [14]).

These facts readily justify the suitability of the particular mixed norm regularization for object cat-
egorization. Indeed  in-sync with ﬁndings of [12]  kernels from different feature descriptors (com-
ponents) are combined using non-trivial weights (i.e. 1
). Moreover  only the “best” kernels from
γ∗
each feature descriptor (component) are utilized by the model. This sparsity feature leads to bet-
ter interpretability as well as computational beneﬁts during the prediction stage. In the following
section an efﬁcient iterative algorithm for solving the dual (4) is presented.

j

3 Efﬁcient Algorithm for Solving the Dual

This section presents an efﬁcient algorithm for solving the dual (4). Note that typically in object cat-
egorization or other such multi-modal learning tasks  the number of feature descriptors (i.e. number
of groups of kernels  n) is low (< 10). However the kernels constructed from each feature descriptor
can be very high in number i.e.  nj ∀ j can be quite high. Also  it is frequent to encounter datasets
with huge number of training data points  m. Hence it is desirable to derive algorithms which scale
well wrt. m and nj. We assume n is small and almost O(1). Consider the dual formulation (4).
Using the minimax theorem [15]  one can interchange the min over λjs and max over γ to obtain:

 min

λj∈∆nj ∀j

(cid:124)

 max
(cid:124)

α∈Sm(C)

(cid:18)(cid:80)nj

 n(cid:88)
(cid:123)(cid:122)

j=1

gγ (λ1 ... λn)

1(cid:62)α − 1

2 α(cid:62)
(cid:123)(cid:122)

f (γ)

−

min
γ∈∆n

k=1 λjkQjk

γj

(cid:19) α



(cid:125)

(cid:125)

We have restated the maximum over γ as a minimization problem by introducing a minus sign.
The proposed algorithm performs alternate minimization over the variables γ and (λ1  . . .   λn  α).
In other words  in one step the variables (λ1  . . .   λn  α) are assumed to be constant and (5) is
optimized wrt. γ. This leads to the following optimization problem:

where Wj = α(cid:62)(cid:80)nj

k=1 λjkQjkα. This problem has an analytical solution given by:

(5)

(6)

j=1

Wj
γj

n(cid:88)
(cid:112)Wj(cid:80)
(cid:112)Wj

j

min
γ∈∆n

γj =

In the subsequent step γ is assumed to be ﬁxed and (5) is optimized wrt. (λ1  . . .   λn  α). For
this f(γ) needs to be evaluated by solving the corresponding optimization problem (refer (5) for
deﬁnition of f). Now  the per-step computational complexity of the iterative algorithm will depend
on how efﬁciently one evaluates f for a given γ.
In the following we present a mirror-descent
(MD) based algorithm which evaluates f to sufﬁcient accuracy in O(log [maxj nj])O(SVMm). Here
O(SVMm) represents the computational complexity of solving an SVM with m training data points.
Neglecting the log term  the overall per-step computational effort for the alternate minimization can
be assumed to be O(SVMm) and hence nearly-independent of the number of kernels. Alternatively 
one can employ the strategy of [14] and compute f using projected steepest-descent (SD) methods.
The following points highlight the merits and de-merits of these two methods:

• In case of SD  the per-step auxiliary problem has no closed form solution and projections
onto the feasibility set need to be done which are computationally intensive especially for
problems with high dimensions. In case of MD  the auxiliary problem has an analytical
solution (refer (8)).
• The step size needs to be computed using 1-d line search in case of SD; whereas the step-

sizes for MD can be easily computed using analytical expressions (refer (9)).

4

• The computational complexity of evaluating f using MD is nearly-independent of no. ker-
nels. However no such statement can be made for SD (unless feasibility set is of Euclidean
geometry  which is not so in our case).

(cid:20)

(cid:21)

1
st

The MD based algorithm for evaluating f(γ) i.e. solving minλj∈∆nj ∀j gγ(λ1  . . .   λn) is detailed
below. Let λ represent the vector [λ1 . . . λn](cid:62). Also let values at iteration ‘t’ be indicated using the
super-script ‘(t)’. Similar to any gradient-based method  at each step ‘t’ MD works with a linear
γ (λ) = gγ(λ(t)) + (λ − λ(t))(cid:62)∇gγ(λ(t)) and follows the below update
approximation of gγ: ˆg(t)
rule:

ω(λ(t)  λ)

2(cid:107)x(cid:107)2

ˆg(t)
γ (λ) +

λ(t+1) = argminλ∈∆n1×...×∆nn

(7)
where  ω(x  y) ≡ ω(y) − ω(x) − (y − x)(cid:62)∇ω(x) is the Bregman-divergence (prox-function) asso-
ciated with ω(x)  a continuously differentiable strongly convex distance-generating function. st is
a regularization parameter and also determines the step-size. (7) is usually known as the auxiliary
problem and needs to be solved at each step. Intuitively (7) minimizes a weighted sum of the local
linear approximation of the original objective and a regularization term that penalizes solutions far
from the current iterate. It is easy to show that the update rule in (7) leads to the SD technique
if ω(x) = 1
2 and step-size is chosen using 1-d line search. The key idea in MD is to choose
the distance-generating function based on the feasibility set  which in our case is direct product of
simplexes  such that (7) is very easy to solve. Note that for SD  with feasibility set as direct product
of simplexes  (7) is not easy to solve especially in higher dimensions.
We choose the distance-generating function as the following modiﬁed entropy function: ω(x) ≡

−1(cid:1) where δ is a small positive number

−1(cid:1) log(cid:0)xjkn−1 + δn−1nj

(cid:0)xjkn−1 + δn−1nj

(say  10e − 16). Now  let ˜gγ
(t) ≡ st∇gγ(λ(t)) − ∇ω(λ(t)). Note that gγ is nothing but the optimal
objective of SVM with kernel Kef f . Since it is assumed that each given kernel is positive deﬁnite 
the optimal of the SVM is unique and hence gradient of gγ wrt. λ exists [5]. Gradient of gγ can
be computed using ∂gγ
where α(t) is the optimal α obtained by solving an
∂λ

(α(t))(cid:62)
Qjkα(t)
γj

= − 1

(cid:80)nj

(cid:80)n

k=1

j=1

SVM with kernel as(cid:80)n

(t)
jk

j=1

(cid:18)Pnj

2

(t)
jk Kjk

k=1 λ
γj

(cid:19)

update (7) has the following analytical form4:

. With this notation  it is easy to show that the optimal

(cid:111)

(cid:110)− ˜gγ
(cid:110)− ˜gγ

(t)
jk n

(t)
jk n

exp

(cid:80)nj

k=1 exp

λ(t+1)
jk =

(cid:111)

(8)

The following text discusses the convergence issues with MD. Let the modulus of strong convexity
of ω wrt. (cid:107) · (cid:107) ≡ (cid:107) · (cid:107)1 be σ. Also  let the ω-size of feasibility set be deﬁned as follows: Θ ≡
ω(u  v). It is easy to verify that σ = O(1)n−2 and Θ = O (log [maxj nj]) in
maxu v∈∆n1×...×∆nn
our case. The convergence and its efﬁciency follow from this result [3  2  9]:

√

Θσ
(cid:107)∇gγ(cid:107)∗

one has the following bound on error after iteration
√

√
Result 1 With step-sizes:st =
t
T :T = mint≤T gγ(λ(t)) − gγ(λ∗) ≤ O(1)
where (cid:107)·(cid:107)∗ is the dual norm of the norm wrt. which the modulus of strong convexity was computed
(in our case (cid:107) · (cid:107)∗ = (cid:107) · (cid:107)∞) and L(cid:107)·(cid:107)(h) is Lipschitz constant of function h wrt. norm (cid:107) · (cid:107) (in our
case (cid:107) · (cid:107) = (cid:107) · (cid:107)1 and it can be shown that the Lipschitz constant exists for gγ). Substituting the
particular values for our case  we obtain

ΘL(cid:107)·(cid:107)(gγ )

√

σT

(cid:112)log [maxj nj]

n(cid:107)∇gγ(cid:107)∞

√
t

st =

√

and T ∝
. In other words  for reaching a reasonable approximation of the optimal 
the number iterations required are O(log [maxj nj])  which is nearly-independent of the number

log[maxj nj ]

√
T

(9)

4Since the term involving δ is (cid:28) λjk  it is neglected in this computation.

5

of kernels. Since the computations in each iteration are dominated by the SVM optimization  the
overall complexity of MD is (nearly) O(SV Mm). Note that the iterative algorithm can be improved
by improving the algorithm for solving the SVM problem. The overall algorithm is summarized in
algorithm 15. The MKL formulation presented here exploits the special structure in the kernels and

Algorithm 1: Mirror-descent based alternate minimization algorithm
Data: Labels and gram-matrices of training eg.  component-id of each kernel  regularization

parameter (C)

Result: Optimal values of α  γ  λ in (4)
begin

Set γ  λ to some initial feasible values.
while stopping criteria for γ is not met do

while stopping criteria for λ is not met do

Solve SVM with current kernel weights and update α
Compute ˜gγ

(t) and update λ using (8)

/* Alternate minimization loop */
/* Mirror-descent loop */

Compute Wj and update γ using (6)

Return values of α  γ  λ

end

leads to non-trivial combinations of the kernels belonging to different components and selections
among the kernels of the same component. Moreover the proposed iterative algorithm solves the
formulation with a per-step complexity of (almost) O(SV Mm)  which is the same as that with tra-
ditional MKL formulations (which do not exploit this structure). As discussed earlier  this efﬁciency
is an outcome of employing state-of-the-art mirror-descent techniques. The MD based algorithm
presented here is of independent interest to the MKL community. This is because  in the special
case where number of components is unity (i.e. n = 1)  the proposed algorithm solves the tradi-
tional MKL formulation. And clearly  owing to the merits of MD over SD discussed earlier  the new
algorithm can potentially be employed to boost the performance of state-of-the-art MKL algorithms.
Our empirical results conﬁrm that the proposed algorithm (with n = 1) outperforms simpleMKL
in terms of computational efﬁciency.

4 Numerical Experiments

This section presents results of experiments which empirically verify the major claims of the pa-
per: a) The proposed formulation is well-suited for object categorization b) In the case n = 1  the
proposed algorithm outperforms simpleMKL wrt. computational effort. In the following  the ex-
periments done on real-world object categorization datasets are summarized. The proposed MKL
formulation is compared with state-of-the-art methodology for object categorization [19  13] that
employs a block l1 regularization based MKL formulation with additional constraints for including
prior information regarding weights of kernels. Since such constraints lead to independent improve-
ments with all formulations  the experiments here compare the following three MKL formulations
without the additional constraints: MixNorm-MKL  the (l∞  l1) mixed-norm based MKL formula-
tion studied in this paper; L1-MKL  the block l1 regularization based MKL formulation [14]; and
L2-MKL  which is nothing but an SVM built using the canonical combination of all kernels i.e.
k=1 Kjk. In case of MixNorm-MKL  the MD based algorithm (section 3) was
used to solve the formulation. The SVM problem arising at each step of mirror-descent is solved
using the libsvm software6. L1-MKL is solved using simpleMKL7. L2-MKL is solved using
libsvm and serves as a baseline for comparison. In all cases  the hyper-parameters of the various
formulations were tuned using suitable cross-validation procedures and the accuracies reported de-
note testset accuracies achieved by the respective classiﬁers using the tuned set of hyper-parameters.

Kef f ≡(cid:80)n

(cid:80)nj

j=1

5Asymptotic convergence can be proved for the algorithm; details omitted due to lack of space.
6Available at www.csie.ntu.edu.tw/˜cjlin/libsvm
7Available at http://asi.insa-rouen.fr/enseignants/˜arakotom/code/mklindex.

html

6

(a) Caltech-5

(b) Caltech-5

(c) Oxford Flowers

(d) Oxford ﬂowers

(e) Caltech-101

(f) Caltech-101

Figure 1: Plot of average gain (%) in accuracy with MixNorm-MKL on the various real-world
datasets.

The following real-world datasets were used in the experiments: Caltech-5 [6]  Caltech-101 [7]
and Oxford Flowers [10]. The Caltech datasets contain digital images of various objects like faces 
watches  ants etc.; whereas the Oxford dataset contains images of 17 varieties of ﬂowers. The
Caltech-101 dataset has 101 categories of objects whereas Caltech-5 dataset is a subset of the
Caltech-101 dataset including images of Airplanes  Car sides  Faces  Leopards and Motorbikes
alone. Most categories of objects in the Caltech dataset have 50 images. The number of images
per category varies from 40 to 800. In the Oxford ﬂowers dataset there are 80 images in each ﬂower
category. In order to make the results presented here comparable to others in literature we have
followed the usual practice of generating training and test sets using a ﬁxed number of pictures from
each object category and repeating the experiments with different random selections of pictures. For
the Caltech-5  Caltech-101 and Oxford ﬂowers datasets we have used 50  15  60 images per object
category as training images and 50  15  20 images per object category as testing images respectively.
Also  in case of Caltech-5 and Oxford ﬂowers datasets  the accuracies reported are the testset ac-
curacies averaged over 10 such randomly sampled training and test datasets. Since the Caltech-101
dataset has large number of classes and the experiments are computationally intensive (100 choose
2 classiﬁers need to be built in each case)  the results are averaged over 3 sets of training and test
datasets only. In case of the Caltech datasets  ﬁve feature descriptors8 were employed: SIFT  Op-
ponentSIFT  rgSIFT  C-SIFT  Transformed Color SIFT. Whereas in case of Oxford ﬂowers dataset 
following strategy of [11  10]  seven feature descriptors9 were employed. Using each feature de-
scriptor  nine kernels were generated by varying the width-parameter of the Gaussian kernel. The
kernels can be grouped based on the feature descriptor they were generated from and the proposed
formulation can be employed to construct classiﬁers well-suited for object categorization. For eg. in
case of the Caltech datasets  n = 5 and nj = 9 ∀ j and in case of Oxford ﬂowers dataset  n = 7 and
nj = 9 ∀ j. In all cases  the 1-vs-1 methodology was employed to handle the multi-class problems.
The results of the experiments are summarized in ﬁgure 1. Each plot shows the % gain in accuracy
achieved by MixNorm-MKL over L1-MKL and L2-MKL for each object category. Note that for

8Code at http://staff.science.uva.nl/˜ksande/research/colordescriptors/
9Distance matrices available at http://www.robots.ox.ac.uk/˜vgg/data/flowers/17/

index.html

7

123450123456789Object CategoriesAverage gain wrt. L1−MKL (%)1234500.511.522.533.544.5Object CategoriesAverage gain wrt. L2−MKL (%)024681012141618−4−2024681012Object CategoriesAverage gain wrt. L1−MKL (%)02468101214161800.511.522.533.544.5Object CategoriesAverage gain wrt. L2−MKL (%)020406080100−1000100200300400500600700800Object CategoriesAverage gain wrt. L1−MKL (%)020406080100−1000100200300400500600Object CategoriesAverage gain wrt. L2−MKL (%)Figure 2: Scaling plots comparing scalability of mirror-descent based algorithm and simpleMKL.

most object categories  the gains are positive and moreover quite high. The best results are seen
in case of the Caltech-101 dataset: the peak and avg. gains over L1-MKL are 800%  37.57% re-
spectively and over L2-MKL are 600%  21.75% respectively. The gain in terms of numbers for the
other two datasets are not as high merely because the baseline accuracies were themselves high.
The baseline accuracies i.e.  the average accuracy achieved by L2-MKL (over all categories) were
93.84%  34.81% and 85.97% for the Caltech-5  Caltech-101 and Oxford ﬂowers datasets respec-
tively. The ﬁgures clearly show that the proposed formulation outperforms state-of-the-art object
categorization techniques and is hence highly-suited for such tasks. Another observation was that the
average sparsity (% of kernels with zero weightages) with the methods MixNorm-MKL  L1-MKL
and L2-MKL is 57%  96% and 0% respectively. Also  it was observed that L1-MKL almost always
selected kernels from one or two components (feature descriptors) only whereas MixNorm-MKL
(and ofcourse L2-MKL) selected kernels from all the components. These observations clearly show
that the proposed formulation combines important kernels while eliminating redundant and noisy
kernels using the information embedded in the group structure of the kernels.
In the following  the results of experiments which compare the scalability of simpleMKL and
the proposed mirror-descent based algorithm wrt. the number of kernels are presented. Note that
in the special case  n = 1  the proposed formulation is exactly same as the l1 regularization based
formulation. Hence the mirror-descent based iterative algorithm proposed here can also be employed
for solving l1 regularization based MKL. Figure 2 shows plots of the training times as a function
of number of kernels with the algorithms on two binary classiﬁcation problems encountered in the
object categorization experiments. The plots clearly show that the proposed algorithm outperforms
simpleMKL in terms of computational effort. Interestingly  it was found in our experiments that 
in most cases  the major computational effort at every iteration of SimpleMKL was in computing
the projection onto the feasible set! On the contrary Mirror descent allows an easily computable
closed form solution for the per-step auxiliary problem. We think this is the crucial advantage of
the proposed iterative algorithm over the gradient-decent based algorithms which were traditionally
employed for solving the MKL formulations.

5 Conclusions

This paper makes two important contributions: a) a speciﬁc mixed-norm regularization based MKL
formulation which is well-suited for object categorization and multi-modal tasks b) An efﬁcient
mirror-descent based algorithm for solving the new formulation. Empirical results on real-world
datasets show that the new formulation achieves far better generalization than state-of-the-art ob-
ject categorization techniques.
In some cases  the average gain in testset accuracy compared to
state-of-the-art was as high as 37%. The mirror-descent based algorithm presented in the paper not
only solves the proposed formulation efﬁciently but also outperforms simpleMKL in solving the
traditional l1 regularization based MKL. The speed-up was as high as 12 times in some cases. Appli-
cation of proposed methodology to various other multi-modal tasks and study of improved variants
of mirror-decent algorithm [4] for faster convergence are currently being explored by us.
Acknowledgements CB was supported by grants from Yahoo! and IBM.

8

1.522.533.540100200300400500600log10(Number of Kernels)Training Time (seconds) MixNorm−MKLL1−MKL1.522.533.540100200300400500600700800900log10(Number of Kernels)Training Time (seconds) MixNorm−MKLL1−MKLReferences
[1] F. Bach  G. R. G. Lanckriet  and M. I. Jordan. Multiple Kernel Learning  Conic Duality  and

the SMO Algorithm. In International Conference on Machine Learning  2004.

[2] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods

for convex optimization. Operations Research Letters  31:167–175  2003.

[3] Aharon Ben-Tal  Tamar Margalit  and Arkadi Nemirovski. The Ordered Subsets Mirror De-
scent Optimization Method with Applications to Tomography. SIAM Journal of Optimization 
12(1):79–108  2001.

[4] Aharon Ben-Tal and Arkadi Nemirovski. Non-euclidean Restricted Memory Level Method for

Large-scale Convex Optimization. Mathematical Programming  102(3):407–456  2005.

[5] O. Chapelle  V. Vapnik  O. Bousquet  and S. Mukerjhee. Choosing multiple parameters for

SVM. Machine Learning  46:131–159  2002.

[6] R. Fergus  P. Perona  and A. Zisserman. Object class recognition by unsupervised scale-
In IEEE Computer Society Conference on Computer Vision and Pattern

invariant learning.
Recognition  volume 2  2003.

[7] R. Fergus L. Fei-Fei and P. Perona. Learning generative visual models from few training
examples: an incremental bayesian approach tested on 101 object categories. In IEEE. CVPR
2004  Workshop on Generative-Model Based Vision.  2004.

[8] G.R.G. Lanckriet  N. Cristianini  P. Bartlett  L. El Ghaoui  and M.I. Jordan. Learning the
Kernel Matrix with Semideﬁnite Programming. Journal of Machine Learning Research  5:27–
72  2004.

[9] Arkadi Nemirovski. Lectures on modern convex optimization (chp.5.4). Available at www2.

isye.gatech.edu/˜nemirovs/Lect_ModConvOpt.pdf.

[10] M-E. Nilsback and A. Zisserman. A visual vocabulary for ﬂower classiﬁcation. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  2006.

[11] M-E. Nilsback and A Zisserman. Automated ﬂower classiﬁcation over a large number of
classes. In Proceedings of the Indian Conference on Computer Vision  Graphics and Image
Processing  2008.

[12] Maria-Elena Nilsback and Andrew Zisserman. A Visual Vocabulary for Flower Classiﬁca-
tion. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition  volume 2  pages 1447–1454  2006.

[13] Maria-Elena Nilsback and Andrew Zisserman. Automated Flower Classiﬁcation over a Large
Number of Classes. In Proceedings of the Sixth Indian Conference on Computer Vision  Graph-
ics & Image Processing  2008.

[14] A. Rakotomamonjy  F. Bach  S. Canu  and Y Grandvalet. SimpleMKL. Journal of Machine

Learning Research  9:2491–2521  2008.

[15] R. T. Rockafellar. Convex Analysis. Princeton University Press  1970.
[16] Soren Sonnenburg  Gunnar Ratsch  Christin Schafer  and Bernhard Scholkopf. Large Scale

Multiple Kernel Learning. Journal of Machine Learning Research  7:1531–1565  2006.

[17] M. Szafranski  Y. Grandvalet  and A. Rakotomamonjy. Composite Kernel Learning. In Pro-

ceedings of the Twenty-ﬁfth International Conference on Machine Learning (ICML)  2008.

[18] Vladimir Vapnik. Statistical Learning Theory. Wiley-Interscience  1998.
[19] M. Varma and D. Ray. Learning the Discriminative Power Invariance Trade-off. In Proceedings

of the International Conference on Computer Vision  2007.

[20] Zenglin Xu  Rong Jin  Irwin King  and Michael R. Lyu. An Extended Level Method for

Multiple Kernel Learning. In Advances in Neural Information Processing Systems  2008.

9

,Yuhong Guo
Isabel Valera
Zoubin Ghahramani
Ting-Chun Wang
Ming-Yu Liu
Jun-Yan Zhu
Guilin Liu
Andrew Tao
Jan Kautz
Bryan Catanzaro