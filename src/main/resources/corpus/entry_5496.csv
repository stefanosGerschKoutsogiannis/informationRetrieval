2013,The Pareto Regret Frontier,Performance guarantees for online learning algorithms typically take the form of regret bounds  which express that the cumulative loss overhead compared to the best expert in hindsight is small. In the common case of large but structured expert sets we typically wish to keep the regret especially small compared to simple experts  at the cost of modest additional overhead  compared to more complex others. We study which such regret trade-offs can be achieved  and how.  We analyse regret w.r.t. each individual expert as a multi-objective criterion in the simple but fundamental case of absolute loss. We characterise the achievable and Pareto optimal trade-offs  and the corresponding optimal strategies for each sample size both exactly for each finite horizon and asymptotically.,The Pareto Regret Frontier

Wouter M. Koolen

Queensland University of Technology
wouter.koolen@qut.edu.au

Abstract

Performance guarantees for online learning algorithms typically take the form of
regret bounds  which express that the cumulative loss overhead compared to the
best expert in hindsight is small. In the common case of large but structured expert
sets we typically wish to keep the regret especially small compared to simple
experts  at the cost of modest additional overhead compared to more complex
others. We study which such regret trade-offs can be achieved  and how.
We analyse regret w.r.t. each individual expert as a multi-objective criterion in
the simple but fundamental case of absolute loss. We characterise the achievable
and Pareto optimal trade-offs  and the corresponding optimal strategies for each
sample size both exactly for each ﬁnite horizon and asymptotically.

1

Introduction

One of the central problems studied in online learning is prediction with expert advice. In this task
a learner is given access to K strategies  customarily referred to as experts. He needs to make a
sequence of T decisions with the objective of performing as well as the best expert in hindsight.
This goal can be achieved with modest overhead  called regret. Typical algorithms  e.g. Hedge [1]

with learning rate η =(cid:112)8/T ln K  guarantee
T ≤ (cid:112)T /2 ln K

LT − Lk

for each expert k.

(1)

T are the cumulative losses of the learner and expert k after all T rounds.

where LT and Lk
Here we take a closer look at that right-hand side. For it is not always desirable to have a uniform
regret bound w.r.t. all experts. Instead  we may want to single out a few special experts and demand
to be really close to them  at the cost of increased overhead compared to the rest. When the number
of experts K is large or inﬁnite  such favouritism even seems unavoidable for non-trivial regret
bounds. The typical proof of the regret bound (1) suggests that the following can be guaranteed as
well. For each choice of probability distribution q on experts  there is an algorithm that guarantees

T ≤ (cid:112)T /2(− ln q(k))

LT − Lk

for each expert k.

(2)

However  it is not immediately obvious how this can be achieved. For example  the Hedge learning
rate η would need to be tuned differently for different experts. We are only aware of a single
(complex) algorithm that achieves something along these lines [2]. On the ﬂip side  it is also not
obvious that this trade-off proﬁle is optimal.
In this paper we study the Pareto (achievable and non-dominated) regret trade-offs. Let us say that a
candidate trade-off (cid:104)r1  . . .   rK(cid:105) ∈ RK is T -realisable if there is an algorithm that guarantees

LT − Lk

T ≤ rk

for each expert k.

Which trade-offs are realisable? Among them  which are optimal? And what is the strategy that
witnesses these realisable strategies?

1

1.1 This paper

We resolve the preceding questions for the simplest case of absolute loss  where K = 2. We
ﬁrst obtain an exact characterisation of the set of realisable trade-offs. We then construct for each
realisable proﬁle a witnessing strategy. We also give a randomised procedure for optimal play that
extends the randomised procedures for balanced regret proﬁles from [3] and later [4  5].
We then focus on the relation between priors and regret bounds  to see if the particular form (2)
is achievable  and if so  whether it is optimal. To this end  we characterise the asymptotic Pareto
frontier as T → ∞. We ﬁnd that the form (2) is indeed achievable but fundamentally sub-optimal.
This is of philosophical interest as it hints that approaching absolute loss by essentially reducing
it to information theory (including Bayesian and Minimum Description Length methods  relative
entropy based optimisation (instance of Mirror Descent)  Defensive Forecasting etc.) is lossy.
Finally  we show that our solution for absolute loss equals that of K = 2 experts with bounded linear
loss. We then show how to obtain the bound (1) for K ≥ 2 experts using a recursive combination
of two-expert predictors. Counter-intuitively  this cannot be achieved with a balanced binary tree of
predictors  but requires the most unbalanced tree possible. Recursive combination with non-uniform
prior weights allows us to obtain (2) (with higher constant) for any prior q.

1.2 Related work

Our work lies in the intersection of two lines of work  and uses ideas from both. On the one hand
there are the game-theoretic (minimax) approaches to prediction with expert advice. In [6] Cesa-
Bianchi  Freund  Haussler  Helmbold  Schapire and Warmuth analysed the minimax strategy for
absolute loss with a known time horizon T . In [5] Cesa-Bianchi and Shamir used random walks to
implement it efﬁciently for K = 2 experts or K ≥ 2 static experts. A similar analysis was given
by Koolen in [4] with an application to tracking. In [7] Abernethy  Langford and Warmuth obtained
the optimal strategy for absolute loss with experts that issue binary predictions  now controlling
the game complexity by imposing a bound on the loss of the best expert. Then in [3] Abernethy 
Warmuth and Yellin obtained the worst case optimal algorithm for K ≥ 2 arbitrary experts. More
general budgets were subsequently analysed by Abernethy and Warmuth in [8]. Connections be-
tween minimax values and algorithms were studied by Rakhlin  Shamir and Sridharan in [9].
On the other hand there are the approaches that do not treat all experts equally. Freund and Schapire
obtain a non-uniform bound for Hedge in [1] using priors  although they leave the tuning problem
open. The tuning problem was addressed by Hutter and Poland in [2] using two-stages of Follow
the Perturbed Leader. Even-Dar  Kearns  Mansour and Wortman characterise the achievable trade-
offs when we desire especially small regret compared to a ﬁxed average of the experts’ losses in
[10]. Their bounds were subsequently tightened by Kapralov and Panigrahy in [11]. An at least
tangentially related problem is to ensure smaller regret when there are several good experts. This
was achieved by Chaudhuri  Freund and Hsu in [12]  and later reﬁned by Chernov and Vovk in [13].

2 Setup

The absolute loss game is one of the core decision problems studied in online learning [14]. In it 
the learner sequentially predicts T binary outcomes. Each round t ∈ {1  . . .   T} the learner assigns
a probability pt ∈ [0  1] to the next outcome being a 1  after which the actual outcome xt ∈ {0  1} is
revealed  and the learner suffers absolute loss |pt − xt|. Note that absolute loss equals expected 0/1
loss  that is  the probability of a mistake if a “hard” prediction in {0  1} is sampled with bias p on 1.
Realising that the learner cannot avoid high cumulative loss without assumptions on the origin of
the outcomes  the learner’s objective is deﬁned to ensure low cumulative loss compared to a ﬁxed
set of baseline strategies. Meeting this goal ensures that the easier the outcome sequence (i.e. for
which some reference strategy has low loss)  the lower the cumulative loss incurred by the learner.

2

3

0

2

0

r1

1

1

0

1

0

0
0

2

1
1

2
2

r0

3
3

(a) The Pareto trade-off proﬁles for small T . The
sets GT consist of the points to the north-east of
each curve.

(b) Realisable trade-off proﬁles for T = 0  1  2  3.
The vertices on the proﬁle for each horizon T are
numbered 0  . . .   T from left to right.

Figure 1: Exact regret trade-off proﬁle

The regret w.r.t. the strategy k ∈ {0  1} that always predicts k is given by 1

T(cid:88)
(cid:0)|pt − xt| − |k − xt|(cid:1).

Rk

T :=

t=1

Minimising regret  deﬁned in this way  is a multi-objective optimisation problem. The classical
T   that is  to ensure small regret
approach is to “scalarise” it into the single objective RT := maxk Rk
compared to the best expert in hindsight. In this paper we study the full Pareto trade-off curve.
Deﬁnition 1. A candidate trade-off (cid:104)r0  r1(cid:105) ∈ R2 is called T -realisable for the T -round absolute
loss game if there is a strategy that keeps the regret w.r.t. each k ∈ {0  1} below rk  i.e. if

∃p1∀x1 ···∃pT∀xT : R0

T ≤ r0 and R1

T ≤ r1

where pt ∈ [0  1] and xt ∈ {0  1} in each round t. We denote the set of all T -realisable pairs by GT .
This deﬁnition extends easily to other losses  many experts  fancy reference combinations of ex-
perts (e.g. shifts  drift  mixtures)  protocols with side information etc. We consider some of these
extension in Section 5  but for now our goal is to keep it as simple as possible.

3 The exact regret trade-off proﬁle
In this section we characterise the set GT ⊂ R2 of T -realisable trade-offs. We show that it is a
convex polygon  that we subsequently characterise by its vertices and edges. We also exhibit the
optimal strategy witnessing each Pareto optimal trade-off and discuss the connection with random
walks. We ﬁrst present some useful observations about GT .
The linearity of the loss as a function of the prediction already renders GT highly regular.
Lemma 2. The set GT of T -realisable trade-offs is convex for each T .
Proof. Take rA and rB in GT . We need to show that αrA + (1− α)rB ∈ GT for all α ∈ [0  1]. Let
A and B be strategies witnessing the T -realisability of these points. Now consider the strategy that
in each round t plays the mixture αpA
t . As the absolute loss is linear in the prediction 
this strategy guarantees LT = αLA

k for each k ∈ {0  1}.

k +(1−α)rB

T ≤ Lk

T +αrA

t + (1 − α)pB
T +(1−α)LB
Guarantees violated early cannot be restored later.
Lemma 3. A strategy that guarantees Rk

T ≤ rk must maintain Rk

t ≤ rk for all 0 ≤ t ≤ T .

1One could deﬁne the regret Rk

T for all static reference probabilities k ∈ [0  1]  but as the loss is minimised

by either k = 0 or k = 1  we immediately restrict to only comparing against these two.

3

 0 1 2 3 4 0 1 2 3 4regret w.r.t. 1regret w.r.t. 0T=1T=2T=3T=4T=5T=6T=7T=8T=9T=10T = Lk

t > rk at some t < T . An adversary may set all

t . As LT ≥ Lt  we have Rk

Proof. Suppose toward contradiction that Rk
xt+1 . . . xT to k to ﬁx Lk
The two extreme trade-offs (cid:104)0  T(cid:105) and (cid:104)T  0(cid:105) are Pareto optimal.
Lemma 4. Fix horizon T and r1 ∈ R. The candidate proﬁle (cid:104)0  r1(cid:105) is T -realisable iff r1 ≥ T .
Proof. The static strategy pt = 0 witnesses (cid:104)0  T(cid:105) ∈ GT for every horizon T . To ensure R1
any strategy will have to play pt > 0 at some time t ≤ T . But then it cannot maintain R0
t = 0.

T < T  

T = LT −Lk

T ≥ Lt−Lk

t = Rk

t > rk.

It is also intuitive that maintaining low regret becomes progressively harder with T .
Lemma 5. G0 ⊃ G1 ⊃ . . .
Proof. Lemma 3 establishes ⊇  whereas Lemma 4 establishes (cid:54)=.
We now come to our ﬁrst main result  the characterisation of GT . We will directly characterise its
south-west frontier  that is  the set of Pareto optimal trade-offs. These frontiers are graphed up to
T = 10 in Figure 1a. The vertex numbering we introduce below is illustrated by Figure 1b.
Theorem 6. The Pareto frontier of GT is the piece-wise linear curve through the T + 1 vertices

i(cid:88)

j=0

(cid:18)T − j − 1
(cid:19)

T − i − 1

.

(cid:10)fT (i)  fT (T − i)(cid:11)

for i ∈ {0  . . .   T}

where

fT (i) :=

j2j−T

Moreover  for T > 0 the optimal strategy at vertex i assigns to the outcome x = 1 the probability

pT (0) := 0 

pT (T ) := 1 

and pT (i) :=

for

0 < i < T 

and the optimal probability interpolates linearly in between consecutive vertices.

fT−1(i) − fT−1(i − 1)

2

Proof. By induction on T . We ﬁrst consider the base case T = 0. By Deﬁnition 1

G0 = (cid:8)(cid:104)r0  r1(cid:105)(cid:12)(cid:12) r0 ≥ 0 and r1 ≥ 0(cid:9)

is the positive orthant  which has the origin as its single Pareto optimal vertex  and indeed
(cid:104)f0(0)  f0(0)(cid:105) = (cid:104)0  0(cid:105). We now turn to T ≥ 1. Again by Deﬁnition 1 (cid:104)r0  r1(cid:105) ∈ GT if

∃p ∈ [0  1]∀x ∈ {0  1} :(cid:10)r0 − |p − x| + |0 − x|  r1 − |p − x| + |1 − x|(cid:11) ∈ GT−1 
∃p ∈ [0  1] :(cid:10)r0 − p  r1 − p + 1(cid:11) ∈ GT−1 and(cid:10)r0 + p  r1 + p − 1(cid:11) ∈ GT−1.

that is if

min
p∈[0 1]

By the induction hypothesis we know that the south-west frontier curve for GT−1 is piecewise linear.
We will characterise GT via its frontier as well. For each r0  let r1(r0) and p(r0) denote the value
and minimiser of the optimisation problem

(cid:12)(cid:12) both (cid:104)r0  r1(cid:105) ± (cid:104)p  p − 1(cid:105) ∈ GT−1

(cid:8)r1

(cid:9).

We also refer to (cid:104)r0  r1(r0)(cid:105) ± (cid:104)p(r0)  p(r0) − 1(cid:105) as the rear(−) and front(+) contact points. For
r0 = 0 we ﬁnd r1(0) = T   with witness p(0) = 0 and rear/front contact points (cid:104)0  T + 1(cid:105) and
(cid:104)0  T − 1(cid:105)  and for r0 = T we ﬁnd r1(T ) = 0 with witness p(T ) = 1 and rear/front contact
points (cid:104)T − 1  0(cid:105) and (cid:104)T + 1  0(cid:105). It remains to consider the intermediate trajectory of r1(r0) as
r0 runs from 0 to T . Initially at r0 = 0 the rear contact point lies on the edge of GT−1 entering
vertex i = 0 of GT−1  while the front contact point lies on the edge emanating from that same
vertex. So if we increase r0 slightly  the contact points will slide along their respective lines. By
Lemma 11 (supplementary material)  r1(r0) will trace along a straight line as a result. Once we
increase r0 enough  both the rear and front contact point will hit the vertex at the end of their
edges simultaneously (a fortunate fact that greatly simpliﬁes our analysis)  as shown in Lemma 12
(supplementary material). The contact points then transition to tracing the next pair of edges of
GT−1. At this point r0 the slope of r1(r0) changes  and we have discovered a vertex of GT .
Given that at each such transition (cid:104)r0  r1(r0)(cid:105) is the midpoint between both contact points  this
implies that all midpoints between successive vertices of GT−1 are vertices of GT . And in addition 
there are the two boundary vertices (cid:104)0  T(cid:105) and (cid:104)T  0(cid:105).

4

(a) Normal scale

(b) Log-log scale to highlight the tail behaviour

Figure 2: Pareto frontier of G  the asymptotically realisable trade-off rates. There is no noticeable
difference with the normalised regret trade-off proﬁle GT /
T for T = 10000. We also graph the

curve(cid:10)(cid:112)− ln(q) (cid:112)− ln(1 − q)(cid:11) for all q ∈ [0  1].

√

3.1 The optimal strategy and random walks

In this section we describe how to follow the optimal strategy. First suppose we desire to witness a
T -realisable trade-off that happens to be a vertex of GT   say vertex i at (cid:104)fT (i)  fT (T − i)(cid:105). With T
rounds remaining and in state i  the strategy predicts with pT (i). Then the outcome x ∈ {0  1} is
revealed. If x = 0  we need to witness in the remaining T − 1 rounds the trade-off (cid:104)fT (i)  fT (T −
i)(cid:105) − (cid:104)pT (i)  pT (i) + 1(cid:105) = (cid:104)fT−1(i − 1)  fT−1(T − 1)(cid:105)  which is vertex i − 1 of GT−1. So the
strategy transition to state i − 1. Similarly upon x = 1 we update our internal state to i. If the state
ever either exceeds the number of rounds remaining or goes negative we simply clamp it.
Second  if we desire to witness a T -realisable trade-off that is a convex combination of successive
vertices  we simply follow the mixture strategy as constructed in Lemma 2. Third  if we desire to
witness a sub-optimal element of GT   we may follow any strategy that witnesses a Pareto optimal
dominating trade-off.
The probability p issued by the algorithm is sometimes used to randomly sample a “hard prediction”
from {0  1}. The expression |p − x| then denotes the expected loss  which equals the probability
of making a mistake. We present  following [3]  a random-walk based method to sample a 1 with
probability pT (i). Our random walk starts in state (cid:104)T  i(cid:105). In each round it transitions from state (cid:104)T  i(cid:105)
to either state (cid:104)T − 1  i(cid:105) or state (cid:104)T − 1  i − 1(cid:105) with equal probability. It is stopped when the state
(cid:104)T  i(cid:105) becomes extreme in the sense that i ∈ {0  T}. Note that this process always terminates. Then
the probability that this process is stopped with i = T equals pT (i). In our case of absolute loss 
evaluating pT (i) and performing the random walk both take T units of time. The random walks
considered in [3] for K ≥ 2 experts still take T steps  whereas direct evaluation of the optimal
strategy scales rather badly with K.

4 The asymptotic regret rate trade-off proﬁle

In the previous section we obtained for each time horizon T a combinatorial characterisation of the
set GT of T -realisable trade-offs. In this section we show that properly normalised Pareto frontiers
for increasing T are better and better approximations of a certain intrinsic smooth limit curve. We
obtain a formula for this curve  and use it to study the question of realisability for large T .
Deﬁnition 7. Let us deﬁne the set G of asymptotically realisable regret rate trade-offs by

G := lim
T→∞

GT√

T

.

Despite the disappearance of the horizon T from the notation  the set G still captures the trade-offs
that can be achieved with prior knowledge of T . Each achievable regret rate trade-off (cid:104)ρ0  ρ1(cid:105) ∈ G

5

 0 0.5 1 1.5 2 0 0.5 1 1.5 2normalised regret w.r.t. 1normalised regret w.r.t. 0T=10000asymptotically realisablesqrt min-log-prior 1 10 1e-50 1e-40 1e-30 1e-20 1e-10 1normalised regret w.r.t. 1normalised regret w.r.t. 0T=10000asymptotically realisablesqrt min-log-prior√
may be witnessed by a different strategy for each T . This is ﬁne for our intended interpretation of

T G as a proxy for GT . We brieﬂy mention horizon-free algorithms at the end of this section.

The literature [2] suggests that  for some constant c  (cid:104)(cid:112)−c ln(q) (cid:112)−c ln(1 − q)(cid:105) should be asymp-

totically realisable for each q ∈ [0  1]. We indeed conﬁrm this below  and determine the optimal
constant to be c = 1. We then discuss the philosophical implications of the quality of this bound.
We now come to our second main result  the characterisation of the asymptotically realisable trade-
√
off rates. The Pareto frontier is graphed in Figure 2 both on normal axes for comparison to Figure 1a 
and on a log-log scale to show its tails. Note the remarkable quality of approximation to GT /
T .
Theorem 8. The Pareto frontier of the set G of asymptotically realisable trade-offs is the curve

(cid:10)f (u)  f (−u)(cid:11)
(cid:82) u
0 e−v2 dv is the error function. Moreover  the optimal strategy converges to

and erf(u) = 2√
π

2u(cid:1) +

for u ∈ R 

e−2u2√

where

+ u 

2π

f (u) := u erf(cid:0)√
2u(cid:1)

.

1 − erf(cid:0)√

2

√
Proof. We calculate the limit of normalised Pareto frontiers at vertex i = T /2 + u

T   and obtain

p(u) =

(cid:0)T /2 + u

√

T(cid:1)

√

T

fT

lim
T→∞

= lim
T→∞

1√
T

√

T /2+u

T(cid:88)
(cid:90) T /2+u

j=0

j2j−T
√

T

1√
T

(cid:90) u

0

j2j−T
√

= lim
T→∞

= lim
T→∞

(cid:90) u
(cid:90) u

−∞

−∞

=

=

T /2

(u − v)2(u−v)
−√
√
(u − v) lim
T→∞ 2(u−v)
e− 1
2 (u+v)2
√

(u − v)

dv

2π

√

(cid:19)

T − 1

√
T /2 − u

(cid:18) T − j − 1
(cid:19)
(cid:18) T − j − 1
(cid:18)T − (u − v)
(cid:18)T − (u − v)
2u(cid:1) +
= u erf(cid:0)√

(cid:19)√
dj
T − 1
√
T − 1
√
(cid:19)√
T /2 − u
T − 1
√
T − 1
√
T − 1
T /2 − u
e−2u2√

T /2 − u
T−T

T−T

+ u

2π

T dv

T dv

In the ﬁrst step we replace the sum by an integral. We can do this as the summand is continuous in j 
and the approximation error is multiplied by 2−T and hence goes to 0 with T . In the second step we
perform the variable substitution v = u− j/
T . We then exchange limit and integral  subsequently
evaluate the limit  and in the ﬁnal step we evaluate the integral.
To obtain the optimal strategy  we observe the following relation between the slope of the Pareto
curve and the optimal strategy for each horizon T . Let g and h denote the Pareto curves at times T
and T + 1 as a function of r0. The optimal strategy p for T + 1 at r0 satisﬁed the system of equations

√

h(r0) + p − 1 = g(u + p)
h(r0) − p + 1 = g(u − p)

to which the solution satisﬁes

1 − 1
p

=

g(r0 + p) − g(r0 − p)

2p

≈ dg(r0)
dr0

 

so that

p ≈

1

1 − dg(r0)
dr0

.

Since slope is invariant under normalisation  this relation between slope and optimal strategy be-
comes exact as T tends to inﬁnity  and we ﬁnd

1 − erf(cid:0)√

2u(cid:1)

.

2

p(u) =

1

1 + df (r0(u))
dr0(u)

=

1

1 + f(cid:48)(u)
f(cid:48)(−u)

=

We believe this last argument is more insightful than a direct evaluation of the limit.

6

4.1 Square root of min log prior

Results for Hedge suggest — modulo a daunting tuning problem — that a trade-off featuring square
root negative log prior akin to (2) should be realisable. We ﬁrst show that this is indeed the case  we
then determine the optimal leading constant and we ﬁnally discuss its sub-optimality.

Theorem 9. The parametric curve(cid:10)(cid:112)−c ln(q) (cid:112)−c ln(1 − q)(cid:11) for q ∈ [0  1] is contained in G

(i.e. asymptotically realisable) iff c ≥ 1.

Proof. By Theorem 8  the frontier of G is of the form (cid:104)f (u)  f (−u)(cid:105). Our argument revolves around
the tails (extreme u) of G. For large u (cid:29) 0  we ﬁnd that f (u) ≈ 2u. For small u (cid:28) 0  we ﬁnd that
f (u) ≈ e−2u2
2πu2 . This is obtained by a 3rd order Taylor series expansion around u = −∞. We need
to go to 3rd order since all prior orders evaluate to 0. The additive approximation error is of order
e−2u2

u−4  which is negligible. So for large r0 (cid:29) 0  the least realisable r1 is approximately

√
4

With the candidate relation r0 =(cid:112)−c ln(q) and r1 =(cid:112)−c ln(1 − q)  still for large r0 (cid:29) 0 so that

q is small and − ln(1 − q) ≈ q  we would instead ﬁnd least realisable r1 approximately equal to

(3)

2π

.

r1 ≈ e− r2
2 −2 ln r0
√

0

r1 ≈ √

ce− r2
0
2c .

(4)

The candidate tail (4) must be at least the actual tail (3) for all large r0. The minimal c for which
this holds is c = 1. The graphs of Figure 2 illustrate this tail behaviour for c = 1  and at the same
time verify that there are no violations for moderate u.

√

ln 2 ≈ 0.833  whereas f (0) = 1/

Even though the sqrt-min-log-prior trade-off is realisable  we see that its tail (4) exceeds the actual
tail (3) by the factor r2
2π  which gets progressively worse with the extremity of the tail r0. Fig-
ure 2a shows that its behaviour for moderate (cid:104)r0  r1(cid:105) is also not brilliant. For example it gives us a
0
√
symmetric bound of
For certain log loss games  each Pareto regret trade-off is witnessed uniquely by the Bayesian mix-
ture of expert predictions w.r.t. a certain non-uniform prior and vice versa (not shown). In this sense
the Bayesian method is the ideal answer to data compression/investment/gambling. Be that as it
may  we conclude that the world of absolute loss is not information theory: simply putting a prior
is not the deﬁnitive answer to non-uniform guarantees. It is a useful intuition that leads to the con-
venient sqrt-min-log-prior bounds. We hope that our results contribute to obtaining tighter bounds
that remain manageable.

2π ≈ 0.399 is optimal.

√

4.2 The asymptotic algorithm

√

The previous theorem immediately suggests an approximate algorithm for ﬁnite horizon T . To
T(cid:104)f (u)  f (−u)(cid:105) is closest to it.
approximately witness (cid:104)r0  r1(cid:105)  ﬁnd the value of u for which
Then play p(u). This will not guarantee (cid:104)r0  r1(cid:105) exactly  but intuitively it will be close. We leave
analysing this idea to the journal version. Conversely  by taking the limit of the game protocol 
which involves the absolute loss function  we might obtain an interesting protocol and “asymptotic”
loss function2  for which u is the natural state  p(u) is the optimal strategy  and u is updated in
a certain way. Investigating such questions will probably lead to interesting insights  for example
T ≤ ρk for all T simultaneously. Again this will be
horizon-free strategies that maintain Rk
pursued for the journal version.

√

T /

2 We have seen an instance of this before. When the Hedge algorithm with learning rate η plays weights w
and faces loss vector (cid:96)  its dot loss is given by wT (cid:96). Now consider the same loss vector handed out in identical
pieces (cid:96)/n over the course of n trials  during which the weights w update as usual. In the limit of n → ∞  the
resulting loss becomes the mix loss − 1

η ln(cid:80)

k w(k)e−η(cid:96)k.

7

5 Extension

5.1 Beyond absolute loss

T(cid:88)

t − T(cid:88)

In this section we consider the general setting with K = 2 expert  that we still refer to as 0 and
1. Here the learner plays p ∈ [0  1] which is now interpreted as the weight allocated to expert 1 
the adversary chooses a loss vector (cid:96) = (cid:104)(cid:96)0  (cid:96)1(cid:105) ∈ [0  1]2  and the learner incurs dot loss given by
(1 − p)(cid:96)0 + p(cid:96)1. The regrets are now redeﬁned as follows

Rk

T :=

t + (1 − pt)(cid:96)0

pt(cid:96)1

(cid:96)k
t

for each expert k ∈ {0  1}.

Theorem 10. The T -realisable trade-offs for absolute loss and K = 2 expert dot loss coincide.

t=1

t=1

Proof. By induction on T . The loss is irrelevant in the base case T = 0. For T > 0  a trade-off
(cid:104)r0  r1(cid:105) is T -realisable for dot loss if

∃p ∈ [0  1]∀(cid:96) ∈ [0  1]2 : (cid:104)r0 + p(cid:96)1 + (1 − p)(cid:96)0 − (cid:96)0  r1 + p(cid:96)1 + (1 − p)(cid:96)0 − (cid:96)1(cid:105) ∈ GT−1

that is if
We recover the absolute loss case by restricting δ to {−1  1}. These requirements are equivalent
since GT is convex by Lemma 2.

∃p ∈ [0  1]∀δ ∈ [−1  1] : (cid:104)r0 − pδ  r1 + (1 − p)δ(cid:105) ∈ GT−1 .

5.2 More than 2 experts

In the general experts problem we compete with K instead of 2 experts. We now argue that an al-
gorithm guaranteeing Rk
cT ln K w.r.t. each expert k can be obtained. The intuitive approach 
combining the K experts in a balanced binary tree of two-expert predictors  does not achieve this

goal: each internal node contributes the optimal symmetric regret of(cid:112)T /(2π). This accumulates

T ≤ √

√

to Rk

T ≤ ln K

cT   where the log sits outside the square root.

Counter-intuitively  the maximally unbalanced binary tree does result in a
ln K factor when the
internal nodes are properly skewed. At each level we combine K experts one-vs-all  permitting large
regret w.r.t. the ﬁrst expert but tiny regret w.r.t. the recursive combination of the remaining K − 1
experts. The argument can be found in Appendix A.1. The same argument shows that  for any prior
q on k = 1  2  . . .  combining the expert with the smallest prior with the recursive combination of

the rest guarantees regret(cid:112)−cT ln q(k) w.r.t. each expert k.

√

6 Conclusion

T(cid:104)(cid:112)− ln(q) (cid:112)− ln(1 − q)(cid:105) trade-off is achievable for any prior probability q ∈ [0  1]  but that it

We studied asymmetric regret guarantees for the fundamental online learning setting of the absolute
loss game. We obtained exactly the achievable skewed regret guarantees  and the corresponding
√
optimal algorithm. We then studied the proﬁle in the limit of large T . We conclude that the expected
is not tight. We then showed how our results transfer from absolute loss to general linear losses  and
to more than two experts.
Major next steps are to determine the optimal trade-offs for K > 2 experts  to replace our traditional
√
D∞ [18]  ∆T [19]
√

√
[17] 
T ≤ ρk
etc. and to ﬁnd the Pareto frontier for horizon-free strategies maintaining Rk

[16] (cid:112)Varmax

T budget by modern variants

(cid:113) Lk

T at any T .

T [15] 
Lk

T (T−Lk
T )

T

(cid:113)

T

Acknowledgements

This work beneﬁted substantially from discussions with Peter Gr¨unwald.

8

References
[1] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences  55:119–139  1997.
[2] Marcus Hutter and Jan Poland. Adaptive online prediction by following the perturbed leader.

Journal of Machine Learning Research  6:639–660  2005.

[3] Jacob Abernethy  Manfred K. Warmuth  and Joel Yellin. When random play is optimal against
an adversary. In Rocco A. Servedio and Tong Zhang  editors  COLT  pages 437–446. Omni-
press  2008.

[4] Wouter M. Koolen. Combining Strategies Efﬁciently: High-quality Decisions from Conﬂicting
Advice. PhD thesis  Institute of Logic  Language and Computation (ILLC)  University of
Amsterdam  January 2011.

[5] Nicol`o Cesa-Bianchi and Ohad Shamir. Efﬁcient online learning via randomized rounding.
In J. Shawe-Taylor  R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger  editors 
Advances in Neural Information Processing Systems 24  pages 343–351  2011.

[6] Nicol`o Cesa-Bianchi  Yoav Freund  David Haussler  David P. Helmbold  Robert E. Schapire 
and Manfred K. Warmuth. How to use expert advice. Journal of the ACM  44(3):427–485 
1997.

[7] Jacob Abernethy  John Langford  and Manfred K Warmuth. Continuous experts and the Bin-

ning algorithm. In Learning Theory  pages 544–558. Springer  2006.

[8] Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries. In
J. Lafferty  C. K. I. Williams  J. Shawe-Taylor  R.S. Zemel  and A. Culotta  editors  Advances
in Neural Information Processing Systems 23  pages 1–9  2010.

[9] Sasha Rakhlin  Ohad Shamir  and Karthik Sridharan. Relax and randomize : From value to
In P. Bartlett  F.C.N. Pereira  C.J.C. Burges  L. Bottou  and K.Q. Weinberger 

algorithms.
editors  Advances in Neural Information Processing Systems 25  pages 2150–2158  2012.

[10] Eyal Even-Dar  Michael Kearns  Yishay Mansour  and Jennifer Wortman. Regret to the best

vs. regret to the average. Machine Learning  72(1-2):21–37  2008.

[11] Michael Kapralov and Rina Panigrahy. Prediction strategies without loss. In J. Shawe-Taylor 
R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger  editors  Advances in Neural
Information Processing Systems 24  pages 828–836  2011.

[12] Kamalika Chaudhuri  Yoav Freund  and Daniel Hsu. A parameter-free hedging algorithm. In
Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in
Neural Information Processing Systems 22  pages 297–305  2009.

[13] Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts.

In Peter Gr¨unwald and Peter Spirtes  editors  UAI  pages 117–125. AUAI Press  2010.

[14] Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction  Learning  and Games. Cambridge Uni-

versity Press  2006.

[15] Peter Auer  Nicol`o Cesa-Bianchi  and Claudio Gentile. Adaptive and self-conﬁdent on-line

learning algorithms. Journal of Computer and System Sciences  64(1):48–75  2002.

[16] Nicol`o Cesa-Bianchi  Yishay Mansour  and Gilles Stoltz. Improved second-order bounds for

prediction with expert advice. Machine Learning  66(2-3):321–352  2007.

[17] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by varia-

tion in costs. Machine learning  80(2-3):165–188  2010.

[18] Chao-Kai Chiang  Tianbao Yangand Chia-Jung Leeand Mehrdad Mahdaviand Chi-Jen Lu-
and Rong Jin  and Shenghuo Zhu. Online optimization with gradual variations. In Proceedings
of the 25th Annual Conference on Learning Theory  number 23 in JMLR W&CP  pages 6.1 –
6.20  June 2012.

[19] Steven de Rooij  Tim van Erven  Peter D. Gr¨unwald  and Wouter M. Koolen. Follow the leader

if you can  Hedge if you must. ArXiv  1301.0534  January 2013.

9

,Wouter Koolen
Oluwasanmi Koyejo
Rajiv Khanna
Joydeep Ghosh
Russell Poldrack
Zhaobin Kuang
Sinong Geng
David Page
Jianlong Chang
xinbang zhang
Yiwen Guo
GAOFENG MENG
SHIMING XIANG
Chunhong Pan