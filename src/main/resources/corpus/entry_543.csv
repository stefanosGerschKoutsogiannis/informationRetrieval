2014,Analysis of Brain States from Multi-Region LFP Time-Series,The local field potential (LFP) is a source of information about the broad patterns of brain activity  and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a ``brain state '' relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states  based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions  and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs  with state- and region-dependent mixture weights  and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets  using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.,Analysis of Brain States

from Multi-Region LFP Time-Series

Kafui Dzirasa 2

and Lawrence Carin 1

1 Department of Electrical and Computer Engineering
2 Department of Psychiatry and Behavioral Sciences

Kyle Ulrich 1  David E. Carlson 1  Wenzhao Lian 1  Jana Schaich Borg 2 

{kyle.ulrich  david.carlson  wenzhao.lian  jana.borg 

Duke University  Durham  NC 27708

kafui.dzirasa  lcarin}@duke.edu

Abstract

The local ﬁeld potential (LFP) is a source of information about the broad patterns
of brain activity  and the frequencies present in these time-series measurements
are often highly correlated between regions. It is believed that these regions may
jointly constitute a “brain state ” relating to cognition and behavior. An inﬁnite
hidden Markov model (iHMM) is proposed to model the evolution of brain states 
based on electrophysiological LFP data measured at multiple brain regions. A
brain state inﬂuences the spectral content of each region in the measured LFP.
A new state-dependent tensor factorization is employed across brain regions  and
the spectral properties of the LFPs are characterized in terms of Gaussian pro-
cesses (GPs). The LFPs are modeled as a mixture of GPs  with state- and region-
dependent mixture weights  and with the spectral content of the data encoded in
GP spectral mixture covariance kernels. The model is able to estimate the number
of brain states and the number of mixture components in the mixture of GPs. A
new variational Bayesian split-merge algorithm is employed for inference. The
model infers state changes as a function of external covariates in two novel elec-
trophysiological datasets  using LFP data recorded simultaneously from multiple
brain regions in mice; the results are validated and interpreted by subject-matter
experts.

Introduction

1
Neuroscience has made signiﬁcant progress in learning how activity in speciﬁc neurons or brain ar-
eas correlates with behavior. One of the remaining mysteries is how to best represent and understand
the way whole-brain activity relates to cognition: in other words  how to describe brain states [1].
Although different brain regions have different functions  neural activity across brain regions is of-
ten highly correlated. It has been proposed that the speciﬁc way brain regions are correlated at any
given time may represent a “state” designed speciﬁcally to optimize neural computations relevant
to the behavioral context an organism is in [2]. Unfortunately  although there is great interest in the
concept of global brain states  little progress has been made towards developing methods to identify
or characterize them.
The study of arousal is an important area of research relating to brain states. Arousal is a hotly
debated topic that generally refers to the way the brain dynamically responds to varying levels of
stimulation [3]. One continuum of arousal used in the neuroscience literature is sleep (low arousal) to
wakefulness (higher arousal). Another is calm (low arousal) to excited or stressed (high arousal) [4].
A common electrophysiological measurement used to determine arousal levels is local ﬁeld poten-
tials (LFPs)  or low-frequency (< 200 Hz) extracellular neural oscillations that represent coordinated

1

...

s(a)
w−1

z(ar)
w−1

y(ar)
w−1

γ

R

y1

y2

F⇐⇒

Classify yw

REM

SWS

WK

s(a)
w

z(ar)
w

y(ar)
w

k(cid:96)(τ )

R

s(a)
w+1

z(ar)
w+1

y(ar)
w+1

...

R

A

θ(cid:96)

L

Figure 1: Left: Graphical representation of our state space model.
We ﬁrst assign a sequence of brain states  {s(a)
1   with Markovian
dynamics to animal a. Given state s(a)
w   each region is assigned to
a cluster  z(ar)
is generated
from a Gaussian process with covariance function k(τ ; θ(cid:96)  γ). Top:
Example of two windows of an LFP time-series; we wish to classify
each window based on spectral content. Spectral densities of known
sleep states (REM  SWS  WK) in the hippocampus are shown.

w = (cid:96) ∈ {1  . . .   L}  and the data y(ar)

w }W

w

neural activity across distributed spatial and temporal scales. LFPs are useful for describing overall
brain states since they reﬂect activity across many neural networks. We examine brain states under
different levels of arousal by recording LFPs simultaneously in multiple regions of the mouse brain 
ﬁrst  as mice pass through different stages of sleep  and second  as mice are moved from a familiar
environment to a novel environment to induce interest and exploration.
In neuroscience  the analysis of electrophysiological time-series data is largely centered around dy-
namic causal modeling (DCM) [5]  where continuous state-space models are formulated based on
differential equations that are speciﬁcally crafted around knowledge of underlying neurobiological
processes. However  DCM is not suitable for exploratory analysis of data  such as inferring un-
known arousal levels  for two reasons: because the differential equations are driven by inputs of
experimental conditions  and the analysis is dependent on a priori hypotheses about which neuronal
populations and interactions are important. This work focuses on methods suitable for exploratory
analysis.
Previously published neuroscience studies distinguished between slow-wave sleep (SWS)  rapid-
eye-movement (REM)  and wake (WK) using proportions of high-frequency (33-55 Hz) gamma
oscillations and lower frequency theta (4-9 Hz) oscillations in a brain area called the Hippocam-
pus [6  7]. As an alternative approach  recent statistical methods for tensor factorization [8] can be
applied to short time Fourier transform (STFT) coefﬁcients by factorizing a 3–way LFP tensor  with
dimensions of brain region  frequency band and time. Distinct sleep states may then be revealed by
clustering the inferred sequence of time-varying score vectors.
Although good ﬁrst steps  the above two methods have several shortcomings: 1) They do not con-
sider the time dependency of brain activity  and therefore cannot capture state-transition properties.
2) They cannot directly work on raw data  but require preprocessing that only considers spectral
content in predeﬁned frequency bins  thus leading to information loss. 3) They do not allow for in-
dividual brain regions to take on their own set of sub-state characteristics within a given global brain
state. 4) Finally  they cannot leverage the shared information of LFP data across multiple animals.
In this paper we overcome the shortcomings of previously published brain-state methods by deﬁning
a sequence of brain states over a sliding window of raw  ﬁltered LFP data  where we impose an
inﬁnite hidden Markov model (iHMM) [9] on these state assignments. Conditioned on this brain
state  each brain region is assigned to a cluster in a mixture model. Each cluster is associated with a
speciﬁc spectral content (or density) pattern  manifested through a spectral mixture kernel [10] of a
Gaussian process. Each window of LFP data is generated as a draw from this mixture of Gaussian
processes. Thus  all animals share an underlying brain state space  of which  all brain regions share
the underlying components of the mixture model.
2 Model
For each animal a ∈ {1  . . . A}  we have time-series of the LFP in R different regions  mea-
∈ RN
sured simultaneously. These time-series are split into sequential  sliding windows  y(ar)
for w ∈ {1  . . .   W}  such that windows are common across regions. These windows are chosen
to be overlapping  thereby sharing data points between consecutive windows; nonoverlapping win-

w

2

0.511.52−2−1012NormalizedPotentialSecondsExampleLFPData4.28.312.516.700.020.040.060.08Frequency HzSpectralDensity1   . . .   s(a)

dows may also be used. Each window is considered as a single observation vector  and we wish to
model the generative process of these observations  {y(ar)
w }.
The proposed model aims to describe the spectral content in each of these LFP signals  as a function
of brain region and time. This is done by ﬁrst assigning a joint “brain state” to each time window 
{s(a)
W }  shared across all brain regions {1  . . .   R}. The brain state is assumed to evolve in
time as a latent Markov process. The LFP data from a particular brain region is assumed drawn from
a mixture of Gaussian processes. The characteristics of each mixture component are shared across
brain states and brain regions  with mixture weights that are dependent on these two entities.
2.1 Brain state assignment
Within the generative process  each animal has a latent brain state for every time window  w. This
brain state is represented through a categorical latent variable s(a)
w   and an inﬁnite hidden Markov
model (iHMM) is placed on the state dynamics [9  12]. This process is formulated as

) 

s(a)
w−1

h

w ∼ Categorical(λ(a)
s(a)

β ∼ GEM(γ0) 
(1)
h ∼ Beta(1  γ0). Here 
where GEM is the stick-breaking process βh = β(cid:48)
{βh}H
h=1 represents global transition probabilities to each state in a potentially inﬁnite state space.
For the stick-breaking process  H → ∞  but in a ﬁnite collection of data only a ﬁnite number of
state transitions will be used and H can be efﬁciently truncated. Since the state space is shared
1 ∼
across animals  we cannot predeﬁne initial state assignments  s(a)
Categorical(ψ(a)) and place a discrete uniform prior on ψ(a) over the truncated state space.

g ∼ DP(α0β) 
(cid:81)h−1
λ(a)
i=1 (1 − β(cid:48)

1 . To remedy this  we allow s(a)

i) with β(cid:48)

g

such that the transition from state g to state h for animal a is λ(a)

Each animal is given a transition matrix Λ(a)  where each row of this matrix is a transition proba-
bility vector λ(a)
gh   each centered
around the global transition vector β. Because each animal’s brain can be structured differently
(e.g.  as an extreme case  consider a central nervous system disorder)  we allow Λ(a) to vary from
animal to animal.
2.2 Assigning brain regions to clusters
For each brain state  mixture weights are drawn to deﬁne the distribution over clusters independently
for each region r  centered around a global mixture η using a hierarchical Dirichlet Process [12]:

h ∼ DP(α1η) 
φ(r)

η ∼ GEM(γ1) 

(2)

where φ(r)
cluster assignment can be written as

h(cid:96) is the probability of assigning region r of a window with brain state h to cluster (cid:96). This

w |s(a)
z(ar)

w ∼ Categorical(φ(r)

s(a)
w

).

(3)

For each cluster (cid:96) there is a set of parameters  θ(cid:96)  describing a Gaussian process (GP)  detailed in
Section 2.3. One could consider the joint probability over cluster assignments for all brain regions
as an extension of a latent nonnegative PARAFAC tensor decomposition [11  13]. We refer to the
Supplemental Material for details. Our clustering model differs from the inﬁnite tensor factorization
(ITF) model of [11] in three signiﬁcant ways: we place Markovian dynamics on state assignments
for each animal  we model separate draws from the prior jointly for each animal  and we share
cluster atoms across all regions through use of an HDP.
2.3
2.3.1 Gaussian processes and the spectral mixture kernel

Inﬁnite mixture of Gaussian processes

w ∈ RN   we wish to model the data in the limit of a continuous-
For a single window of data  y(ar)
time function (allowing N → ∞)  motivating a GP formulation  and we are interested in the spectral
properties of the LFP signal in this window. Previous research has established a link between the
kernel function of a GP and its spectral properties [10]. We write a distribution over the time-series:

y(t) ∼ GP(m(t)  k(t  t(cid:48))) 

(4)

3

where m(t) is known as the mean function  and k(t  t(cid:48)) is the covariance function [14]. This frame-
work provides a ﬂexible  structured method to model time-series data. The structure of observations
in the output space  y  is deﬁned through a careful choice of the covariance function. Since this work
aims to model the spectral content of the LFP signal  we set the mean function to 0  and use a re-
cently proposed spectral mixture (SM) kernel [10]. This kernel is deﬁned through a spectral domain
representation  S(s)  of the stationary kernel  represented by a mixture of Q Gaussian components:

φ(s) =

ωqN (s; µq  νq) 

S(s) =

[φ(s) + φ(−s)] 

1
2

(5)

q=1

(cid:88)Q

(cid:88)Q

where φ(s) is reﬂected about the origin to obtain a valid spectral density  and µq  νq  and ωq respec-
tively deﬁne the mean  variance  and relative weight of the q-th Gaussian component in the spectral
domain. Priors may be placed on these parameters; for example  we use the uninformative priors
µq ∼ Uniform(µmin  µmax)  νq ∼ Uniform(0  νmax) and ωq ∼ Gamma(e0  f0). A bandpass ﬁlter
is applied to the LFP signal from µmin to µmax Hz as a preprocessing step  so this prior knowledge
is justiﬁed. Also  νmax is set to prevent overﬁtting  and e0 and f0 are set to manifest a broad prior.
We assume that only a noisy version of the true function is observed  so the kernel is deﬁned as the
Fourier transform of the spectral density S(s) plus white Gaussian noise:

q=1

k(τ ; θ  γ) = f (τ ; θ) + γ−1δτ  

ωq exp{−2π2τ 2νq} cos(2πτ µq) 

f (τ ; θ) =

(6)
where the set of parameters θ = {ω  µ  ν} and γ deﬁne the covariance kernel  τ = |t− t(cid:48)|  and δτ is
the Kronecker delta function which equals one if τ = 0. We set the prior γ ∼ Gamma(e1  f1) where
the hyperparemeters e1 and f1 are chosen to manifest a broad prior. The formulation of (6) results
in an interpretable kernel in the spectral domain  where the weights ωq correspond to the relative
contribution of each component  the means µq represent spectral peaks  and the variances νq play a
role similar to an inverse length-scale.
Through a realization of this Gaussian process  an analytical representation is obtained for the
marginal likelihood of the observed data y given the parameters {θ  γ}  and the observation loca-
tions t  p(y|θ  γ  t). The optimal set of kernel parameters {θ  γ} can then be chosen as the set that
maximizes the marginal likelihood. Further discussions on the inference for the Gaussian process
parameters is presented in Section 3.
2.3.2 Generating observed data
To combine the clustering model with our SM kernel  each cluster (cid:96) is associated with a distinct
∈ RN has
set of kernel parameters θ(cid:96). To generate the observations {y(ar)
observation times t = {t1  . . .   tN} such that |ti − tj| = |i − j|τ for all i and j  we consider a draw
from the multivariate normal distribution:

w }  where each y(ar)

w

w ∼ N (0  Σz(ar)
y(ar)

w

) 

(Σ(cid:96))ij = k(|ti − tj|; θ(cid:96)  γ) 

(7)

w

w

where each observation is generated from the cluster indicated by z(ar)
(described in Section 2.2) 
and each cluster is represented uniquely by a covariance matrix  Σ(cid:96)  whose elements are deﬁned
through the covariance kernel k(τ ; θ(cid:96)  γ). Therefore  the parameters θz(ar)
describe the auto-
correlation content associated with each y(ar)
w .
We address two concerns with this formulation. First  this observation model ignores complex
cross-covariance functions between regions. Although LFP measurements exhibit coherence pat-
terns across regions  the generative model in (7) only weakly couples the spectral densities of each
region through the brain state. In principle  the generative model could be extended to incorporate
this coherence information. Second  (7) does not model the time-series itself as a stochastic process 
but rather the preprocessed  ‘independent’ observation vectors. This shortcoming is not ideal  but
the windowing process allows for efﬁcient computation via the mixture of Gaussian processes.
3
In the following  latent model variables are represented by Ω = {Z  S  Φ  η  Λ  β  Ψ}  the kernel
parameters to be optimized are Θ = {{θ(cid:96)}L
1   γ}  and H and L are upper limit truncations on the
number of brain states and clusters  respectively. As described throughout this section  the proposed
algorithm adaptively adjusts the truncation levels on the number of brain states  H  and clusters  L 

Inference

4

through a series of split-merge moves. The joint probability of the proposed model is

p(Y  Ω  Θ) = p(Y |Z  Θ)p(Z  S|Φ  Λ  Ψ)p(Φ|η)p(η)p(Λ|β)p(β)p(Ψ)p(Θ)

w   Θ)p(z(ar)

w |s(a)
(cid:89)W

a r w

p(s(a)

a

w |z(ar)
p(y(ar)
1 |ψ(a))p(ψ(a))
(cid:89)Q

w=2

(cid:105)(cid:104)
w   Φ)
w |s(a)

p(η|γ1)

p(φ(r)

(cid:89)
(cid:21)(cid:104)

r h

p(β|γ0)

(cid:21)

(cid:105)
(cid:105)
h |η  α1)
(cid:89)
g |β  α0)

p(λ(a)

a g

p(s(a)

w−1  Λ(a))

=

(cid:104)(cid:89)
(cid:20)(cid:89)
(cid:20)

(cid:89)
(cid:89)
(cid:89)

a

a

a r w

Cat(z(ar)
w }W

w=1) 

q({s(a)

δψ(a)∗ (ψ(a)) 

q(S) =

q(Ψ) =

p(γ|e1  f1)

p(ωq|e0  f0)p(µq|µmin  µmax)p(νq|νmax)

.

q=1

(8)

A variational inference scheme is developed to update Ω and Θ.
3.1 Variational inference
that
With variational
is similar
This variational
posterior is assumed to have a factorization into simpler distributions  where q(Ω  Θ) =
q(Z)q(S)q(Φ)q(η)q(Λ)q(β)q(Ψ)q(Θ)  with further factorization
Dir(φ(r)

to the true posterior distribution  q(Ω  Θ) ≈ p(Ω  Θ|Y ).

inference  an approximate variational posterior distribution is sought

q(η) = δη∗ (η) 

q(Z) =

q(Φ) =

w ; ζ(ar)

w ) 

h ; ν(r)
h ) 

q(Λ) =

q(Θ) =

Dir(λ(a)

g ; κ(a)

g ) 

δΘ∗

j

(Θj) 

j

q(β) = δβ∗ (β) 

(9)

w }W

j}  respectively.

w = (cid:96)) = 0 for (cid:96) > L and q(s(a)

where only necessary sufﬁcient statistics of the latent factors q({s(a)
w=1) are required  and the
approximate posteriors of η  β  {ψ(a)} and {Θj} are represented by point estimates at η∗  β∗ 
{ψ(a)∗} and {Θ∗
The degenerate distributions δη∗ (η) and δβ∗ (β) are described in previous work on variational in-
ference for HDPs [15  16]. The idea is that the point estimates of the stick-breaking processes
simplify the derivation of the variational posterior  and the authors of [16] show that obtaining a
full posterior distribution on the stick-breaking weights has little impact on model ﬁtting since the
variational lower bound is not heavily inﬂuenced by the terms dependent on η and β. Further-
more  the Dirichlet process is truncated for both the number of states and the number of clusters
such that q(z(ar)
w = h) = 0 for h > H. This truncation method
(see [17] for details) is notably different than other common truncation methods of the DP (e.g.  [18]
and [19])  and is primarily important for facilitating the split-merge inference techniques described
in Section 3.2.
In mean-ﬁeld variational inference  the variational distribution q(Ω  Θ) is chosen such that the
Kullback-Leibler divergence of p(Ω  Θ|Y ) from q(Ω  Θ)  DKL(q(Ω  Θ)||p(Ω  Θ|Y ))  is mini-
mized. This is equivalent to maximizing the evidence lower bound (also known as the variational
free energy in the DCM literature)  L(q) = Eq[log p(Y   Ω  Θ)] − Eq[log q(Ω  Θ)]  where both
expectations are taken with respect to the variational distribution. The resulting lower bound is
L(q) = E[ln p(Y |Z  Θ)] + E[ln p(Z  S|Φ  Λ  Ψ)] + E[ln p(Φ|η)] + E[ln p(η)] + E[ln p(Λ|β)]
(10)
where all expectations are with respect to the variational distribution  the hyperparameters are ex-
cluded for notational simplicity  and we deﬁne H[q(·)] as the sum over the entropies of the individual
factors of q(·). Due to the degenerate approximations for q(η)  q(β)  q(Ψ) and q(Θ)  these full
posterior distributions are not obtained  and  therefore  the terms H[q(η)]  H[q(β)]  H[q(Ψ)] and
H[q(Θ)] are set to zero in the lower bound.
The updates for ζ(ar)
are standard. Variational inference for the HDP-HMM is detailed
in other work (e.g.  see [20  21]); using these methods  updates for κ(a)
g   ψ(a) and the necessary
expected sufﬁcient statistics of the factors of q(S) are realized. Finally  updates for β∗  η∗ and
{Θj} are non-conjugate  so a gradient-ascent method is performed to optimize these values. We
use a simple resilient back-propagation (Rprop)  though most line-search methods should sufﬁce.
Details on all updates and taking the gradient of L(q) with respect to β  η and {Θj} are found in
the Supplemental Material.

+ E[ln p(β)] + E[ln p(Ψ)] + E[ln p(Θ)] + H[q(Z)] + H[q(S)] + H[q(Φ)] + H[q(Λ)] 

and ν(r)
h

w

(cid:89)
(cid:89)
(cid:89)

h r

g a

5

(cid:96)

(cid:96)

h(cid:96)(cid:48)(cid:48)  η∗

(cid:96) = η∗

(cid:96)(cid:48) + η∗

(cid:96)

gh(cid:48)(cid:48)  β∗

(cid:96)(cid:48)(cid:48)  and θnew

h = β∗

h(cid:48) + β∗

gh = κ(a)

gh(cid:48) + κ(a)

wh(cid:48) + ρ(a)

wh(cid:48)(cid:48)  κ(a)

h(cid:96) = ν(r)

h(cid:96)(cid:48) + ν(r)

h = v(a)

h(cid:48) + v(a)

w(cid:96)(cid:48) + ζ (ar)

w(cid:96)(cid:48)(cid:48)   ν(r)

h(cid:48)(cid:48)  and v(a)

3.2 Split-merge moves
During inference  a series of split and merge operations are used to help the algorithm jump out of
local optima [22]. This work takes the viewpoint that two clusters (or states) should merge only if
the variational lower bound increases  and  when a split is proposed for a cluster (or state)  it should
always be accepted  whether or not the split increases the variational lower bound. If the split is
not appropriate  a future merge step is expected to undo this operation. In this way  the opportunity
is provided for cluster and state assignments to jump out of local optima  allowing the inference
algorithm to readjust assignments as desired.
Merge states: To merge states h(cid:48) and h(cid:48)(cid:48) into a new state h  new parameters are initialized as:
ρ(a)
wh = ρ(a)
h(cid:48)(cid:48)   such
that the model now has a truncation at H new = H − 1 states. In order to account for problems
with merging two states in an HMM  a single restricted iteration is allowed  where only the state-
dependent variational parameters in Ωnew are updated  producing a new distribution q(Ωnew). The
merge is accepted (i.e.  Ω = Ωnew) if L(q(Ωnew)) > L(q(Ω)). Since these computations are not
excessive  all possible state merges are computed and a small number of merges are accepted per
iteration.
Merge clusters: To merge clusters (cid:96)(cid:48) and (cid:96)(cid:48)(cid:48) into a new cluster (cid:96)  new parameters are initialized as:
= θ∗  such that there is a
ζ (ar)
w(cid:96) = ζ (ar)
truncation at Lnew = L − 1 clusters. We set θ∗ = θ(cid:96)(cid:48) for simplicity  and allow a restricted iteration
. The merge is accepted (i.e.  Ω = Ωnew and Θ = Θnew) if the lower
of updates to Ωnew and θnew
bound is improved  L(q(Ωnew  Θnew)) > L(q(Ω  Θ)). Since the restricted iteration for θnew
is
expensive  only a few cluster merges may be proposed at a time. Therefore  merges are proposed
for clusters with the smallest earth mover’s distance [23] between their spectral densities.
Split step: When splitting states and clusters  the opposite process to the initialization of the merging
procedures described above is performed. For clusters  data points within a cluster (cid:96) are randomly
chosen to stay in cluster (cid:96) or split to a new cluster (cid:96)(cid:48). For splitting state h  the cluster assignment
vector φ(r)
h is replicated and windows within state h are randomly chosen to stay in state h or split
to a new cluster h(cid:48). Regardless of how this effects the lower bound  a split step is always accepted.
For implementation details  we allow the model to accept 3 state merges every third iteration  pro-
pose 5 cluster merges every third iteration  and split one state and one cluster every third iteration.
Therefore  every iteration may affect the truncation level of either the number of states or clusters.
A ‘burn-in’ period is allowed before starting the proposing of splits/merges  and a ‘burn-out’ period
is employed in which split proposals cease. In this way  the algorithm has guarantees of improving
the lower bound only during iterations when a split is not proposed  and convergence tests are only
considered during the burn-out period.
4 Datasets
Three datasets are considered in this work  as follows:
Toy data: Data is generated for a single animal according to the proposed model in Section 2. The
purpose of this dataset is to ensure the inference scheme can recover known ground truth  since
ground truth information is not known for the real datasets. We set L = 5 and H = 3. For each
cluster  a spectral density was generated with Q = 4  µq ∼ Unif(4  50)  νq ∼ Unif(1  50) and
ω ∼ Dir(1  . . .   1). The cluster usage probability vector was drawn φ(r)
10 ). State
transition probabilities were drawn according to λgh ∼ Unif(0  1) + 10δ(g=h). States were assigned
to W = 1000 windows according to an HMM with transition matrix Λ  and cluster assignments
were drawn conditioned on this state. Data with N = 200 was drawn for each window.
Sleep data: Twelve hours of LFP data from sixteen different brain regions were recorded from three
mice naturally transitioning through different levels of sleep arousal. Due to the high number of
brain regions  we present only three hours of sleep data from a single mouse for simplicity. The
multi-animal analysis is reserved for the novel environment dataset.
Novel environment data: Thirty minutes of LFP data from ﬁve brain regions was recorded from
ﬁve mice who were moved from their home cage to a novel environment approximately nine minutes
into the recording. Placing animals into novel environments has been shown to increase arousal  and

h ∼ Dir( 1

10   . . .   1

6

should therefore result in (at least one) network state change [3]. Data acquisition methods for the
latter two datasets are discussed in [24].
5 Results
For all results  we set Q = 10  H = 15  L = 25  stop the ‘burn-in’ period after iteration 6  and start
the subsequent computation period after iteration 25. Hyperparameters were set to γ0 = γ1 = .01 
α0 = α1 = 1  µmin = 0  µmax = 50  νmax = 10  and e0 = f0 = 10−6. In all results  the model
was seen to converge to a local optima after 30 iterations  and each iteration took on the order of 20
seconds using Matlab code on a PC with a 2.30GHz quad-core CPU and 8GB RAM.
Figure 2 shows results on the toy data. The model correctly recovers exactly 3 states and 5 clusters 
and  as seen in the ﬁgure  the state assignments and spectral densities of each cluster component
are recovered almost perfectly. The model was implemented for different values of the noise vari-
ance  γ−1  and  though not shown  in all cases the noise variance was recovered accurately during
inference  implying the spectral mixture kernels are not overﬁtting the noise. In this way  we con-
ﬁrm that the inference scheme recovers a ground truth. For further model veriﬁcation  ten-fold
cross-validation was used to compute predictive probabilities for held-out data (reported in Table 1) 
where we compare to two simpler versions of our model: 1) the HDP-HMM on brain states in (1) is
replaced with an HDP  and 2) a single brain state. For the HDP-HMM  the hold-out data was consid-
ered as ‘missing data’ in the training data and the window index was used to assign time-dependent
probabilities over clusters  whereas in the HDP and Single State models it was simply withheld from
the training data. We see large predictive performance gains when considering multiple brain states 
and even more improvement on average (though modest) when considering an HDP-HMM.

Figure 2: Toy data results. Top row shows the generated toy data. From left to right: the ﬁve spectral functions 
each associated with a component in the mixture model; the probability of each of these ﬁve components
occurring for all ﬁve regions in each brain state; the generated brain state assignments from a 3-state HMM
along with the generated cluster assignments for the ﬁve simulated regions. The bottom row shows the results
of our model. On the left  a comparison of the recovered state vs. the true state for all time; on the right  an
alignment of the ﬁve recovered kernels to the spectral density ground truth.

e
r
o
c

p
p
i
H
D

c
A
N

C
F
O

A
T
V

e
r
o
c

p
p
i
H
D

c
A
N

C
F
O

A
T
V

e
r
o
c

p
p
i
H
D

c
A
N

C
F
O

A
T
V

Figure 3: Sleep data results. Top: A comparison of brain state assignments from our method to two other
methods. Bottom Left: Spectral density of the 7 inferred clusters. Middle Left: Cluster assignments over
time for 16 different brain regions  sorted by similarity. Middle Right: Given brain states 1  2 and 3  we show
cluster assignment probabilities for 4 different brain regions: the hippocampus (D Hipp)  nucleus accumbens
core (NAc core)  orbitofrontal cortex (OFC) and ventral tegmental area (VTA) from left to right  respectively.
Right: State assignments of our method and the tensor method conditioned on the method of [6].

7

01020304050−7−6−5−4−3−2Frequency HzLogSpectralDensity12300.20.40.60.81BrainState (5ChannelsperState)ClusterUsageforeachBrainStateClust 1Clust 2Clust 3Clust 4Clust 5MinutesSimulatedBrainActivityoverTime51015 ’Region 1’ ’Region 2’ ’Region 3’ ’Region 4’ ’Region 5’ Clust 1Clust 2Clust 3Clust 4Clust 5MinutesStateAssignmentComparison51015True StateInferred StateState 1State 2State 302550−8−6−4−2Freq HzLogSpectralDensity02550Freq Hz02550Freq Hz02550Freq Hz02550Freq HzMinutesBrainActivityoverTime51015202530354045505560Dzirasa et al.Tensor MethodOur MethodState 1State 2State 3State 4State 5State 6State 7State 8246810121400.050.10.150.2Frequency HzSpectralDensityMinutesBrainActivityoverTime51015202530DLSDMSFrAM1M_OFC_CxOFCBasal_AmyD_HippL_HbNAc_CoreNAc_ShellMD_ThalPrL_CxSubNigraV1VTAClust 1Clust 2Clust 3Clust 4Clust 5Clust 6Clust 7132BrainState(Showing4/16Regions)ClusterUsagegivenState/RegionClust 1Clust 2Clust 3Clust 4Clust 5Clust 6Clust 7TensorOur00.20.40.60.81StateUsageGiven(Dzirasaetal.)State 1State 2State 3State 4State 5State 6State 7State 8Figure 4: Novel environment data results. Left: The log spectral density of the 6 inferred clusters. Middle:
State assignments for all 9 animals over a 30 minute period. There are 7 inferred states  and each state has a
distribution over clusters for each region  as seen on the right.

HDP-HMM

Dataset
−1.686 (±0.053) −1.688 (±0.053) −1.718 (±0.054)
Toy (×105)
Sleep (×106) −1.677 (±0.030) −1.682 (±0.020) −1.874 (±0.019)
Novel (×105) −5.932 (±0.040) −5.973 (±0.034) −6.962 (±0.063)

HDP

Single State

Table 1: Average held-out log predictive probability for different priors on brain states: HDP-HMM  HDP 
and a single state. The data consists of W time-series windows for R regions of A animals; at random  10% of
these time-series windows were held-out  and the predictive distribution was used to determine their likelihood.

The sleep and novel environment results are presented in Figures 3 and 4  respectively. With the
sleep dataset  our results are compared with the two methods discussed in the Introduction: that
of [6  7]  and the tensor method of [8]. We refer to the Supplemental Material for exact speciﬁcations
of the tensor method.
For each of these datasets  we infer the intended arousal states.
In the novel environment data 
we observe broad arousal changes at 9–minutes for all animals  as expected. In the sleep data  we
successfully uncover at least as many states as the simple approach of [6  7]  to include SWS  REM
and WK states. Thus far neuroscientists have focused primarily on 2 stages of sleep (NREM and
REM)  but as many as 5 have been discussed (4 different stages of NREM sleep  and 1 stage of
REM). Different stages of sleep affect memory and behavior in different ways (e.g.  see [25])  as
does the number of times animals transition between these states [26]. Our results suggest that there
may be even more levels of sleep that should be considered (e.g.  transition states and sub states).
This is very interesting and important for neuroscientists to know  because it is possible that each
of our newly observed states could affect memory and behavior in different ways. There is no other
published method that has provided evidence of these other states.
In addition to brain states  we infer spectral information for each brain region through cluster assign-
ments. Though not the primary focus of this work  it is interesting that groups of brain regions tend
to share similar attributes. In Figure 3  we have sorted brain regions into groups based on cluster
assignment similarity  essentially recovering a ‘network’ of the brain. This underscores the power of
the proposed method: not only do we develop unsupervised methods to classify whole-brain activity
into states  we infer the cross-region/animal relationships within these states.
6 Conclusion
The contributions of this paper are three-fold. First  we design an extension of the inﬁnite tensor
mixture model  incorporating time dependency. Second  we develop variational inference for the
proposed generative model  including an efﬁcient inference scheme using split-merge moves for two
general models: the ITM and iHMM. To the authors’ knowledge  neither of these inference schemes
have been developed previously. Finally  with respect to neuroscience application  we model brain
states given multi-channel LFP data in a principled manner  showing signiﬁcant advantages over
other potential approaches to modeling brain states. Using the proposed framework  we discover
distinct brain states directly from the raw  ﬁltered data  deﬁned by their spectral content and network
properties  and we can infer relationships between and share statistical strength across data from
multiple animals.
Acknowledgments
The research reported here was funded in part by ARO  DARPA  DOE  NGA and ONR.

8

246810121400.050.10.150.20.250.30.35Frequency HzSpectralDensityMinutesBrainActivityoverTime510152025Animal 1Animal 2Animal 3Animal 4Animal 5Animal 6Animal 7Animal 8Animal 9State 1State 2State 3State 4State 5State 6State 7123456700.20.40.60.81BrainState (5ChannelsperState)ClusterUsageforeachBrainStateClust 1Clust 2Clust 3Clust 4Clust 5Clust 6References
[1] C D Gilbert and M Sigman  “Brain States: Top-down Inﬂuences in Sensory Processing. ” Neuron  vol.

54  no. 5  pp. 677–96  June 2007.

[2] A Kohn  A Zandvakili  and M A Smith  “Correlations and Brain States: from Electrophysiology to

Functional Imaging. ” Curr. Opin. Neurobiol.  vol. 19  no. 4  Aug. 2009.

[3] D Pfaff  A Ribeiro  J Matthews  and L Kow  “Concepts and Mechanisms of Generalized Central Nervous

System Arousal ” ANYAS  Jan. 2008.

[4] P J Lang and M M Bradley  “Emotion and the Motivational Brain ” Biol. Psychol.  vol. 84  no. 3  pp.

437–50  July 2010.

[5] K J Friston  L Harrison  and W Penny  “Dynamic Causal Modelling ” NeuroImage  vol. 19  no. 4  pp.

1273–1302  2003.

[6] K Dzirasa  S Ribeiro  R Costa  L M Santos  S C Lin  A Grosmark  T D Sotnikova  R R Gainetdinov  M G
Caron  and M A L Nicolelis  “Dopaminergic Control of Sleep–Wake States ” J. Neurosci.  vol. 26  no.
41  pp. 10577–10589  2006.

[7] D Gervasoni  S C Lin  S Ribeiro  E S Soares  J Pantoja  and M A L Nicolelis  “Global Forebrain Dynamics
Predict Rat Behavioral States and their Transitions ” J. Neurosci.  vol. 24  no. 49  pp. 11137–11147  2004.
[8] P Rai  Y Wang  S Guo  G Chen  D Dunson  and L Carin  “Scalable Bayesian Low-Rank Decomposition

of Incomplete Multiway Tensors ” ICML  2014.

[9] M J Beal  Z Ghahramani  and C E Rasmussen  “The Inﬁnite Hidden Markov Model ” NIPS  2002.
[10] A G Wilson and R P Adams  “Gaussian Process Kernels for Pattern Discovery and Extrapolation ” ICML 

2013.

[11] J Murray and D B Dunson  “Bayesian learning of joint distributions of objects ” AISTATS  2013.
[12] Y W Teh  M I Jordan  M J Beal  and D M Blei  “Sharing Clusters Among Related Groups : Hierarchical

Dirichlet Processes ” NIPS  2005.

[13] R A Harshman  “Foundations of the Parafac Procedure ” Work. Pap. Phonetics  1970.
[14] C E Rasmussen and C K I Williams  Gaussian Processes for Machine Learning  vol. 14  Apr. 2006.
[15] M Bryant and E B Sudderth  “Truly nonparametric online variational inference for hierarchical Dirichlet

processes ” NIPS  pp. 1–9  2012.

[16] P Liang  S Petrov  M I Jordan  and D Klein  “The Inﬁnite PCFG using Hierarchical Dirichlet Processes ”

Conf. Empir. Methods Nat. Lang. Process. Comput. Nat. Lang. Learn.  pp. 688–697  2007.

[17] Y W Teh  K Kurihara  and M Welling  “Collapsed Variational Inference for HDP ” NIPS  2007.
[18] D M Blei and M I Jordan  “Variational Inference for Dirichlet Process Mixtures ” Bayesian Anal  2004.
[19] K Kurihara  M Welling  and N Vlassis  “Accelerated Variational Dirichlet Process Mixtures ” NIPS 

2007.

[20] M J Beal  “Variational Algorithms for Approximate Bayesian Inference ” Diss. Univ. London  2003.
[21] J Paisley and L Carin  “Hidden Markov Models With Stick-Breaking Priors ” IEEE Trans. Signal Pro-

cess.  vol. 57  no. 10  pp. 3905–3917  2009.

[22] S Jain and R M Neal  “Splitting and merging components of a nonconjugate Dirichlet process mixture

model ” Bayesian Anal  Sept. 2007.

[23] Y Rubner  C Tomasi  and L J Guibas  “The Earth Movers Distance as a Metric for Image Retrieval ” Int.

J. Comput. Vis.  vol. 40  no. 2  pp. 99–121  2000.

[24] K Dzirasa  R Fuentes  S Kumar  J M Potes  and M A L Nicolelis  “Chronic in Vivo Multi-Circuit Neuro-

physiological Recordings in Mice ” J. Neurosci. Methods  vol. 195  no. 1  pp. 36–46  Jan. 2011.

[25] M A Tucker  Y Hirota  E J Wamsley  H Lau  A Chaklader  and W Fishbein  “A Daytime Nap Containing
Solely Non-REM Sleep Enhances Declarative but not Procedural Memory. ” Neurobiol. Learn. Mem. 
vol. 86  no. 2  pp. 241–7  Sept. 2006.

[26] A Rolls  D Colas  A Adamantidis  M Carter  T Lanre-Amos  H C Heller  and L de Lecea  “Optogenetic
Disruption of Sleep Continuity Impairs Memory Consolidation ” PNAS  vol. 108  no. 32  pp. 13305–10 
Aug. 2011.

9

,Beomjoon Kim
Amir-massoud Farahmand
Joelle Pineau
Doina Precup
Kyle Ulrich
David Carlson
Wenzhao Lian
Jana Borg
Kafui Dzirasa
Lawrence Carin