2012,Meta-Gaussian Information Bottleneck,We present a reformulation of the information bottleneck (IB) problem in terms of copula  using the equivalence between mutual information and negative copula  entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities  also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers.,Meta-Gaussian Information Bottleneck

M´elanie Rey

Department of Mathematics and Computer Science

University of Basel

melanie.rey@unibas.ch

Volker Roth

Department of Mathematics and Computer Science

University of Basel

volker.roth@unibas.ch

Abstract

We present a reformulation of the information bottleneck (IB) problem in terms
of copula  using the equivalence between mutual information and negative cop-
ula entropy. Focusing on the Gaussian copula we extend the analytical IB solu-
tion available for the multivariate Gaussian case to distributions with a Gaussian
dependence structure but arbitrary marginal densities  also called meta-Gaussian
distributions. This opens new possibles applications of IB to continuous data and
provides a solution more robust to outliers.

1

Introduction

The information bottleneck method (IB) [1] considers the concept of relevant information in the
data compression problem  and takes a new perspective to signal compression which was classically
treated using rate distortion theory. The IB method formalizes the idea of relevance  or meaning-
ful information  by introducing a relevance variable Y . The problem is then to obtain an optimal
compression T of the data X which preserves a maximum of information about Y . Although the
IB method beautifully formalizes the compression problem under relevance constraints  the prac-
tical solution of this problem remains difﬁcult  particularly in high dimensions  since the mutual
informations I(X; T )  I(Y ; T ) must be estimated. The IB optimization problem has no available
analytical solution in the general case. It can be solved iteratively using the generalized Blahut-
Arimoto algorithm which  however  requires us to estimate the joint distribution of the potentially
high-dimensional variables X and Y . A formal analysis of the difﬁculties of this estimation problem
was conducted in [2]. In the continuous case  estimation of multivariate densities becomes arduous
and can be a major impediment to the practical application of IB. A notable exception is the case of
joint Gaussian (X  Y ) for which an analytical solution for the optimal representation T exists [3].
The optimal T is jointly Gaussian with (X  Y ) [4] and takes the form of a noisy linear projection
to eigenvectors of the normalised conditional covariance matrix. The existence of an analytical so-
lution opens new application possibilities and IB becomes practically feasible in higher dimensions
[5]. Finding closed form solutions for other continuous distribution families remains an open chal-
lenge. The practical usefulness of the Gaussian IB (GIB)  on the other hand  suffers from its missing
ﬂexibility and the statistical problem of ﬁnding a robust estimate of the joint covariance matrix of
(X  Y ) in high-dimensional spaces.
Compression and relevance in IB are deﬁned in terms of mutual information (MI) of two random
vectors V and W   which is deﬁned as the reduction in the entropy of V by the conditional entropy
of V given W . MI bears an interesting relationship to copulas: mutual information equals negative
copula entropy [6]. This relation between two seemingly unrelated concepts might appear surpris-

1

ing  but it directly follows from the deﬁnition of a copula as the object that captures the “pure”
dependency structure of random variables [7]: a multivariate distribution consists of univariate ran-
dom variables related to each other by a dependence mechanism  and copulas provide a framework
to separate the dependence structure from the marginal distributions. In this work we reformulate
the IB problem for the continuous variables in terms of copulas and enlighten that IB is completely
independent of the marginal distributions of X  Y . The IB problem in the continuous case is in fact
to ﬁnd the optimal copula (or dependence structure) of T and X  knowing the copula of X and the
relevance variable Y . We focus on the case of Gaussian copula and on the consequences of the
IB reformulation for the Gaussian IB. We show that the analytical solution available for GIB can
naturally be extended to multivariate distributions with Gaussian copula and arbitrary marginal den-
sities  also called meta-Gaussian densities. Moreover  we show that the GIB solution depends only a
correlation matrix  and not on the variance. This allows us to use robust rank correlation estimators
instead of unstable covariance estimators  and gives a robust version of GIB.

2

Information Bottleneck and Gaussian IB

2.1 General Information Bottleneck.
Consider two random variables X and Y with values in the measurable spaces X and Y. Their
joint distribution pXY (x  y) will also be denoted p(x  y) for simplicity. We construct a compressed
representation T of X that is most informative about Y by solving the following variational problem:

L | L ≡ I(X; T ) − βI(T ; Y ) 

min
p(t|x)

(1)

where the Lagrange parameter β > 0 determines the trade-off between compression of X and
preservation of information about Y . Since the compressed representation is conditionally indepen-
dent of Y given X as illustrated in Figure 1  to fully characterize T we only need to specify its joint
distribution with X  i.e. p(x  t). No analytical solution is available for the general problem deﬁned
by (1) and this joint distribution must be calculated with an iterative procedure. In the case of dis-
crete variables X and Y   p(x  t) is obtained iteratively by self-consistent determination of p(t|x) 
p(t) and p(y|t) in the generalized Blahut-Arimoto algorithm. The resulting discrete T then deﬁnes
(soft) clusters of X. In the case of continuous X and Y   the same set of self-consistent equations
for p(t|x)  p(t) and p(y|t) are obtained. These equations also translate into two coupled eigenvector
problems for ∂ log p(x|t)/∂t and ∂ log p(y|t)/∂t  but a direct solution of these problems is very
difﬁcult in practice. However  when X and Y are jointly multivariate Gaussian distributed  this
problem becomes analytically tractable.

Figure 1: Graphical representation of the conditional independence structure of IB.

2.2 Gaussian IB.

Consider two joint Gaussian random vectors (rv) X and Y with zero mean:

(cid:18)

(cid:18) Σx

ΣT
xy
Σxy Σy

(cid:19)(cid:19)

(X  Y ) ∼ N

0p+q  Σ =

 

(2)

where p is the dimension of X  q is the dimension of Y and 0p+q is the zero vector of dimension
p + q. In [4] it is proved that the optimal compression T is also jointly Gaussian with X and Y . This
implies that T can be expressed as a noisy linear transformation of X:

T = AX + ξ 

2

(3)

where ξ ∼ N (0p  Σξ) is independent of X and A ∈ Rp×p. The minimization problem (1) is then
reduced to solving:

(4)
For a given trade-off parameter β  the optimal compression is given by T ∼ N (0p  Σt) with Σt =
AΣxAT + Σξ and the noise variance can be ﬁxed to the identity matrix Σξ = Ip  as shown in [3].
The transformation matrix A is given by:

L|L ≡ I(X; T ) − βI(T ; Y ).

min
A Σξ



A =

(cid:2)0T ; . . . ; 0T(cid:3)
(cid:2)α1vT
1 ; 0T ; . . .   0T(cid:3)
2 ; 0T ; . . . ; 0T(cid:3) βc
(cid:2)α1vT

1 ; α2vT

0 ≤ β ≤ βc
1 ≤ β ≤ βc
βc
2 ≤ β ≤ βc

3

1

2



...

(5)

1   . . .   vT

(cid:113) β(1−λi)−1

p are left eigenvectors of Σx|yΣ−1

where vT
x sorted by their corresponding increasing eigen-
i = (1 − λi)−1  and the αi coefﬁcients are deﬁned
values λ1  . . .   λp. The critical β values are βc
i Σxvi. In the above  0T is a p-dimensional row vector and
by αi =
semicolons separate rows of A. We can see from equation (5) that the optimal projection of X is a
combination of weighted eigenvectors of Σx|yΣ−1
x . The number of selected eigenvectors  and thus
the effective dimension of T   depends on the parameter β.

with ri = vT

λiri

3 Copula and Information Bottleneck

3.1 Copula and Gaussian copula.

A multivariate distribution consists of univariate random variables related to each other by a de-
pendence mechanism. Copulas provide a framework to separate the dependence structure from
the marginal distributions. Formally  a d-dimensional copula is a multivariate distribution function
C : [0  1]d → [0  1] with standard uniform margins. Sklar’s theorem [7] states the relationship be-
tween copulas and multivariate distributions. Any joint distribution function F can be represented
using its marginal univariate distribution functions and a copula:

F (z1  . . .   zd) = C (F1 (z1)   . . .   Fd (zd)) .

(6)
If the margins are continuous  then this copula is unique. Conversely  if C is a copula and F1  . . .   Fd
are univariate distribution functions  then F deﬁned as in (6) is a valid multivariate distribution
function with margins F1  . . .   Fd. Assuming that C has d-th order partial derivatives we can deﬁne
  u1  . . .   ud ∈ [0  1]  The density corre-
the copula density function: c(u1  . . .   ud) = ∂C(u1 ... ud)
sponding to (6) can then be rewritten as a product of the marginal densities and the copula density

function: f (z1  . . .   zd) = c (F1(z1)  . . .   Fd(zd))(cid:81)d

∂u1...∂ud

j=1 fj(zj).

Gaussian copulas constitute an important class of copulas. If F is a Gaussian distribution Nd (µ  Σ)
then the corresponding C fulﬁlling equation (6) is a Gaussian copula. Due to basic invariance
properties (cf. [8])  the copula of Nd (µ  Σ) is the same as the copula of Nd (0  P )  where P is the
correlation matrix corresponding to the covariance matrix Σ. Thus a Gaussian copula is uniquely
determined by a correlation matrix P and we denote a Gaussian copula by CP . Using equation
(6) with CP   we can construct multivariate distributions with arbitrary margins and a Gaussian
dependence structure. These distributions are called meta-Gaussian distributions. Gaussian copulas
conveniently have a copula density function:

cP (u) = |P|− 1

(7)
where Φ−1(u) is a short notation for the univariate Gaussian quantile function applied to each com-
ponent Φ−1(u) = (Φ−1(u1)  . . .   Φ−1(ud)).

2 exp

 

Φ−1(u)T (P −1 − I)Φ−1(u)

(cid:26)

− 1
2

(cid:27)

3.2 Copula formulation of IB.

At the heart of the copula formulation of IB is the following identity: for a continuous random vector
Z = (Z1  . . .   Zd) with density f (z) and copula density cZ(u) the multivariate mutual information

3

(cid:90)

or multi-information is the negative differential entropy of the copula density:

I(Z) ≡ Dkl(f (z) (cid:107) f0(z)) =

cZ(u) log cZ(u)du = −H(cZ) 

(8)

where u = (u1  . . .   ud) ∈ [0  1]d  Dkl denotes the Kullback-Leibler divergence  and f0(z) =
f1(z1)f2(z2) . . . fd(zd). For continuous multivariate X  Y and T   equation (8) implies that:

[0 1]d

I(X; T ) = Dkl(f (x  t) (cid:107) f0(x  t)) − Dkl(f (x)||f0(x)) − Dkl(f (t)||f0(t)) 

= −H(cXT ) + H(cX ) + H(cT ) 
I(Y ; T ) = −H(cY T ) + H(cY ) + H(cT ) 

where cXT is the copula density of the vector (X1  . . .   Xp  T1  . . .   Tp). The above derivation then
leads to the following proposition.
Proposition 3.1. Copula formulation of IB
The Information Bottleneck minimization problem (1) can be reformulated as:

L | L = −H(cXT ) + H(cX ) + H(cT ) − β{−H(cY T ) + H(cY ) + H(cT )}.

(9)

min
cXT

The minimization problem deﬁned in (1) is solved under the assumption that the joint distribution
of (X  Y ) is known  this now translates in the assumption that the copula copula density cXY (and
thus cX) is assumed to be known. The density cT is entirely determined by cXT   and using the
conditional independence structure it is clear that cY T is also determined by cXT when cXY is
known. Since the joint density of (X  Y  T ) decomposes as:

f (x  y  t) = f (t  y|x)f (x) = f (t|x)f (y|x)f (x) 

the corresponding copula density then also decomposes as:

cXY T (ux  uy  ut) = RT|X (ux  ut)RY |X (ux  uy)cX (ux) 

(10)

(11)

(12)

where

RT|X (ux  ut) =

cXT (ux  ut)

cX (ux)

  ux ∈ [0  1]p  uy ∈ [0  1]q  ut ∈ [0  1]p 

as shown in [9]. We can ﬁnally rewrite the copula density of (Y  T ) as:

(cid:90) cXT (ux  ut)cXY (ux  uy)

cX (ux)

(cid:90)

cY T (uy  ut) =

cXY T (ux  uy  ut)dux =

dux.

(13)

The IB optimization problem actually reduces to ﬁnding an optimal copula density cXT . This im-
plies that in order to construct the compression variable T   the only relevant aspect is the copula
dependence structure between X  T and Y .

4 Meta-Gaussian IB

4.1 Meta-Gaussian IB formulation.

The above reformulation of IB is of great practical interest when we focus on the special case of
the Gaussian copula. The only known case for which a simple analytical solution to the IB problem
exists is when (X  Y ) are joint Gaussians. Equation (9) shows that actually an optimal solution
does not depend of the margins but only on the copula density cXY . From this observation the idea
naturally follows that an analytical solution should also exist for any joint distribution of (X  Y )
which has a Gaussian copula  and that regardless of its margins. We show below in proposition 4.1
that this is indeed the case. The notation ˜X and ˜Y is used to represent the normal scores:

˜X = (Φ−1 ◦ FX1(X1)  . . .   Φ−1 ◦ FXp (Xp)).

(14)

Since copulas are invariant to strictly increasing transformations the normal scores have the same
copulas as the original variables X and Y .

4

Proposition 4.1. Optimality of meta-Gaussian IB
Consider rv X  Y with a Gaussian dependence structure and arbitrary margins:

FX Y (x  y) ∼ CP (FX1 (x1)  . . .   FXp (xp)  FY1(y1)  . . .   FYq (yq)) 

(15)
where FXi  FYi are the marginal distributions of X  Y and CP is a Gaussian copula parametrized by
a correlation matrix P . Then the optimum of the minimization problem (1) is obtained for T ∈ T  
where T is the set of all rv T such that (X  Y  T ) has a Gaussian copula and T has Gaussian
margins.

Before proving proposition 4.1 we give a short lemma.
Lemma 4.1. T ∈ T ⇔ ( ˜X  ˜Y   T ) are jointly Gaussian.

Proof.

1. If T ∈ T then (X  Y  T ) has a Gaussian copula which implies that ( ˜X  ˜Y   T ) also
has a Gaussian copula. Since ˜X  ˜Y   T all have normally distributed margins it follows that
( ˜X  ˜Y   T ) has a joint Gaussian distribution.

2. If ( ˜X  ˜Y   T ) are jointly Gaussian then ( ˜X  ˜Y   T ) has a Gaussian copula which implies
that (X  Y  T ) has again a Gaussian copula. Since T has normally distributed margins  it
follows that T ∈ T .

Proposition 4.1 can now be proven by contradiction.
Proof of proposition 4.1. Assume there exists T ∗ /∈ T such that:
L(X  Y  T ∗) := I(X; T ∗) − βI(Y ; T ∗) < min

p(t|x) T∈T I(X; T ) − βI(T ; Y )

(16)

Since ( ˜X  ˜Y   T ) has the same copula as (X  Y  T )  we have that I( ˜X; T ) = I(X; T ) and I( ˜Y ; T ) =
I(Y ; T ). Using Lemma 4.1 the right hand part of inequality (16) can be rewritten as :

min

p(t|x) T∈T

L(X  Y  T ) = min

p(t|x) T∈T

L( ˜X  ˜Y   T ) =

min

p(t|˜x) ( ˜X  ˜Y  T )∼N

L( ˜X  ˜Y   T ).

(17)

Combining equations (16) and (17) we obtain:

I( ˜X; T ∗) − βI( ˜Y ; T ∗) <

min

p(t|˜x) ( ˜X  ˜Y  T )∼N

I( ˜X; T ) − βI(T ; ˜Y ).

This is in contradiction with the optimality of Gaussian information bottleneck  which states that the
optimal T is jointly Gaussian with (X  Y ). Thus the optimum for meta-Gaussian (X  Y ) is attained
for T with normal margins such that (X  Y  T ) also is meta-Gaussian.

Corollary 4.1. The optimal projection T o obtained for ( ˜X  ˜Y ) is also optimal for (X  Y ).

Proof. By the above we know that an optimal compression for (X  Y ) can be obtained in the set of
variables T such that ( ˜X  ˜Y   T ) is jointly Gaussian  since ˜L = L it is clear that T o is also optimal
for (X  Y ).

As a consequence of Proposition 4.1  for any random vector (X  Y ) having a Gaussian copula depen-
dence structure  an optimal projection T can be obtained by ﬁrst calculating the vector of the normal
scores ( ˜X  ˜Y ) and then computing T = A ˜X + ξ. A is here entirely determined by the covariance
matrix of the vector ( ˜X  ˜Y ) which also equals its correlation matrix (the normal scores have unit
variance by deﬁnition)  and thus the correlation matrix P parametrizing the Gaussian copula CP . In
practice the problem is reduced to the estimation the Gaussian copula of (X  Y ). In particular  for
the traditional Gaussian case where (X  Y ) ∼ N (0  Σ)  this means that we actually do not need to
estimate the full covariance Σ but only the correlations.

5

4.2 Meta-Gaussian mutual information.

The multi-information for a meta-Gaussian random vector Z = (Z1  . . .   Zd) with copula CPz is:

I(Z) = I( ˜Z) = − 1

2 log |corr( ˜Z)| = − 1

2 log |Pz| 

2 log |cov( ˜Z)| = − 1

2 log |Σ˜z| = − 1

(18)
where |.| denotes the determinant. A direct derivation of the multi-information for meta-Gaussian
random variables is also given in the supplementary material. The mutual information between
X and Y is then I(X; Y ) = − 1
2 log |Px|+ 1
. It
is obvious that the formula for the meta-Gaussian is similar to the formula for the Gaussian case
2 log |Σy|  but uses the correlation matrix parametrizing
IGauss(X; Y ) = − 1
the copula instead of the data covariance matrix. The two formulas are equivalent when X  Y are
jointly Gaussian.

2 log |P|+ 1
2 log |Σx|+ 1

(cid:18) Px Pyx

2 log |Py|  where P =

2 log |Σ|+ 1

(cid:19)

Pxy

Py

4.3 Semi-parametric copula estimation.

Semi-parametric copula estimation has been studied in [10]  [11] and [12]. The main idea is to
combine non-parametric estimation of the margins with a parametric copula model  in our case the
Gaussian copulas family. If the margins F1  . . .   Fd of a random vector Z are known  P can be
estimated by the matrix ˆP with elements given by:

(cid:80)n
i=1 Φ−1(Fk(zik))Φ−1(Fl(zil))

1
n

 

(19)

ˆP(k l) =

(cid:104) 1

n

(cid:80)n

i=1 [Φ−1(Fk(zik))]2 1

n

where zik denotes the i-th observation of dimension k. ˆP is assured to be positive semi-deﬁnite. If
the margins are unknown we can instead use the rescaled empirical cumulative distributions:

i=1 [Φ−1(Fl(zil))]2(cid:105)1/2
(cid:80)n
(cid:33)

(cid:32)

n(cid:88)

i=1

ˆFj(t) =

n

n + 1

1
n

Izij≤t

.

(20)

The estimator resulting from using the rescaled empirical distributions (20) in equation (19) is given
in the following deﬁnition.
Deﬁnition 4.1 (Normal scores rank correlation coefﬁcient). The normal scores rank correlation
coefﬁcient is the matrix ˆP n with elements:

(cid:80)n
(cid:80)n
i=1 Φ−1( R(zik)

(cid:16)

i=1

(cid:17)2
n+1 )Φ−1( R(zil)
n+1 )
Φ−1( i

n+1 )

ˆP n

(k l) =

 

(21)

where R(zik) denotes the rank of the i-th observation for dimension k. Robustness properties of
the estimator (21) have been studied in [13]. Using (21) we compute an estimate of the correlation
matrix P parametrizing cXY and obtain the transformation matrix A as detailed in Algorithm 1.

Algorithm 1 Construction of the transformation matrix A

1. Compute the normal scores rank correlation estimate ˆP n of the correlation matrix P
parametrizing cXY :
for k  l = 1  . . .   p + q do

Set the (k  l)-th element of ˆP n to
the i-th row of z is the concatenation of the i-th rows of x and y: zi∗ = (xi∗  yi∗) ∈ Rp+q.

as in equation (21) and where

n+1 ))2

i

end for
2. Compute the estimated conditional covariance matrix of the normal scores: ˆΣ˜x|˜y = ˆP n
ˆP n
xy( ˆP n
3. Find the eigenvectors and eigenvalues of ˆΣ˜x|˜y( ˆP n
4. Construct the transformation matrix A as in equation (5).

y )−1 ˆP n
yx.

x )−1.

x −

(cid:80)n
(cid:80)n
i=1 Φ−1( R(zik )
i=1(Φ−1(

n+1 )Φ−1( R(zil )
n+1 )

6

5 Results

5.1 Simulations

We tested meta-Gaussian IB (MGIB) in two different setting  ﬁrst when the data is Gaussian but con-
tains outliers  second when the data has a Gaussian copula but non-Gaussian margins. We generated
a training sample with n = 1000 observations of X and Y with dimensions ﬁxed to dx = 15 and
dy = 15. A covariance matrix was drawn from a Wishart distribution centered at a correlation matrix
populated with a few high correlation values to ensure some dependency between X and Y . This
matrix was then scaled to obtain the correlation matrix parametrizing the copula. In the ﬁrst setting
the data was sampled with N (0  1) margins. A ﬁxed percentage of outliers  8%  was then introduced
to the sample by randomly drawing a row and a column in the data matrix and replacing the current
value with a random draw from the set [−6 −3] ∪ [3  6]. In the second setting data points were
drawn from meta-Gaussian distributions with three different type of margins: Student with df = 4 
exponential with λ = 1  and beta with α1 = 0.5 = α2. For each training sample two projection
matrices AG and AC were computed  AG was calculated based on the sample covariance ˆΣn and
AC was obtained using the normal scores rank correlation ˆP n. The compression quality of the pro-
jection was then tested on a test sample of n = 10(cid:48)000 observations generated independently from
the same distribution (without outliers). Each experiment was repeated 50 times. Figure 2 shows the
information curves obtained by varying β from 0.1 to 200. The mutual informations I(X; T ) and
(Y ; T ) can be reliably estimated on the test sample using (18) and (21). The information curves start
with a very steep slope  meaning that a small increase in I(X; T ) leads to a signiﬁcant increase in
I(Y ; T )  and then slowly saturate to reach their asymptotic limit in I(Y ; T ). The best information
curves are situated in the upper left corner of the ﬁgure  since for a ﬁxed compression value I(X; T )
we want to achieve the highest relevant information content (I; T ). We clearly see in Figure 2 that
MGIB consistently outperforms GIB in that it achieves higher compression rates.

Figure 2: Information curves for Gaussian data with outliers  data with Student  Exponential and
Beta margins. Each panel shows 50 curves obtained for repetitions of the MGIB (red) and the GIB
(black). The curves stop when they come close to saturation. For higher values of β the information
I(X; T ) would continue to grow while I(Y ; T ) would reach its limit leading to horizontal lines 
but such high beta values lead to numerical instability. Since GIB suffers from a model mismatch
problem when the margins are not Gaussian  the curves saturate for smaller values of I(Y ; T ).

7

0102030024681012Gaussian with outliersI(X;T)I(Y;T)MGIBGIB01020304002468101214Student marginsI(X;T)I(Y;T)MGIBGIB01020304002468101214Exponential marginsI(X;T)I(Y;T)MGIBGIB01020304002468101214Beta marginsI(X;T)I(Y;T)MGIBGIB5.2 Real data

We further applied MGIB to the Communities and Crime data set from the UCI repository 1. The
data set contains observations of predictive and target variables. After removing missing values we
retained n = 2195 observations. In a pre-processing step we selected the dx = 10 dimensions
with the strongest absolute rank correlation to one of the relevance variables. Plotting empirical
information curves as in the synthetic examples above was impossible  because even for this setting
with drastically decreased dimensionality all mutual information estimates we tried (including the
nearest-neighbor graph method in [14]) were too unstable to draw empirical information curves. To
still give a graphical representation of our results we show in Figure 3 non-parametric density esti-
mates of the one dimensional compression T split in 5 groups according to corresponding values of
the ﬁrst relevance variable. We used GIB  MGIB and Principal Component analysis (PCA) to reduce
X to a 1-dimensional variable. For PCA this is the ﬁrst principal component  for GIB and MGIB
we independently selected the highest value of β leading to a 1-dimensional compression. It is obvi-
ous from Figure 3 that the one-dimensional MGIB compression nicely separates the different target
classes  whereas the GIB and PCA projections seem to contain much less information about the
target variable. We conclude that similar to our synthetic examples above  the MGIB compression
contains more information about the relevance variable than GIB at the same compression rate.

Figure 3: Parzen density estimates of the univariate projection of X split in 5 groups according to
values of the ﬁrst relevance variable. We see more separation between groups for MGIB than for
GIB or PCA  which indicates that the projection is more informative about the relevance variable.

6 Conclusion

We present a reformulation of the IB problem in terms of copula which gives new insights into data
compression with relevance constraints and opens new possible applications of IB for continuous
multivariate data. Meta-Gaussian IB naturally extends the analytical solution of Gaussian IB to
multivariate distributions with Gaussian copula and arbitrary marginal density. It can be applied
to any type of continuous data  provided the assumption of a Gaussian dependence structure is
reasonable  in which case the optimal compression can easily be obtained by semi-parametric copula
estimation. Simulated experiments showed that MGIB clearly outperforms GIB when the marginal
densities are not Gaussian  and even in the Gaussian case with a tiny amount of outliers MGIB has
been shown to signiﬁcantly beneﬁt from the robustness properties of rank estimators. In future work 
it would be interesting to see if the copula formulation of IB admits analytical solutions for other
copula families.

Acknowledgments

M. Rey is partially supported by the Swiss National Science Foundation  grant CR32I2 127017 / 1.

1http://archive.ics.uci.edu/ml/

8

Meta−Gaussian IBFirst component of compression TGaussian IBFirst component of compression TPCAfirst PCA projectionY1 in (−3.5 0)Y1 in (0 0.5)Y1 in (0.5 1)Y1 in (1 1.5)Y1 in (1.5 3.5)References
[1] N. Tishby  F.C. Pereira  and W. Bialek. The information bottleneck method. The 37th annual Allerton

Conference on Communication  Control  and Computing  (29-30):368–377  1999.

[2] O. Shamir  S. Sabato  and N. Tishby. Learning and generalization with the information bottleneck. Theor.

Comput. Sci.  411(29-30):2696–2711  2010.

[3] G. Chechik  A. Globerson  N. Tishby  and Y. Weiss.

Journal of Machine Learning Research  6:165–188  2005.

Information bottleneck for Gaussian variables.

[4] A. Globerson and N. Tishby. On the optimality of the Gaussian information bottleneck curve. Hebrew

University Technical Report  2004.

[5] R.M. Hecht  E. Noor  and N. Tishby. Speaker recognition by Gaussian information bottleneck. INTER-

SPEECH  pages 1567–1570  2009.

[6] J. Ma and Z. Sun. Mutual information is copula entropy. arXiv:0808.0845v1  2008.
[7] A. Sklar. Fonctions de r´epartition `a n dimensions et leurs marges. Publications de l’Institut de Statistique

de l’Universit´e de Paris  8:229–231  1959.

[8] A. J. McNeil  R. Frey  and P. Embrechts. Quantitative Risk Management. Princeton Series in Finance.

Princeton University Press  2005.

[9] G. Elidan. Copula bayesian networks. Proceedings of the Neural Information Processing Systems (NIPS) 

2010.

[10] C. Genest  K. Ghoudhi  and L.P. Rivet. A semiparametric estimation procedure of dependence parameters

in multivariate families of distributions. Biometrika  82(3):543–552  1995.

[11] H. Tsukahara. Semiparametric estimation in copula models. The Canadian Journal of Statistics 

33(3):357–375  2005.

[12] Peter D. Hoff. Extending the rank likelihood for semiparametric copula estimation. Annals of Applied

Statistics  1(1):273  2007.

[13] K. Boudt  J. Cornelissen  and C. Croux. The gaussian rank correlation estimator: Robustness properties.

Statistics and Computing  22:471–483  2012.

[14] D. P´al  B. P´oczos  and C. Szepesv´ari. Estimation of R´enyi entropy and mutual information based on
generalized nearest-neighbor graphs. Proceedings of the Neural Information Processing Systems (NIPS) 
2010.

9

,Yizhe Zhang
Xiangyu Wang
Changyou Chen
Ricardo Henao
Kai Fan
Lawrence Carin