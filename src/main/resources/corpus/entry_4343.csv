2019,Power analysis of knockoff filters for correlated designs,The knockoff filter introduced by Barber and Cand\`es 2016 is an elegant framework for controlling the false discovery rate in variable selection. 
While empirical results indicate that this methodology  is not too conservative 
there is no conclusive theoretical result on its power. When the predictors are i.i.d.\ Gaussian  it is known that as the signal to noise ratio tend to infinity  the knockoff filter is consistent in the sense that one can make FDR go to 0 and power go to 1 simultaneously. In this work we study the case where the predictors have a general covariance matrix $\bsigma$. We introduce a simple functional called \emph{effective signal deficiency (ESD)} of the covariance matrix of the predictors
that predicts consistency of various variable selection methods. 
In particular 
ESD reveals that the structure of the precision matrix 
plays a central role in consistency and therefore  so does the conditional independence structure of the predictors. To leverage this connection  we introduce \emph{Conditional Independence knockoff}  a simple procedure that is able to compete with the more sophisticated knockoff filters and that is defined when the predictors obey a Gaussian tree graphical models (or when the graph is sufficiently sparse).  Our theoretical results are supported by numerical evidence on synthetic data.,Power analysis of knockoff ﬁlters for correlated

designs

Jingbo Liu

Institute for Data  Systems  and Society
Massachusetts Institute of Technology

Cambridge  MA 02139

jingbo@mit.edu

Philippe Rigollet

Department of Mathematics

Massachusetts Institute of Technology

Cambridge  MA 02139

rigollet@math.mit.edu

Abstract

The knockoff ﬁlter introduced by Barber and Candès 2016 is an elegant framework
for controlling the false discovery rate in variable selection. While empirical
results indicate that this methodology is not too conservative  there is no conclusive
theoretical result on its power. When the predictors are i.i.d. Gaussian  it is known
that as the signal to noise ratio tend to inﬁnity  the knockoff ﬁlter is consistent in
the sense that one can make FDR go to 0 and power go to 1 simultaneously. In this
work we study the case where the predictors have a general covariance matrix Σ.
We introduce a simple functional called effective signal deﬁciency (ESD) of the
covariance matrix of the predictors that predicts consistency of various variable
selection methods. In particular  ESD reveals that the structure of the precision
matrix plays a central role in consistency and therefore  so does the conditional
independence structure of the predictors. To leverage this connection  we introduce
Conditional Independence knockoff  a simple procedure that is able to compete
with the more sophisticated knockoff ﬁlters and that is deﬁned when the predictors
obey a Gaussian tree graphical models (or when the graph is sufﬁciently sparse).
Our theoretical results are supported by numerical evidence on synthetic data.

1

Introduction

Variable selection is a cornerstone of modern high-dimensional statistics and  more generally  of
data-driven scientiﬁc discovery. Examples include selecting a few genes correlated to the incidence
of a certain disease  or discovering a number of demographic attributes correlated to crime rates.
A fruitful theoretical framework to study this question is the linear regression model in which we
observe n independent copies of the pair (X  Y ) ∈ Rp × R such that

Y = X(cid:62)θ + ξ  

where θ ∈ Rp is an unknown vector of coefﬁcients  and ξ ∼ N (0  nσ2) is a noise random variable.
Throughout this work we assume that X ∼ N (0  Σ) for some known covariance matrix Σ. Note
that for notational simplicity our linear regression model is multiplied by
n compared to standard
scaling in high-dimensional linear regression [BRT09]. Clearly  this scaling  also employed in [JM14]
has no effect on our results. In this work  we consider asymptotics where n/p → δ is ﬁxed.
In this model  a variable selection procedure is a sequence of test statistics ψ1  . . .   ψp ∈ {0  1} for
each of the hypothesis testing problem

√

(1)
When p is large  a simultaneous control of all the type I errors leads to overly conservative procedures
that impedes statistical signiﬁcant variables  and ultimately  scientiﬁc discovery. The False Discovery

: θj = 0  

vs.

1

H (j)

0

H (j)

: θj (cid:54)= 0   j = 1 . . .   p

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Rate (FDR) is a less conservative alternative to global type I error. The FDR of a procedure
(ψ1  . . .   ψp) is the expected proportion of erroneoulsy rejected tests. Formally

(cid:104) #{j : ψj = 1  θj = 0}

#{j : ψj = 1} ∨ 1

(cid:105)

FDR := E

Since its introduction more than two decades ago  various procedures have been developed to provably
control this quantity under various assumptions. Central among these is the Benjamini-Hochberg
procedure which is guaranteed to lead to a desired FDR control under the assumption that the
design matrix X = (X1  . . .   Xn)(cid:62) ∈ Rn×p formed by the concatenation of the n column vectors
X1  . . .   Xn is deterministic and orthogonal [BH95  STS04].
In the presence of correlation between the variables  that is when the design matrix fails to be
orthogonal  the problem becomes much more difﬁcult. Indeed  if the variables Xj and Xk are highly
correlated  any standard procedure will tend to output a similar coefﬁcient for both  or in the case of
Lasso for example  simply chose one of the two variables rather than both.
Recently  the knockoff ﬁlter of Barber and Candès [BC15  CFJL18] has emerged as a competitive
alternative to the Benjamini-Hochberg procedure for FDR control in the presence of correlated
variables  and has demonstrated great empirical success [KS19  SKB+]. The terminology “knockoffs"
refers to a vector ˜X ∈ Rp that is easy to mistake for the original vector X but is crucially independent
of Y given X. Formally  ˜X is a knockoff of X if (i) ˜X is independent of Y given X and (ii) for any
S ⊂ {1  . . .   p}  it holds

(2)
where d= denotes equality in distribution and (X  ˜X)swap(S) is the vector Z ∈ R2p with jth coordinate
given by

(X  ˜X)swap(S)

d=(X  ˜X)

if j ∈ ({1  . . .   p} \ S) ∪ (S + {p})
if j ∈ S ∪ ({p + 1  . . .   2p} \ (S + {p})

(cid:26) Xj

˜Xj

Zj =

In words  for any vector R2p  the operator (·)swap(S) swaps each coordinate in j ∈ S with the
coordinate j + p and leaves the other coordinates unchanged. We call a knockoff mechanism any
probability family of probability distributions (Px  x ∈ Rp) over Rp such that ˜X ∼ PX is a knockoff
of X. Since the knockoff is constructed independently of Y   it serves as a benchmark to evaluate
how much of the coefﬁcient of a certain variable is due to its correlation with Y and how much of it
is due to its correlation with the other variables.
With this idea in mind  the knockoff ﬁlter is then constructed from the following four steps:

1. Generate knockoffs. For i = 1  . . .   n  given Xi ∈ Rp  generate knockoff ˜Xi ∼ PXi and
form the n × 2p design matrix [X  ˜X] where ˜X = ( ˜X1  . . .   ˜Xn)(cid:62) ∈ Rn×p is obtained by
concatenating the knockoff vectors.

2. Collect scores for each variable. Deﬁne the 2p dimensional vector 1 ˆθ as the Lasso

estimator

1
2n

(cid:107)Y − [X  ˜X]θ(cid:107)2

2 + λ(cid:107)θ(cid:107)1  

ˆθ = argmin
θ∈R2p

(3)
where Y = (Y1  . . .   Yn)(cid:62) is the response vector and  collect the differences of absolute
coefﬁcients between variables and knockoffs into a set D = {|∆j|   j = 1  . . .   p} \ {0}
where ∆j’s are any constructed statistics satisfying certain symmetry conditions [BC15]. A
frequent choice is

In this work we replace ˆθ by the debiased version ˆθu (see (7) ahead) in the above deﬁnition.

3. Threshold. Given a desired FDR bound q ∈ (0  1)  deﬁne the threshold

∆j := |ˆθj| − |ˆθj+p|   j = 1  . . .   p.
(cid:26)

#{j : ∆j ≤ −t}
#{j : ∆j ≥ t} ∨ 1

t ∈ D :

(cid:27)

≤ q

.

T := min

1Regression problems with knockoffs are 2p dimensional rather than p dimensional. To keep track of this

fact  we use · to denote a 2p dimensional vector.

2

4. Test. For all j = 1  . . .   p  answer the hypothesis testing problem (1) with test

Ψj = 1{∆j ≥ T} .

This procedure is guaranteed to satisfy FDR ≤ q [BC15  Theorem 1] no matter the choice of
knockoffs. Clearly  ˜X = X is a valid choice for knockoffs but it will inevitably lead to no discoveries.
The ability of a variable selection procedure (ψ1  . . .   ψp) to discover true positive is captured its
power (or true positive proportion) deﬁned as

(cid:104) #{j : ψj = 1  θj (cid:54)= 0}

(cid:105)

PWR = E

#{j : θj (cid:54)= 0}

Intuitively  to maximize power  knockoffs should be as uncorrelated with X as possible while
satisfying the exchangeability property (2). Following this principle  various knockoff mechanisms
have been proposed in different settings  which typically involves solving an optimization to minimize
a heuristic notion of correlation [BC15  CFJL18  RSC18]. Because of this optimization problem 
knockoff mechanisms with analytical expressions are rare  with the exception of the equi-knockoff
[BC15] and metropolized knockoff sampling [BCJW19]). Partly due to this  the theoretical analysis
of the power of the knockoff ﬁlter has been very limited  even in the Gaussian setting. In the special
case where X ∼ N (0  D) for some diagonal matrix  i.e. when the variables are independent  one
can simply take ˜X ∼ N (0  D) independent of X. In this case  the power of the knockoff ﬁlter tends
to 1 as the signal-to-noise ratio tends to inﬁnity [WBC17].
When predictors are correlated  [FDLL19] proved a lower bound on the power  where the limiting
power as n → ∞ is bounded below in terms of the number p of predictors and extremal eigenvalues
of the covariance matrix of the true and knockoff variables. While this lower bound provides a
sufﬁcient condition for situations when the power tends to 1  it is loose in certain scenarios. For
example  if all predictors are independent except that two of them are almost surely equal  then the
minimum eigenvalue of the covariance matrix is zero and yet  experimental results indicate that the
FDR and the power of the knockoff ﬁlter are almost unchanged.
Our contribution. In this paper  we revisit the statistical performance of the knockoff ﬁlter X ∼
N (0  Σ) and characterize the situation the knockoff ﬁlter is consistent  that is when its FDR tends to
0 and its power tends to 1 simultaneously. More speciﬁcally  under suitable limit assumptions  we
show that the knockoff ﬁlter is consistent if and only if the empirical distribution of the diagonal
elements of the precision matrix of P := Σ−1 converges to 0  where Σ denotes the covariance matrix
of [X  ˜X] ∈ R2p converges to a point mass at 0. In turn  we propose an explicit criterion  called
effective signal deﬁciency deﬁned formally in (8) to practically evaluate consistency or lack thereof.
Here the term “signal" refers to the covariance structure Σ of X and the effective signal deﬁciency
essentially how much weak such a signal should be for a knockoff mechanism to be consistent.
A second contribution is to propose a new knockoffs mechanism  called Conditionally Independent
Knockoffs (CIK)  which possesses both simple analytic expressions and excellent experimental
performance. CIK does not exist for all Σ  but we show its existence for tree graphical models or
other sufﬁciently sparse graphs. Note that in practice  the so-called model-X knockoff ﬁlter requires
the knowledge of Σ  an estimation of which is often prohibitive except when the graph has sparse or
tree structures. CIK has simple explicit expressions of the effective signal deﬁciency for tree models 
since the empirical distribution of the diagonals of Σ−1 is the same as that of (P2
j=1. We
remark that CIK is different than metropolized knockoff sampling studied in [BCJW19] (originally
appeared in [CFJL18  Section 3.4.1])  even in the case of Gaussian Markov chains. The latter exists
for generic distributions and is computationally efﬁcient for Markov chains.

jjΣjj)p

Notation. We write [n] := {1  . . .   n} and 1 to denote the all-ones vector. For any vector θ  let (cid:107)θ(cid:107)0
and (cid:107)θ(cid:107)1 denote its (cid:96)0 and (cid:96)1 norms. Given a vector x  we denote by diag(x) the diagonal matrix
whose diagonal elements are given by the entries of x and for a matrix M  we denote by diag(M) the
vector whose entries are given by the diagonal entries of M. For a standard Gaussian random variable
ξ ∼ N (0  1) and any real number r  we denote by Q(r) = P[ξ > r]  the Gaussian tail probability.
Finally we use the notation A (cid:22) B to indicate the loewner order: B − A is positive semideﬁnite.

3

2 Existing work

We focus this discussion on the case of Gaussian design X.
condition (2) implies that [X  ˜X] has a covariance matrix of the form

In this case  the exchangeability

Σ =

Σ

Σ − diag(s)

Σ − diag(s)

Σ

.

(4)

(cid:20)

(cid:21)

As observed in [BC15]  positive semi-deﬁniteness of this matrix is equivalent to

0 (cid:22) diag(s) (cid:22) 2Σ

For some s ∈ Rp. As a result  ﬁnding a knockoff mechanism consists in ﬁnding s.
The seminal work [BC15][CFJL18] introduce the following knockoff mechanisms:
EQUI-KNOCKOFFS: The vector s is chosen of the form s = s1 for some s ≥ 0. In light of (5) the
smallest value possible for s is 2λmin(Σ). Assuming the normalization diag(Σ) = 1  [CFJL18]
recommend choosing

(5)

(6)

s = 2λmin(Σ) ∧ 1 

with the goal of minimizing the correlation between Xj and ˜Xj.
SDP-KNOCKOFFS: The vector s is chosen to solve the following semideﬁnite program:

min (cid:107) diag(Σ) − s(cid:107)1

s.t.

0 (cid:22) diag(s) (cid:22) diag(Σ)

diag(s) (cid:22) 2Σ.

ASDP-KNOCKOFFS: Assume the normalization diag(Σ) = 1. Choose an approximation Σa of Σ
(see [CFJL18]) and solve:

minimize (cid:107)1 − ˆs(cid:107)1
subject to ˆs ≥ 0  diag(ˆs) (cid:22) 2Σa

and then solve:

minimize γ
subject to diag(γˆs) (cid:22) 2Σ

and put s = γˆs.
We do not discuss other knockoff constructions  such as the exact construction [CFJL18  Section 3.4.1]
and deep knockoff [RSC18]  which mostly target at general non-Gaussian distributions.
As alluded  previously  [WBC17] performed power analysis in the linear (ﬁxed n/p) regime for
Σ = Ip  in which case all the above knockoff mechanisms give the same answer of s = 1. For a
general Σ  [FDLL19] derived lower bounds on the power in terms of the minimum eigenvalue of the
extended covariance matrix Σ (no speciﬁc knockoff mechanism is assumed).

3 Overview of the main results

In the paper  we focus on the so-called linear regime where the sampling n/p converges to a constant
δ. We allow for general Σ and for simplicity  rather than using the Lasso estimator ˆθ deﬁned in (3) 
we employ a debiased version [ZZ14  vdGBRD14  JM14]

d
n

Σ−1X(cid:62)(Y − Xˆθ) 

ˆθu := ˆθ +

(7)
where 1/d = 1−(cid:107)ˆθ(cid:107)0/n. To allow for asymptotic results  we consider a sequence {(Σ(p)  θ(p))}p≥1
where Σ(p) are covariance matrices of size m(p) × m(p) and θ(p) ∈ R
(p) are vectors of coefﬁcients.
Note that we will only consider the cases where m(p) = p or m(p) = 2p  depending on whether we
consider predictors with or without knockoffs.
At ﬁrst glance  it is unclear that for such general sequences  any meaningful result can be said about
the debiased Lasso estimator ˆθu deﬁned in (7). To overcome this obvious limitation  we consider the
asymptotic setting where a standard distributional limit exists in the sense [JM14  Deﬁnition 4.1].

4

Deﬁnition 1 (Standard distributional limit). Assume constant sampling rate n(p) = δm(p). A
sequence {(Σ(p)  θ(p))}p≥1 is said to have a standard distributional limit with sparsity (α  β)  if
(i) there exist τ (cid:54)= 0 deterministic and d  possibly random  such that the empirical measure

δ(cid:0)θj  

1

m(p)

m(p)(cid:88)

j=1

(cid:1)(p)

−θj

ˆθu
j

τ

 (Σ−1)jj

converges almost surely weakly to a probability measure ν on R3 as p → ∞. Here  ν is the
probability distribution of (Θ  Υ1/2Z  Υ)  where Z ∼ N (0  1)  and Θ and Υ are some random
variables independent of Z. Moreover  we ask that
(ii) as p → ∞  it holds almost surely that

(cid:107)θ(p)(cid:107)0 → α := P[|Θ| > 0]  

and

1
p

(cid:107)θ(p)(cid:107)1 → β := E[|Θ|] .

1
p

Note that (i) implies that lim inf p→∞ (cid:107)θ(p)(cid:107)1/p ≥ E[|Θ|]  and lim inf p→∞ (cid:107)θ(p)(cid:107)0/p ≥ P[|Θ| > 0] 
almost surely. We further impose that equalities are achieved in (ii).
As mentioned in [JM14]  characterizing instances having a standard distributional limit is highly
nontrivial. Yet  at least  the deﬁnition is non-empty since it contains the case of standard Gaussian
design. Moreover  a non-rigorous replica argument indicates that the standard distributional limit
exists as long as a certain functional deﬁned on R2 has a differentiable limit [JM14  Replica Method
Claim 4.6]  which is always satisﬁed for block diagonal Σ where the empirical distribution of the
blocks converges.
We remark that in the sparse regime where (cid:107)θ(cid:107)0 = o(p)  rigorous results  that do not appeal to the
replica method  show that the weak convergence of the distribution of {(θj  Pjj)}p
j=1is essentially
sufﬁcient for the existence of a standard distributional limit ([JM14  Theorem 4.5])  although the
present paper does not concern that regime.
We now introduce the key criterion to characterize consistency of a knockoff mechanism and more
generally of a variable selection procedure.
Deﬁnition 2 (Effective signal deﬁciency). For a given variable selection procedure  ESD(p) ≥ 0 is a
function of Σ(p) with the following property: for the class of sequences (θ(p)  Σ(p))p≥1 satisfying
suitable distributional limit conditions  vanishing ESD is equivalent to consistency of the test:

(cid:8)FDR(p) + (1 − PWR(p))(cid:9) → 0 .

ESD := lim sup
p→∞

ESD(p) → 0 ⇐⇒ lim sup
p→∞

When we consider knockoff ﬁlters  ESD is frequently expressed in terms of the extended covariance
matrix Σ  which is in turn a function of Σ for a given knockoff mechanism. In that setting  the
“suitable distributional limit conditions” in the above deﬁnition requires that the sequence of extended
instances (θ(p)  Σ(p))p≥1 has a standard distributional limit.
Note that by deﬁnition  ESD is not unique  and our goal is to ﬁnd simple representations of its
equivalence class. ESD is a potentially useful concept in comparing or evaluating different ways
of generating knockoff matrices. As an analogy  think of the various notions of convergences of
probability measures. A sequence of probability measures may converge in one topology but not
in another. Similarly  one may cook up different functionals of the covariance matrix  such as
limp→∞ p Tr−1(Σ) and limp→∞ p Tr(Σ−1)  which both intuitively characterize some sort of signal
deﬁciency since they tend to be small when the signal gets stronger. However  they are not equivalent 
and the second convergence to 0 is stronger in the sense that the ﬁrst must vanish when the second
vanishes. ESD is intended to be the correct notion of “convergence” that characterizes FDR tending
to 0 and power tending to 1.
Of course  by deﬁnition it is not obvious that a succinct expression of such an effective signal
deﬁciency exists. Remarkably  we ﬁnd that the effective signal deﬁciency can be characterized by
the convergence of certain empirical distribution derived from Σ. The effective signal deﬁciency for
various (old and new) variable selection procedures is as follows:

5

LASSO: The debiased Lasso [JM14] is a popular method for high-dimensional statistical inference.
It is implemented by ﬁrst computing a Lasso estimator

(cid:26) 1

2n

ˆθ = argmin
t∈Rp

(cid:107)Y − Xθ(cid:107)2 + λ(cid:107)θ(cid:107)1

(cid:27)

where λ > 0 can be chosen as any ﬁxed positive number independent of p. Instead of a direct
threshold test on ˆθ  we ﬁrst compute an “unbiased version” ˆθu deﬁned in (7)  as in [JM14]  and pass
a threshold to select non-nulls. We show in Theorem 3 and Proposition 4 that we may chose

(cid:0) 1

p(cid:88)

p

j=1

ESD = lim

p→∞ dLP

δP(p)

jj

  δ0)  

where dLP denotes the Lévy-Prokhorov distance between deﬁned for any two measures µ and ν
deﬁned over a metric space as

dLP(µ  ν) := inf{ > 0 : µ(A) ≤ ν(A) +   ν(A) ≤ µ(A) +  ∀A}  

where A denotes the -neighborhood of A. In particular  we have

(cid:0) 1

p(cid:88)

p

j=1

dLP

δP(p)

jj

  δ0) := inf

 > 0 :

#{j : P(p)

jj ≥ }
p

≤ 

(cid:40)

(cid:41)

.

(8)

The assumption of the standard distributional limit ensures the weak convergence of the empirical
j=1  and hence the convergence of (8). Hereafter  for any vector x ∈ Rm  we
distribution of (P(p)
use the shorthand (abusive) notation

jj )p

(cid:107)(xj)j(cid:107)LP := dLP

δxj   δ0) .

(cid:0) 1

m(cid:88)

m

j=1

This characterization if ESD is  in fact tight: ESD → 0 is a necessary and sufﬁcient condition for
consistency of thresholded Lasso as a variable selection procedure (see Proposition 4)

GENERAL KNOCKOFF: for a general knockoff construction  including variational formulations such
as SDP-knockoffs  it seems hopeless to ﬁnd simple expressions of ESD in terms of Σ. Nevertheless 
if (θ(p)  Σ(p)) has a standard distributional limit  we can choose ESD = limp→∞ (cid:107)(P(p)
jj )j(cid:107)LP where
we recall that P is the extended precision matrix of [X  ˜X].

EQUI-KNOCKOFF: Specializing the above result to the equi-knockoff case  we see that we can choose
ESD = limp→∞ λmax(P(p))  achieved when s = aλmin(Σ) for any a ∈ (0  2). Note that this is
slightly different from the choice (6) prescribed in [BC15  CFJL18] where s := min{1  2λmin(Σ)}.

CI-KNOCKOFF: We introduce a new method for generating the knockoff matrix  called conditional
independence knockoff or CI-knockoff in short. If the Gaussian graphical model associated to X
if the sparsity pattern of Σ−1 corresponds to the adjacency matrix of a tree  then
is a tree  i.e.
the conditional independence knockoff always exists and ESD = limp→∞ (cid:107)(P(p)
jj Σjj)j(cid:107)LP . For
example  in the independent case where Σ is diagonal  we get ESD = 1 which readily yields
consistency.
The last knockoff construction  conditional independence knockoff  appears to be new. It is both
analytically simple and empirically competitive. Comparing equi- and CI- knockoffs: the latter is
more robust  since having a small fraction of j with large P2
jjΣjj does not increase its ESD much.
For example  two predictors are identical  then the ESD for conditional independence knockoff almost
does not change  but equi-knockoff completely fails. Compared to other previous knockoffs  we ﬁnd
that CI-knockoff usually shows similar or improved performance empirically  while being easier to
compute and to manipulate.

6

4 Baseline: Lasso with oracle threshold

Consider a variable selection algorithm in which the Lasso parameters with absolute values above a
threshold are selected  and suppose that the threshold which controls the FDR is given by an oracle.
Note that the knockoff ﬁlter is based on the Lasso estimator but it must choose threshold in a data
driven fashion. As a result  the Lasso with oracle threshold presents a strong baseline against which
the performance of a given knockoff ﬁlter should be compared. Not surprisingly  and also as noted in
[FDLL19]  although the knockoff ﬁlter has the advantage of controlling FDR  it usually has a lower
power than Lasso with oracle threshold. This fact will become more transparent as we determine
their ESD.
Theorem 3. Let λ > 0 be arbitrary and let {(Σ(p)  θ(p))}p≥1 admit a standard distributional limit 
and denote the distributional limit by (Θ  Υ1/2Z  Υ)  where Z ∼ N (0  1)  and Θ and Υ are some
random variables independent of Z. Assume further that L := limp→∞ (cid:107)(P(p)
jj )j(cid:107)LP where the limit
exists almost surely by the standard distributional limit assumption. Consider the algorithm which
selects j for which |ˆθu

j | ≥ t  where ˆθu is deﬁned in (7). Then with the choice of t = L1/4 

{FDR(p) + (1 − PWR(p))} ≤ CL µΘ τ

lim sup
p→∞

where limL→0 CL µΘ τ = 0 for any µΘ with P[|Θ| > 0] > 0 and τ as in the deﬁnition of the standard
distributional limit. In particular  if δ > 1  then τ can be bounded in terms of σ  λ  δ and µΘ only
(independent of µΥ)  and hence CL µΘ τ in the above inequality can be replaced by CL µΘ σ λ δ
where limL→0 CL µΘ σ λ δ = 0.
The above theorem implies that L → 0 is a sufﬁcient condition for consistency; this is in fact also
necessary  as indicated by the following complementary lower bound.
Proposition 4. (Lower bound). In the previous theorem  assume further that Υ is independent of Θ.
Then for any t > 0 

p→∞ {FDR(p) + (1 − PWR(p))} ≥ cL σ µΘ.

lim inf

where cL σ µΘ is increasing in L  strictly positive as long as L > 0.

Combining the above two results  we get the following interpretation. Suppose that the distribution
of Θ and the values of σ are ﬁxed  and suppose that the parameters λ and t in the algorithm optimally
tuned (i.e. minimizing lim supp→∞{FDR(p) + (1 − PWR(p))} for any given distributions). If δ > 1 
then  remarkably  the variable selection procedure is consistent if and only if L being small – as
long as Υ is independent of Θ  while other characteristics of the law of Υ are not necessary to know.
In other words  we proved that ESD = L := limp→∞ (cid:107)(P(p)
jj )j(cid:107)LP. If δ ≤ 1  small L may not be
sufﬁcient for consistency since CL µΘ σ λ δ also depends on µΥ through τ.

5 Results for general knockoff mechanisms
Given Σ  let Σ be the extended 2p × 2p covariance matrix for the true predictors and their knockoffs.
Let θ = [θ  0] ∈ R2p. Consider the procedure of the knockoff ﬁlter described in Section 2  with a
slight tweak: deﬁne ∆j := |ˆθ

j+p|  where

u

u

j | − |ˆθ

u
ˆθ

= ˆθ +

d
n

Σ−1[X  ˜X]

(cid:62)

(Y − [X  ˜X]ˆθ)

and ˆθ is deﬁned in (3). This modiﬁcation still fulﬁlls the sufﬁciency and antisymmetry condition
in [BC15  Section 2.2]  so its FDR can still be controlled. This change allows us to perform
analysis using results in [JM14]. We also assume that the Lasso parameter λ is an arbitrary number
independent of p.
Theorem 5. Let {(Σ(p)  θ(p))}p≥1 admit a standard distributional limit for a given λ ≥ 0  and
denote the distributional limit by (Θ  Υ1/2Z  Υ)  where Z ∼ N (0  1)  and Θ and Υ are some random
variables independent of Z. Assume further that L := limp→∞ (cid:107)(P(p)
jj )j(cid:107)LP where the limit exists

7

almost surely under the standard distributional limit assumption. Then the knockoff ﬁlter with FDR
budget q ∈ (0  1) satisﬁes:

p→∞ PWR(p) ≥ 1 − CL q τ µΘ 

lim inf

where limL→0 CL q τ µΘ = 0 for any given q  τ  µΘ. Further if δ > 2  then CL q τ µΘ in the above
inequality can be replaced by CL q λ σ δ µΘ.
Taking q → 0 in the above theorem implies that L → 0 is sufﬁcient for consistency; the following
result shows the necessity in a representative setting:
Proposition 6. In the previous theorem  further assume that θj = 1{j ∈ H1} where |H1| = αp
(α > 0) is selected uniformly at random. Then  under a suitable distributional limit assumption  the
knockoff ﬁlter with FDR budget q ∈ (0  αLQ2( 1
√

L

)) satisﬁes:
PWR(p) ≤ 3/4.

σ

lim sup
p→∞

u

j − θj  ˆθ

j+p − θj+p)p

The “suitable distributional limit assumption” in Proposition 6 postulates a Gaussian limit for the
u
empirical distribution of the pair (ˆθ
j=1  which is stronger than the marginal
Gaussian limit assumption in Deﬁnition 1  but nevertheless supported by the replica heuristics.
Moreover  this condition can be rigorously shown for the case of δ > 2  λ = 0 (least squares) and
block diagonal Σ. The assumption that θj = 1 under H1 in Proposition 6 facilitates the proof but we
expect that a similar inconsistency result holds for general µΘ. The assumption that H1 is selected
uniformly at random is a counterpart of the independence of Θ and Υ in Proposition 4.
Together  Theorem 5 and Proposition 6 show that for the knockoff ﬁler  ESD = limp→∞ (cid:107)(P(p)
jj )j(cid:107)LP
in the regime of δ > 1. This suggests that one should construct the knockoff variables so that the
empirical distribution of (Pjj)2p

j=1 converges to 0 weakly.

6 Conditional independence knockoff and ESD

We introduce the conditional independence knockoff  where Xj and ˜Xj are independent conditionally
on X¬j := {Xk  k ∈ [p] \ {j}}  for each j = 1  . . .   p. This condition implies that

Therefore recalling that s1  . . .   sp are as deﬁned in (4)  we get

E[Xj ˜Xj] = E(cid:2)E[Xj ˜Xj|X¬j](cid:3) = E(cid:2)(E[Xj|X¬j])2(cid:3)
j |X¬j](cid:3) − E(cid:2)(E[Xj|X¬j])2(cid:3)

= E(cid:2)E[X 2

sj = Σjj − E[Xj ˜Xj]

= E[Var(Xj|X¬j)] = P −1
jj .

(9)

However such an s may violate the positive semideﬁnite assumption for the joint covariance matrix
(examples exist already in the case p = 3). Yet  interestingly  we ﬁnd that in the case of tree graphical
models  this construction always exists. In many practical scenarios  the predictors X p comes from a
tree graphical model  and we can estimate the underlying graph sing the Chow-Liu algorithm [CL68].
Theorem 7. The covariance matrix Σ deﬁned in (4) is positive semideﬁnite with s deﬁned in (9)  if
either 1) Σ is the covariance matrix of a tree graphical model; or 2) P is diagonally dominant.

Either condition in the theorem intuitive imposes that the graph is sparse. In practice  Σ needs to be
estimated  which is generally only feasible with some sparse structure (e.g. via graphical lasso).
Assuming the existence of a standard distributional limit and δ > 1  we have the following results:
Theorem 8. For tree graphical models  ESD = limp→∞ (cid:107)(P(p)
Theorem 9. ESD = λmax(Σ) for EQUI-KNOCKOFF if sj = aλmin(Σ)  a ∈ (0  2)  j = 1  . . .   p.

jj Σjj)j(cid:107)LP for CI-KNOCKOFF.

8

Figure 1: Comparisons of EQUI-KNOCKOFF  ASDP-KNOCKOFF  and CI-KNOCKOFF. Left: Binary
tree  equal correlations. Right: Markov chain  randomly chosen correlation strengths.

7 Experimental results
First consider the setting where X1  . . .   Xp ∼ N (0  1) and the conditional independence graph
forms a binary tree. The correlations between adjacent nodes are all equal to 0.5. Choose k = 100
out of p = 1000 indices uniformly at random as the support of θ  and set θj = 4.5 for j in the support.
Generate n = 1000 independent copies of (X  Y ) in Y = X(cid:62)θ + ξ where ξ ∼ N (0  n).
Figure 1  left shows the box plots of the power and FDR for EQUI-KNOCKOFF  ASDP-KNOCKOFF 
and CI-KNOCKOFF  where s is deﬁned as in (6) for CI-KNOCKOFF. The FDR is controlled at the
target q = 0.1 in all three cases. The powers are not statistically signiﬁcantly different  but the rough
trend is PWRe < PWRa < PWRc. We then compare the effective signal deﬁciency. Note that in the
current setting  Var(X j|X¬j) ≤ 1  and hence Pjj ≥ 1  for each j = 1  . . .   2p  and we always have
(cid:107)(Pjj)2p
j=1(cid:107)LP = 1 by deﬁnition (8)  which cannot reveal any useful information for comparison. To
resolve this  we can scale down Pjj by a common factor before computing the LP distances  noting
that it yields a valid effective signal deﬁciency. Lacking a systematic way of choosing such a scaling
factor  heuristically we choose it as 2000 so that the LP distances for the three algorithms are all
“bounded away from 0 and 1”. We ﬁnd that dLP e (cid:39) 0.501  dLP a (cid:39) 0.048 and dLP c (cid:39) 0.002 and
their ordering matches the ordering of the powers.
In the previous example  the simplest EQUI-KNOCKOFF has a highly competitive performance.
However  this is an artifact of the fact that the data covariance is highly structured (i.e.  correlations
are all the same). If the correlations have high ﬂuctuations  and in particular  a small number of
node pairs are highly correlated  then the equi-knockoff has a much worse performance. This
is demonstrated in the next example. Consider the setting where X1  . . .   Xp forms a Markov
chain  in which X1  . . .   Xp ∼ N (0  1). In other words  the Gaussian graphical model is a path
graph. The correlation between Xj and Xj+1 is ρj := Gj 1{|Gj| ≤ 1}  where Gj ∼ N (0  0.25) 
j = 1  . . .   p − 1 are chosen independently. Choose k = 100 out of p = 1000 indices uniformly at
random as the support of θ  and set θj = 4.5 for j in the support. Generate n = 1200 independent
copies of (X  Y ) in Y = X(cid:62)θ + ξ where ξ ∼ N (0  0.49n).
Figure 1 Right shows the box plots of the power and FDR for the knockoff ﬁlter with three different
knockoff constructions. The target FDR is q = 0.1. Since the correlations are now chosen randomly 
with high probability there exist highly correlated nodes  and hence λmin(Σ) can be very small  in
which case the equi-knockoff performs poorly. However PWRc is similar to PWRa  with the median
of the former slightly higher. To compare the ESD  ﬁrst scale down Pjj by a heuristically chosen
factor 100. We ﬁnd dLP e (cid:39) 0.9995  dLP a (cid:39) 0.8660  and dLP c (cid:39) 0.1075 and their ordering matches
the ordering of the powers of the three knockoff constructions.

Acknowledgments

JL was supported by the IDSS Wiener Fellowship. PR was supported by NSF awards IIS-BIGDATA-
1838071  DMS-1712596 and CCF-TRIPODS- 1740751; ONR grant N00014-17-1-2147.

9

Power_ePower_aPower_cFDR_eFDR_aFDR_c0102030405060708090100Power_ePower_aPower_cFDR_eFDR_aFDR_c0102030405060708090100References

[BC15] Rina Foygel Barber and Emmanuel J. Candés. Controlling the false discovery rate via

knockoffs. The Annals of Statistics  43(5):2055–2085  2015.

[BCJW19] Stephen Bates  Emmanuel Candès  Lucas Janson  and Wenshuo Wang. Metropolized

knockoff sampling. arXiv preprint arXiv:1903.00434  2019.

[BH95] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical
and powerful approach to multiple testing. Journal of the Royal statistical society:
series B (Methodological)  57(1):289–300  1995.

[BRT09] Peter J. Bickel  Ya’acov Ritov  and Alexandre B. Tsybakov. Simultaneous analysis of

Lasso and Dantzig selector. Ann. Statist.  37(4):1705–1732  2009.

[CFJL18] Emmanuel Candes  Yingying Fan  Lucas Janson  and Jinchi Lv. Panning for
gold:‘model-x’knockoffs for high dimensional controlled variable selection. Journal
of the Royal Statistical Society: Series B (Statistical Methodology)  80(3):551–577 
2018.

[CL68] C Chow and Cong Liu. Approximating discrete probability distributions with depen-

dence trees. IEEE Transactions on Information Theory  14(3):462–467  1968.

[FDLL19] Yingying Fan  Emre Demirkaya  Gaorong Li  and Jinchi Lv. Rank: large-scale
inference with graphical nonlinear knockoffs. Journal of the American Statistical
Association  pages 1–43  2019.

[JM14] Adel Javanmard and Andrea Montanari. Hypothesis testing in high-dimensional
IEEE

regression under the gaussian random design model: Asymptotic theory.
Transactions on Information Theory  60(10):6522–6554  2014.

[KS19] Eugene Katsevich and Chiara Sabatti. Multilayer knockoff ﬁlter: Controlled variable
selection at multiple resolutions. Ann. Appl. Stat. The Annals of Applied Statistics 
13(1):1–33  2019.

[RSC18] Yaniv Romano  Matteo Sesia  and Emmanuel Candès. Deep knockoffs. Journal of the

American Statistical Association (to appear)  2018.

[SKB+] Matteo Sesia  Eugene Katsevich  Stephen Bates  Emmanuel Candès  and Chiara
Sabatti. Multi-resolution localization of causal variants across the genome. bioRxiv
(2019).

[STS04] John D Storey  Jonathan E Taylor  and David Siegmund. Strong control  conservative
point estimation and simultaneous conservative consistency of false discovery rates:
a uniﬁed approach. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  66(1):187–205  2004.

[vdGBRD14] Sara van de Geer  Peter Bühlmann  Yaacov Ritov  and Ruben Dezeure. On asymptoti-
cally optimal conﬁdence regions and tests for high-dimensional models. Ann. Statist. 
42(3):1166–1202  06 2014.

[WBC17] Asaf Weinstein  Rina Barber  and Emmanuel Candes. A power and prediction analysis

for knockoffs with lasso statistics. arXiv preprint arXiv:1712.06465  2017.

[ZZ14] Cun-Hui Zhang and Stephanie S. Zhang. Conﬁdence intervals for low dimensional
parameters in high dimensional linear models. Journal of the Royal Statistical Society:
Series B (Statistical Methodology)  76(1):217–242  2014.

10

,Jingbo Liu
Philippe Rigollet