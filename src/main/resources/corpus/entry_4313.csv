2011,Policy Gradient Coagent Networks,We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present  analyze theoretically  and empirically evaluate an update rule for each module  which requires only local information: the module's input  output  and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difficult and are also desirable to increase the biological plausibility of reinforcement learning methods.,Policy Gradient Coagent Networks

Philip S. Thomas

Department of Computer Science

University of Massachusetts Amherst

Amherst  MA 01002

pthomas@cs.umass.edu

Abstract

We present a novel class of actor-critic algorithms for actors consisting of sets
of interacting modules. We present  analyze theoretically  and empirically eval-
uate an update rule for each module  which requires only local information: the
module’s input  output  and the TD error broadcast by a critic. Such updates are
necessary when computation of compatible features becomes prohibitively difﬁ-
cult and are also desirable to increase the biological plausibility of reinforcement
learning methods.

1

Introduction

Methods for solving sequential decision problems with delayed reward  where the problems are for-
mulated as Markov decision processes (MDPs)  have been compared to the learning mechanisms of
animal brains [3  4  9  10  13  20  22]. These comparisons stem from similarities between activa-
tion of dopaminergic neurons and reward prediction error [19]  also called the temporal difference
(TD) error [21]. Dopamine is broadcast to large portions of the human brain  suggesting that it may
be used in a similar manner to the TD error in reinforcement learning (RL) [23] systems  i.e.  to
facilitate improvements to the brain’s decision rules.
Systems with a critic that computes and broadcasts the TD error to another module called the actor 
which stores the current decision rule  are called actor-critic architectures. Chang et al. [7] present a
compelling argument that the ﬂy brain is an actor-critic by ﬁnding the neurons making up the critic
and then artiﬁcially activating them to train the actor portions of the brain. However  current actor-
critic methods in the artiﬁcial intelligence community remain biologically implausible because each
component of the actor can only be updated with detailed knowledge of the entire actor. This forces
computational neuroscientists to either create novel methods [14] or alter existing methods from the
artiﬁcial intelligence community in order to enforce locality constraints (e.g.  [16]).

Figure 1: Example modular actor.

The actor in an actor-critic maintains a decision rule  π  called a policy  parameterized by a vector
θ  that computes the probability of an action (decision)  a  given an estimate of the current state of
the world  st  and the current parameters  θt. In some cases  an actor can be broken into multiple
interacting modules  each of which computes an action given some input  x  which may contain
elements of s as well as the outputs of other modules. An example of such a modular actor is
provided in Figure 1. This actor consists of three modules  A1  A2  and A3  with parameters θ1  θ2 

1

A1AItθ1θ3Otta1a3x1x3A2A3Input  sθ2θOutput  aa2x2Input………………Layer 1Layer 2OutputLayerand θ3  respectively. The ith module takes input xi  which is a subset of the state features and the
outputs of other modules. It then produces its action ai according to its policy  πi(xi  ai  θi) =
Pr(ai|xi  θi). The output  a  of the whole modular actor is one of the module outputs—in this case
a = a3. Later we modify this to allow the action a to follow any distribution with the state and
module outputs as parameters. This modular policy can also be written as a non-modular policy that

is a function of θ =(cid:8)θ1  θ2  θ3(cid:9)  i.e.  π(s  a  θ) = Pr(a|s  θ). We assume that the modular policy is

not recurrent. Such modular policies appear frequently in models of the human brain  with modules
corresponding to neurons or collections thereof [12  16].
Current actor-critic methods (e.g. [11  15  23  24]) require knowledge of ∂π/∂θi in order to update
θi. However  ∂π/∂θi often depends on the current values of all other parameters as well as the
structure deﬁning how the parameters are combined to produce the decision rule. This is akin to
assuming that a neuron (or cluster of neurons)  Ai  must know its inﬂuence on the ﬁnal decision
rule implemented. Were another module to modify its policy such that ∂π/∂θi changes  a message
must be sent to alert Ai of the exact changes so that it can update its estimate of ∂π/∂θi  which is
biologically implausible.
Rather than keeping a current estimate of ∂π/∂θi  one might attempt to compute it on the ﬂy via
the error backpropagation learning algorithm [17]. In this algorithm  each module  Ai  beginning
with the output modules  computes its own update and then sends a message containing ∂π/∂aj
to each Aj that Ai uses as input (we call these Aj parents  and Ai a child of Aj). Once all of
Ai’s children have updated  it will have all of the information required to compute ∂π/∂θi. Though
an improvement upon the naive message passing scheme  backpropagation remains biologically
implausible because it would require rapid transmission of information backwards along the axon 
which has not been observed [8  28]. However  gradient descent remains one of the most frequently
used methods. For example  Rivest et al. [16] use gradient descent to update a modular actor  and
are forced to assume that certain derivatives are always one in order to maintain realistic locality
constraints.
This raises the question: could each module update given only local information that does not in-
clude explicit knowledge of ∂π/∂θi? We assume that a critic exists that broadcasts the TD error  so
a module’s local information would consist of its input xi  which is not necessarily a Markov state
representation  its output ai  and the TD error. Though this has been achieved for tasks with imme-
diate rewards [3  26  27]  we are not aware of any such methods for tasks with delayed rewards. In
this paper we present a class of algorithms  called policy gradient coagent networks (PGCNs)  that
do exactly this: they allow modules to update given only local information.
PGCNs are also a viable technique for non-biological reinforcement learning applications in which
∂π/∂θ is prohibitively difﬁcult to compute. For example  consider an artiﬁcial neural network where
the output of each neuron follows some probability distribution over the reals. Though this would
allow for exploration at every level  rather than just at the level of primitive actions of the output
layer  expressions for π(s  a  θ) would require a nested integral for every node and ∂π/∂θ would be
difﬁcult to compute or approximate for networks with many neurons and layers. Because PGCNs
do not require knowledge of ∂π/∂θ  they remain simple even in such cases  making them a practical
choice for complex parameterized policies.

2 Background

An MDP is a tuple M = (S A P R  ds0)  where S and A are the sets of possible states and actions
respectively  P gives state transition probabilities: P(s  a  s(cid:48)) = Pr(st+1=s(cid:48)|st=s  at=a)  where t
is the current time step  R(s  a) = E[rt|st=s  at=a] is the expected reward when taking action a in
state s  and ds0(s) = Pr(s0=s). An agent A with time-variant parameters θt ∈ Θ (typically function
approximator weights  learning rates  etc.) observes the current state st  selects an action  at  based
on st and θt  which is used to update the state according to P. It then observes the resulting state 
st+1  receives uniformly bounded reward rt according to R  and updates its parameters to θt+1.
A policy is a mapping from states to probabilities of selecting each possible action. A’s policy π
may be parameterized by a vector  θ  such that π(s  a  θ) = Pr(at=a|st=s  θt=θ). We assume that
∂π(s  a  θ)/∂θ exists for all s  a  and θ. Let dθ
M (s) denote the stationary distribution over states

2

under the policy induced by θ. We can then write the average reward for θ as

(cid:34)T−1(cid:88)

t=0

(cid:35)

(cid:12)(cid:12)(cid:12)M  θ

rt

.

(1)

JM (θ) = lim
T→∞

1
T

E

∞(cid:88)

t=1

V θ
M (s) =

The state-value function  which maps states to the difference between the average reward and the
expected reward if the agent follows the policy induced by θ starting in the provided state  is

Lastly  we deﬁne the TD error to be δt = rt − JM (θ) + V θ

E[rt − J(θ)|M  s0 = s  θ].
M (st+1) − V θ

M (st).

(2)

2.1 Policy Gradient

One approach to improving a policy for an MDP is to adjust the parameters θ to ascend the policy
gradient  ∇θJM (θ). For reviews of policy gradient methods  see [5  15  24]. A common variable
in policy gradient methods is the compatible features  ψsa = ∇θ log π(s  a  θ). Bhatnagar et al. [5]
showed that δtψsa is an unbiased estimate of ∇θJM (θ) if s ∼ dθ
M (·) and a ∼ π(s ·  θ). This results
in a simple actor-critic algorithm  which we reproduce from [5]:

ˆJt+1 =(1 − cαt) ˆJt + cαtrt

δt =rt − ˆJt+1 + vt · φ(st+1) − vt · φ(st)

(3)
(4)
(5)
(6)
where ˆJ is a scalar estimate of J  δt remains the scalar TD error  φ is any function taking S to a
feature space for linear value function approximation  v is a vector of weights for the approximation
v · φ(s) ≈ V θ

M (s)  c is a constant  and αt and βt are learning rate schedules such that

vt+1 =vt + αtδtφ(st)
θt+1 =θt + βtδtψstat 

∞(cid:88)

∞(cid:88)

∞(cid:88)

αt =

βt = ∞ 

(α2

t + β2

t ) < ∞  αt = o(βt).

(7)

t=0

t=0

t=0

(cid:101)∇θJM (θ) = G(θ)−1∇θJM (θ) 

One example of such a schedule would be αt = ααc
βC +t  for some constants
α  αC  β  and βC. We call this algorithm the vanilla actor-critic (VAC). Bhatnagar et al. [5] show
that under certain mild assumptions and in the limit as t → ∞  VAC will converge to a θt that is
within a small neighborhood of a local maximum of JM (θ).
Some more advanced actor-critic methods ascend the natural policy gradient [1  5  15] 

αc+t2/3 and βt = ββC

(8)
M (·) a∼π(s · θ)[∇θ log π(s  a  θ)∇θ log π(s  a  θ)T ] is the Fisher information
where G(θ) = Es∼dθ
matrix of the policy. To help differentiate between the two types of policy gradients  we refer to
the non-natural policy gradient as the vanilla policy gradient hereafter. One view of the natural
gradient is that it corrects for the skewing of the vanilla gradient that is induced by a particular
parameterization of the policy [2]. Empirical studies have found that ascending the natural gradient
results in faster convergence [1  5  15]. One algorithm for ascending the natural policy gradient is
the Natural-Gradient Actor-Critic with Advantage Parameters [5]  which we abbreviate as NAC
and use in our case study.
VAC and NAC have a property  which we reference later as Property 1  that is common to almost all
other actor-critic methods: if the policy is a function of x = f (s)  for any f  such that π(s  a  θ) can
be written as π(x  a  θ) or π(f (s)  a  θ)  then updates to the policy parameters θ are independent of
s given x  a  and δt. For example  if s = (s1  s2) and f (s) = s1 so that the policy is a function of
only s1  then the update to θ requires knowledge of only s1  a  and δt  and not s2. This is one crucial
property will allow the actor to update given only local information.
VAC and NAC  as well as all other algorithms referenced  require computation of ∇θ log π(s  a  θ).
Hence  none of these methods allow for local updates to modular policies  which makes them un-
desirable from a biological standpoint  and impractical for policies for which this derivative is pro-
hibitively difﬁcult to compute. However  by combining these methods with the CoMDP framework
reviewed in Section 2.2 and by taking advantage of Property 1  the updates to the actor can be
modiﬁed to satisfy the locality constraint.

3

2.2 Conjugate Markov Decision Processes

j Aj  where(cid:81)

deﬁning its policy. We deﬁne ¯θi = (cid:83)
number of other agents: si ∈ S ×(cid:81)

In this section we review the aspects of the conjugate Markov decision process (CoMDP) framework
that are relevant to this work. Though Thomas and Barto [25] present the CoMDP framework for the
discounted reward setting with ﬁnite state  action  and reward spaces  the extension to the average
reward and inﬁnite setting used here is straightforward. To solve M  one may create a network of
agents A1  A2  . . .   An  where Ai has output ai ∈ Ai  where Ai is any space  though typically the
reals or integers. All agents receive the same reward. We focus on the case where Ai = {Ai  Ci}
are all actor critics  i.e.  they contain an actor  Ai  and a critic  Ci. The action at ∈ A for M is
computed as at ∼ Γ(s  a1  a2  . . .   an)  for some distribution Γ. Each agent Ai has parameters θi
j∈{1 2 ... n}−{i} θj to be the parameters of all agents other
than Ai. Each agent takes as input si  which contains the state of M and the outputs of an arbitrary
j Aj is the Cartesian product of the output sets
of all the Aj whose output is used as input to Ai. Notice that si are not the components of s  but
rather s is the state of M  while si is the input to Ai. We require the graph with nodes for each Ai
and a directed edge from Ai to Aj if Aj takes ai as part of its input  to be acyclic. Thus  the network
of agents must be feed-forward  so we can assume an ordering of Ai such that if aj is part of si 
then j < i. When executing the modular policy  the policies of the Ai can be executed in this order
so that all requisite information for computing a module’s output is always available. Thomas and
Barto [25] call each Ai a coagent and the entire network a coagent network.
An agent Ai may treat the rest of the network and M as its environment  where it sees states si
t and
(cid:81)
t+1. This en-
takes actions ai
vironment is called a conjugate Markov decision process (CoMDP)  which is an MDP M i = (S ×
Pr(cid:8)si
j Aj is the state space  Ai is the action space  P i(si  ai  ˆsi) =
j Aj Ai P i Ri  di

) where S ×(cid:81)
t = ai  M  ¯θi(cid:9)  Ri(si  ai) = E(cid:2)rt|si

t resulting in reward rt (the same for all Ai) and a transition to state si

t = ai  M  ¯θi(cid:3) gives the ex-

t+1 = ˆsi|si

t = si  ai

t = si  ai

s0 is the distribution over initial states of M i.
pected reward when taking action a in state s  and di
We write πi(si  ai  θi) to denote Ai’s policy for M i. Notice that M i depends on ¯θi. Thus  as the
policies of other coagents change  so too does the CoMDP with which Ai interacts. While [25]
considers generic methods for handling this nonstationarity  we focus on the special case in which
all Ai are policy gradient methods.
Theorem 3 of [25] states that the policy gradient of M can be decomposed into the policy gradients
for all of the CoMDPs  M i:
∂JM (θ1  θ2  . . .   θn)

∂JM (θ1  θ2  . . .   θn)

∂JM (θ1  θ2  . . .   θn)

(cid:21)

s0

∂[θ1  θ2  . . .   θn]

(cid:20) ∂JM (θ1  θ2  . . .   θn)
(cid:20) ∂JM 1(θ1)

∂θ1

∂JM 2(θ2)

 

 

∂θ1

∂θ2

=

=

(cid:21)

∂θ2

  . . .  

∂JM n (θn)

∂θn

  . . .  

∂θn

.

(9)

Thus  if each coagent computes and follows the policy gradient based on the local environment that
it sees  the coagent network will follow its policy gradient on M.
Thomas and Barto [25] also show that the value functions for M and all the CoMDPs are the same
for all st  if the additional state components of M i are drawn according to the modular policy:

(10)

M (st).

V θ1
t ) = V θ2
M 1(s1
M (st+1) − V θ

t ) = . . . = V θn

M 2 (s2
t ) = V θ
M (st) = rt − JM i(θi) + V θi

M n (sn

The state-value based TD error is therefore the same as well:

t+1) − V θi

t) ∀i.

δt = rt − JM (θ) + V θ

M i (si

M i(si

(11)
This means that  if the coagents require δt  we can maintain a global critic  C  that keeps an estimate
of V θ
M   which can be used to replace every Ci by computing δt and broadcasting it to each Ai.
Because all Ai share a global critic  C  all that remains of each module is the actor Ai. We therefore
refer to each Ai as a module.
Notice that the CoMDPs  M i  and thus the coagents  Ai  have S as part of their state space. This is
required for M i to remain Markov. However  if the actor’s policy is a function of some xi = f (si)
for any f  i.e.  the policy can be written as πi(xi  ai  θi)  then  by Property 1  updates to the actor’s
policy require only the TD error  ai  and xi. Hence  the full Markovian state representation is only
needed by the global critic  C. The modules  Ai  will be able to perform their updates given only
their input: the xi portion of the state of M i.

4

3 Methods

The CoMDP framework tells us that  if each module is an actor that computes the policy gradient for
its local environment (CoMDP)  then the entire modular actor will ascend its policy gradient. Actor-
critics satisfying Property 1 are able to perform their policy updates given only local information:
the policy’s input xt  the most recent action at  and the TD error δt. Combining these two  each
module Ai can compute its update given only its local input xi
t  and the TD
error δt. We call any network of coagents  each using policy gradient methods  a policy gradient
coagent network (PGCN). One PGCN is the vanilla coagent network (VCN)  which uses VAC for
all modules (coagents)  and maintains a global critic that computes and broadcasts δt. The VCN
xi ai = ∇θi log πi(xi  ai  θi) are the
algorithm is depicted diagramatically in Figure 2  where ψi
is an unbiased estimate of the policy
compatible features for the ith module. Notice that δtψi
tai
xi
t
gradient for M i [5]  which is an unbiased estimate of part of the policy gradient for M by Equation
9.

t  most recent action ai

Figure 2: Diagram of the vanilla coagent network (VCN) algorithm. The global critic observes
st  rt  st+1 tuples  updates its estimate ˆJ of the average reward  which it uses to compute the TD
error δt  which is then broadcast to all of the modules  Ai. Lastly  it updates the parameters  v  of its
t) and then computes updates
state-value estimate. Each module Ai draws its actions from πi(xi
to θi given its input xi

t  and the TD error  δt  which was broadcast by the global critic.

t  action ai

t ·  θi

t   a2

t and then at =
To implement VCN  observe the current state st  compute the module outputs ai
t ). This action will result in a transition to st+1 with reward rt. Given st  rt  and
Γ(st  a1
st+1 the global critic can execute to produce δt  which can then be used to train each module Ai.
Notice that the Ai can update concurrently. This process then repeats.

t   . . .   an

4 The Decomposed Natural Policy Gradient

Another interesting PGCN  which we call a natural coagent network (NCN)  would use coagents
that ascend the natural policy gradient  e.g.  NAC. However  Equation 9 does not hold for natural

(cid:104)(cid:101)∇θ1JM 1(θ1) (cid:101)∇θ2 JM 2(θ2)  . . .  (cid:101)∇θn JM n (θn)

gradients: (cid:101)∇θJM (θ) (cid:54)=
where θ =(cid:8)θ1  θ2  . . .   θn(cid:9) and (cid:98)∇θJM (θ) is an estimate of the natural policy gradient that we call
not follow the natural policy gradient  but rather (cid:98)∇θJM (θ) = (cid:98)G(θ)−1∇θJM (θ)  an approximation
thereto  where (cid:98)G(θ) is an approximation of G(θ)  constructed by:

the decomposed natural policy gradient  which has an implicit dependence on how θ is partitioned
into n components. Hence  a PGCN  where each module computes its natural policy gradient  would

(cid:105) ≡ (cid:98)∇θJM (θ) 

(12)

(cid:26)

(cid:98)G(θ)ij =

0
G(θk)ij

if the i and jth elements of θ are in different modules
if the i and jth elements of θ are both in module Ak

 

where G(θk) is the Fisher information matrix of the kth module’s policy:

G(θk) = Esk∼dθk

M k (·) ak∼πk(xk · θk)[∇θk log πk(xk  ak  θk)∇θk log πk(xk  ak  θk)T ] 

where G(θk)ij in Equation 13 denotes the entry corresponding to the i and jth elements of θ  which
are elements of θk.
The decomposed natural policy gradient is intuitively a trade-off between the natural policy gradient
and the vanilla policy gradient depending on the granularity of modularization. For example  if the

5

(13)

(14)

AiGlobal Critic1(1)tttttJcJcrAtiiii1  tttsrstitxita1()ttttt11tttttttrJvsvs1tttttvvs1Act:  Train:iittiiiitttiiittttxaax  iitttxapolicy is one module  A1  and Γ(s  a1) = a1  then the decomposed natural policy gradient is triv-
ially the same as the natural policy gradient. On the other hand  as the policy is broken into more
and more modules  the gradient begins to differ more and more from the natural policy gradient 
because the structure of the modular policy begins to inﬂuence the direction of the gradient. With

ﬁner granularity  (cid:98)G(θ) will tend to a diagonal approximation of the identity matrix. If the modular
(cid:98)G(θ)−1 = I  in which case the decomposed natural policy gradient will be equivalent to the vanilla

actor contains one parameter per module and the module inputs are normalized  it is possible for

policy gradient. Hence  the more coarse the modularization (fewer modules)  the closer the decom-
posed natural policy gradient is to the natural policy gradient  while the ﬁner the modularization
(more modules)  the closer the decomposed natural policy gradient may come to the vanilla policy
gradient.
Each term of the decomposed natural policy gradient is within ninety degrees of the vanilla policy
gradient  so a system will converge to a local optimum if it follows the decomposed natural policy
gradient and the step size is decayed appropriately.

5 Variance of Gradient Estimates
Let ψs a i = ∇θi log π(s  a  θ) be the components of ψs a that correspond to the parameters of Ai.
xi ai  the update to the parameters of Ai by VCN  and δtψs a i  the update by VAC  are
Both δtψi
unbiased estimates of ∇θiJM i(θi) = ∇θiJM (θ). This means that E[δtψs a i] = E[δtψi
xi ai]  which
is particularly interesting because δt is the same for both  so the only difference between the two
are the compatible features used. Whereas ψs a i requires computation of the derivative of the entire
modular policy  π  ψi
xi ai only requires differentiation of πi. Thus  the latter satisﬁes the locality
constraint  and is also easier to compute. However  this beneﬁt comes at the cost of higher variance.
This increase in variance appears regardless of the actor-critic method used. In this section we focus
on VAC due to its simplicity  though the argument that stochasticity in the CoMDP is the root cause
of the variance of gradient estimates carries over to PGCNs using other actor-critic methods as well.
This increase in variance has also been observed in multi-agent reinforcement learning research as
additional stochasticity in one agent’s environment when another explores [18].
Consider using VAC on any MDP. Bhatnagar et al. [5] show that E[δt|st = s  at = a  M  θ] can
be viewed as the advantage of taking action at in state st over following the policy induced by
θ. If it is positive  it means taking at in st is better than following π. If it is negative  then at is
worse. So  following E[δtψst at] increases the likelihood of at if it is advantageous  and decreases
the likelihood of at if it is disadvantageous. However  our updates use samples rather than the
expected value  so an action at that is actually worse could  due to stochasticity in the environment 
result in a TD error that suggests it is advantageous. Thus  the gradient estimates are inﬂuenced by
the stochasticity of the transition function P and reward function R. If P or R is very stochastic 
the same s  a pair will result in seemingly random TD errors  which manifests as large variance in
δtψst at samples.
Now consider the stochasticity in M and M i. The state transitions of M i depend not only on
M’s transition function  but may also depend on the actions selected by some or all Aj  j (cid:54)= i.
Consider the modular actor from Figure 1 in the case where the transitions and rewards of M are
deterministic. The transition function for M 3  the CoMDP for A3  remains relatively deterministic
because its actions completely determine the transitions of M. We therefore expect the variance in
the gradient estimate for the parameters of A3 to be only slightly higher for VCN than it is for VAC.
However  the actions of A1 and A2 inﬂuence the transitions of M indirectly through the actions
of A3  which adds a layer of stochasticity to their transition functions. We therefore expect policy
gradient estimates for their parameters to have higher variance. In summary  the stochasticity in the
CoMDPs is responsible for VCN’s policy gradient estimates having higher variance than those of
VAC.
We performed a simple study using the modular actor from Figure 1 on a 10 × 10 gridworld
with deterministic actions {up  down  lef t  right}  a reward of −1 for all transitions  factored
state (¯x  ¯y)  and with a terminal state at (10  10). For the modular actor  A1 = A2 = {0  1} 
A3 = {up  down  lef t  right}  A1 and A2 both received the full state (¯x  ¯y)  and all modules used

6

(a)

(b)

Figure 3: (a) Variance of the VAC and VCN updates for weights in each of the three modules. (b)
Variance of updates using VCN with various ε. Standard error bars are provided (n = 100).

linear function approximation rather than a tabular state representation. All modules also used soft-
max action selection:

πi(xi  a  θi) =

(cid:80)

a·xi
eτ θi
ˆa∈Ai eτ θi

ˆa·xi  

(15)

where τ is a constant scaling the amount of exploration  and where the parameters θi for the ith
a for each action a ∈ Ai. The critic is common to both methods 
module contain a weight vector θi
and our goal is not to compare methods for value function approximation  so we used a tabular critic.
With all actor weights ﬁxed and selected randomly with uniform distribution from (−1  1)  we ﬁrst
observed that the mean of the updates δtψst at i and δtψi
xi ai are approximately equal  as expected 
and then computed the variance of both updates. The results are shown in Figure 3(a). As predicted 
the variance of the gradient estimates for each parameter of A1 and A2 is larger for VCN  though
the variance of the gradient estimate for each parameter of A3 is similar for VCN and VAC.

6 Variance Mitigation

To mitigate the increase in the variance of gradient estimates  we observe that  in general  the addi-
tional variance due to the other modules can be completely removed for a module Ai if every other
module is made to be deterministic. This is not practical because every module must explore in order
to learn. However  we can approximate it by decreasing the exploration of each module  making its
policy less stochastic and more greedy. For example  every module could take a deterministic greedy
action without performing any updates with probability 1 − ε for some ε ∈ [0  1). With probability
ε the module would act using softmax action selection and update its parameters. As ε → 0  the
probability of two modules exploring simultaneously goes to zero  decreasing the variance in M i
but also decreasing the percent of time steps during which each module trains. When ε = 1  every
module explores and updates on every step  so the algorithm is the original PGCN algorithm (VCN
if using VAC for each module).
We repeated the gridworld study of the variance in gradient estimates for various ε. The results 
shown in Figure 3(b)  show that smaller ε can be effective in reducing the variance of gradient
estimates. Notice that VCN using ε = 1 is equivalent to VCN as described previously  so the
points for ε = 1 in Figure 3(b) correspond exactly to the VCN data in Figure 3(a). Thus  if the
variance in gradient estimates precludes learning  we suggest making the policies of the modules
more deterministic by decreasing exploration and increasing exploitation.
Several questions remain. First  though the variance decreases  the amount of exploration also de-
creases  so what is the net effect on learning speed? Second  how does PGCN compare to an actor-
critic where ∇θπ(s  a  θ) is known? Lastly  is there a signiﬁcant loss in performance when using the
decomposed natural policy gradient as opposed to the true natural policy gradient? We attempt to
answer these questions in the following section.

7

0.160.180.2010.120.140.16ance0.060.080.1VariaVACVCN00.020.04A1A2A3Module0.160.20.12ance0.08VariaA1A2A300.04A300.20.40.60.81εAlgorithm

VAC
VCN
NAC
NCN

α
0.75
0.25
0.5
0.5

β
0.25
0.1
0.1
0.1

c

0.13
0.04
0.02
0.02

τ12
0.5
0.1
0.05
0.05

τ3 Average Reward
2.5
3.5
1
1

−23.13
−29.15
−24.91
−28.32

Standard Error

0.09
0.09
0.08
0.14

Table 1: Best parameters found for each algorithm. The average reward per episode and standard
error are computed using 10000 samples (each a lifetime of 75 episodes). The optimization tested
each parameter set for 300 lifetimes  so the best parameters found still occasionally perform poorly.
We found the above parameters to perform poorly (average reward less than −200) approximately
one in 500 lifetimes. These outliers were removed for the average reward calculations. Random
policy parameters average less than −5000 reward per episode.

7 Case Study

In this section we compare the learning speed of VAC  VCN  NAC  and NCN. Our goal is to de-
termine whether VCN and NCN perform similarly to VAC and NAC  which are established meth-
ods [6]  even though VCN and NCN’s modules do not have access to ∂π/∂θi. To perform a thorough
analysis  we again use the modular actor depicted in Figure 1  as in Section 5. We therefore require
a problem with a simple optimal policy. We select the gridworld from Section 5  and again use a
tabular critic in order to focus on the difference in policy improvements. To decrease the size of the
parameter space  we did not decay α nor β. For all four algorithms  we performed a grid search
for the α  β  c  τ12  and τ3 that maximize the average reward over 75 episodes  where τ12 is the τ
used by A1 and A2  while τ3 is that of A3. The best parameters are provided in Table 1. Recall
that the increased variance in VCN updates arises because A1 and A2’s actions only inﬂuence the
transitions of M indirectly through the actions of A3. Though decreased exploration is beneﬁcial in
general  for this particular modular policy it is therefore particularly important that A3’s exploration
be decreased by increasing τ3. The optimization does just this  balancing the trade-off between
exploration and the variance of gradient estimates by selecting larger τ3 for VCN than VAC. The
mean ratio τ3/τ12 for the top 25 of the 202300 parameters tested was 5.48 for VAC and 31.04 for
VCN  further emphasizing the relatively smaller exploration of A3. For NAC and NCN  the explo-
ration parameters are identical  suggesting that the additional variance of gradient estimates was not
signiﬁcant. This is likely due to the policy gradient estimates being ﬁltered before being used.
The average rewards during a lifetime are similar  suggesting that  even though the variance of
gradient estimates can be orders larger for VCN with τ12 = τ3 = 1 (Figure 3(a))  exploration can
be tuned such that learning speed is not signiﬁcantly diminished.

8 Conclusion

We have devised a class of algorithms  policy gradient coagent networks (PGCNs)  and two spe-
ciﬁc instantiations thereof  the natural coagent network (NCN) and vanilla coagent network (VCN) 
which allow modules within an actor to update given only local information. We show that the
NCN ascends the decomposed natural policy gradient  an approximation to the natural policy gra-
dient  while VCN ascends the vanilla policy gradient. We discussed the theoretical properties of
both the decomposed natural policy gradient and the increase in the variance of gradient estimates
when using PGCNs. Lastly  we presented a case study to compare NCN and VCN to two existing
actor-critic methods  NAC and VAC. We showed that  even though NAC and VAC are provided with
additional non-local information  VCN and NCN perform comparably. We point out how VCN’s
similar performance is achieved by decreasing exploration in order to decrease the stochasticity of
each module’s CoMDP  and thus the variance of the gradient estimates.

Acknowledgements

We would like to thank Scott Kuindersma  Scott Niekum  Bruno Castro da Silva  Andrew Barto 
Sridhar Mahadevan  the members of the Autonomous Learning Laboratory  and the reviewers for
their feedback and contributions to this paper.

8

References
[1] S. Amari. Natural gradient works efﬁciently in learning. Neural Computation  10(2):251–276  1998.
[2] S. Amari and S. Douglas. Why natural gradient? In Proceedings of the 1998 IEEE International Confer-

ence on Acoustics  Speech  and Signal Processing (ICASSP ’98)  volume 2  pages 1213–1216  1998.

[3] A. G. Barto. Learning by statistical cooperation of self-interested neuron-like computing elements. Hu-

man Neurobiology  4:229–256  1985.

[4] A. G. Barto. Adaptive critics and the basal ganglia. Models of Information Processing in the Basal

Ganglia  pages 215–232  1995.

[5] S. Bhatnagar  R. S. Sutton  M. Ghavamzadeh  and M. Lee. Natural actor-critic algorithms. Automatica 

45(11):2471–2482  2009.

[6] S. Bhatnagar  R. S. Sutton  M. Ghavamzadeh  and M. Lee. Natural actor-critic algorithms. Technical

Report TR09-10  University of Alberta Department of Computing Science  June 2009.

[7] A. Claridge-Chang  R. Roorda  E. Vrontou  L. Sjulson  H. Li  J. Hirsh  and G. Miesenbock. Writing

memories with light-addressable reinforcement circuitry. Cell  193(2):405–415  2009.

[8] F. H. C. Crick. The recent excitement about neural networks. Nature  337:129–132  1989.
[9] N. Daw and K. Doya. The computational neurobiology of learning and reward. Current Opinion in

Neurobiology  16:199–204  2006.

[10] K. Doya. What are the computations of the cerebellum  the basal ganglia and the cerebral cortex? Neural

Networks  12:961–974  1999.

[11] K. Doya. Reinforcement learning in continuous time and space. Neural Computation  12(1):219–245 

2000.

[12] M. J. Frank and E. D. Claus. Anatomy of a decision: Striato-orbitofrontal interactions in reinforcement

learning  decision making  and reversal. Psychological Review  113(2):300–326  2006.

[13] E. Ludvig  R. Sutton  and E. Kehoe. Stimulus representation and the timing of reward-prediction errors

in models of the dopamine system. Neural Computation  20:3034–3035  2008.

[14] R. C. O’Reilly. The LEABRA model of neural interactions and learning in the neocortex. PhD thesis 

Carnegie Mellon University.

[15] J. Peters and S. Schaal. Natural actor critic. Neurocomputing  71:1180–1190  2008.
[16] F. Rivest  Y. Bengio  and J. Kalaska. Brain inspired reinforcement learning.

Information Processing Systems  pages 1129–1136  2005.

In Advances in Neural

[17] D. E. Rumelhart and J. L. McClelland. Parallel distributed processing. Volume 1: Foundations. MIT

Press  Cambridge  MA  1986.

[18] T. W. Sandholm and R. H. Crites. Multiagent reinforcement learning in the iterated prisoner’s dilemma.

Biosystems  37:147–166  1996.

[19] W. Schultz  P. Dayan  and P. Montague. A neural substrate of prediction and reward. Science  275:1593–

1599  1992.

[20] A. Stocco  C. Lebiere  and J. Anderson. Conditional routing of information to the cortex: A model of the

basal ganglia’s role in cognitive coordination. Psychological Review  117(2):541–574  2010.

[21] R. Sutton. Learning to predict by the methods of temporal differences. Machine Learning  3:9–44  1988.
[22] R. Sutton and A. Barto. Toward a modern theory of adaptive networks: Expectation and prediction.

Psychological Review  88:135–140  1981.

[23] R. Sutton and A. Barto. Reinforcement learning: An introduction. MIT Press  Cambridge  MA  1998.
[24] R. Sutton  D. McAllester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement learning
with function approximation. In Advances in Neural Information Processing Systems 12  pages 1057–
1063  2000.

[25] P. Thomas and A. Barto. Conjugate Markov decision processes. In Proceedings of the Twenty-Eighth

International Conference on Machine Learning  2011.

[26] R. J. Williams. A class of gradient-estimating algorithms for reinforcement learning in neural networks.

In Proceedings of the IEEE First International Conference on Neural Networks  1987.

[27] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.

Machine Learning  8(3):229–256  1992.

[28] D. Zipser and R. A. Andersen. A back propagation programmed network that simulates response proper-

ties of a subset of posterior parietal neurons. Nature  331:679–684  1988.

9

,Qian Wang
Jiaxing Zhang
Sen Song
Zheng Zhang
Guruprasad Raghavan
Matt Thomson