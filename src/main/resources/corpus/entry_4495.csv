2012,Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging,Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods  however  neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is  higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity  geodesics)  supervised (correlation in errors)  or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \succeq 0  one can impose a desired covariance structure on mixing coefficient selection  and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem  where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here  our new model outperforms the state of the art (p-values << 10−3 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity).,Q-MKL: Matrix-induced Regularization in
Multi-Kernel Learning with Applications to

Neuroimaging∗

Chris Hinrichs†‡ Vikas Singh†‡

Jiming Peng§

Sterling C. Johnson†‡

†University of Wisconsin

Madison  WI

§University of Illinois
Urbana-Champaign  IL

‡Geriatric Research Education & Clinical Center

Wm. S. Middleton Memorial VA Hospital  Madison  WI

{ hinrichs@cs  vsingh@biostat  scj@medicine }.wisc.edu

pengj@illinois.edu

Abstract

Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one
simultaneously trains a linear classiﬁer and chooses an optimal combination of
given base kernels. Model complexity is typically controlled using various norm
regularizations on the base kernel mixing coefﬁcients. Existing methods neither
regularize nor exploit potentially useful information pertaining to how kernels in
the input set ‘interact’; that is  higher order kernel-pair relationships that can be
easily obtained via unsupervised (similarity  geodesics)  supervised (correlation
in errors)  or domain knowledge driven mechanisms (which features were used
to construct the kernel?). We show that by substituting the norm penalty with an
arbitrary quadratic function Q (cid:23) 0  one can impose a desired covariance struc-
ture on mixing weights  and use this as an inductive bias when learning the con-
cept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm
MKL objectives. We explore the model’s utility via experiments on a challeng-
ing Neuroimaging problem  where the goal is to predict a subject’s conversion to
Alzheimer’s Disease (AD) by exploiting aggregate information from many dis-
tinct imaging modalities. Here  our new model outperforms the state of the art
(p-values (cid:28) 10−3). We brieﬂy discuss ramiﬁcations in terms of learning bounds
(Rademacher complexity).

Introduction

1
Kernel learning methods (such as Support Vector Machines) are conceptually simple  strongly rooted
in statistical learning theory  and can often be formulated as a convex optimization problem. As a
result  SVMs have come to dominate the landscape of supervised learning applications in bioinfor-
matics  computer vision  neuroimaging  and many other domains. A standard SVM-based ‘learning
system’ may be conveniently thought of as a composition of two modules [1  2  3  4]: (1) Feature
pre-processing  and (2) a core learning algorithm. The design of a kernel (feature pre-processing)
may involve using different sets of extracted features  dimensionality reductions  or parameteriza-
tions of the kernel functions. Each of these alternatives produces a distinct kernel matrix. While
much research has focused on efﬁcient methods for the latter (i.e.  support vector learning) step 
speciﬁc choices of feature pre-processing are frequently a dominant factor in the system’s overall
performance as well  and may involve signiﬁcant user effort. Multi-kernel learning [5  6  7] trans-
fers a part of this burden from the user to the algorithm. Rather than selecting a single kernel  MKL
offers the ﬂexibility of specifying a large set of kernels corresponding to the many options (i.e.  ker-
nels) available  and additively combining them to construct an optimized  data-driven Reproducing
∗Supported by NIH (R01AG040396)  (R01AG021155); NSF (RI 1116584)  (DMS 09-15240 ARRA) 
and (CMMI-1131690); Wisconsin Partnership Proposal; UW ADRC; UW ICTR (1UL1RR025011); AFOSR
(FA9550-09-1-0098); and NLM (5T15LM007359). The authors would like to thank Maxwell Collins and
Sangkyun Lee for many helpful discussions.

1

Kernel Hilbert Space (RKHS) – while simultaneously ﬁnding a max-margin classiﬁer. MKL has
turned out to be very successful in many applications: on several important Vision problems (such
as image categorization)  some of the best known results on community benchmarks come from
MKL-type methods [8  9]. In the context of our primary motivating application  the current state of
the art in multi-modality neuroimaging-based Alzheimer’s Disease (AD) prediction [10] is achieved
by multi-kernel methods [3  4]  where each imaging modality spawns a kernel  or set of kernels.
In allowing the user to specify an arbitrary number of base kernels for combination MKL provides
more expressive power  but this comes with the responsibility to regularize the kernel mixing coef-
ﬁcients so that the classiﬁer generalizes well. While the importance of this regularization cannot be
overstated  it is also a fact that commonly used (cid:96)p norm regularizers operate on kernels separately 
without explicitly acknowledging dependencies and interactions among them. To see how such de-
pendencies can arise in practice  consider our neuroimaging learning problem of interest: the task
of learning to predict the onset of AD. A set of base kernels K1  . . .   KM are derived from sev-
eral different medical imaging modalities (MRI; PET)  image processing methods (morphometric;
anatomical modelling)  and kernel functions (linear; RBF). Some features may be shared between
kernels  or kernel functions may use similar parameters. As a result we expect the kernels’ behaviors
to exhibit some correlational  or other cluster structure according to how they were constructed. (See
Fig. 2 (a) and related text  for a concrete discussion of these behaviors in our problem of interest.)
We will denote this relationship as Q ∈ RM×M .
Ideally  the regularizer should reﬂect these dependencies encoded by Q  as they can signiﬁcantly
impact the learning characteristics of a linearly combined kernel. Some extensions work at the level
of group membership (e.g.  [11])  but do not explicitly quantify these interactions. Instead  rather
than penalizing covariances or inducing sparsity among groups of kernels  it may be beneﬁcial to
reward such covariances  so as to better reﬂect a latent cluster structure between kernels. In this
paper  we show that a rich class of regularization schemes are possible under a new MKL formulation
which regularizes on Q directly – the model allows one to exploit domain knowledge (as above) and
statistical measures of interaction between kernels  employ estimated error covariances in ways that
are not possible with (cid:96)p-norm regularization  or  encourage sparsity  group sparsity or non-sparsity
as needed – all within a convex optimization framework. We call this form of multi-kernel learning 
Q-norm MKL or “Q-MKL”. This paper makes the following contributions: (a) presents our new
Q-MKL model which generalizes 1- (and 2-) norm MKL models  (b) provides a learning theoretic
result showing that Q-MKL can improve MKL’s generalization error rate  (c) develops efﬁcient
optimization strategies (to be distributed with the Shogun toolbox)  and (d) provides empirical results
demonstrating statistically signiﬁcant gains in accuracy on the important AD prediction problem.
Background. The development of MKL methods began with [5]  which showed that the problem
of learning the right kernel for an input problem instance could be formulated as a Semi-Deﬁnite
Program (SDP). Subsequent papers have focused on designing more efﬁcient optimization methods 
which have enabled its applications to large-scale problem domains. To this end  the model in [5]
was shown to be solvable as a Second Order Cone Program [12]  a Semi-Inﬁnite Linear Program
[6]  and via gradient descent methods in the dual and primal [7  13]. More recently  efforts have
focused on generalizing MKL to arbitrary p-norm regularizers where p > 1 [13  14] while main-
taining overall efﬁciency. In [14]  the authors brieﬂy mentioned that more general norms may be
possible  but this issue was not further examined. A non-linear “hyperkernel” method was proposed
[15] which implicitly maps the kernels themselves to an implicit RKHS  however this method is
computationally very demanding  (it has 4th order interactions among training examples). The au-
thors of [16] proposed to ﬁrst select the sub-kernel weights by minimizing an objective function
derived from Normalized Cuts  and subsequently train an SVM on the combined kernel. In [17  2] 
a method was proposed for selecting an optimal ﬁnite combination from an inﬁnite parameter space
of kernels. Contemporary to these results  [18] showed that if a large number of kernels had a desir-
able shared structure (e.g.  followed directed acyclic dependencies)  extensions of MKL could still
be applied. Recently in [8]  a set of base classiﬁers were ﬁrst trained using each kernel and were
then boosted to produce a strong multi-class classiﬁer. At this time  MKL methods [8  9] provide
some of the best known accuracy on image categorization datasets such as Caltech101/256 (see
www.robots.ox.ac.uk/˜vgg/software/MKL/). Next  we describe in detail the motiva-
tion and theoretical properties of Q-MKL .

2

(cid:32) M(cid:88)

(cid:32) M(cid:88)

(cid:33)

(cid:33)

2 From MKL to Q-MKL
MKL Models. Adding kernels corresponds to taking a direct sum of Reproducing Kernel Hilbert
spaces (RKHS)  and scaling a kernel by a constant c scales the axes of it’s RKHS by
c. In the
MKL setting  the SVM margin regularizer 1
over
contributions from RKHS’s H1  . . .  HM   where the vector of mixing coefﬁcients β scales each
respective RKHS [14]. A norm penalty on β ensures that the units in which the margin is measured
are meaningful (provided the base kernels are normalized). The MKL primal problem is given as

2(cid:107)w(cid:107)2 becomes a weighted sum 1

(cid:80)M

(cid:107)wm(cid:107)2Hm

√

m=1

βm

2

min

w b β≥0 ξ≥0

1
2

(cid:107)wm(cid:107)2Hm

βm

+ C

M(cid:88)

m

n(cid:88)

ξi + (cid:107)β(cid:107)2

p

s.t. yi

(cid:104)wm  φm(xi)(cid:105)Hm + b

≥ 1 − ξi 

(1)

i

m

where φm(x) is the (potentially unknown) transformation from the original data space to the mth
RKHS Hm. As in SVMs  we turn to the dual problem to see the role of kernels:
(cid:107)G(cid:107)q  G ∈ RM ; Gm = (α ◦ y)T Km(α ◦ y) 

(2)

max
0≤α≤C

αT 1 − 1
2

p + 1

where ◦ denotes element-wise multiplication  and the dual q-norm follows the identity 1
q = 1.
Note that the primal norm penalty (cid:107)β(cid:107)2
p becomes a dual-norm on the vector G. At optimality 
wm = βm(α ◦ y)T φm(X)  and so the term Gm = (α ◦ y)T Km(α ◦ y) =
is the vector
of scaled classiﬁer norms. This shows that the dual norm is tied to how MKL measures the margin
in each RKHS.
The Q-MKL model. The key characteristic of Q-MKL is that the standard (squared) (cid:96)p-norm
penalty on β  along with the corresponding dual-norm penalty in (2)  is substituted with a more
general class of quadratic penalty functions  expressed as βT Qβ = (cid:107)β(cid:107)2
Mahalanobis (matrix-induced) norm so long as Q (cid:23) 0. In this framework  the burden of choosing
a kernel is deferred to a choice of Q-function. This approach gives the algorithm greater ﬂexibility
while controlling model complexity  as we will discuss shortly. The model we optimize is 

Q. (cid:107)β(cid:107)Q =(cid:112)βT Qβ is a

(cid:107)wm(cid:107)2Hm

β2
m

n(cid:88)

||wm||2Hm

M(cid:88)
(cid:112)GT Q−1G.

βm

m

min

w b β≥0 ξ≥0

1
2

+ C

ξi + βT Qβ s.t. yi

i

m

(cid:104)wm  φm(xi)(cid:105)Hm + b

≥ 1 − ξi 

(3)

where the last objective term provides a bias relative to βT Qβ. The dual problem becomes
maxα αT 1 − 1
It is easy to see that if Q = 1M×M   we obtain the p = 1 form
of (1)  i.e.  1-norm MKL  as a special case because βT 1M×M β = (cid:107)β(cid:107)2
1. On the other hand  setting
Q to IM×M (identity)  reduces to 2-norm MKL.

2

3 The case for Q-MKL
Extending the MKL regularizer to arbitrary quadratics Q (cid:23) 0 signiﬁcantly expands the richness
of the MKL framework; yet we can show that for reasonable choices of Q  this actually decreases
MKL’s learning-theoretic complexity. Joachims et al. [19] derived a theoretical generalization error
bound on kernel combinations which depends on the degree of redundancy between support vectors
in SVMs trained on base kernels individually. Using this type of correlational structure  we can
derive a Q function between kernels to automatically select a combination of kernels which will
maximize this bound. This type of Q function can be shown to have lower Rademacher complexity 
(see below ) while simultaneously decreasing the error bound from [19]  which does not directly
depend on Rademacher complexity.

3.1 Virtual Kernels  Rademacher Complexity and Renyi Entropy
If we decompose Q into its component eigen-vectors  we can see that each eigen-vector deﬁnes
a linear combination of kernels. This observation allows us to analyze Q-MKL in terms of these
objects  which we will refer to as Virtual Kernels. We ﬁrst show that as Q−1’s eigen-values decay 
so do the traces of the virtual kernels. Assuming Q−1 has a bounded  non-uniform spectrum  this
property can then be used to analyze  (and bound)  Q-MKL’s Rademacher complexity  which has
been shown to depend on the traces of the base kernels. We then offer a few observations on how
Q−1’s Renyi entropy [20] relates to these learning theoretic bounds.

3

(cid:88)

Virtual Kernels. In the following  assume that Q (cid:31) 0  and has eigen-decomposition Q = V ΛV  
with V = {v1 ···   vM}. First  observe that because Q’s eigen-vectors provide an orthonormal basis
of RM   β ∈ RM can be expressed as a linear combination in this basis with γ as its coefﬁcients:

i γivi = V γ. Substituting in βT Qβ we have

β =(cid:80)

βT Qβ = (γT V T )V ΛV T (V γ) = γT (V T V )Λ(V T V )γ = γT Λγ =

γ2
i λi

(4)

i

RKHS. This can be ensured by choosing Q in a speciﬁc way  if desired. This leads to the following
result:

This simple observation offers an alternate view of what Q-MKL is actually optimizing. Each
eigen-vector vi of Q can be used to deﬁne a linear combination of kernels  which we will refer to as

virtual kernel(cid:101)Ki =(cid:80)
m vi(m)Km. Note that if(cid:101)Ki (cid:23) 0  ∀ i  then they each deﬁne an independent
Lemma 1. If (cid:101)Ki (cid:23) 0 ∀i  then Q-MKL is equivalent to 2-norm MKL using virtual kernels instead
= (cid:80)M
i µi(cid:101)Ki  where (cid:101)Ki =
2(cid:80)M

(cid:80)M
i γivi(m)Km = (cid:80)M
m βmKm
m vi(m)Km is the ith virtual kernel. The learned kernel K∗ is a weighted combination

λ− 1
of virtual kernels  and the coefﬁcients are regularized under a squared 2-norm.

2(cid:80)M
m vi(m)Km = (cid:80)M

4) and K∗ = (cid:80)

Then βT Qβ = (cid:107)µ(cid:107)2
2 

Proof. Let µi = γi

of base kernels.

i µiλ− 1

(eq.

λi.

√

m

n

(5)

p) ≤

p + 1

q = 1.

(cid:112)η0q(cid:107)u(cid:107)q

Rademacher Complexity in MKL. With this result in hand  we can now evaluate the Rademacher
complexity of Q-MKL by using a recent result for p-norm MKL. We ﬁrst state a theorem from [21] 
which relates the Rademacher complexity of MKL to the traces of its base kernels.
Theorem 1. ([21]) The empirical Rademacher complexity on a sample set S of size n  with M base
22 ) 
kernels is given as follows (with η0 = 23
RS(HM
where u = [Tr(K1) ···   Tr(KM )]T and 1
The bound in (5) shows that the Rademacher complexity RS(·) depends on (cid:107)u(cid:107)q  a norm on the base
kernels’ traces. Assuming they are normalized to have unit trace  the bound for p = q = 2-norm
MKL is governed by (cid:107)u(cid:107)2 =
M. However  in Q-MKL the virtual kernels traces are not equal 
. With this expression for the traces of the virtual kernels 
we can now prove that the bound given in (5) is strictly decreased as long as the eigen-values ψi of
Q−1 are in the range (0  1]. (Adding 1 to the diagonal of Q is sufﬁcient to guarantee this.)

and are in fact given by Tr((cid:101)Ki) = 1T vi√
Theorem 2. If Q−1 (cid:54)= IM×M and (cid:101)Ki (cid:23) 0 ∀i then the bound on Rademacher complexity given in
kernel traces  decreases. As shown above  the virtual kernel traces are given as Tr((cid:101)Ki) =

(5) is strictly lower for Q-MKL than for 2-norm MKL.
Proof. By Lemma 1  we have that the bound in (5) will decrease if (cid:107)u(cid:107)2  the norm on the virtual
ψi1T vi 
meaning that (cid:107)u(cid:107)2
i 1 = 1T Q−11. Clearly  this sum is maxi-
mal for ψi = 1  ∀i  which is true if and only if Q−1 = IM×M . This means that when Q (cid:54)= IM×M  
then the bound in (5) is strictly decreased.

i ψi(1T vi)2 =(cid:80)N

2 =(cid:80)N

i ψi1T vivT

√

√

λi

Note that requiring the virtual kernels to be p.s.d.  while achievable (see supplements ) is somewhat
restrictive. In practice  such a Q matrix may not differ substantially from IN×N . We therefore
provide the following result which frees us from this restriction  and has more practical signiﬁcance.
Theorem 3. Q-MKL is equivalent to the following model:

M(cid:88)
(cid:32) M(cid:88)

m

(cid:107)wm(cid:107)2Vm

µm

min

w b µ ξ≥0

1
2

s.t. yi

n(cid:88)

i

+ C

ξi + (cid:107)µ(cid:107)2

2

(cid:33)

(cid:104)wm  φm(xi)(cid:105)Vm + b

≥ 1 − ξi  Q

2 µ ≥ 0 
− 1

(6)

where φm() is the feature transform mapping data space to the mth virtual kernel  denoted as Vm.

m

4

While the virtual kernels themselves may be indeﬁnite  recall that µ = Q 1
2 β  and so the constraint
2 µ ≥ 0 is equivalent to β ≥ 0  guaranteeing that the combined kernel will be p.s.d. This
Q− 1
formulation is slightly different than the 2-norm MKL formulation  however it does not alter the
theoretical guarantee of [21]  providing a stronger result.
Renyi Entropy. Renyi entropy [20] signiﬁcantly generalizes the usual notion of Shannon entropy
[22  23  24]  has applications in Statistics and many other ﬁelds  and has recently been proposed as
an alternative to PCA [22]. Thm. 2 points to an intuitive explanation of where the beneﬁt from a
Q regularizer comes from as well  if we analyze the Renyi entropy of the distribution on kernels
deﬁned by Q−1  if we treat Q−1 as a kernel density estimator. The quadratic Renyi entropy of a
probability measure is given as 

(cid:90)

H(p) = − log

p2(x)dx.

virtual kernel traces to the Renyi entropy estimator of Q−1 as(cid:82) ˆp2(x)dx = 1

Now  if we use a kernel function (i.e.  Q−1)  and a ﬁnite sample (i.e.  base kernels)  as a kernel
density estimator  (cf. [15] ) then with some normalization we can derive an estimate of the un-
derlying probability ˆp  which is a distribution over base kernels. We can then interpret its Renyi
entropy as a complexity measure on the space of combined kernels. Eq. (5.2) in [23] relates the
N 2 1T Q−11 1 which
leads to a nice connection to Thm. 2. This view informs us that setting Q−1 = IM×M   (i.e.  2-norm
MKL)  has maximal Renyi entropy because it is maximally uninformative; adding structure to Q−1
concentrates ˆp  reducing both its Renyi entropy  and Rademacher complexity together.
This series of results suggests an entirely new approach to analyzing the Rademacher complexity of
MKL methods. The proof of Thm. 2 relies on decreasing a norm on the virtual kernel traces  which
we now see directly relates to the Renyi entropy of Q−1  as well as with decreasing the Rademacher
complexity of the search space of combined kernels. It is even possible that by directly analyzing
Renyi entropy in a multi-kernel setting  this conjecture may be useful in deriving analogous bounds
in  e.g.  Indeﬁnite Kernel Learning [25]  because the virtual kernels are indeﬁnite in general.

3.2 Special Cases: Q-SVM and relative margin
Before describing our optimization strategy  we discuss several variations on the Q-MKL model.
Q-SVM. An interesting special case of Q-MKL is Q-SVM  which generalizes several recent  (but
independently developed ) models in the literature [26  27]. If the base kernels are rank-1  (i.e. 
singleton features ) then each coefﬁcient βm effectively becomes a feature weight  and a 2-norm
penalty on β is a penalty on weights. Q-MKL therefore reduces to a form of SVM in which (cid:107)w(cid:107)2
becomes wT Qw. Thus  in such cases we can reduce the Q-MKL model to a simple QP  which we
call Q-SVM . Please refer to the supplements for details  and some experimental results.
Relative Margin. Several interesting extensions to the SVM and MKL frameworks have been
proposed which focus on the relative margin methods [28  29] which maximize the margin relative
to the spread of the data. In particular Q-MKL can be easily modiﬁed to incorporate the Relative
Margin Machine (RMM) model [28] by replacing Module 1 as in (7) with the RMM objective. Our
alternating optimization approach  (described next ) is not affected by this addition; however  the
additional constraints would mean that SMO-based strategies would not be applicable.

4 Optimization
We now present the core engine to solve (3). Most MKL implementations make use of an alternating
minimization strategy which ﬁrst minimizes the objective in terms of the SVM parameters  and
then with respect to the sub-kernel weights β. Since the MKL problem is convex  this method
leads to global convergence [7  14] and minor modiﬁcations to standard SVM implementations are
sufﬁcient. Q-MKL generalizes (cid:107)β(cid:107)2
p to arbitrary convex quadratic functions  while the feasible set
is the same as for MKL. This directly gives that the Q-MKL model in (3) is convex. We will broadly
follow this strategy  but as will become clear shortly  interaction between sub-kernel weights makes
the optimization of β more involved (than [6  14])  and requires alternative solution mechanisms.
We may consider this process as a composition of two modules: one which solves for SVM dual
parameters (α) with ﬁxed β  and the other for solving for β with ﬁxed α:

1Note that this involves a Gaussian assumption  but [24] provides extensions to non-Gauss kernels.

5

(Module 1)

αT 1 − αT Y KY α s.t.αT y = 0

(7)

max
0≤α≤C

min
β≥0

(cid:88)

m

(Module 2)
(cid:107)wm(cid:107)2
βm

s.t.βT Qβ ≤ 1

(8)

Using a result from [14] we can replace the βT Qβ objective term with a quadratic constraint  which
gives the problem in (8). Notice that (8) has a sum of ratios with optimization variables in the de-
nominator  while the constraint is quadratic – this means that standard convex optimization toolkits
may not be able to solve this problem without signiﬁcant reformulation from its canonical form in
(8). Our approach is to search for a stationary point by representing the gradient as a non-linear
system. Writing the gradient in terms of the Lagrange multiplier δ  and setting it equal to 0 gives:

(cid:107)wm(cid:107)2Hm

β2
m

− δ(Qβ)m = 0  ∀m ∈ {1 ···   M}.

(9)

We now seek to eliminate δ so that the non-linear system will be limited to quadratic terms in
β  allowing us to use a non-linear system solver. Let W = Diag((cid:107)w1(cid:107)2H1
)  and
β−2 = (β−2
M ). We can then write Wβ−2 = δ(Qβ). Now  solving for β (on the right hand
side) gives

  . . .  (cid:107)wM(cid:107)2HM

1   . . .   β−2

(10)
Because Q (cid:31) 0  and β ≥ 0  at optimality the constraint βT Qβ ≤ 1 must be active. So  we can
plug in the above identity to solve for δ 

β =

Q

−1Wβ

−2

1
δ

(cid:19)T

(cid:18) 1
δ =(cid:112)(Wβ−2)T Q−1(Wβ−2) = (cid:107)Wβ

(cid:18) 1

−1Wβ

−1Wβ

1 =

−2

Q

Q

Q

δ

δ

−2

(cid:19)
−2(cid:107)Q−1  

(11)
which shows that δ effectively normalizes Wβ−2 according to Q−1. We can now solve (10) in
terms of β using a nonlinear root ﬁnder  such as the GNU Scientiﬁc Library; in practice this method
is quite efﬁcient  typically requiring 10 to 20 outer iterations. Putting these parts together  we can
propose following algorithm for optimizing Q-MKL:

Algorithm 1. Q-MKL

Input: Kernels {K1 ···   KM}; Q (cid:23) 0 ∈ RM×M ; labels y ∈ {±1}N .
Outputs: α  b  β
β(0) = 1
while not optimal do
m β(t)

K (t) ←(cid:80)

M ; t = 0 (iterations)

m Km

α(t)  b(t) ← SVM(K (t)  C  y) (Module 1  (7))
Wmm = α(t)T K (t)
β(t+1) ← arg min (Problem(8)) (Module 2  (8))
t = t + 1

m α(t)(β(t)

m )2

end while

4.1 Convergence
We can show that our model can be solved optimally by noting that Module 2 can be precisely
optimized at each step. If Module 2 cannot be solved precisely  then Algorithm 1 may not converge.
The following result assures us that indeed Module 2 can be solved precisely by reducing it to a
convex Semi-Deﬁnite Program (SDP).
Theorem 4. The solution to Problem (8) is the same as the solution to the following SDP:

min

ν≥0 β≥0 Z∈RM×M

wT ν

(cid:20) νm

(cid:21)

(cid:20) 1

(cid:21)

subject to

βT
Z
Proof. The ﬁrst PSD constraint (13) requires that νm = β−1
m   meaning that objective (12) is the
same as that of Problem (8). From the second we have Z = ββT   and so Tr(QZ) = βT Qβ;
therefore the feasible sets are equivalent.

Tr(QZ) ≤ 1.

(cid:23) 0  ∀m

(cid:23) 0 

1
βm

(13)

β

1

(12)

6

(a)

(b)

(c)

(d)

Figure 1: Comparison of spatial smoothness of weights chosen by Q-SVM and SVM with gray matter (GM)
density maps. Left (a-b): weights given by a standard SVM; Right (c-d): weights given by Q-SVM .

The last PSD constraint is only necessary to ensure that βT Qβ ≤ 1  and can be replaced with that
quadratic constraint. Doing so yields a Second-Order Cone Program (SOCP) which is also amenable
to standard solvers. Note that it is not necessary to solve for β as an SDP  though it may nevertheless
be an effective solution mechanism  depending on the size and characteristics of the problem.

5 Experiments
We performed extensive experiments to validate Q-MKL  examine the effect it has on β  and to
assess its advantages in the context of our motivating neuroimaging application.
In these main
experiments  we demonstrate how domain knowledge can be adapted to improve the algorithm’s
performance. Our focus on a practical application is intended as a demonstration of how domain
knowledge can be seamlessly incorporated into a learning model  giving signiﬁcant gains in accu-
racy. We also performed experiments on the UCI repositories  which are described in detail in the
supplements. Brieﬂy  in these experiments Q-MKL performed as well as  or better than  1- and
2-norm MKL on most datasets  showing that even in the absence of signiﬁcant domain knowledge 
Q-MKL can still perform about as well as existing MKL methods.
Image preprocessing. In out main experiments we used brain scans of AD patients and Cognitively
Normal healthy controls (CN) from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) [30]
in a set of cross-validation experiments. ADNI is a landmark study sponsored by the NIH  major
pharmaceuticals and others to determine the extent to which multi-modal brain imaging can help pre-
dict on-set  and monitor progression of  AD. To this end  MKL-type methods have already deﬁned
the state of the art for this application [3  4]. For our experiments  48 AD subjects and 66 controls
were chosen who had both T1-weighted MR scans and Fluoro-Deoxy-Glucose PET (FDG-PET)
scans at two time-points two years apart. Standard diffeomorphic methods  known generally as
Voxel-Based Morphometry (VBM)  (see SPM  www.fil.ion.ucl.ac.uk/spm/) were used
to register scans to a common template and calculate Gray Matter (GM) densities at each voxel in
the MR scans. We also used Tensor-Based Morphometry (TBM) to calculate maps of longitudinal
voxel-wise expansion or contraction over a two year period. Feature selection was performed sep-
arately in each set of images by sorting voxels by t-statistic (calculated using training data)  and
choosing the highest 2000  5000  10000 . . .  250000 voxels in 8 stages. We used linear  quadratic 
and Gaussian kernels: a total of 24 kernels per set  (GM maps  TBM maps  baseline FDG-PET 
FDG-PET at 2-year follow up) for a total of 96 kernels. For Q-matrix we used the Laplacian of
covariance between single-kernel α parameters  (recall the motivation from [19] in Section 3 ) plus
a block-diagonal representing clusters of kernels derived from the same imaging modalities.

5.1 Spatial SVM
Before describing out main experiments  we ﬁrst return to the Q-SVM model brieﬂy mentioned
in 3.2. To demonstrate that Q-regularizers indeed inﬂuence the learned classiﬁer  we performed
classiﬁcation experiments with the Laplacian of the inverse distance between voxels as a Q matrix 
and voxel-wise GM density (VBM) as features. Using 10-fold cross-validation with 10 realizations 
Q-SVM ’s accuracy was 0.819  compared to the regular SVM’s accuracy of 0.792. These accuracies
are signiﬁcantly different at the α = 0.0005 level under a paired t-test.
In Fig. 1 we show a
comparison of weights trained by a regular SVM (a–b)  and those trained by a spatially regularized
SVM  (c–d). Note the greater spatial smoothness  and lower incidence of isolated “pockets”.

7

Acc.
0.864
0.875
0.875
0.884
0.884
0.888

Spec.
0.931
0.936
0.938
0.942
0.955
0.956

Covα

Lap.(Covα)

Lap.(Covα) + diag

Sens.
0.771
0.790
0.789
0.780
0.785
0.786

Regularizer
(cid:107)β(cid:107)1-MKL
(cid:107)β(cid:107)1.5-MKL
(cid:107)β(cid:107)2-MKL

Table 1: Comparison of Q-MKL & MKL. Bold
numerals indicate methods not differing from the
best at the 0.01 level using a paired t-test. Lap. =
“Laplacian”; diag = “Block-diagonal”.

5.2 Multi-modality Alzheimer’s disease (AD) prediction
Next  we performed multi-modality AD prediction
experiments using all 96 kernels across two modal-
ities: MR provides structural information  while
FDG-PET assesses hypo-metabolism. Further  we
may use several image processing pipelines. Due
to the inherent similarities in how the various ker-
nels are derived  there are clear cluster structures /
behaviors among the kernels  which we would like
to exploit using Q-MKL. We used 10-fold cross-
validation with 30 realizations  for a total of 300
folds. Accuracy  sensitivity and speciﬁcity were av-
eraged over all folds. For comparison we also exam-
ined 1-  1.5-  and 2-norm MKL. As MKL methods
have emerged as the state of the art in this domain [3  4]  and have performed favorably in exten-
sive evaluations against various baselines such as single-kernel methods  and na¨ıve combinations 
we therefore focus our analysis on comparison with existing MKL methods. Results are shown in
Table 1. Q-MKL had the highest performance overall  reducing the error rate from 12.5% to 11.2%.
(Signiﬁcant at the α = 0.001 level.) Note that the in vivo diagnostic error rate for AD is believed to
be near 8–10%  meaning that this improvement is quite signiﬁcant. The primary beneﬁt of current
sparse MKL methods is that they effectively ﬁlter out uninformative or noisy kernels  however  the
kernels used in these experiments are all derived from clinically relevant neuroimaging data  and are
thus highly reliable. Q-MKL’s performance suggests that it boosts the overall accuracy.
Virtual kernel analysis. We next turn to an analysis of the covariance structures found in the data
empirically as a concrete demonstration of the type of patterns towards which the Q-MKL regular-
izer biases β. Recall that Q’s eigen-vectors can show which patterns are encouraged or deterred 
in proportion to their eigen-values. In Fig. 2  we compare the Q matrix used in the ADNI exper-
iments  based on the correlations of single-kernel α parameters (a)  the 3 least eigenvectors of its
graph Laplacian (b–d)  and the β vector optimized by Q-MKL . In (a)  we can see that while the
VBM (ﬁrst block of 24 kernels) and TBM (second block of kernels) are highly correlated  they ap-
pear to be fairly uncorrelated to one another. The FDG-PET kernels (last 48 kernels) are much more
strongly interrelated. Interestingly  the ﬁrst eigenvector is almost entirely devoted to two large blocks
of kernels – those which come from MRI data  and those which come from FDG-PET data. The
positive elements in the off-diagonal encourage sparsity within these two super-blocks of kernels.
Somewhat to the contrary  the next two eigenvecors have negative weights in the region between
TBM and VBM kernels  encouraging non-sparsity between these two blocks. In (e) we see that the
optimized β discards most TBM kernels  (but not all ) putting the strongest weight on a few VBM
kernels  and keeps a wider distribution of the FDG-PET kernels.
Conclusion. MKL is an elegant method for aggregating multiple data views  and is being exten-
sively adopted for a variety of problems in machine learning  computer vision  and neuroimaging.
Q-MKL extends this framework to exploit higher order interactions between kernels using super-
vised  unsupervised  or domain-knowledge driven measures. This ﬂexibility can impart greater
control over how the model utilizes cluster structure among kernels  and effectively encourage can-
cellation of errors wherever possible. We have presented a convex optimization model to efﬁciently
solve the resultant model  and shown experiments on a challenging problem of identifying AD
based on multi modal brain imaging data (obtaining statistically signiﬁcant improvements). Our im-
plementation will be made available within the Shogun toolbox (www.shogun-toolbox.org).

(a)

(b)

(c)

(d)

(e)

Figure 2: Cov. Q used in AD experiments (a); three least graph Laplacian eigen-vectors (b-d); outer product
of optimized β (e). Note the block structure in (a–d) relating to the imaging modalities and kernel functions.

8

References
[1] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. JMLR  3:1157–1182  2003.
[2] P. V. Gehler and S. Nowozin. Let the kernel ﬁgure it out; principled learning of pre-processing for kernel

classiﬁers. CVPR  2009.

[3] C. Hinrichs  V. Singh  G. Xu  and S.C. Johnson. Predictive markers for AD in a multi-modality frame-

work: An analysis of MCI progression in the ADNI population. Neuroimage  55(2):574–589  2011.

[4] D. Zhang  Y. Wang  L. Zhou  H. Yuan  and D. Shen. Multimodal Classiﬁcation of Alzheimer’s Disease

and Mild Cognitive Impairment. NeuroImage  55(3):856–867  2011.

[5] G. R. G. Lanckriet  N. Cristianini  P. Bartlett  L. El Ghaoui  and M. Jordan. Learning the kernel matrix

with semideﬁnite programming. JMLR  5:27–72  2004.

[6] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large scale multiple kernel learning. JMLR 

7:1531–1565  2006.

[7] A. Rakotomamonjy  F. Bach  S. Canu  and Y. Grandvalet. SimpleMKL. JMLR  9:2491–2521  2008.
[8] P. V. Gehler and S. Nowozin. On feature combination for multiclass object classiﬁcation. In ICCV  2009.
[9] J. Yang  Y. Li  Y. Tian  L. Duan  and W. Gao. Group-sensitive multiple kernel learning for object catego-

rization. In ICCV  2009.

[10] P. Vemuri  J.L. Gunter  M. L. Senjem  J. L. Whitwell  K. Kantarci  D. S. Knopman  et al. Alzheimer’s
disease diagnosis in individual subjects using structural MR images: validation studies. Neuroimage 
39(3):1186–1197  2008.

[11] M. Szafranski  Y. Grandvalet  and A. Rakotomamonjy. Composite kernel learning. Machine learning 

79(1):73–103  2010.

[12] F. R. Bach  G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and the SMO algo-

rithm. In ICML  2004.

[13] F. Orabona  L. Jie  and B. Caputo. Online-Batch Strongly Convex Multi Kernel Learning. In CVPR  2010.
[14] M. Kloft  U. Brefeld  S. Sonnenburg  and A. Zien. (cid:96)p-Norm Multiple Kernel Learning. JMLR  12:953–

997  2011.

[15] C.S. Ong  A. Smola  and B. Williamson. Learning the kernel with hyperkernels. JMLR  6:1045–1071 

2005.

[16] L. Mukherjee  V. Singh  J. Peng  and C. Hinrichs. Learning Kernels for variants of Normalized Cuts:

Convex Relaxations and Applications. CVPR  2010.

[17] P. V. Gehler and S. Nowozin. Inﬁnite kernel learning. Technical Report 178  Max-Planck Institute for

Biological Cybernetics  10 2008.

[18] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In NIPS  2008.
[19] T. Joachims  N. Cristianini  and J. Shawe-Taylor. Composite kernels for hypertext categorisation.

ICML  2001.

In

[20] A. Renyi. On measures of entropy and information. In Fourth Berkeley Symposium on Mathematical

Statistics and Probability  pages 547–561  1961.

[21] C. Cortes  M. Mohri  and A. Rostamizadeh. Generalization bounds for learning kernels. In ICML  2010.
[22] R. Jenssen. Kernel entropy component analysis. IEEE Trans. PAMI  pages 847–860  2009.
[23] M. Girolami. Orthogonal series density estimation and the kernel eigenvalue problem. Neural Computa-

tion  14(3):669–688  2002.

[24] D. Erdogmus and J.C. Principe. Generalized information potential criterion for adaptive system training.

IEEE Trans. Neural Networks  13(5):1035–1044  2002.

[25] M. Kowalski  M. Szafranski  and L. Ralaivola. Multiple indeﬁnite kernel learning with mixed norm

regularization. In ICML  2009.

[26] S. Bergsma  D. Lin  and D. Schuurmans.

Improved Natural Language Learning via Variance-

Regularization Support Vector Machines. In CoNLL  2010.

[27] R. Cuingnet  M. Chupin  H. Benali  and O. Colliot. Spatial and anatomical regularization of SVM for

brain image analysis. In NIPS  2010.

[28] P. Shivaswamy and T. Jebara. Maximum relative margin and data-dependent regularization. JMLR 

11:747–788  2010.

[29] K. Gai  G. Chen  and C. Zhang. Learning kernels with radiuses of minimum enclosing balls. In NIPS 

2010.

[30] S. G. Mueller  M. W. Weiner  et al. Ways toward an early diagnosis in Alzheimers disease: The

Alzheimer’s Disease Neuroimaging Initiative. J. of the Alzheimer’s Association  1(1):55–66  2005.

9

,Siddartha Ramamohan
Arun Rajkumar
Shivani Agarwal
Shivani Agarwal