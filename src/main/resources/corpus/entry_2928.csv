2011,Learning Higher-Order Graph Structure with Features by Structure Penalty,In discrete undirected graphical models  the conditional independence of node labels Y is specified by the graph structure. We study the case where there is another input random vector X (e.g. observed features) such that the distribution P (Y | X) is determined by functions of X that characterize the (higher-order) interactions among the Y ’s. The main contribution of this paper is to learn the graph structure and the functions conditioned on X at the same time. We prove that discrete undirected graphical models with feature X are equivalent to mul- tivariate discrete models. The reparameterization of the potential functions in graphical models by conditional log odds ratios of the latter offers advantages in representation of the conditional independence structure. The functional spaces can be flexibly determined by kernels. Additionally  we impose a Structure Lasso (SLasso) penalty on groups of functions to learn the graph structure. These groups with overlaps are designed to enforce hierarchical function selection. In this way  we are able to shrink higher order interactions to obtain a sparse graph structure.,Learning Higher-Order Graph Structure with

Features by Structure Penalty

Shilin Ding1∗  Grace Wahba1 2 3∗  and Xiaojin Zhu2∗

Department of {1Statistics  2Computer Sciences  3Biostatistics and Medical Informatics}

University of Wisconsin-Madison  WI 53705

{sding  wahba}@stat.wisc.edu  jerryzhu@cs.wisc.edu

Abstract

In discrete undirected graphical models  the conditional independence of node
labels Y is speciﬁed by the graph structure. We study the case where there is
another input random vector X (e.g. observed features) such that the distribution
P (Y | X) is determined by functions of X that characterize the (higher-order)
interactions among the Y ’s. The main contribution of this paper is to learn the
graph structure and the functions conditioned on X at the same time. We prove
that discrete undirected graphical models with feature X are equivalent to mul-
tivariate discrete models. The reparameterization of the potential functions in
graphical models by conditional log odds ratios of the latter offers advantages
in representation of the conditional independence structure. The functional spaces
can be ﬂexibly determined by kernels. Additionally  we impose a Structure Lasso
(SLasso) penalty on groups of functions to learn the graph structure. These groups
with overlaps are designed to enforce hierarchical function selection. In this way 
we are able to shrink higher order interactions to obtain a sparse graph structure.

1

Introduction

In undirected graphical models (UGMs)  a graph is deﬁned as G = (V  E)  where V = {1  · · ·   K}
is the set of nodes and E ⊂ V × V is the set of edges between the nodes. The graph structure spec-
iﬁes the conditional independence among nodes. Much prior work has focused on graphical model
structure learning without conditioning on X. For instance  Meinshausen and B¨uhlmann [1] and
Peng et al. [2] studied sparse covariance estimation of Gaussian Markov Random Fields. The co-
variance matrix fully determines the dependence structure in the Gaussian distribution. But it is not
the case for non-elliptical distributions  such as the discrete UGMs. Ravikumar et al. [3] and H¨oﬂing
and Tibshirani [4] studied variable selection of Ising models based on l1 penalty. Ising models are
special cases of discrete UGMs with (usually) only pairwise interactions  and without features. We
focused on discrete UGMs with both higher order interactions and features. It is important to note
that the graph structure may change conditioned on different X’s  thus our approach may lead to
better estimates and interpretation.

In addressing the problem of structure learning with features  Liu et al. [5] assumed Gaussian dis-
tributed Y given X  and they partitioned the space of X into bins. Schmidt et al. [6] proposed a
framework to jointly learn pairwise CRFs and parameters with block-l1 regularization. Bradley and
Guestrin [7] learned tree CRF that recovers a max spanning tree of a complete graph based on heuris-
tic pairwise link scores. These methods utilize only pairwise information to scale to large graphs.
The closest work is Schmidt and Murphy [8]  which examined the higher-order graphical structure

∗SD wishes to acknowledge the valuable comments from Stephen J. Wright and Sijian Wang. Research of
SD and GW is supported in part by NIH Grant EY09946  NSF Grant DMS-0906818 and ONR Grant N0014-
09-1-0655. Research of XZ is supported in part by NSF IIS-0953219  IIS-0916038.

1

learning problem without considering features. They used an active set method to learn higher order
interactions in a greedy manner. Their model is over-parameterized  and the hierarchical assumption
is sufﬁcient but not necessary for conditional independence in the graph.

To the best of our knowledge  no previous work addressed the issue of graph structure learning of
all orders while conditioning on input features. Our contributions include a reparemeterization of
UGMs with bivariate outcomes into multivariate Bernoulli (MVB) models. The set of conditional
log odds ratios in MVB models are complete to represent the effects of features on responses and
their interactions at all levels. The sparsity in the set of functions are sufﬁcient and necessary for the
conditional independence in the graph  i.e.  two nodes are conditionally independent iff the pairwise
interaction is constant zero; and the higher order interaction among a subset of nodes means none of
the variables is separable from the others in the joint distribution.

To obtain a sparse graph structure  we impose Structure Lasso (SLasso) penalty on groups of func-
tions with overlaps. SLasso can be viewed as group lasso with overlaps. Group lasso [9] leads to
selection of variables in groups. Jacob et al. [10] considered the penalty on groups with arbitrary
overlaps. Zhao et al. [11] set up the general framework for hierarchical variable selection with over-
lapping groups  which we adopt here for the functions. Our groups are designed to shrink higher
order interactions similar to hierarchical inclusion restriction in Schimdt and Murphy [8]. We give
a proximal linearization algorithm that efﬁciently learns the complete model. Global convergence is
guaranteed [12]. We then propose a greedy search algorithm to scale our method up to large graphs
as the number of parameters grows exponentially.

2 Conditional Independence in Discrete Undirected Graphical Models

In this section  we ﬁrst discuss the relationship between the multivariate Bernoulli (MVB) model
and the UGM whose nodes are binary  i.e. Yi = 0 or 1. At the end  we will give the representation
of the general discrete UGM where Yi takes value in {0  · · ·   m − 1}. In UGMs  the distribution of
multivariate discrete random variables Y1  . . .   YK given X is:

P (Y1 = y1  . . .   YK = yK|X) =

ΦC(yC; X)

(1)

1

Z(X) YC∈C

where Z(X) is the normalization factor. The distribution is factorized according to the cliques in
the graph. A clique C ⊆ Ω = {1  . . .   K} is the set of nodes that are fully connected. ΦC(yC; X) is
the potential function on C  indexed by yC = (yi)i∈C. This factorization follows from the Markov
property: any two nodes not in a clique are conditionally independent given others [13]. So C does
not have to comply with the graph structure  as long as it is sufﬁcient. For example  the most general
choice for any given graph is C = {Ω}. See Theorem 2.1 and Example 2.1 for details.

(a) Graph 1

(b) Graph 2

(c) Graph 3

(d) Graph 4

Figure 1: Graphical model examples.

Given the graph structure  the potential functions characterize the distribution on the graph. But if
the graph is unknown in advance  estimating the potential functions on all possible cliques tends
to be over-parameterized [8]. Furthermore  log ΦC(yC; X) = 0 is sufﬁcient for the conditional
independence among the nodes but not necessary (see Example 2.1). To avoid these problems  we
introduce the MVB model that is equivalent to (1) with binary nodes  i.e. Yi = 0 or 1. The MVB
distribution is:

P (Y1 = y1  . . .   YK = yk|X = x) = exp(cid:8) Xω∈ΨK
= exp(cid:8)y1f 1(x) + · · · + yKf K (x) + · · · + y1y2f 1 2(x) + · · · + y1 . . . yKf 1 ... K (x) − b(f )(cid:9)

yωf ω − b(f )(cid:9)

(2)

2

Here  we use the following notations. Let ΨK be the power set of Ω = {1  . . .   K}  and
use ΨK = ΨK − {∅} to index the 2K − 1 f ω’s in (2). Let ω denotes a set in ΨK  de-

ﬁne Y = (y1  · · ·   yω  · · ·   yΩ) be the augmented response with yω = Qi∈ω yi. And f =

(f 1  . . .   f ω  . . .   f Ω) is the vector of conditional log odds ratios [14]. We assume f ω is in a Repro-
ducing Kernel Hilbert Space (RKHS) Hω with kernel K ω [15]. For example  in our simulation we
choose f ω to be B-spline (see supplementary mateiral). We focus on estimating the set of f ω(x)
with feature x where the sparsity in the set speciﬁes the graph structure.
We present the following lemma and theorem which show the equivalence between UGM and MVB:
Lemma 2.1. In a MVB model  deﬁne the odd-even partition of the power set of ω as: Ψω
odd = {κ ⊆
ω | |κ| = |ω| − k  where k is odd}  and Ψω
even = {κ ⊆ ω | |κ| = |ω| − k  where k is even}. Note
|Ψω

odd| = |Ψω

even| = 2|ω|−1. The following property holds:

f ω = log Qκ∈Ψω
Qκ∈Ψω

even

odd

P (Yi = 1  i ∈ κ; Yj = 0  j ∈ Ω\κ|X)
P (Yi = 1  i ∈ κ; Yj = 0  j ∈ Ω\κ|X)

 

b(f ) = log

Z(x)

QC∈C ΦC(0; x)

(3)

Theorem 2.1. A UGM of the general form (1) with binary nodes is equivalent to a MVB model of
(2). In addition  the following are equivalent: 1) There is no |C|-order interaction in {Yi  i ∈ C};
2) There is no clique C ∈ ΨK in the graph; 3) f ω = 0 for all ω such that C ⊆ ω.

A proof is given in Appendix.
It states that there is a clique C in the graph  iff there is ω ⊇
C  f ω 6= 0 in MVB model. The advantage of modeling by MVB is that the sparsity in f ω’s is
sufﬁcient and necessary for the conditional independence in the graph  thus fully specifying the
graph structure. Speciﬁcally  Yi  Yj are conditionally independent iff f ω = 0  ω ⊇ {i  j}. This
showed the interaction is non-zero iff all the nodes involved are not conditionally independent.
Example 2.1. When K = 2  Ω = {1  2}  C = {Ω}  denote ΦΩ(Y1 = 1  Y2 = 1; X) as Φ11
for simplicity  then P (Y1 = 1  Y2 = 1|X) = 1
Z Φ11. Deﬁne Φ10  Φ01  Φ00 similarly  then the
distribution with UGM parameterization is determined. The relation between UGM and MVB is

f 1 = log

Φ10
Φ00

 

f 2 = log

Φ01
Φ00

 

f 1 2 = log

Φ11 · Φ00
Φ01 · Φ10

Note  the independence between Y1 and Y2 implies: f 1 2 = 0 or Φ11 · Φ00 = Φ01 · Φ10. Therefore 
f 1 2 being zero in MVB model is sufﬁcient and necessary for the conditional independence in the
model. On the other hand  log ΦC = 0 is a sufﬁcient condition but not necessary.

The distribution of a general discrete UGM where Yk ∈ {0  · · ·   m − 1} can be extended from (2).
Lemma 2.2. Let V = {1  . . .   m − 1}  yω = (yi)i∈ω  then

P (Y1 = y1  · · ·   YK = yK|X) = exp(cid:8)

Ω

Xω=1 Xv∈V |ω|

I(yω = v)f ω

v − b(f )(cid:9)

(4)

where I is an indicator function and V n is the tensor product of n V ’s. Each f ω is a |V ||ω| vector.

3 Structure Penalty

In many applications  the assumption is that the graph has very few large cliques. Similar to the
hierarchical inclusion restriction in Schmidt and Murphy [8]  we will include a higher order inter-
action only when all its subsets are included. Our model is very ﬂexible in that f ω(x) can be in an
arbitrary RKHS.
Let y(i) = (y1(i)  . . .   yK (i))  x(i) = (x1(i)  . . .   xp(i)) be the ith data point. There are |ΨK| =
2K − 1 functions in total. We ﬁrst consider learning the full model when K is small  and later
propose a greedy search algorithm to scale to large graphs. The penalized log likelihood model is:

min Iλ(f ) = L(f ) + λJ(f ) =

n

Xi=1(cid:16) − Y(i)T f (x(i)) + b(f )(cid:17) + λJ(f )

(5)

3

where L(f ) is the negative log likelihood and J(·) is the structure penalty. The hierarchical assump-
tion is that if there is no interaction on clique C  then all f ω should be zero  for ω ⊇ C. The penalty
is designed to shrink such f ω toward zero. We consider the Structure Lasso (SLasso) penalty guided
by the lattice in Figure 2. The lattice T has 2K − 1 nodes: 1  . . .   ω  . . .   Ω. There is an edge from
ω1 to ω2 if and only if ω1 ⊂ ω2 and |ω1| + 1 = |ω2|. Jenatton et al. [16] discussed how to deﬁne the
groups to achieve different nonzero patterns.

Figure 2: Hierarchical lattice for penalty

Let Tv = {ω ∈ ΨK|v ⊆ ω} be the subgraph rooted at v in T   including all the descendants
of v. Denote f Tv = (f ω)ω∈Tv . All the functions are categorized into groups with overlaps as
Hω where pv is

(T1  . . .   TΩ). The SLasso penalty on the group Tv is: J(f Tv ) = pvqPω∈Tv kf ωk2

the weight for the penalty on Tv  empirically chosen as

|Tv| . Then  the objective is:

1

min

f

Iλ(f ) = L(f ) + λXv

pvsXω∈Tv

kf ωk2

Hω

(6)

The following theorem shows that by minimizing the objective (6)  f ω1 will enter the model before
f ω2 if ω1 ⊂ ω2. That is to say  if f ω1 is zero  there will be no higher order interactions on ω2. It is
an extension of Theorem 1 in Zhao et al. [11] and the proof is given in Appendix.
Theorem 3.1. Objective (6) is convex  thus the minimal is attainable. Let ω1  ω2 ∈ ΨK and ω1 ⊂
ω2. If ˆf is the minimizer of (6) given the observations  that is  0 ∈ ∂Iλ( ˆf ) which is the subgradient
of Iλ at ˆf  then ˆf ω2 = 0 almost surely if ˆf ω1 = 0.
Example 3.1. If K = 3  f = (f 1  f 2  f 3  f 1 2  f 1 3  f 2 3  f 1 2 3). The group at node 1 in Figure 2

is f T1 = (f 1  f 1 2  f 1 3  f 1 2 3) and J(f T1 ) = p1pkf 1k2 + kf 1 2k2 + kf 1 3k2 + kf 1 2 3k2.

4 Parameter Estimation

1 for simplicity. {1} refers to the constant function space  and Hω

In this section  we discuss parameter estimation where the ωth function space is linear as Hω =
1 is a RKHS with a linear
{1} ⊕ Hω
kernel. The functions in Hω have the form f ω(x) = cω
j xj. Its norm is kf ωkHω = kcωk 
p )T ∈ Rp+1 as a vector
where k · k stands for Euclidean l2 norm. Here  we denote cω = (cω
of length p + 1 and c = (cω)ω∈ΨK ∈ R ˜p is the concatenated vector of all parameters of length
˜p = (p + 1) · |ΨK|. Let cTv = (cω)ω∈Tv be a (p + 1) · |T v| vector  then the objective (6) is now:

0 +Pp

0   . . .   cω

j=1 cω

min

c

Iλ(c) = L(c) + λXv

pvkcTv k

(7)

4.1 Estimating the complete model on small graphs

Many applications do not involve a large amount of responses  so it is desirable to learn the complete
model when the graph is small for consistency reasons. We propose a method to optimize (7) of the

4

Algorithm 1 Proximal Linearization Algorithm

Input: c0  α0  ζ > 1  tol > 0
repeat

Choose αk ∈ [αmin  αmax]
Solve Eq (8) for dk = c − ck
while δk = Iλ(ck) − Iλ(ck + dk) < kdkk3 do

// Insufﬁcient decrease
Set αk = max(αmin  ζαk)
Solve Eq (8) for dk

end while
Set αk+1 = αk/ζ
Set ck+1 = ck + dk

until δk < tol

complete model with all interaction levels by iteratively solving the following proximal linearization
problem as discussed in Wright [12]:

min

c

Lk + ∇LT

k (c − ck) +

αk
2

kc − ckk2 + λJ(c)

(8)

where Lk = L(ck)  and αk is a positive scalar chosen adaptively at kth step. With slight abuse
of notation  we denote ck as the value of c at kth step. Algorithm 1 summarized the framework of
solving (7). Following the analysis in Wright [12]  we can ensure that the proximal linearization
algorithm will converge for the negative log-likelihood loss function with the SLasso penalty.

However  solving group lasso with overlaps is not trivial due to the non-smoothness at the singular
point. In recent years  several papers have addressed this problem. Jacob et al. [10] duplicated the
design matrix columns that appear in group overlaps  then solved the problem as group lasso without
overlaps. Kim and Xing [17] reparameterized the group norm with additional dummy variables.
They alternatively optimized the model parameters and the dummy ones at each step. It is efﬁcient
for the quadratic loss function on Gaussian data  but might not scale well in our case. Instead  we
solve (8) by its smooth and convex dual problem [18].The details are in the supplementary material.

4.2 Estimating large graphs

The above algorithm is efﬁcient on small graphs (K < 20). It usually terminates within 20 iterations
in our experiments. However  the issue of estimating a complete model is the exponential number
of f ω’s and the same amount of groups involved in objective (7). It is intractable when the graph
becomes large. The hierarchical assumption and the SLasso penalty lend themselves naturally to a
greedy search algorithm:

1. Start from the set of main effects as A0 = {f 1  · · ·   f K}.
2.

In step i  remove the nodes that are not in Ai from the lattice in Figure 2. Obtain a sparse
i.
estimation of the functions in Ai by algorithm (1). Denote the resulting sparse set A′

3. Let Ai+1 = A′

i. Keep adding a higher order interaction into Ai+1 if all its subsets of

interactions are included in A′

i. And also add this node into the lattice in Figure 2.

Iterate step 2 and 3 until convergence. The algorithm is similar to the active set method in Schmidt
and Murphy [8]. It has multiple runs of algorithm (1) to enforce the hierarchical assumption. It is
not guaranteed to converge to the global optimum. Nonetheless  our empirical experiments show its
ability to scale to large graphs.

5 Experiments

5.1 Toy Data

In the simulation  we create 6 toy graphs. The ﬁrst four graphs are depicted in Figure 1. Graph 5
has 100 nodes where the ﬁrst 8 nodes have the same structure as in Figure 1(c) and the others are
independent. Graph 6 also has 100 nodes where the ﬁrst 10 nodes have the same connection as in
Figure 1(d) and the others are independent. We generate 100 datasets for each structure to evaluate

5

j=1 gω

j (xj) where gω

k=1 cω

0 +P5

j (xj) = PD

the performance. The sample size of each dataset is 1000. Here is how the ﬁrst data set is generated:
The length of the feature vector  p  is set to 5 in our experiment  i.e. X = (X1  . . .   X5). Each
jkBk(xj) is spanned by the B-spline basis
f ω(x) = cω
functions {Bk(·)}k=1 ···  D (see the supplementary material)  where D is chosen to be 5. The true
jk  is uniformly sampled from {−5  −4  · · ·   5}. We set the intercepts
set of the model parameters  cω
0 in main effects to 1  and those in second or higher order interactions to 2. The features  Xj  are
cω
i.i.d uniform on [-1  1]. Then  Y is sampled according to the probability in equation (2).
We use GACV (generalized approximate cross validation) and BGACV (B-type GACV) [19] to
choose the regularization parameter λ for the complete model (graphs 1-4). We call these variants
of SLasso Complete-GACV and Complete-BGACV. We use AIC for greedy search (Greedy-AIC)
in graphs 5 and 6 due to computational consideration. The range of λ is chosen according to Koh
et al. [20]. The details of the tuning methods are discussed in the supplementary material. The R
package  BMN  is used as a baseline [4].

Table 1: Number of true positive and false positive functions

Graph Method

1

2

3

4

5

6

BMN
Complete-GACV
Complete-BGACV
BMN
Complete-GACV
Complete-BGACV
BMN
Complete-GACV
Complete-BGACV
BMN
Complete-GACV
Complete-BGACV
BMN
Greedy-AIC
BMN
Greedy-AIC

f 1 2
60
100
86
44
100
88
72
91
36
48
92
68
38
99
28
100

f 1 3
76
100
83
50
99
91
64
87
22
34
98
68
28
99
26
100

f 2 3
70
100
83
38
100
88
60
81
23
37
94
71
26
98
14
100

f 3 4
60
94
72
58
99
78
60
92
93
29
90
62
22
97
26
99

f 1 2 3

0
84
14
0
83
33
0
62
0
0
54
0
0
22
0
24

f 5 7 8

-
-
-
-
-
-
0
71
39
0
45
0
0
21
0
15

f 5 6 7 8

-
-
-
-
-
-
0
33
0
-
-
-
0
0
-
-

FP
162
136
11
412
341
64
830
412
162
774
693
144
9476
1997
9672
3458

In Table 1  we count  for each function f ω  the number of runs out of 100 where f ω is recovered
(kcωk 6= 0). If a recovered function is in the true model  it is considered a true positive  otherwise a
false positive. The main effects are always detected correctly  thus are not listed in the table. SLasso
is more effective compared to BMN which only considers pairwise interactions.

In Figure 3  we show the learning results in terms of true positive rate (TPR) as sample size increases
from 100 to 1000. The experimental setting is the same as before. The TPRs improve with increas-
ing sample size. GACV achieves better TPR  but higher FPR compared to BGACV. Our method
outperforms BMN in all six graphs.

5.2 Case Study: Census Bureau County Data

We use the county data from U.S. Census Bureau1 to validate our method. We remove the counties
that have missing values and obtain 2668 entries in total. The outcomes of this study are summarized
in Table 2. “Vote” [21] is coded as 1 if the Republican candidate won in the 2004 presidential
election. To dichotomize the remaining outcomes  the national mean is selected as a threshold. The
data is standardized to mean 0 and variance 1. The following features are included: Housing unit
change in percent from 2000-2006  percent of ethnic groups  percent foreign born  percent people
over 65  percent people under 18  percent people with a high school education  percent people
with a bachelors degree; birth rate  death rate  per capita government expenditure in dollars. By
adjusting λ  we observe new interactions enter the model. The graph structure of λ = 0.1559 is

1http://www.census.gov/statab/www/ccdb.html

6

0
.
1

0
.
1

0
.
1

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

GACV
BGACV
BMN

800

1000

200

400

600

Sample Size

(a) Graph 1 (5%)

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

200

400

600

800

1000

Sample Size

(b) Graph 2 (5%)

AIC
BMN

200

400

600

800

1000

Sample Size

(c) Graph 3 (1%)

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

5
.
0

200

400

600

Sample Size

(d) Graph 4 (0.5%)

5
.
0

4
.
0

800

1000

200

5
.
0

4
.
0

800

1000

200

400

600

800

1000

Sample Size

400

600

Sample Size

(e) Graph 5 (< 10−20)

(f) Graph 6 (< 10−20)

Figure 3: The True Positive Rate (TPR) of graph structure learning methods with increasing sam-
ple size. The percentage in the bracket is the upper bound of False Positive Rate (FPR) in each
experiment. BMN always has larger FPR compared to SLasso.

Table 2: Selected response variables

Response Description
Vote
Poverty
VCrime
PCrime
URate
PChange

2004 votes for Republican presidential candidate
Poverty Rate
Violent Crime Rate  eg. murder  robbery
Property Crime Rate  eg. burglary
Unemployment Rate
Population change in percent from 2000 to 2006

Positive%
81.11
52.70
23.09
6.82
51.35
64.96

shown in Figure 4(a). The results of BMN (the tuning parameter is 0.015) is in Figure 4(b). The
unemployment rate plays an important role as a hub as discovered by SLasso  but not by BMN.

(a) SLasso-Complete

(b) BMN

Figure 4: Interactions of response variables in the Census Bureau data. The ﬁrst number on the edge
is the order at which the link is recovered. The number in bracket is the function norm on the clique
and the absolute value of the elements in the concentration matrix  respectively. We note SLasso
discovers at 7th step two third-order interactions which are displayed by two circles in (a).

We analyze the link between “Vote” and “PChange”. Though the marginal correlation between
them (without X) is only 0.0389  which is the second lowest absolute pairwise correlation  the

7

link is ﬁrstly recovered by SLasso. It has been suggested that there is indeed a connection2. This
shows that after taking features into account  the dependence structure of response variables may
change and hidden relations could be discovered. The main factors in this case are “percentage of
housing unit change” (X1) and “population percentage of people over 65” (X2). The part of the
ﬁtted model shown below suggests that as housing units increase  the counties are more likely to
have both positive results for “Vote” and “PChange”. But this tendency will be counteracted by the
increase of people over 65: the responses are less likely to take both positive values.

ˆf V ote = 0.2913 · X1 + 0.3475 · X2 + · · ·
ˆf P Change = 1.4726 · X1 − 0.3709 · X2 + · · ·
ˆf V ote P Change = 0.1358 · X1 − 0.0458 · X2 + · · ·

6 Conclusions

Our SLasso method can learn the graph structure that is speciﬁed by the conditional log odds ratios
conditioned on input features X  which allows the graphical model depending on features. The
modeling interprets well  since f ω = 0 iff there is no such clique. An efﬁcient algorithm is given
to estimate the complete model. A greedy approach is applied when the graph is large. SLasso
can be extended to model a general discrete UGM  where Yk takes value in {0  . . .   m − 1}. Also 
there exist rich selections of the function forms  which makes the model more ﬂexible and powerful 
though modiﬁcation is needed in solving the proximal subproblem for non-parametric families.

A Proof

A.1 Proof of Theorem 2.1

Proof. Given UGM (1)  the corresponding parameterization in MVB model is shown in (3) of
Lemma 2.1. Conversely  given the MVB model of (2)  the cliques can be determined by the nonzero
f ω: clique C exists if C = ω and f ω 6= 0. Then the maximal cliques can be inferred from the
graph structure. And suppose they are C1  . . .   Cm. Let ωi = Ci  for i = 1  . . .   m  and κ1 = ∅ 
κi = Ci ∩ (Ci−1 ∪ · · · ∪ C1)  i = 2  . . .   m. Then the parameterization is:

and Z(x) = exp(b(f ))

(9)

ΦCi (yCi ; x) = exp(cid:0)Sωi(y; x) − Sκi (y; x)(cid:1)

where Sω(y; x) =Pκ⊆ω yκf κ(x). Thus  UGM (1) with bivariate nodes is equivalent to MVB (2).

In the latter part of the theorem  1 ⇒ 2 and 3 ⇒ 1 follow naturally from the Markov property of
graphical models. To show 2 ⇒ 3  let yω
i )i∈C where
C = yκ′
i = 1 if i ∈ ω and yω
C . For
yω
any possible v = κ ∩ C  κ′ ∈ {κ|κ = v ∪ u  s.t. u ⊆ ω − v} will satisfy the condition: κ′ ∩ C = v.
There are 2|ω−v| such κ′ in total due to the choice of u. Also  they appear in the nominator and
denominator of equation (3) equally. So  for any C ∈ C 

C = (yω
i = 0 otherwise. Notice that whenever κ∩C = κ′ ∩C  we have yκ

C be a realization of yC such that yω

It follows that f ω = 0 by (3).

Yκ∈Ψω

even

ΦC(yκ

C; x) = Yκ∈Ψω

odd

ΦC(yκ

C; x)

(10)

A.2 Proof of Theorem 3.1

Proof. We give the proof for the linear case. The convexity of Iλ is easy to check  since L and
J(f Tv ) are all convex in c. Suppose there is some ω2 ⊃ ω1 s.t. ˆcω2 6= 0 and ˆcω1 = 0  by the groups
constructed through Figure 2  kˆcTv k = k(ˆcω)v⊆ωk 6= 0 for all v ⊆ ω1. So the partial derivative of
the objective (7) with respect to cω1 at ˆcω1 is

Thus  the probability of {ˆcω2 6= 0} equals to the probability of { ∂L

2http://www.ipsos-mori.com/researchpublications/researcharchive/2545/Analysis-Population-change-turnout-the-election.aspx

∂L

∂cω1(cid:12)(cid:12)(cid:12)(cid:12)cω1 =ˆcω1

+ λ Xv⊆ω1

pv

ˆcω1
kˆcTv k

= 0

(11)

∂cω1(cid:12)(cid:12)cω1 =ˆcω1 = 0}  which is 0.

8

References

[1] N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the lasso. The

Annals of Statistics  34(3):1436–1462  2006.

[2] J. Peng  P. Wang  N. Zhou  and J. Zhu. Partial correlation estimation by joint sparse regression models.

Journal of the American Statistical Association  104(486):735–746  2009.

[3] P. Ravikumar  M.J. Wainwright  and J. Lafferty. High-dimensional Ising model selection using l1-

regularized logistic regression. Annals of Statistics  38(3):1287–1319  2010.

[4] H. H¨oﬂing and R. Tibshirani. Estimation of sparse binary pairwise markov networks using pseudo-

likelihoods. The Journal of Machine Learning Research  10:883–906  2009.

[5] Han Liu  Xi Chen  John Lafferty  and Larry Wasserman. Graph-valued regression. In J. Lafferty  C. K. I.
Williams  J. Shawe-Taylor  R.S. Zemel  and A. Culotta  editors  Advances in Neural Information Process-
ing Systems 23  pages 1423–1431. 2010.

[6] M. Schmidt  K. Murphy  G. Fung  and R. Rosales. Structure learning in random ﬁelds for heart motion
In IEEE Conference on Computer Vision and Pattern Recognition  pages 1–8 

abnormality detection.
2008.

[7] J.K. Bradley and C. Guestrin. Learning tree conditional random ﬁelds.

International Conference on Machine learning  pages 127–134  2010.

In Proceedings of the 27th

[8] M. Schmidt and K. Murphy. Convex structure learning in log-linear models: Beyond pairwise potentials.
In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2010.
[9] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  2006.

[10] L. Jacob  G. Obozinski  and J.P. Vert. Group Lasso with overlap and graph Lasso. In Proceedings of the

26th Annual International Conference on Machine Learning  pages 433–440  2009.

[11] P. Zhao  G. Rocha  and B. Yu. The composite absolute penalties family for grouped and hierarchical

variable selection. Annals of Statistics  37(6A):3468–3497  2009.

[12] S.J. Wright. Accelerated block-coordinate relaxation for regularized optimization. Technical report 

Department of Computer Science  University of Wisconsin-Madison  2010.

[13] M.J. Wainwright and M.I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends R(cid:13) in Machine Learning  1:1–305  2008.

[14] F. Gao  G. Wahba  R. Klein  and B. Klein. Smoothing Spline ANOVA for multivariate Bernoulli ob-
servations  with application to ophthalmology data. Journal of the American Statistical Association 
96(453):127  2001.

[15] G. Wahba. Spline Models for Observational Data. Society for Industrial Mathematics  1990.
[16] R. Jenatton  J.Y. Audibert  and F. Bach. Structured variable selection with sparsity-inducing norms.

arXiv:0904.3523  2009.

[17] S. Kim and E.P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity.

In
Proceedings of 27th International Conference on Machine Learning  pages 543–550  Haifa  Israel  2010.

[18] J. Liu and J. Ye. Fast overlapping group lasso. arXiv:1009.0306v1  2010.
[19] Xiwen Ma. Penalized Regression in Reproducing Kernel Hilbert Spaces With Randomized Covariate

Data. PhD thesis  Department of Statistics  University of Wisconsin-Madison  2010.

[20] K. Koh  S.J. Kim  and S. Boyd. An interior-point method for large-scale l1-regularized logistic regression.

Journal of Machine learning research  8(8):1519–1555  2007.

[21] R.M. Scammon  A.V. McGillivray  and R. Cook. America Votes 26: 2003-2004  Election Returns By

State. CQ Press  2005.

9

,Maksim Lapin
Matthias Hein
Bernt Schiele
Zihan Li
Matthias Fresacher
Jonathan Scarlett