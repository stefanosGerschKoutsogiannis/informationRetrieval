2018,Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation,It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive  there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work  we move a tiny step towards a theory and better understanding of the representations. Specifically  we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations.  We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches  where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally  we conduct extensive experiments using our algorithms. Experimental results suggest that  surprisingly  representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected  at least in terms of subspace match.,Towards Understanding Learning Representations:
To What Extent Do Different Neural Networks Learn

the Same Representation

Liwei Wang1 2 Lunjia Hu3

Jiayuan Gu1 Yue Wu1

Zhiqiang Hu1 Kun He4

John Hopcroft5

1Key Laboratory of Machine Perception  MOE  School of EECS  Peking University
2Center for Data Science  Peking University  Beijing Institute of Big Data Research

3Computer Science Department  Stanford University
4Huazhong University of Science and Technology

5Cornell University

wanglw@cis.pku.edu.cn

lunjia@stanford.edu

{gujiayuan  frankwu  huzq}@pku.edu.cn

brooklet60@hust.edu.cn  jeh17@cornell.edu

Abstract

It is widely believed that learning good representations is one of the main reasons
for the success of deep neural networks. Although highly intuitive  there is a lack of
theory and systematic approach quantitatively characterizing what representations
do deep neural networks learn.
In this work  we move a tiny step towards a
theory and better understanding of the representations. Speciﬁcally  we study a
simpler problem: How similar are the representations learned by two networks
with identical architecture but trained from different initializations. We develop
a rigorous theory based on the neuron activation subspace match model. The
theory gives a complete characterization of the structure of neuron activation
subspace matches  where the core concepts are maximum match and simple match
which describe the overall and the ﬁnest similarity between sets of neurons in two
networks respectively. We also propose efﬁcient algorithms to ﬁnd the maximum
match and simple matches. Finally  we conduct extensive experiments using our
algorithms. Experimental results suggest that  surprisingly  representations learned
by the same convolutional layers of networks trained from different initializations
are not as similar as prevalently expected  at least in terms of subspace match.

1

Introduction

It is widely believed that learning good representations is one of the main reasons for the success of
deep neural networks [Krizhevsky et al.  2012  He et al.  2016]. Taking CNN as an example  ﬁlters 
shared weights  pooling and composition of layers are all designed to learn good representations of
images. Although highly intuitive  it is still illusive what representations do deep neural networks
learn.
In this work  we move a tiny step towards a theory and a systematic approach that characterize
the representations learned by deep nets.
In particular  we consider a simpler problem: How
similar are the representations learned by two networks with identical architecture but trained from
different initializations. It is observed that training the same neural network from different random
initializations frequently yields similar performance [Dauphin et al.  2014]. A natural question arises:
do the differently-initialized networks learn similar representations as well  or do they learn totally

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

distinct representations  for example describing the same object from different views? Moreover 
what is the granularity of similarity: do the representations exhibit similarity in a local manner 
i.e. a single neuron is similar to a single neuron in another network  or in a distributed manner  i.e.
neurons aggregate to clusters that collectively exhibit similarity? The questions are central to the
understanding of the representations learned by deep neural networks  and may shed light on the
long-standing debate about whether network representations are local or distributed.
Li et al. [2016] studied these questions from an empirical perspective. Their approach breaks down
the concept of similarity into one-to-one mappings  one-to-many mappings and many-to-many
mappings  and probes each kind of mappings by ad-hoc techniques. Speciﬁcally  they applied linear
correlation and mutual information analysis to study one-to-one mappings  and found that some
core representations are shared by differently-initialized networks  but some rare ones are not; they
applied a sparse weighted LASSO model to study one-to-many mappings and found that the whole
correspondence can be decoupled to a series of correspondences between smaller neuron clusters;
and ﬁnally they applied a spectral clustering algorithm to ﬁnd many-to-many mappings.
Although Li et al. [2016] provide interesting insights  their approach is somewhat heuristic  especially
for one-to-many mappings and many-to-many mappings. We argue that a systematic investigation
may deliver a much more thorough comprehension. To this end  we develop a rigorous theory
to study the questions. We begin by modeling the similarity between neurons as the matches of
subspaces spanned by activation vectors of neurons. The activation vector [Raghu et al.  2017]
shows the neuron’s responses over a ﬁnite set of inputs  acting as the representation of a single
neuron.1 Compared with other possible representations such as the weight vector  the activation
vector characterizes the essence of the neuron as an input-output function  and takes into consideration
the input distribution. Further  the representation of a neuron cluster is represented by the subspace
spanned by activation vectors of neurons in the cluster. The subspace representations derive from the
fact that activations of neurons are followed by afﬁne transformations; two neuron clusters whose
activations differ up to an afﬁne transformation are essentially learning the same representations.
In order to develop a thorough understanding of the similarity between clusters of neurons  we give a
complete characterization of the structure of the neuron activation subspace matches. We show the
unique existence of the maximum match  and we prove the Decomposition Theorem: every match
can be decomposed as the union of a set of simple matches  where simple matches are those which
cannot be decomposed any more. The maximum match characterizes the whole similarity  while
simple matches represent minimal units of similarity  collectively giving a complete characterization.
Furthermore  we investigate how to characterize these simple matches so that we can develop efﬁcient
algorithms for ﬁnding them.
Finally  we conduct extensive experiments using our algorithms. We analyze the size of the maximum
match and the distribution of sizes of simple matches. It turns out  contrary to prevalently expected 
representations learned by almost all convolutional layers exhibit very low similarity in terms of
matches. We argue that this observation reﬂects the current understanding of learning representation
is limited.
Our contributions are summarized as follows.

1. We develop a theory based on the neuron activation subspace match model to study the
similarity between representations learned by two networks with identical architecture
but trained from different initializations. We give a complete analysis for the structure of
matches.

2. We propose efﬁcient algorithms for ﬁnding the maximum match and the simple matches 

which are the central concepts in our theory.

3. Experimental results demonstrate that representations learned by most convolutional layers

exhibit low similarity in terms of subspace match.

The rest of the paper is organized as follows. In Section 2 we formally describe the neuron activation
subspace match model. Section 3 will present our theory of neuron activation subspace match. Based
on the theory  we propose algorithms in Section 4. In Section 5 we will show experimental results
and make analysis. Finally  Section 6 concludes. Due to the limited space  all proofs are given in the
supplementary.

1Li et al. [2016] also implicitly used the activation vector as the neuron’s representation.

2

2 Preliminaries

In this section  we will formally describe the neuron activation subspace match model that will
be analyzed throughout this paper. Let X and Y be the set of neurons in the same layer2 of two
networks with identical architecture but trained from different initializations. Suppose the networks
are given d input data a1  a2 ···   ad. For ∀v ∈ X ∪ Y  let the output of neuron v over ai be
zv(ai). The representation of a neuron v is measured by the activation vector [Raghu et al.  2017]
of the neuron v over the d inputs  zv := (zv(a1)  zv(a2) ···   zv(ad)). For any subset X ⊆ X   we
denote the vector set {zx : x ∈ X} by zX for short. The representation of a subset of neurons
X ⊆ X is measured by the subspace spanned by the activation vectors of the neurons therein 
λzx zx : ∀λzx ∈ R}. Similarly for Y ⊆ Y. In particular  the representation of

zx∈zX

span(zX ) := { (cid:80)
layer of X   we have z˜x(ai) = ReLU((cid:80)
{wx : x ∈ X} there exists {wy : y ∈ Y } such that ∀ai (cid:80)

an empty subset is span(∅) := {0}  where 0 is the zero vector in Rd.
The reason why we adopt the neuron activation subspace as the representation of a subset of neurons
is that activations of neurons are followed by afﬁne transformations. For any neuron ˜x in the following
x∈X wxzx(ai) + b)  where {wx : x ∈ X} and b are the
parameters. Similarly for neuron ˜y in the following layer of Y. If span(zX ) = span(zY )  for any
y∈Y wyzy(ai)  and

x∈X wxzx(ai) =(cid:80)

vice versa. Essentially ˜x and ˜y receive the same information from either X or Y .
We now give the formal deﬁnition of a match.
Deﬁnition 1 (-approximate match and exact match). Let X ⊆ X and Y ⊆ Y be two subsets of
neurons. ∀ ∈ [0  1)  we say (X  Y ) forms an -approximate match in (X  Y)  if

1. ∀x ∈ X  dist(zx  span(zY )) ≤ |zx| 
2. ∀y ∈ Y  dist(zy  span(zX )) ≤ |zy|.

Here we use the L2 distance: for any vector z and any subspace S  dist(z  S) = minz(cid:48)∈S (cid:107)z − z(cid:48)(cid:107)2.
We call a 0-approximate match an exact match. Equivalently  (X  Y ) is an exact match if span(zX ) =
span(zY ).

3 A Theory of Neuron Activation Subspace Match

In this section  we will develop a theory which gives a complete characterization of the neuron
activation subspace match problem. For two sets of neurons X  Y in two networks  we show the
structure of all the matches (X  Y ) in (X  Y). It turns out that every match (X  Y ) can be decomposed
as a union of simple matches  where a simple match is an atomic match that cannot be decomposed
any further.
Simple match is the most important concept in our theory. If there are many one-to-one simple
matches (i.e. |X| = |Y | = 1)   it implies that the two networks learn very similar representations at
the neuron level. On the other hand  if all the simple matches have very large size (i.e. |X| |Y | are
both large)  it is reasonable to say that the two networks learn different representations  at least in
details.
We will give mathematical characterization of the simple matches. This allows us to design efﬁcient
algorithms ﬁnding out the simple matches (Sec.4). The structures of exact and approximate match
are somewhat different. In Section 3.1  we present the simpler case of exact match  and in Section
3.2  we describe the more general -approximate match. Without being explicitly stated  when we
say match  we mean -approximate match.
We begin with a lemma stating that matches are closed under union.
Lemma 2 (Union-Close Lemma). Let (X1  Y1) and (X2  Y2) be two -approximate matches in
(X  Y). Then (X1 ∪ X2  Y1 ∪ Y2) is still an -approximate match.
The fact that matches are closed under union implies that there exists a unique maximum match.
Deﬁnition 3 (Maximum Match). A match (X∗  Y ∗) in (X  Y) is the maximum match if every match
(X  Y ) in (X  Y) satisﬁes X ⊆ X∗ and Y ⊆ Y ∗.

2In this paper we focus on neurons of the same layer. But the method applies to an arbitrary set of nerons.

3

The maximum match is simply the union of all matches. In Section 4 we will develop an efﬁcient
algorithm that ﬁnds the maximum match.
Now we are ready to give a complete characterization of all the matches. First  we point out that there
can be exponentially many matches. Fortunately  every match can be represented as the union of some
simple matches deﬁned below. The number of simple matches is polynomial for the setting of exact
match given (zx)x∈X and (zy)y∈Y being both linearly independent  and under certain conditions for
approximate match as well.
Deﬁnition 4 (Simple Match). A match ( ˆX  ˆY ) in (X  Y) is a simple match if ˆX ∪ ˆY is non-empty
and there exist no matches (Xi  Yi) in (X  Y) such that

2. ˆX =(cid:83)

Xi  ˆY =(cid:83)

1. ∀i  (Xi ∪ Yi) (cid:40) ( ˆX ∪ ˆY );

Yi.

i

i

With the concept of the simple matches  we will show the Decomposition Theorem: every match
can be decomposed as the union of a set of simple matches. Consequently  simple matches fully
characterize the structure of matches.
Theorem 5 (Decomposition Theorem). Every match (X  Y ) in (X  Y) can be expressed as a union
ˆYi.

of simple matches. Formally  there are simple matches ( ˆXi  ˆYi) satisfying X =(cid:83)

ˆXi and Y =(cid:83)

i

i

3.1 Structure of Exact Matches

The main goal of this and the next subsection is to understand the simple matches. The deﬁnition
of simple match only tells us it cannot be decomposed. But how to ﬁnd the simple matches? How
many simple matches exist? We will answer these questions by giving a characterization of the
simple match. Here we consider the setting of exact match  which has a much simpler structure than
approximate match.
An important property for exact match is that matches are closed under intersection.
Lemma 6 (Intersection-Close Lemma). Assume (zx)x∈X and (zy)y∈Y are both linearly independent.
Let (X1  Y1) and (X2  Y2) be exact matches in (X  Y). Then  (X1 ∩ X2  Y1 ∩ Y2) is still an exact
match.

It turns out that in the setting of exact match  simple matches can be explicitly characterized by
v-minimum match deﬁned below.
Deﬁnition 7 (v-Minimum Match). Given a neuron v ∈ X ∪ Y  we deﬁne the v-minimum match to
be the exact match (Xv  Yv) in (X  Y) satisfying the following properties:

1. v ∈ Xv ∪ Yv;
2. any exact match (X  Y ) in (X  Y) with v ∈ X ∪ Y satisﬁes Xv ⊆ X and Yv ⊆ Y .

Every neuron v in the maximum match (X∗  Y ∗) has a unique v-minimum match  which is the
intersection of all matches that contain v. For a neuron v not in the maximum match  there is no
v-minimum match because there is no match containing v.
The following theorem states that the simple matches are exactly v-minimum matches.
Theorem 8. Assume (zx)x∈X and (zy)y∈Y are both linearly independent. Let (X∗  Y ∗) be the
maximum (exact) match in (X  Y). ∀v ∈ X∗ ∪ Y ∗  the v-minimum match is a simple match  and
every simple match is a v-minimum match for some neuron v ∈ X∗ ∪ Y ∗.
Theorem 8 implies that the number of simple exact matches is at most linear with respect to the
number of neurons given the activation vectors being linearly independent  because the v-minimum
match for each neuron v is unique. We will give a polynomial time algorithm in Section 4 to ﬁnd out
all the v-minimum matches.

3.2 Structure of Approximate Matches

The structure of -approximate match is more complicated than exact match. A major difference is
that in the setting of approximate matches  the intersection of two matches is not necessarily a match.
As a consequence  there is no v-minimum match in general. Instead  we have v-minimal match.

4

Deﬁnition 9 (v-Minimal Match). v-minimal matches are matches (Xv  Yv) in (X  Y) with the
following properties:
1. v ∈ Xv ∪ Yv;
2. if a match (X  Y ) with X ⊆ Xv and Y ⊆ Yv satisﬁes v ∈ X ∪ Y   then (X  Y ) = (Xv  Yv).
Different from the setting of exact match where v-minimum match is unique for a neuron v  there
may be multiple v-minimal matches for v in the setting of approximate match  and in this setting
simple matches can be characterized by v-minimal matches instead. Again  for any neuron v not in
the maximum match (X∗  Y ∗)  there is no v-minimal match because no match contains v.
Theorem 10. Let (X∗  Y ∗) be the maximum match in (X  Y). ∀v ∈ X∗ ∪ Y ∗  every v-minimal
match is a simple match  and every simple match is a v-minimal match for some v ∈ X∗ ∪ Y ∗.
Remark 1. We use the notion v-minimal match for v ∈ X ∪ Y. That is  the neuron can be in either
networks. We emphasize that this is necessary. Restricting v ∈ X (or v ∈ Y) does not yield Theorem
10 anymore. In other word  v-minimal matches for v ∈ X do not represent all simple matches. See
Remark A.1 in the Supplementary Material for details.
Remark 2. One may have the impression that the structure of match is very simple. This is not
exactly the case. Here we point out the complicated aspect:

1. Matches are not closed under the difference operation  even for exact matches. More gener-
ally  let (X1  Y1) and (X2  Y2) be two matches with X1 (cid:40) X2  Y1 (cid:40) Y2. (X2\X1  Y2\Y1)
is not necessarily a match.
2. The decomposition of a match into the union of simple matches is not necessarily unique.

See Section C in the Supplementary Material for details.

4 Algorithms

In this section  we will give an efﬁcient algorithm that ﬁnds the maximum match. Based on this
algorithm  we further give an algorithm that ﬁnds all the simple matches  which are precisely the
v-minimum/minimal matches as shown in the previous section. The algorithm for ﬁnding the
maximum match is given in Algorithm 1. Initially  we guess the maximum match (X∗  Y ∗) to be
X∗ = X   Y ∗ = Y. If there is x ∈ X∗ such that dist(zx  span(zY ∗ )) >   then we remove x from
X∗. Similarly  if for some y ∈ Y ∗ such that y cannot be linearly expressed by zX∗ within error  
then we remove y from Y ∗. X∗ and Y ∗ are repeatedly updated in this way until no such x  y can be
found.

Algorithm 1 max_match((zv(cid:48))v(cid:48)∈X∪Y   )
1: (X∗  Y ∗) ← (X  Y)
2: changed ← true
3: while changed do
changed ← false
4:
for x ∈ X∗ do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: return (X∗  Y ∗)

if changed then
changed ← false
for y ∈ Y ∗ do

Y ∗ ← Y ∗\{y}
changed ← true

if dist(zx  span(zY ∗ )) >  then

X∗ ← X∗\{x}
changed ← true

if dist(zy  span(zX∗ )) >  then

Theorem 11. Algorithm 1 outputs the maximum match and runs in polynomial time.
Our next algorithm (Algorithm 2) is to output  for a given neuron v ∈ X ∪ Y  the v-minimum match
(for exact match given the activation vectors being linearly independent) or one v-minimal match (for

5

approximate match). The algorithm starts from (Xv  Yv) being the maximum match and iteratively
ﬁnds a smaller match (Xv  Yv) keeping v ∈ Xv ∪ Yv until further reducing the size of (Xv  Yv)
would have to violate v ∈ Xv ∪ Yv.

Algorithm 2 min_match((zv(cid:48))v(cid:48)∈X∪Y   v  )
1: (Xv  Yv) ← max_match((zv(cid:48))v(cid:48)∈X∪Y   )
2: if v /∈ Xv ∪ Yv then
return “failure”
3:
4: while there exists u ∈ Xv ∪ Yv unchecked do
Pick an unchecked u ∈ Xv ∪ Yv and mark it as checked
5:
if u ∈ Xv then
6:
7:
else
8:
9:
(X∗  Y ∗) ← max_match((zv(cid:48))v(cid:48)∈X∪Y   )
10:
if v ∈ (X∗  Y ∗) then
11:
12:
13: return (Xv  Yv)

(X  Y ) ← (Xv\{u}  Yv)
(X  Y ) ← (Xv  Yv\{u})

(Xv  Yv) ← (X∗  Y ∗)

Theorem 12. Algorithm 2 outputs one v-minimal match for the given neuron v. If  = 0 (exact
match)  the algorithm outputs the unique v-minimum match provided (zx)x∈X and (zy)y∈Y are both
linearly independent. Moreover  the algorithm always runs in polynomial time.

Finally  we show an algorithm (Algorithm 3) that ﬁnds all the v-minimal matches in time LO(Nv).
Here  L is the size of the input (L = (|X| + |Y|) · d) and Nv is the number of v-minimal matches
for neuron v. Note that in the setting of  = 0 (exact match) with (zx)x∈X and (zy)y∈Y being both
linearly independent  we have Nv ≤ 1  so Algorithm 3 runs in polynomial time in this case.
Algorithm 3 ﬁnds all the v-minimal matches one by one by calling Algorithm 2 in each iteration. To
make sure that we never ﬁnd the same v-minimal match twice  we always delete a neuron in every
previously-found v-minimal match before we start to ﬁnd the next one.

Algorithm 3 all_min_match((zv(cid:48))v(cid:48)∈X∪Y   v  )
1: S ← ∅
2: found ← true
3: while found do
4:
5:
6:

unchecked do

7:

and mark it as checked

found ← false
Let S = {(X1  Y1)  (X2  Y2) ···   (X|S|  Y|S|)}
while ¬found and there exists (u1  u2 ···   u|S|) ∈ (X1∪Y1)×(X2∪Y2)×···×(X|S|∪Y|S|)
Pick the next unchecked (u1  u2 ···   u|S|) ∈ (X1∪Y1)×(X2∪Y2)×···×(X|S|∪Y|S|)
(X  Y ) ← (X  Y)
for i = 1  2 ···  |S| do
if ui ∈ X then
X ← X\{ui}
else
Y ← Y \{ui}

8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: return S

if min_match((zv(cid:48))v(cid:48)∈X∪Y   v  ) doesn’t return “failure” then

(Xv  Yv) ← min_match((zv(cid:48))v(cid:48)∈X∪Y   v  )
S ← S ∪ {(Xv  Yv)}
found ← true

Theorem 13. Algorithm 3 outputs all the Nv different v-minimal matches in time LO(Nv). With
Algorithm 3  we can ﬁnd all the simple matches by exploring all v ∈ X ∪ Y based on Theorem 10.

6

In the worst case  Algorithm 3 is not polynomial time  as Nv is not upper bounded by a constant
in general. However  under assumptions we call strong linear independence and stability  we show
that Algorithm 3 runs in polynomial time. Speciﬁcally  we say (zx)x∈X satisﬁes θ-strong linear
independence for θ ∈ (0  π
2 ] if 0 /∈ zX and for any two non-empty disjoint subsets X1  X2 ⊆ X  
the angle between span(zX1) and span(zX2 ) is at least θ. Here  the angle between two subspaces
is deﬁned to be the minimum angle between non-zero vectors in the two subspaces. We deﬁne
θ-strong linear independence for (zy)y∈Y similarly. We say (zx)x∈X and (zy)y∈Y satisfy (  λ)-
stability for  ≥ 0 and λ > 1 if ∀x ∈ X  ∀Y ⊆ Y  dist(zx  span(zY )) /∈ (|zx|  λ|zx|] and
∀y ∈ Y ∀X ⊆ X   dist(zy  span(zX )) /∈ (|zy|  λ|zy|]. We prove the following theorem.
Theorem 14. Suppose ∃θ ∈ (0  π
2 ] such that (zx)x∈X and (zy)y∈Y both satisfy θ-strong linear
sin θ + 1)-stability. Then  ∀v ∈ X ∪ Y  Nv ≤ 1. As a consequence  Algorithm
independence and ( 
3 ﬁnds all the v-minimal matches in polynomial time  and we can ﬁnd all the simple matches in
polynomial time by exploring all v ∈ X ∪ Y based on Theorem 10.

2

5 Experiments

We conduct experiments on architectures of VGG[Simonyan and Zisserman  2014] and ResNet [He
et al.  2016] on the dataset CIFAR10[Krizhevsky et al.] and ImageNet[Deng et al.  2009]. Here
we investigate multiple networks initialized with different random seeds  which achieve reasonable
accuracies. Unless otherwise noted  we focus on the neurons activated by ReLU.
The activation vector zv mentioned in Section 2 is deﬁned as the activations of one neuron v
over the validation set. For a fully connected layer  zv ∈ Rd  where d is the number of im-
ages. For a convolutional layer  the activations of one neuron v  given the image Ii  is a fea-
ture map zv(ai) ∈ Rh×w. We vectorize the feature map as vec(zv(ai)) ∈ Rh×w  and thus
zv := (vec(zv(a1))  vec(zv(a2)) ···   vec(zv(ad))) ∈ Rh×w×d.

5.1 Maximum Match

We introduce maximum matching similarity to measure the overall similarity between sets of neurons.
Given two sets of neurons X  Y and   algorithm 1 outputs the maximum match X∗  Y ∗. The
maximum matching similarity s under  is deﬁned as s() =

|X∗|+|Y ∗|
|X|+|Y|

Here we only study neurons in the same layer of two networks with same architecture but initialized
with different seeds. For a convolutional layer  we randomly sample d from h × w × d outputs to
form an activation vector for several times  and average the maximal matching similarity.
Different Architecture and Dataset We examine several architectures on different dataset. For each
experiment  ﬁve differently initialized networks are trained  and the maximal matching similarity is
averaged over all the pairs of networks given . The similarity values show little variance among
different pairs  which indicates that this metric reveals a general property of network pairs. The detail
of network structures and validation accuracies are listed in the Supplementary Section E.2.
Figure 1 shows maximal matching similarities of all the layers of different architectures under various
. From these results  we make the following conclusions:

1. For most of the convolutional layers  the maximum match similarity is very low. For deep
neural networks  the similarity is almost zero. This is surprising  as it is widely believed that
the convolutional layers are trained to extract speciﬁc patterns. However  the observation
shows that different CNNs (with the same architecture) may learn different intermediate
patterns.

2. Although layers close to the output sometimes exhibit high similarity  it is a simple conse-
quence of their alignment to the output: First  the output vector of two networks must be
well aligned because they both achieve high accuracy. Second  it is necessary that the layers
before output are similar because if not  after a linear transformation  the output vectors will
not be similar. Note that in Fig 1 (b) layers close to the output do not have similarity. This
is because in this experiment the accuracy is relatively low. (See also in Supplementary
materials that  for a trained and an untrained networks which have very different accuracies
and therefore layers close to output do not have much similarity.)

7

(a) CIFAR10-ResNet18

(b) ImageNet-VGG16

(c) CIFAR10-ResNet34

Figure 1: Maximal matching similarities of different architectures on different datasets under various
. The x-axis is along the propagation. (a) shows ResNet18 on CIFAR10 validation set  we leave
other classical architectures like VGG in Supplementary material; (b) shows VGG16 on ImageNet
validation set; (c) shows a deeper ResNet on CIFAR10.

3. There is also relatively high similarity of layers close to the input. Again  this is the
consequence of their alignment to the same input data as well as the low-dimension nature
of the low level layers. More concretely  the fact that each low-level ﬁlter contains only a
few parameters results in a low dimension space after the transformation; and it is much
easier to have high similarity in low dimensional space than in high dimensional space.

5.2 Simple Match

The maximum matching illustrates the overall similarity but does not provide information about the
relation of speciﬁc neurons. Here we analyze the distribution of the size of simple matches to reveal
the ﬁner structure of a layer. Given  and two sets of neurons X and Y  algorithm 3 will output all the
simple matches.
For more efﬁcient implementation  given   we run the randomized algorithm 2 over each v ∈ X ∪ Y
to get one v-minimal match for several iterations. The ﬁnal result is the collection of all the v-minimal
matches found (remove duplicated matches)   which we use to estimate the distribution.
Figure 2 shows the distribution of the size of simple matches on layers close to input or output
respectively. We make the following observations:

1. While the layers close to output are similar overall  it seems that they do not show similarity
in a local manner. There are very few simple matches with small sizes. It is also an evidence
that such similarity is the result of its alignment to the output  rather than intrinsic similar
representations.

2. The layer close to input shows lower similarity in the ﬁner structure. Again  there are few

simple matches with small sizes.

In sum  almost no single neuron (or a small set of neurons) learn similar representations  even in
layers close to input or output.

8

(a) Layer close to input

(b) Layer close to output

Figure 2: The distribution of the sizes of minimal matches of layers close to input and output
respectively

6 Conclusion

In this paper  we investigate the similarity between representations learned by two networks with iden-
tical architecture but trained from different initializations. We develop a rigorous theory and propose
efﬁcient algorithms. Finally  we apply the algorithms in experiments and ﬁnd that representations
learned by convolutional layers are not as similar as prevalently expected.
This raises important questions: Does our result imply two networks learn completely different
representations  or subspace match is not a good metric for measuring the similarity of representations?
If the former is true  we need to rethink not only learning representations  but also interpretability of
deep learning. If from each initialization one learns a different representation  how can we interpret
the network? If  on the other hand  subspace match is not a good metric  then what is the right metric
for similarity of representations? We believe this is a fundamental problem for deep learning and
worth systematic and in depth studying.

7 Acknowledgement

This work is supported by National Basic Research Program of China (973 Program) (grant no.
2015CB352502)  NSFC (61573026) and BJNSF (L172037) and a grant from Microsoft Research
Asia.

References
Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems  pages 2933–2941  2014.

J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical

Image Database. In CVPR09  2009.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. Cifar-10 (canadian institute for advanced

research). URL http://www.cs.toronto.edu/~kriz/cifar.html.

Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems  pages 1097–1105 
2012.

Yixuan Li  Jason Yosinski  Jeff Clune  Hod Lipson  and John Hopcroft. Convergent learning: Do
different neural networks learn the same representations? In International Conference on Learning
Representation (ICLR ’16)  2016.

Maithra Raghu  Justin Gilmer  Jason Yosinski  and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In Advances in
Neural Information Processing Systems  pages 6078–6087  2017.

9

010203040size of simple matches0.000.020.040.060.080.10density = 0.300  |X*| = 44859095100105110size of simple matches0.000.010.020.030.040.050.06density = 0.152  |X*| = 118Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556  2014.

10

,Liwei Wang
Lunjia Hu
Jiayuan Gu
Zhiqiang Hu
Yue Wu
Kun He
John Hopcroft
Andrew Cotter
Maya Gupta
Harikrishna Narasimhan