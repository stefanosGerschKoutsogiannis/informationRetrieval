2017,Spectrally-normalized margin bounds for neural networks,This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized "spectral complexity": their Lipschitz constant  meaning the product of the spectral norms of the weight matrices  times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the MNIST and CIFAR10 datasets  with both original and random labels;  the bound  the Lipschitz constants  and the excess risks are all in direct correlation  suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task  and secondly that the presented bound is sensitive to this complexity.,Spectrally-normalized margin bounds

for neural networks

Peter L. Bartlett∗

Dylan J. Foster†

Matus Telgarsky‡

Abstract

This paper presents a margin-based multiclass generalization bound for neural net-
works that scales with their margin-normalized spectral complexity: their Lipschitz
constant  meaning the product of the spectral norms of the weight matrices  times
a certain correction factor. This bound is empirically investigated for a standard
AlexNet network trained with SGD on the mnist and cifar10 datasets  with both
original and random labels; the bound  the Lipschitz constants  and the excess risks
are all in direct correlation  suggesting both that SGD selects predictors whose
complexity scales with the difﬁculty of the learning task  and secondly that the
presented bound is sensitive to this complexity.

1 Overview
Neural networks owe their astonishing success not only to their ability to ﬁt any data set: they also
generalize well  meaning they provide a close ﬁt on unseen data. A classical statistical adage is that
models capable of ﬁtting too much will generalize poorly; what’s going on here?
Let’s navigate the many possible explanations provided by statistical theory. A ﬁrst observation is
that any analysis based solely on the number of possible labellings on a ﬁnite training set — as is
the case with VC dimension — is doomed: if the function class can ﬁt all possible labels (as is the
case with neural networks in standard conﬁgurations [Zhang et al.  2017])  then this analysis can not
distinguish it from the collection of all possible functions!

Figure 1: An analysis of AlexNet [Krizhevsky et al.  2012] trained with SGD on cifar10  both with
original and with random labels. Triangle-marked curves track excess risk across training epochs (on
a log scale)  with an ‘x’ marking the earliest epoch with zero training error. Circle-marked curves
track Lipschitz constants  normalized so that the two curves for random labels meet. The Lipschitz
constants tightly correlate with excess risk  and moreover normalizing them by margins (resulting in
the square-marked curve) neutralizes growth across epochs.

∗<peter@berkeley.edu>; University of California  Berkeley and Queensland University of Technology.
†<djf244@cornell.edu>; Cornell University.
‡<mjt@illinois.edu>; University of Illinois  Urbana-Champaign.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

epoch 10epoch 100excess risk 0.3excess risk 0.9cifar excess riskcifar Lipschitzcifar Lipschitz/margincifar [random] excess riskcifar [random] LipschitzNext let’s consider scale-sensitive measures of complexity  such as Rademacher complexity and
metric entropy  which work directly with real-valued function classes  and moreover are sensitive
to their magnitudes. Figure 1 plots the excess risk (the test error minus the training error) across
training epochs against one candidate scale-sensitive complexity measure  the Lipschitz constant of
the network (the product of the spectral norms of their weight matrices)  and demonstrates that they
are tightly correlated (which is not the case for  say  the l2 norm of the weights). The data considered
in Figure 1 is the standard cifar10 dataset  both with original and with random labels  which has
been used as a sanity check when investigating neural network generalization [Zhang et al.  2017].
There is still an issue with basing a complexity measure purely on the Lipschitz constant (although
it has already been successfully employed to regularize neural networks [Cisse et al.  2017]): as
depicted in Figure 1  the measure grows over time  despite the excess risk plateauing. Fortunately 
there is a standard resolution to this issue: investigating the margins (a precise measure of conﬁdence)
of the outputs of the network. This tool has been used to study the behavior of 2-layer networks 
boosting methods  SVMs  and many others [Bartlett  1996  Schapire et al.  1997  Boucheron et al. 
2005]; in boosting  for instance  there is a similar growth in complexity over time (each training
iteration adds a weak learner)  whereas margin bounds correctly stay ﬂat or even decrease. This
behavior is recovered here: as depicted in Figure 1  even though standard networks exhibit growing
Lipschitz constants  normalizing these Lipschitz constants by the margin instead gives a decaying
curve.
1.1 Contributions
This work investigates a complexity measure for neural networks that is based on the Lipschitz
constant  but normalized by the margin of the predictor. The two central contributions are as follows.
• Theorem 1.1 below will give the rigorous statement of the generalization bound that is
the basis of this work. In contrast to prior work  this bound: (a) scales with the Lipschitz
constant (product of spectral norms of weight matrices) divided by the margin; (b) has
no dependence on combinatorial parameters (e.g.  number of layers or nodes) outside of
log factors; (c) is multiclass (with no explicit dependence on the number of classes); (d)
measures complexity against a reference network (e.g.  for the ResNet [He et al.  2016]  the
reference network has identity mappings at each layer). The bound is stated below  with a
general form and analysis summary appearing in Section 3 and the full details relegated to
the appendix.

• An empirical investigation  in Section 2  of neural network generalization on the standard
datasets cifar10  cifar100  and mnist using the preceding bound. Rather than using the
bound to provide a single number  it can be used to form a margin distribution as in Figure 2.
These margin distributions will illuminate the following intuitive observations: (a) cifar10
is harder than mnist; (b) random labels make cifar10 and mnist much more difﬁcult;
(c) the margin distributions (and bounds) converge during training  even though the weight
matrices continue to grow; (d) l2 regularization (“weight decay”) does not signiﬁcantly
impact margins or generalization.

A more detailed description of the margin distributions is as follows. Suppose a neural network
computes a function f : Rd → Rk  where k is the number of classes; the most natural way to
convert this to a classiﬁer is to select the output coordinate with the largest magnitude  meaning
x (cid:55)→ arg maxj f (x)j. The margin  then  measures the gap between the output for the correct label
and other labels  meaning f (x)y − maxj(cid:54)=y f (x)j.
Unfortunately  margins alone do not seem to say much; see for instance Figure 2a  where the
collections of all margins for all data points — the unnormalized margin distribution — are similar
for cifar10 with and without random labels. What is missing is an appropriate normalization  as in
Figure 2b. This normalization is provided by Theorem 1.1  which can now be explained in detail.
To state the bound  a little bit of notation is necessary. The networks will use L ﬁxed nonlinearities
(σ1  . . .   σL)  where σi : Rdi−1 → Rdi is ρi-Lipschitz (e.g.  as with coordinate-wise ReLU  and
max-pooling  as discussed in Appendix A.1); occasionally  it will also hold that σi(0) = 0. Given
L weight matrices A = (A1  . . .   AL) let FA denote the function computed by the corresponding
network:

FA(x) := σL(ALσL−1(AL−1 ··· σ1(A1x)··· )).

(1.1)

2

(a) Margins.

(b) Normalized margins.

Figure 2: Margin distributions at the end of training AlexNet on cifar10  with and without random
labels. With proper normalization  random labels demonstrably correspond to a harder problem.

The network output FA(x) ∈ RdL (with d0 = d and dL = k) is converted to a class label in
{1  . . .   k} by taking the arg max over components  with an arbitrary rule for breaking ties. Whenever
input data x1  . . .   xn ∈ Rd are given  collect them as rows of a matrix X ∈ Rn×d. Occasionally 
notation will be overloaded to discuss FA(X T )  a matrix whose ith column is FA(xi). Let W denote
the maximum of {d  d1  . . .   dL}. The l2 norm (cid:107) · (cid:107)2 is always computed entry-wise; thus  for a
matrix  it corresponds to the Frobenius norm.
Next  deﬁne a collection of reference matrices (M1  . . .   ML) with the same dimensions as
A1  . . .   AL; for instance  to obtain a good bound for ResNet [He et al.  2016]  it is sensible to
set Mi := I  the identity map  and the bound below will worsen as the network moves farther
from the identity map; for AlexNet [Krizhevsky et al.  2012]  the simple choice Mi = 0 sufﬁces.
Finally  let (cid:107) · (cid:107)σ denote the spectral norm and (cid:107) · (cid:107)p q denote the (p  q) matrix norm  deﬁned by
network FA with weights A is the deﬁned as

(cid:107)A(cid:107)p q :=(cid:13)(cid:13)((cid:107)A: 1(cid:107)p  . . .  (cid:107)A: m(cid:107)p)(cid:13)(cid:13)q for A ∈ Rd×m. The spectral complexity RFA = RA of a

 L(cid:89)

 L(cid:88)

RA :=

ρi(cid:107)Ai(cid:107)σ

i=1

i=1

(cid:107)A(cid:62)

i (cid:107)2/3

2 1

i − M(cid:62)
(cid:107)Ai(cid:107)2/3

σ

3/2

.

(1.2)

The following theorem provides a generalization bound for neural networks whose nonlinearities are
ﬁxed but whose weight matrices A have bounded spectral complexity RA.
Theorem 1.1. Let nonlinearities (σ1  . . .   σL) and reference matrices (M1  . . .   ML) be given as
above (i.e.  σi is ρi-Lipschitz and σi(0) = 0). Then for (x  y)  (x1  y1)  . . .   (xn  yn) drawn iid from
any probability distribution over Rd × {1  . . .   k}  with probability at least 1 − δ over ((xi  yi))n
i=1 
every margin γ > 0 and network FA : Rd → Rk with weight matrices A = (A1  . . .   AL) satisfy

(cid:105) ≤ (cid:98)Rγ(FA) + (cid:101)O

(cid:32)(cid:107)X(cid:107)2RA
1(cid:2)f (xi)yi ≤ γ + maxj(cid:54)=yi f (xi)j

(cid:114)
(cid:3) and (cid:107)X(cid:107)2 =(cid:112)(cid:80)

ln(W ) +

γn

FA(x)j (cid:54)= y

i

ln(1/δ)

n
i (cid:107)xi(cid:107)2
2.

(cid:33)

 

Pr

arg max

where (cid:98)Rγ(f ) ≤ n−1(cid:80)

j

(cid:104)

The full proof and a generalization beyond spectral norms is relegated to the appendix  but a sketch is
provided in Section 3  along with a lower bound. Section 3 also gives a discussion of related work:
brieﬂy  it’s essential to note that margin and Lipschitz-sensitive bounds have a long history in the
neural networks literature [Bartlett  1996  Anthony and Bartlett  1999  Neyshabur et al.  2015]; the
distinction here is the sensitivity to the spectral norm  and that there is no explicit appearance of
combinatorial quantities such as numbers of parameters or layers (outside of log terms  and indices to
summations and products).
To close  miscellaneous observations and open problems are collected in Section 4.
2 Generalization case studies via margin distributions
In this section  we empirically study the generalization behavior of neural networks  via margin
distributions and the generalization bound stated in Theorem 1.1.

3

0cifarrandom0cifarcifar random(a) Mnist is easier than cifar10.

(b) Random mnist is as hard as random cifar10!

(c) cifar100 is (almost) as hard as cifar10 with ran-
dom labels!
Figure 3: A variety of margin distributions. Axes are re-scaled in Figure 3a  but identical in the other
subplots; the cifar10 (blue) and random cifar10 (green) distributions are the same each time.

(d) Random inputs are harder than random labels.

Before proceeding with the plots  it’s a good time to give a more reﬁned description of the mar-
gin distribution  one that is suitable for comparisons across datasets. Given n pattern/label pairs
i=1  with patterns as rows of matrix X ∈ Rn×d  and given a predictor FA : Rd → Rk  the
((xi  yi))n
(normalized) margin distribution is the univariate empirical distribution of the labeled data points
each transformed into a single scalar according to

(x  y) (cid:55)→ FA(x)y − maxi(cid:54)=y FA(x)i

RA(cid:107)X(cid:107)2/n

 

where the spectral complexity RA is from eq. (1.2). The normalization is thus derived from the bound
in Theorem 1.1  but ignoring log terms.
Taken this way  the two margin distributions for two datasets can be interpreted as follows. Consider-
ing any ﬁxed point on the horizontal axis  if the cumulative distribution of one density is lower than
the other  then it corresponds to a lower right hand side in Theorem 1.1. For no reason other than
visual interpretability  the plots here will instead depict a density estimate of the margin distribution.
The vertical and horizontal axes are rescaled in different plots  but the random and true cifar10
margin distributions are always the same.
A little more detail about the experimental setup is as follows. All experiments were implemented in
Keras [Chollet et al.  2015]. In order to minimize conﬂating effects of optimization and regularization 
the optimization method was vanilla SGD with step size 0.01  and all regularization (weight decay 
batch normalization  etc.) were disabled. “cifar” in general refers to cifar10  however cifar100
will also be explicitly mentioned. The network architecture is essentially AlexNet [Krizhevsky et al. 
2012] with all normalization/regularization removed  and with no adjustments of any kind (even to
the learning rate) across the different experiments.
Comparing datasets. A ﬁrst comparison is of cifar10 and the standard mnist digit data. mnist
is considered “easy”  since any of a variety of methods can achieve roughly 1% test error. The
“easiness” is corroborated by Figure 3a  where the margin distribution for mnist places all its mass far
to the right of the mass for cifar10. Interestingly  randomizing the labels of mnist  as in Figure 3b 
results in a margin distribution to the left of not only cifar10  but also slightly to the left of (but
close to) cifar10 with randomized labels.

4

0cifarcifar randommnist0cifarcifar randommnist random0cifarcifar randomcifar1000cifarrandom labelrandom input(a) Margins across epochs for cifar10.

(b) Various levels of l2 regularization for cifar10.

Figure 4

Next  Figure 3c compares cifar10 and cifar100  where cifar100 uses the same input images
as cifar10; indeed  cifar10 is obtained from cifar100 by collapsing the original 100 categories
into 10 groups. Interestingly  cifar100  from the perspective of margin bounds  is just as difﬁcult
as cifar10 with random labels. This is consistent with the large observed test error on cifar100
(which has not been “optimized” in any way via regularization).
Lastly  Figure 3d replaces the cifar10 input images with random images sampled from Gaussians
matching the ﬁrst- and second-order image statistics (see [Zhang et al.  2017] for similar experiments).
Convergence of margins. As was pointed out in Section 1  the weights of the neural networks do
not seem to converge in the usual sense during training (the norms grow continually). However  as
depicted in Figure 4a  the sequence of (normalized) margin distributions is itself converging.
Regularization. As remarked in [Zhang et al.  2017]  regularization only seems to bring minor
beneﬁts to test error (though adequate to be employed in all cutting edge results). This observation
is certainly consistent with the margin distributions in Figure 4b  which do not improve (e.g.  by
shifting to the right) in any visible way under regularization. An open question  discussed further in
Section 4  is to design regularization that improves margins.
3 Analysis of margin bound
This section will sketch the proof of Theorem 1.1  give a lower bound  and discuss related work.
3.1 Multiclass margin bound
The starting point of this analysis is a margin-based bound for multiclass prediction. To state the bound 
ﬁrst recall that the margin operator M : Rk×{1  . . .   k} → R is deﬁned as M(v  y) := vy−max
vi 
i(cid:54)=y
and deﬁne the ramp loss (cid:96)γ : R → R+ as

0

(cid:96)γ(r) :=

r < −γ 

1 + r/γ r ∈ [−γ  0] 
1

r > 0 

deﬁne an empirical counterpart (cid:98)Rγ of Rγ as (cid:98)Rγ(f ) := n−1(cid:80)
and ramp risk as Rγ(f ) := E((cid:96)γ(−M(f (x)  y))). Given a sample S := ((x1  y1)  . . .   (xn  yn)) 
and (cid:98)Rγ respectively upper bound the probability and fraction of errors on the source distribution
i (cid:96)γ(−M(f (xi)  yi)); note that Rγ
R(H|S) := n−1E suph∈H(cid:80)n
and training set. Lastly  given a set of real-valued functions H  deﬁne the Rademacher complexity as
i=1 ih(xi  yi)  where the expectation is over the Rademacher random
variables (1  . . .   n)  which are independent  uniform ±1-valued.
(cid:8)(x  y) (cid:55)→ (cid:96)γ(−M(f (x)  y)) : f ∈ F(cid:9). Then  with probability at least 1 − δ over a sample S
With this notation in place  the basic bound is as follows.
Lemma 3.1. Given functions F with F (cid:51) f : Rd → Rk and any γ > 0  deﬁne Fγ :=
(cid:113) ln(1/δ)
of size n  every f ∈ F satisﬁes Pr[arg maxi f (x)i (cid:54)= y] ≤ (cid:98)Rγ(f ) + 2R((Fγ)|S) + 3

2n .

This bound is a direct consequence of standard tools in Rademacher complexity. In order to instantiate
this bound  covering numbers will be used to directly upper bound the Rademacher complexity term

5

10 epochs20 epochs40 epochs80 epochs160 epochs0106105104√

k [Zhang  2004].

R((Fγ)|S). Interestingly  the choice of directly working in terms of covering numbers seems essential
to providing a bound with no explicit dependence on k; by contrast  prior work primarily handles
multiclass via a Rademacher complexity analysis on each coordinate of a k-tuple of functions  and
pays a factor of
3.2 Covering number complexity upper bounds
This subsection proves Theorem 1.1 via Lemma 3.1 by controlling  via covering numbers  the
Rademacher complexity R((Fγ)|S) for networks with bounded spectral complexity.
The notation here for (proper) covering numbers is as follows. Let N (U   (cid:107) · (cid:107)) denote the least
cardinality of any subset V ⊆ U that covers U at scale  with norm (cid:107) · (cid:107)  meaning

(cid:107)A − B(cid:107) ≤ .

sup
A∈U

min
B∈V

Choices of U that will be used in the present work include both the image F|S of data S under some
function class F   as well as the conceptually simpler choice of a family of matrix products.
The full proof has the following steps: (I) A matrix covering bound for the afﬁne transformation
of each layer is provided in Lemma 3.2; handling whole layers at once allows for more ﬂexible
norms. (II) An induction on layers then gives a covering number bound for entire networks; this
analysis is only sketched here for the special case of norms used in Theorem 1.1  but the full proof in
the appendix culminates in a bound for more general norms (cf. Lemma A.7). (III) The preceding
whole-network covering number leads to Theorem 1.1 via Lemma 3.1 and standard techniques.
Step (I)  matrix covering  is handled by the following lemma. The covering number considers the
matrix product XA  where A will be instantiated as the weight matrix for a layer  and X is the data
passed through all layers prior to the present layer.
Lemma 3.2. Let conjugate exponents (p  q) and (r  s) be given with p ≤ 2  as well as positive reals
(a  b  ) and positive integer m. Let matrix X ∈ Rn×d be given with (cid:107)X(cid:107)p ≤ b. Then

(cid:111)

(cid:19)

(cid:38)

(cid:39)

(cid:18)(cid:110)

lnN

XA : A ∈ Rd×m (cid:107)A(cid:107)q s ≤ a

   (cid:107) · (cid:107)2

≤

a2b2m2/r

2

ln(2dm).

The proof relies upon the Maurey sparsiﬁcation lemma [Pisier  1980]  which is stated in terms
of sparsifying convex hulls  and in its use here is inspired by covering number bounds for linear
predictors [Zhang  2002]. To prove Theorem 1.1  this matrix covering bound will be instantiated
for the case of (cid:107)A(cid:107)2 1. It is possible to instead scale with (cid:107)A(cid:107)2 and (cid:107)X(cid:107)2  but even for the case of
the identity matrix X = I  this incurs an extra dimension factor. The use of (cid:107)A(cid:107)2 1 here thus helps
Theorem 1.1 avoid any appearance of W and L outside of log terms; indeed  the goal of covering
a whole matrix at a time (rather than the more standard vector covering) was to allow this greater
sensitivity and avoid combinatorial parameters.
Step (II) above  the induction on layers  proceeds as follows. Let Xi denote the output of layer i (thus

X0 = X)  and inductively suppose there exists a cover element (cid:98)Xi depending on covering matrices
((cid:98)A1  . . .   (cid:98)Ai−1) chosen to cover weight matrices in earlier layers. Thanks to Lemma 3.2  there also
exists (cid:98)Ai so that (cid:107)Ai(cid:98)Xi − (cid:98)Ai(cid:98)Xi(cid:107)2 ≤ i. The desired cover element is thus (cid:98)Xi+1 = σi((cid:98)Ai(cid:98)Xi) where

σi is the nonlinearity in layer i; indeed  supposing σi is ρi-Lipschitz 

(cid:107)Xi+1 − (cid:98)Xi+1(cid:107)2 ≤ ρi(cid:107)AiXi − (cid:98)Ai(cid:98)Xi(cid:107)2

(cid:16)(cid:107)AiXi − Ai(cid:98)Xi(cid:107)2 + (cid:107)Ai(cid:98)Xi − (cid:98)Ai(cid:98)Xi(cid:107)2
≤ ρi(cid:107)Ai(cid:107)σ(cid:107)Xi − Ai(cid:98)Xi(cid:107)2ρi + i 

≤ ρi

(cid:17)

where the ﬁrst term is controlled with the inductive hypothesis. Since (cid:98)Xi+1 depends on each choice
((cid:98)Ai  . . .   (cid:98)Ai)  the cardinality of the full network cover is the product of the individual matrix covers.

The preceding proof had no sensitivity to the particular choice of norms; it merely required an
operator norm on Ai  as well as some other norm that allows matrix covering. Such an analysis is
presented in full generality in Appendix A.5. Specializing to the particular case of spectral norms
and (2  1) group norms leads to the following full-network covering bound.

6

Theorem 3.3. Let ﬁxed nonlinearities (σ1  . . .   σL) and reference matrices (M1  . . .   ML) be given 
where σi is ρi-Lipschitz and σi(0) = 0. Let spectral norm bounds (s1  . . .   sL)  and matrix (2  1)
norm bounds (b1  . . .   bL) be given. Let data matrix X ∈ Rn×d be given  where the n rows corre-
spond to data points. Let HX denote the family of matrices obtained by evaluating X with all choices
i (cid:107)2 1 ≤ bi

of network FA: HX := (cid:8)FA(X T ) : A = (A1  . . .   AL)  (cid:107)Ai(cid:107)σ ≤ si  (cid:107)A(cid:62)
3
(cid:19)2/3

where each matrix has dimension at most W along each axis. Then for any  > 0 

lnN (HX    (cid:107) · (cid:107)2) ≤ (cid:107)X(cid:107)2

 L(cid:88)

 L(cid:89)

i − M(cid:62)

(cid:9) 

.

2 ln(2W 2)

2

s2
j ρ2
j

j=1

i=1

(cid:18) bi

si

|S

What remains is (III): Theorem 3.3 can be combined with the standard Dudley entropy integral upper
bound on Rademacher complexity (see e.g. Mohri et al. [2012])  which combined with Lemma 3.1
gives Theorem 1.1.
3.3 Rademacher complexity lower bounds
By reduction to the linear case (i.e.  removing all nonlinearities)  it is easy to provide a lower bound
on the Rademacher complexity of the networks studied here. Unfortunately  this bound only scales
with the product of spectral norms  and not the other terms in RA (cf. eq. (1.2)).
Theorem 3.4. Consider the setting of Theorem 3.3  but all nonlinearities are the ReLU z (cid:55)→
max{0  z}  the output dimension is dL = 1  and all non-output dimensions are at least 2 (and hence
W ≥ 2). Let data S := (x1  . . .   xn) be collected into data matrix X ∈ Rn×d. Then there is a c
such that for any scalar r > 0  R

(cid:19)
i (cid:107)Ai(cid:107)σ ≤ r(cid:9)
Note that  due to the nonlinearity  the lower bound should indeed depend on(cid:81)
(cid:107)(cid:81)

≥ c(cid:107)X(cid:107)2r.
i (cid:107)Ai(cid:107)σ and not
i Ai(cid:107)σ; as a simple sanity check  there exist networks for which the latter quantity is 0  but the

(cid:18)(cid:8)FA : A = (A1  . . .   AL) (cid:81)

network does not compute the zero function.
3.4 Related work
To close this section on proofs  it is a good time to summarize connections to existing literature.
The algorithmic idea of large margin classiﬁers was introduced in the linear case by Vapnik [1982]
(see also [Boser et al.  1992  Cortes and Vapnik  1995]). Vapnik [1995] gave an intuitive explanation
of the performance of these methods based on a sample-dependent VC-dimension calculation  but
without generalization bounds. The ﬁrst rigorous generalization bounds for large margin linear
classiﬁers [Shawe-Taylor et al.  1998] required a scale-sensitive complexity analysis of real-valued
function classes. At the same time  a large margins analysis was developed for two-layer networks
[Bartlett  1996]  indeed with a proof technique that inspired the layer-wise induction used to prove
Theorem 1.1 in the present work. Margin theory was quickly extended to many other settings (see
for instance the survey by Boucheron et al. [2005])  one major success being an explanation of the
generalization ability of boosting methods  which exhibit an explicit growth in the size of the function
class over time  but a stable excess risk [Schapire et al.  1997]. The contribution of the present work
is to provide a margin bound (and corresponding Rademacher analysis) that can be adapted to various
operator norms at each layer. Additionally  the present work operates in the multiclass setting  and
avoids an explicit dependence on the number of classes k  which seems to appear in prior work
[Zhang  2004  Tewari and Bartlett  2007].
There are numerous generalization bounds for neural networks  including VC-dimension and fat-
shattering bounds (many of these can be found in [Anthony and Bartlett  1999]). Scale-sensitive
analysis of neural networks started with [Bartlett  1996]  which can be interpreted in the present
setting as utilizing data norm (cid:107)·(cid:107)∞ and operator norm (cid:107)·(cid:107)∞→∞ (equivalently  the norm (cid:107)A(cid:62)
i (cid:107)1 ∞ on
weight matrix Ai). This analysis can be adapted to give a Rademacher complexity analysis [Bartlett
and Mendelson  2002]  and has been adapted to other norms [Neyshabur et al.  2015]  although the
(cid:107) · (cid:107)∞ setting appears to be necessary to avoid extra combinatorial factors. More work is still needed
to develop complexity analyses that have matching upper and lower bounds  and also to determine
which norms are well-adapted to neural networks as used in practice.
The present analysis utilizes covering numbers  and is most closely connected to earlier covering
number bounds [Anthony and Bartlett  1999  Chapter 12]  themselves based on the earlier fat-
shattering analysis [Bartlett  1996]  however the technique here of pushing an empirical cover through

7

(cid:19)3/2

(cid:16)(cid:81)L

(cid:17)(cid:18)(cid:80)L

The original preprint of

layers is akin to VC dimension proofs for neural networks [Anthony and Bartlett  1999]. The use
of Maurey’s sparsiﬁcation lemma was inspired by linear predictor covering number bounds [Zhang 
2002].
Comparison to preprint.
2017]

featured a slightly different version of

al. 
the spectral complexity RA  given by
. In the present version (1.2)  each (cid:107)Ai − Mi(cid:107)1
(cid:107)Ai(cid:107)2/3
i (cid:107)2 1. This is a strict improvement since for any matrix A ∈ Rd×m

term is replaced by (cid:107)A(cid:62)
one has (cid:107)A(cid:107)2 1 ≤ (cid:107)A(cid:107)1  and in general the gap between these two norms can be as large as
On a related note  all of the ﬁgures in this paper use the (cid:96)1 norm in the spectral complexity RA instead
of the (2  1) norm. Variants of the experiments described in Section 2 were carried out using each of

i=1 ρi(cid:107)Ai(cid:107)σ

the l1  (2  1)  and l2 norms in the ((cid:80)L

i=1(·)2/3)3/2 term with negligible difference in the results.

i − M(cid:62)

(cid:107)Ai−Mi(cid:107)2/3

1

√

this paper

[Bartlett

et

i=1

σ

d.

σ

i=1

√
(

(cid:107)Ai(cid:107)2

W(cid:107)Ai−Mi(cid:107)2)2

(cid:16)(cid:81)L

(cid:17) · L

i=1 ρi(cid:107)Ai(cid:107)σ

(cid:16)(cid:80)L
Theorem 1.1 because for any A ∈ Rd×m it holds that(cid:13)(cid:13)A(cid:62)(cid:13)(cid:13)2 1 ≤ √
form ((cid:80)L

(cid:17)1/2
i=1(·)2/3)3/2 appearing in Theorem 1.1 may be replaced by the form L((cid:80)L

Since spectrally-normalized margin bounds were ﬁrst proposed in the preprint [Bartlett et al. 
2017]  subsequent works [Neyshabur et al.  2017  Neyshabur  2017] re-derived a similar spectrally-
normalized bound using the PAC-Bayes framework. Speciﬁcally  these works showed that RA may
be replaced by (up to log(W ) factors):
. Unfortu-
nately  this bound never improves on Theorem 1.1  and indeed can be derived from it as follows. First 
the dependence on the individual matrices Ai in the second term of this bound can be obtained from
d(cid:107)A(cid:107)2. Second  the functional
i=1(·)2)1/2
appearing above by using that (cid:107)α(cid:107)2/3 ≤ L(cid:107)α(cid:107)2 for any α ∈ RL (this inequality following  for
instance  from Jensen’s inequality).
4 Further observations and open problems
Adversarial examples. Adversarial examples are a phenomenon where the neural network predic-
tions can be altered by adding seemingly imperceptible noise to an input [Goodfellow et al.  2014].
This phenomenon can be connected to margins as follows. The margin is nothing more than the
distance an input must traverse before its label is ﬂipped; consequently  low margin points are more
susceptible to adversarial noise than high margin points. Concretely  taking the 100 lowest margin
inputs from cifar10 and adding uniform noise at scale 0.15 yielded ﬂipped labels on 5.86% of the
images  whereas the same level of noise on high margin points yielded 0.04% ﬂipped labels. Can the
bounds here suggest a way to defend against adversarial examples?
Regularization.
It was observed in [Zhang et al.  2017] that explicit regularization contributes little
to the generalization performance of neural networks. In the margin framework  standard weight
decay (l2) regularization seemed to have little impact on margin distributions in Section 2. On the
other hand  in the boosting literature  special types of regularization were developed to maximize
margins [Shalev-Shwartz and Singer  2008]; perhaps a similar development can be performed here?
SGD. The present analysis applies to predictors that have large margins; what is missing is an
analysis verifying that SGD applied to standard neural networks returns large margin predictors!
Indeed  perhaps SGD returns not simply large margin predictors  but predictors that are well-behaved
in a variety of other ways that can be directly translated into reﬁned generalization bounds.
Improvements to Theorem 1.1. There are several directions in which Theorem 1.1 might be
improved. Can a better choice of layer geometries (norms) yield better bounds on practical networks?
Can the nonlinearities’ worst-case Lipschitz constant be replaced with an (empirically) averaged
quantity? Alternatively  can better lower bounds rule out these directions?
Rademacher vs. covering.
with no invocation of covering numbers?
Acknowledgements
The authors thank Srinadh Bhojanapalli  Ryan Jian  Behnam Neyshabur  Maxim Raginsky  Andrew
J. Risteski  and Belinda Tzen for useful conversations and feedback. The authors thank Ben Recht
for giving a provocative lecture at the Simons Institute  stressing the need for understanding of

Is it possible to prove Theorem 1.1 solely via Rademacher complexity 

8

both generalization and optimization of neural networks. M.T. and D.F. acknowledge the use of
a GPU machine provided by Karthik Sridharan and made possible by an NVIDIA GPU grant.
D.F. acknowledges the support of the NDSEG fellowship. P.B. gratefully acknowledges the support
of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian
Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical
and Statistical Frontiers. The authors thank the Simons Institute for the Theory of Computing Spring
2017 program on the Foundations of Machine Learning. Lastly  the authors are grateful to La Burrita
(both the north and the south Berkeley campus locations) for upholding the glorious tradition of the
California Burrito.
References
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge

University Press  1999.

Peter Bartlett  Dylan J Foster  and Matus Telgarsky. Spectrally-normalized margin bounds for neural

networks. arXiv preprint arXiv:1706.08498  2017.

Peter L. Bartlett. For valid generalization the size of the weights is more important than the size of

the network. In NIPS  1996.

Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. JMLR  3:463–482  Nov 2002.

Bernhard E. Boser  Isabelle M. Guyon  and Vladimir N. Vapnik. A training algorithm for optimal
margin classiﬁers. In Proceedings of the Fifth Annual Workshop on Computational Learning
Theory  COLT ’92  pages 144–152  New York  NY  USA  1992. ACM. ISBN 0-89791-497-X.

Stéphane Boucheron  Olivier Bousquet  and Gabor Lugosi. Theory of classiﬁcation: A survey of

some recent advances. ESAIM: Probability and Statistics  9:323–375  2005.

François Chollet et al. Keras. https://github.com/fchollet/keras  2015.

Moustapha Cisse  Piotr Bojanowski  Edouard Grave  Yann Dauphin  and Nicolas Usunier. Parseval

networks: Improving robustness to adversarial examples. In ICML  2017.

Corinna Cortes and Vladimir N. Vapnik. Support-vector networks. Machine Learning  20(3):273–297 

1995.

Ian J. Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial

examples. 2014. arXiv:1412.6572 [stat.ML].

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual

networks. In ECCV  2016.

Alex Krizhevsky  Ilya Sutskever  and Geoffery Hinton. Imagenet classiﬁcation with deep convolu-

tional neural networks. In NIPS  2012.

Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of Machine Learning.

MIT Press  2012.

Behnam Neyshabur. Implicit regularization in deep learning. CoRR  abs/1709.01953  2017. URL

http://arxiv.org/abs/1709.01953.

Behnam Neyshabur  Ryota Tomioka  and Nathan Srebro. Norm-based capacity control in neural

networks. In COLT  2015.

Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nathan Srebro. A pac-bayesian
approach to spectrally-normalized margin bounds for neural networks. CoRR  abs/1707.09564 
2017.

Gilles Pisier. Remarques sur un résultat non publié de b. maurey. Séminaire Analyse fonctionnelle

(dit)  pages 1–12  1980.

9

Robert E. Schapire  Yoav Freund  Peter Bartlett  and Wee Sun Lee. Boosting the margin: A new

explanation for the effectiveness of voting methods. In ICML  pages 322–330  1997.

Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear separabil-

ity: New relaxations and efﬁcient boosting algorithms. In COLT  2008.

J. Shawe-Taylor  P. L. Bartlett  R. C. Williamson  and M. Anthony. Structural risk minimization over

data-dependent hierarchies. IEEE Trans. Inf. Theor.  44(5):1926–1940  September 1998.

Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classiﬁcation methods. Journal

of Machine Learning Research  8:1007–1025  2007.

Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag  New

York  1982.

Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag  New York  1995.

Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. ICLR  2017.

Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal of

Machine Learning Research  2:527–550  2002.

Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods. Journal

of Machine Learning Research  5:1225–1251  2004.

10

,Peter Bartlett
Dylan Foster
Matus Telgarsky