2013,Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression,We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters  such as length-scales  in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering  datasets with high-dimensional inputs.,Variational Inference for Mahalanobis

Distance Metrics in Gaussian Process Regression

Michalis K. Titsias

Department of Informatics

Athens University of Economics and Business

mtitsias@aueb.gr

Miguel L´azaro-Gredilla

Dpt. Signal Processing & Communications
Universidad Carlos III de Madrid - Spain

miguel@tsc.uc3m.es

Abstract

We introduce a novel variational method that allows to approximately integrate
out kernel hyperparameters  such as length-scales  in Gaussian process regression.
This approach consists of a novel variant of the variational framework that has
been recently developed for the Gaussian process latent variable model which ad-
ditionally makes use of a standardised representation of the Gaussian process. We
consider this technique for learning Mahalanobis distance metrics in a Gaussian
process regression setting and provide experimental evaluations and comparisons
with existing methods by considering datasets with high-dimensional inputs.

1

Introduction

Gaussian processes (GPs) have found many applications in machine learning and statistics ranging
from supervised learning tasks to unsupervised learning and reinforcement learning. However  while
GP models are advertised as Bayesian models  it is rarely the case that a full Bayesian procedure is
considered for training. In particular  the commonly used procedure is to ﬁnd point estimates over
the kernel hyperparameters by maximizing the marginal likelihood  which is the likelihood obtained
once the latent variables associated with the GP function have been integrated out (Rasmussen and
Williams  2006). Such a procedure provides a practical algorithm that is expected to be robust to
overﬁtting when the number of hyperparameters that need to be tuned are relatively few compared
to the amount of data. In contrast  when the number of hyperparameters is large this approach will
suffer from the shortcomings of a typical maximum likelihood method such as overﬁtting. To avoid
the above problems  in GP models  the use of kernel functions with few kernel hyperparameters
is common practice  although this can lead to limited ﬂexibility when modelling the data. For
instance  in regression or classiﬁcation problems with high dimensional input data the typical kernel
functions used are restricted to have the simplest possible form  such as a squared exponential with
common length-scale across input dimensions  while more complex kernel functions such as ARD
or Mahalanobis kernels (Vivarelli and Williams  1998) are not considered due to the large number
of hyperparameters needed to be estimated by maximum likelihood. On the other hand  while full
Bayesian inference for GP models could be useful  it is pragmatically a very challenging task that
currently has been attempted only by using expensive MCMC techniques such as the recent method
of Murray and Adams (2010). Deterministic approximations and particularly the variational Bayes
framework has not been applied so far for the treatment of kernel hyperparameters in GP models.
To this end  in this work we introduce a variational method for approximate Bayesian inference over
hyperparameters in GP regression models with squared exponential kernel functions. This approach
consists of a novel variant of the variational framework introduced in (Titsias and Lawrence  2010)
for the Gaussian process latent variable model. Furthermore  this method uses the concept of a
standardised GP process and allows for learning Mahalanobis distance metrics (Weinberger and
Saul  2009; Xing et al.  2003) in Gaussian process regression settings using Bayesian inference. In

1

the experiments  we compare the proposed algorithm with several existing methods by considering
several datasets with high-dimensional inputs.
The remainder of this paper is organised as follows: Section 2 provides the motivation and the-
oretical foundation of the variational method  Section 3 demonstrates the method in a number of
challenging regression datasets by providing also a comprehensive comparison with existing meth-
ods. Finally  the paper concludes with a discussion in Section 4.

2 Theory

Section 2.1 discusses Bayesian GP regression and motivates the variational method. Section 2.2
explains the concept of the standardised representation of a GP model that is used by the variational
method described in Section 2.3. Section 2.4 discusses setting the prior over the kernel hyperpa-
rameters together with a computationally analytical way to reduce the number of parameters to be
optimised during variational inference. Finally  Section 2.5 discusses prediction in novel test inputs.

2.1 Bayesian GP regression and motivation for the variational method
i=1  where each xi ∈ RD and each yi is a real-valued scalar output.
Suppose we have data {yi  xi}n
We denote by y the vector of all output data and by X all input data. In GP regression  we assume
that each observed output is generated according to yi = f (xi) + i  i ∼ N (0  σ2)  where the
full length latent function f (x) is assigned a zero-mean GP prior with a certain covariance or kernel
function kf (x  x(cid:48)) that depends on hyperparameters θ. Throughout the paper we will consider the
following squared exponential kernel function

kf (x  x(cid:48)) = σ2

f e− 1

2 (x−x(cid:48))T WT W(x−x(cid:48)) = σ2

f e− 1

2||Wx−Wx(cid:48)||2

f e− 1
2 d2

W(x x(cid:48)) 

(cid:80)D

kf (x  x(cid:48)) = σ2

f e− 1

(1)
where dW(x  x(cid:48)) = ||Wx − Wx(cid:48)||. In the above  σf is a global scale parameter while the matrix
W ∈ RK×D quantiﬁes a linear transformation that maps x into a linear subspace with dimension
at most K. In the special case where W is a square and diagonal matrix  the above kernel function
reduces to

= σ2

 

(2)
which consists of the well-known ARD squared exponential kernel commonly used in GP regres-
sion applications (Rasmussen and Williams  2006). In other cases where K < D  dW(x  x(cid:48)) deﬁnes
a Mahalanobis distance metric (Weinberger and Saul  2009; Xing et al.  2003) that allows for su-
pervised dimensionality reduction to be applied in a GP regression setting (Vivarelli and Williams 
1998).
In a full Bayesian formulation  the hyperparameters θ = (σf   W) are assigned a prior distribution
p(θ) and the Bayesian model follows the hierarchical structure depicted in Figure 1(a). According
to this structure the random function f (x) and the hyperparameters θ are a priori coupled since the
former quantity is generated conditional on the latter. This can make approximate  and in particular
variational  inference over the hyperparameters to be troublesome. To clarify this  observe that the
joint density induced by the ﬁnite data is

d(xd−x(cid:48)

d=1 w2

d)2

2

p(y  f   θ) = N (y|f   σ2I)N (f|0  Kf  f )p(θ) 

(3)
where the vector f stores the latent function values at inputs X and Kf  f is the n × n kernel matrix
obtained by evaluating the kernel function on those inputs. Clearly  in the term N (f|0  Kf  f ) the
hyperparameters θ appear non-linearly inside the inverse and determinant of the kernel matrix Kf  f .
While there exist a recently developed variational inference method applied to Gaussian process
latent variable model (GP-LVM) (Titsias and Lawrence  2010)  that approximately integrates out
inputs that appear inside a kernel matrix  this method is still not applicable to the case of kernel
hyperparameters such as length-scales. This is because the augmentation with auxiliary variables
used in (Titsias and Lawrence  2010)  that allows to bypass the intractable term N (f|0  Kf  f )  leads
to an inversion of a matrix Ku u that still depends on the kernel hyperparameters. More precisely 
the Ku u matrix is deﬁned on auxiliary values u comprising points of the function f (x) at some
arbitrary and freely optimisable inputs (Snelson and Ghahramani  2006a; Titsias  2009). While this
kernel matrix does not depend on the inputs X any more (which need to be integrated out in the
GP-LVM case)  it still depends on θ  making a possible variational treatment of those parameters

2

intractable. In Section 2.3  we present a novel modiﬁcation of the approach in (Titsias and Lawrence 
2010) which allows to overcome the above intractability. Such an approach makes use of the so-
called standardised representation of the GP model that is described next.

2.2 The standardised representation
Consider a function s(z)  where z ∈ RK  which is taken to be a random sample drawn from a GP
indexed by elements in the low K-dimensional space and assumed to have a zero mean function and
the following squared exponential kernel function:

ks(z  z(cid:48)) = e− 1

2||z−z(cid:48)||2

(4)
where the kernel length-scales and global scale are equal to unity. The above GP is referred to as
standardised process  whereas a sample path s(z) is referred to as a standardised function. The inter-
esting property that a standardised process has is that it does not depend on kernel hyperparameters
since it is deﬁned in a space where all hyperparameters have been neutralised to take the value one.
Having sampled a function s(z) in the low dimensional input space RK  we can deterministically
express a function f (x) in the high dimensional input space RD according to

 

f (x) = σf s(Wx) 

(5)
where the scalar σf and the matrix W ∈ RK×D are exactly the hyperparameters deﬁned in the
previous section. The above simply says that the value of f (x) at a certain input x is the value of the
standardised function s(z)  for z = Wx ∈ RK  times a global scale σf that changes the amplitude
or power of the new function. Given (σf   W)  the above assumptions induce a GP prior on the
function f (x)  which has zero mean and the following kernel function
f e− 1
2 d2

(6)
which is precisely the kernel function given in eq. (1) and therefore  the above construction leads to
the same GP prior distribution described in Section 2.1. Nevertheless  the representation using the
standardised process also implies a reparametrisation of the GP regression model where a priori the
hyperparameters θ and the GP function are independent. More precisely  one can now represent the
GP model according to the following structure:

kf (x  x(cid:48)) = E[σf s(Wx)σf s(Wx(cid:48))] = σ2

W(x x(cid:48)) 

s(z) ∼ GP(0  ks(z  z(cid:48)))  θ ∼ p(θ)
f (x) = σf s(Wx)

yi ∼ N (yi|f (xi)  σ2)  i = 1  . . .   n

(7)
which is depicted graphically in Figure 1(b). The interesting property of this representation is that
the GP function s(z) and the hyperparameters θ interact only inside the likelihood function while
a priori are independent. Furthermore  according to this representation one could now consider a
modiﬁcation of the variational method in (Titsias and Lawrence  2010) so that the auxiliary variables
u are deﬁned to be points of the function s(z) so that the resulting kernel matrix Ku u which needs
to be inverted does not depend on the hyperparameters but only on some freely optimisable inputs.
Next we discuss the details of this variational method.

2.3 Variational inference using auxiliary variables
We deﬁne a set of m auxiliary variables u ∈ Rm such that each ui is a value of the standardised
function so that ui = s(zi) and the input zi ∈ RK lives in dimension K. The set of all inputs
Z = (z1  . . .   zm) are referred to as inducing inputs and consist of freely-optimisable parameters
that can improve the accuracy of the approximation. The inducing variables u follow the Gaussian
density

(8)
where [Ku u]ij = ks(zi  zj) and ks is the standardised kernel function given by eq. (4). Notice that
the density p(u) does not depend on the kernel hyperparameters and particularly on the matrix W.
This is a rather critical point  that essentially allows the variational method to be applicable  and
comprise the novelty of our method compared to the initial framework in (Titsias and Lawrence 
2010). The vector f of noise-free latent function values  such that [f ]i = σf s(Wxi)  covary with
the vector u based on the cross-covariance function

p(u) = N (u|0  Ku u) 

kf u(x  z) = E[σf s(Wx)s(z)] = σf E[s(Wx)s(z)] = σf e− 1

2||Wx−z||2

= σf ks(Wx  z).

(9)

3

θ

s(x)

θ

f (x)

f (x)

y

(a)

y

(b)

(c)

Figure 1: The panel in (a) shows the usual hierarchical structure of a GP model where the middle node cor-
responds to the full length function f (x) (although only a ﬁnite vector f is associated with the data). The
panel in (b) shows an equivalent representation of the GP model expressed through the standardised ran-
dom function s(z)  that does not depend on hyperparameters  and interacts with the hyperparameters at the
data generation process. The rectangular node for f (x) corresponds to a deterministic operation representing
f (x) = σf s(Wx). The panel in (c) shows how the latent dimensionality of the Puma dataset is inferred to be
4  roughly corresponding to input dimensions 4  5  15 and 16 (see Section 3.3).

Based on this function  we can compute the cross-covariance matrix Kf  u and subsequently express
the conditional Gaussian density (often referred to as conditional GP prior):

p(f|u  W) = N (f|Kf  uK−1

u uu  Kf  f − Kf  uK−1

a marginalisation over the inducing variables  i.e. p(f|W) =(cid:82) p(f|u  W)p(u)du. We would like

so that p(f|u  W)p(u) allows to obtain the initial conditional GP prior p(f|W)  used in eq. (3)  after

u uKT

f  u) 

now to apply variational inference in the augmented joint model1

p(y  f   u  W) = N (y|f   σ2I)p(f|u  W)p(u)p(W) 

in order to approximate the intractable posterior distribution p(f   W  u|y). We introduce the varia-
tional distribution

q(f   W  u) = p(f|u  W)q(W)q(u) 

(10)
where p(f|u  W) is the conditional GP prior that appears in the joint model  q(u) is a free-form
variational distribution that after optimisation is found to be Gaussian (see Section B.1 in the sup-
plementary material)  while q(W) is restricted to be the following factorised Gaussian:

K(cid:89)

D(cid:89)

q(W) =

N (wkd|µdk  σ2

kd) 

(11)

k=1

d=1

The variational lower bound that minimises the Kullback Leibler (KL) divergence between the vari-
ational and the exact posterior distribution can be written in the form

F = F1 − KL(q(W)||p(W)) 

(12)
where the analytical form of F1 is given in Section B.1 of the supplementary material  whereas the
KL divergence term KL(q(W)||p(W)) that depends on the prior distribution over W is described
in the next section.
The variational lower bound is maximised using gradient-based methods over the variational pa-
kd}K D
rameters {µkd  σ2
k=1 d=1  the inducing inputs Z (which are also variational parameters) and the
hyperparameters (σf   σ2).

1The scale parameter σf and the noise variance σ2 are not assigned prior distributions  but instead they are
treated by Type II ML. Notice that the treatment of (σf   σ2) with a Bayesian manner is easier and approximate
inference could be done with the standard conjugate variational Bayesian framework (Bishop  2006).

4

1234567891000.511.522.533.544.5Latent dimension (sorted)RelevanceLatent dimension (sorted)Input dimension246810510152025302.4 Prior over p(W) and analytical reduction of the number of optimisable parameters
To set the prior distribution for the parameters W  we follow the automatic relevance determina-
tion (ARD) idea introduced in (MacKay  1994; Neal  1998) and subsequently considered in several
models such as sparse linear models (Tipping  2001) and variational Bayesian PCA (Bishop  1999).
Speciﬁcally  the prior distribution takes the form

p(W) =

N (wkd|0  (cid:96)2
k) 

(13)

K(cid:89)

D(cid:89)

k=1

d=1

k}K

where the elements of each row of W follow a zero-mean Gaussian distribution with a common
variance. Learning the set of variances {(cid:96)2
k=1 can allow to automatically select the dimensionality
associated with the Mahalanobis distance metric dW(x  x(cid:48)). This could be carried out by either
applying a Type II ML estimation procedure or a variational Bayesian approach  where the latter as-
signs a conjugate Gamma prior on the variances and optimises a variational distribution q({(cid:96)2
k=1)
over them. The optimisable quantities in both these procedures can be removed analytically and
optimally from the variational lower bound as described next.
Consider the case where we apply Type II ML for the variances {(cid:96)2
k=1. These parameters appear
only in the KL(q(W)||p(W)) term (denoted by KL in the following) of the lower bound in eq. (12)
which can be written in the form:

k}K

k}K

KL =

1
2

d=1 σ2
dk + µ2
dk
(cid:96)2
k

− D − D(cid:88)

d=1

log

σ2
dk
(cid:96)2
k

(cid:35)

.

By ﬁrst minimizing this term with respect to these former hyperparameters we ﬁnd that

K(cid:88)

k=1

(cid:34)(cid:80)D
(cid:80)D

(cid:96)2
k =

(cid:34) D(cid:88)

K(cid:88)

d=1 σ2
dk + µ2
dk
D

  k = 1  . . .   K 

(14)

(cid:32) D(cid:88)

and then by substituting back these optimal values into the KL divergence we obtain

(cid:35)

(cid:33)
(cid:0)(cid:96)2
(cid:1)−α−1
k}K
D(cid:88)
K(cid:88)

k

KL =

1
2

log σ2

dk − D log

k=1

d=1

d=1

σ2
dk + µ2
dk

+ D log D

 

(15)

which now depends only on variational parameters. When we treat {(cid:96)2
we assign inverse Gamma prior to each variance (cid:96)2
ing a similar procedure as the one above we can remove optimally the variational factor q({(cid:96)2
(see Section B.2 in the supplementary material) to obtain

k=1 in a Bayesian manner 
− β
k . Then  by follow-
(cid:96)2
e
k=1)

k) = βα

k  p((cid:96)2

k}K

Γ(α)

(cid:18) D

2

KL = −

(cid:19) K(cid:88)

(cid:32)

D(cid:88)

(cid:33)

1
2

+ α

log

2β +

µ2
kd + σ2
kd

+

log(σ2

kd) + const 

(16)

k=1

d=1

k=1

d=1

which  as expected  has the nice property that when α = β = 0  so that the prior over variances
becomes improper  it reduces to the quantity in (15).
Finally  it is important to notice that different and particularly non-Gaussian priors for the parameters
W can be also accommodated by our variational method. More precisely  any alternative prior for
W changes only the form of the negative KL divergence term in the lower bound in eq. (12). This
term remains analytically tractable even for priors such as the Laplace or certain types of spike and
slab priors. In the experiments we have used the ARD prior described above while the investigation
of alternative priors is intended to be studied as a future work.

2.5 Predictions
Assume we have a test input x∗ and we would like to predict the corresponding output y∗. The exact
predictive density p(y∗|y) is intractable and therefore we approximate it with the density obtained
by averaging over the variational posterior distribution:

(cid:90)

q(y∗|y) =

N (y∗|f∗  σ2)p(f∗|f   u  W)p(f|u  W)q(u)q(W)df∗df dudW 

(17)

5

where p(f|u  W)q(u)q(W) is the variational distribution and p(f∗|f   u  W) is the conditional GP
prior over the test value f∗ given the training function values f and the inducing variables u. By

performing ﬁrst the integration over f  we obtain (cid:82) p(f∗|f   u  W)p(f|u  W)df = p(f∗|u  W)

which yields as a consequence of the consistency property of the Gaussian process prior. Given
that p(f∗|u  W) and q(u) (see Section B.1 in the supplementary material) are Gaussian densities
with respect to f∗ and u  the above can be further simpliﬁed to

q(y∗|y) =

N (y∗|µ∗(W)  σ2∗(W) + σ2)q(W)dW 

where the mean µ∗(W) and variance σ2∗(W) obtain closed-form expressions and consist of non-
linear functions of W making the above integral intractable. However  by applying Monte Carlo
integration based on drawing independent samples from the Gaussian distribution q(W) we can
efﬁciently approximate the above according to

(cid:90)

T(cid:88)

t=1

q(y∗|y) =

1
T

N (y∗|µ∗(W(t))  σ2∗(W(t)) + σ2) 

(18)

which is the quantity used in our experiments. Furthermore  although the predictive density is not
Gaussian  its mean and variance can be computed analytically as explained in Section B.1 of the
supplementary material.

3 Experiments
In this section we will use standard data sets to assess the performance of the proposed VDMGP
in terms of normalised mean square error (NMSE) and negative log-probability density (NLPD).
We will use as benchmarks a full GP with automatic relevance determination (ARD) and the state-
of-the-art SPGP-DR model  which is described below. Also  see Section A of the supplementary
material for an example of dimensionality reduction on a simple toy example.

3.1 Review of SPGP-DR
The sparse pseudo-input GP (SPGP) from Snelson and Ghahramani (2006a) is a well-known sparse
GP model  that allows the computational cost of GP regression to scale linearly with the number of
samples in a the dataset. This model is sometimes referred to as FITC (fully independent training
conditional) and uses an active set of m pseudo-inputs that control the speed vs. performance trade-
off of the method. SPGP is often used when dealing with datasets containing more than a few
thousand samples  since in those cases the cost of a full GP becomes impractical.
In Snelson and Ghahramani (2006b)  a version of SPGP with dimensionality reduction (SPGP-DR)
is presented. SPGP-DR applies the SPGP model to a linear projection of the inputs. The K × D
projection matrix W is learned so as to maximise the evidence of the model. This can be seen
simply as a specialisation of SPGP in which the covariance function is a squared exponential with a
Mahalanobis distance deﬁned by W(cid:62)W. The idea had already been applied to the standard GP in
(Vivarelli and Williams  1998).
Despite the apparent similarities between SPGP-DR and VDMGP  there are important differences
worth clarifying. First  SPGP’s pseudo-inputs are model parameters and  as such  ﬁtting a large
number of them can result in overﬁtting  whereas the inducing inputs used in VDMGP are varia-
tional parameters whose optimisation can only result in a better ﬁt of the posterior densities. Second 
SPGP-DR does not place a prior on the linear projection matrix W; it is instead ﬁtted using Max-
imum Likelihood  just as the pseudo-inputs. In contrast  VDMGP does place a prior on W and
variationally integrates it out.
These differences yield an important consequence: VDMGP can infer automatically the latent di-
mensionality K of data  but SPGP-DR is unable to  since increasing K is never going to decrease
its likelihood. Thus  VDMGP follows Occam’s razor on the number of latent dimensions K.

3.2 Temp and SO2 datasets
We will assess VDMGP on real-world datasets. For this purpose we will use the two data sets
from the WCCI-2006 Predictive Uncertainty in Environmental Modeling Competition run by Gavin

6

(a) Temp 
std. dev. of avg.

avg. NMSE ± 1

(b) SO2 
std. dev. of avg.

avg. NMSE ± 1

(c) Puma 
std. dev. of avg.

avg. NMSE ± 1

(d) Temp  avg. NLPD ± one
std. dev. of avg.

avg. NLPD ± one

(e) SO2 
std. dev. of avg.

(f) Puma  avg.
std. dev. of avg.

NLPD ± one

Figure 2: Average NMSE and NLPD for several real datasets  showing the effect of different training set sizes.

Cawley2  called Temp and SO2. In dataset Temp  maximum daily temperature measurements have
to be predicted from 106 input variables representing large-scale circulation information. For the
SO2 dataset  the task is to predict the concentration of SO2 in an urban environment twenty-four
hours in advance  using information on current SO2 levels and meteorological conditions.3 These
are the same datasets on which SPGP-DR was originally tested (Snelson and Ghahramani  2006b) 
and it is worth mentioning that SPGP-DR’s only entry in the competition (for the Temp dataset) was
the winning one.
We ran SPGP-DR and VDMGP using the same exact initialisation for the projection matrix on
both algorithms and tested the effect of using a reduced number of training data. For SPGP-DR
we tested several possible latent dimensions K = {2  5  10  15  20  30}  whereas for VDMGP we
ﬁxed K = 20 and let the model infer the number of dimensions. The number of inducing variables
(pseudo-inputs for SPGP-DR) was set to 10 for Temp and 20 for SO2. Varying sizes for the training
set between 100 and the total amount of available samples were considered. Twenty independent
realisations were performed.
Average NMSE as a function of training set size is shown in Figures 2(a) and 2(b). The multiple
dotted blue lines correspond to SPGP-DR with different choices of latent dimensionality K. The
dashed black line represents the full GP  which has been run for training sets up to size 2000. VD-
MGP is shown as a solid red line. Similarly  average NLPD is shown as a function of training set
size in Figures 2(d) and 2(e).
When feasible  the full GP performs best  but since it requires the inversion of the full kernel matrix 
it cannot by applied to large-scale problems such as the ones considered in this subsection. Also 
even in reasonably-sized problems  the full GP may run into trouble if several noise-only input
dimensions are present. SPGP-DR works well for large training set sizes  since there is enough
information for it to avoid overﬁtting and the advantage of using a prior on W is reduced. However 

2Available at http://theoval.cmp.uea.ac.uk/˜gcc/competition/

Temp: 106 dimensions 7117/3558 training/testing data  SO2: 27 dimensions 15304/7652 training/testing data.
3For SO2  which contains only positive labels yn  a logarithmic transformation of the type log(a + yn) was
applied  just as the authors of (Snelson and Ghahramani  2006b) did. However  reported NMSE and NLPD
ﬁgures still correspond to the original labels.

7

10020050010002000500071170.050.10.150.20.250.30.350.40.450.50.55  Full GPVDMGPSPGP−DR100200500100020005000153040.80.911.11.21.31.41.5  Full GPVDMGPSPGP−DR10020050010002000500071680.20.40.60.811.21.4  Full GPVDMGPSPGP−DR100200500100020005000711700.20.40.60.811.21.41.61.82  Full GPVDMGPSPGP−DR100200500100020005000153044.44.64.855.25.45.65.86  Full GPVDMGPSPGP−DR1002005001000200050007168−0.200.20.40.60.811.21.41.61.8  Full GPVDMGPSPGP−DRfor smaller training sets  performance is quite bad and the choice of K becomes very relevant (which
must be selected through cross-validation). Finally  VDMGP results in scalable performance: It is
able to perform dimensionality reduction and achieve high accuracy both on small and large datasets 
while still being faster than a full GP.
3.3 Puma dataset
In this section we consider the 32-input  moderate noise version of the Puma dataset.4 This is
realistic simulation of the dynamics of a Puma 560 robot arm. Labels represent angular accelerations
of one of the robot arm’s links  which have to be predicted based on the angular positions  velocities
and torques of the robot arm. 7168 samples are available for training and 1024 for testing.
It is well-known from previous works (Snelson and Ghahramani  2006a) that only 4 out of the 32
input dimensions are relevant for the prediction task  and that identifying them is not always easy.
In particular  SPGP (the standard version  with no dimensionality reduction)  fails at this task unless
initialised from a “good guess” about the relevant dimensions coming from a different model  as
discussed in (Snelson and Ghahramani  2006a). We thought it would be interesting to assess the
performance of the discussed models on this dataset  again considering different training set sizes 
which are generated by randomly sampling from the training set.
Results are shown in Figures 2(c) and 2(f). VDMGPR determines that there are 4 latent dimen-
sions  as shown in Figure 1(c). The conclusions to be drawn here are similar to those of the previous
subsection: SPGP-DR has trouble with “small” datasets (where the threshold for a dataset being con-
sidered small enough may vary among different datasets) and requires a parameter to be validated 
whereas VDMGPR seems to perform uniformly well.
3.4 A note on computational complexity
The computational complexity of VDMGP is O(N M 2K +N DK)  just as that of SPGP-DR  which
is much smaller than the O(N 3+N 2D) required by a full GP. However  since the computation of the
variational bound of VDMGP involves more steps than the computation of the evidence of SPGP-
DR  VDMGP is slower than SPGP-DR. In two typical cases using 500 and 5000 training points
full GP runs in 0.24 seconds (for 500 training points) and in 34 seconds (for 5000 training points) 
VDMGP runs in 0.35 and 3.1 seconds while SPGP-DR runs in 0.01 and 0.10 seconds.
4 Discussion and further work
A typical approach to regression when the number of input dimensions is large is to ﬁrst use a
linear projection of input data to reduce dimensionality (e.g.  PCA) and then apply some regres-
sion technique. Instead of approaching this method in two steps  a monolithic approach allows the
dimensionality reduction to be tailored to the speciﬁc regression problem.
In this work we have shown that it is possible to variationally integrate out the linear projection
of the inputs of a GP  which  as a particular case  corresponds to integrating out its length-scale
hyperparameters. By placing a prior on the linear projection  we avoid overﬁtting problems that may
arise in other models  such as SPGP-DR. Only two parameters (noise variance and scale) are free in
this model  whereas the remaining parameters appearing in the bound are free variational parameters 
and optimizing them can only result in improved posterior estimates. This allows us to automatically
infer the number of latent dimensions that are needed for regression in a given problem  which is
also not possible using SPGP-DR. Finally  the size of the data sets that the proposed model can
handle is much wider than that of SPGP-DR  which performs badly on small-size data.
One interesting topic for future work is to investigate non-Gaussian sparse priors for the parameters
W. Furthermore  given that W represents length-scales it could be replaced by a random function
W(x)  such a GP random function  which would render the length-scales input-dependent  mak-
ing such a formulation useful in situations with varying smoothness across input space. Such a
smoothness-varying GP is also an interesting subject of further work.
Acknowledgments
MKT greatly acknowledges support from “Research Funding at AUEB for Excellence and Extro-
version  Action 1: 2012-2014”. MLG acknowledges support from Spanish CICYT TIN2011-24533.
4Available from Delve  see http://www.cs.toronto.edu/˜delve/data/pumadyn/desc.

html.

8

References
Bishop  C. M. (1999). Variational principal components.

Conference on Artiﬁcial Neural Networks  ICANN?99  pages 509–514.

In In Proceedings Ninth International

Bishop  C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statis-

tics). Springer  1st ed. 2006 edition.

MacKay  D. J. (1994). Bayesian non-linear modelling for the energy prediction competition. SHRAE

Transactions  4:448–472.

Murray  I. and Adams  R. P. (2010). Slice sampling covariance hyperparameters of latent Gaussian
models. In Lafferty  J.  Williams  C. K. I.  Zemel  R.  Shawe-Taylor  J.  and Culotta  A.  editors 
Advances in Neural Information Processing Systems 23  pages 1723–1731.

Neal  R. M. (1998). Assessing relevance determination methods using delve. Neural Networksand

Machine Learning  pages 97–129.

Rasmussen  C. and Williams  C. (2006). Gaussian Processes for Machine Learning. Adaptive

Computation and Machine Learning. MIT Press.

Snelson  E. and Ghahramani  Z. (2006a). Sparse Gaussian processes using pseudo-inputs. In Ad-

vances in Neural Information Processing Systems 18  pages 1259–1266. MIT Press.

Snelson  E. and Ghahramani  Z. (2006b). Variable noise and dimensionality reduction for sparse

Gaussian processes. In Uncertainty in Artiﬁcial Intelligence.

Tipping  M. E. (2001). Sparse bayesian learning and the relevance vector machine. Journal of

Machine Learning Research  1:211–244.

Titsias  M. K. (2009). Variational learning of inducing variables in sparse Gaussian processes. In

Proc. of the 12th International Workshop on AI Stats.

Titsias  M. K. and Lawrence  N. D. (2010). Bayesian Gaussian process latent variable model. Jour-

nal of Machine Learning Research - Proceedings Track  9:844–851.

Vivarelli  F. and Williams  C. K. I. (1998). Discovering hidden features with Gaussian processes

regression. In Advances in Neural Information Processing Systems  pages 613–619.

Weinberger  K. Q. and Saul  L. K. (2009). Distance metric learning for large margin nearest neighbor

classiﬁcation. J. Mach. Learn. Res.  10:207–244.

Xing  E.  Ng  A.  Jordan  M.  and Russell  S. (2003). Distance metric learning  with application to

clustering with side-information.

9

,Michalis Titsias RC AUEB
Miguel Lazaro-Gredilla