2019,Transfer Learning via Minimizing the Performance Gap Between Domains,We propose a new principle for transfer learning  based on a straightforward intuition: if two domains are similar to each other  the model trained on one domain should also perform well on the other domain  and vice versa. To formalize this intuition  we define the performance gap as a measure of the discrepancy between the source and target domains. We derive generalization bounds for the instance weighting approach to transfer learning  showing that the performance gap can be viewed as an algorithm-dependent regularizer  which controls the model complexity. Our theoretical analysis provides new insight into transfer learning and motivates a set of general  principled rules for designing new instance weighting schemes for transfer learning. These rules lead to gapBoost  a novel and principled boosting approach for transfer learning. Our experimental evaluation on benchmark data sets shows that gapBoost significantly outperforms previous boosting-based transfer learning algorithms.,Transfer Learning via Minimizing the Performance

Gap Between Domains

Boyu Wang

Department of Computer Science

University of Western Ontario

bwang@csd.uwo.ca

Jorge A. Mendez

Department of Computer and Information Science

University of Pennsylvania

mendezme@seas.upenn.edu

Princeton Neuroscience Insititute

Department of Computer and Information Science

Ming Bo Cai

Princeton University
mcai@princeton.edu

Eric Eaton

University of Pennsylvania
eeaton@seas.upenn.edu

Abstract

We propose a new principle for transfer learning  based on a straightforward
intuition: if two domains are similar to each other  the model trained on one domain
should also perform well on the other domain  and vice versa. To formalize this
intuition  we deﬁne the performance gap as a measure of the discrepancy between
the source and target domains. We derive generalization bounds for the instance
weighting approach to transfer learning  showing that the performance gap can be
viewed as an algorithm-dependent regularizer  which controls the model complexity.
Our theoretical analysis provides new insight into transfer learning and motivates a
set of general  principled rules for designing new instance weighting schemes for
transfer learning. These rules lead to gapBoost  a novel and principled boosting
approach for transfer learning. Our experimental evaluation on benchmark data sets
shows that gapBoost signiﬁcantly outperforms previous boosting-based transfer
learning algorithms.

1

Introduction

Transfer learning is based on the idea that learning a new concept is easier after having learned one or
more similar concepts. By extracting knowledge from a set of related concepts (source domains) and
then leveraging this knowledge upon learning the concept of interest (target domain)  the learning
performance can be improved. This is especially beneﬁcial when there is insufﬁcient data to learn
solely from the target domain  but enough knowledge from the source domains is available. Transfer
learning has become increasingly relevant over the last two decades  and consequently during that
time various algorithms have been proposed [10  17  39  22  24  11]  accompanied by theoretical and
empirical justiﬁcations [4  25  3  21  18  19  26].
In order to successfully transfer information from one domain to another  it is critical to understand
the similarities and differences between the domains. Intuitively  the more similar the two domains
are  the more information can be transferred. When the domains are considerably different  but still
related  a common strategy to correct this difference is to minimize some measure of divergence
between the empirical source and target data distributions. Most prior work in this area has focused
on deﬁning discrepancy measures that motivate the design of algorithms that effectively reduce the
dissimilarity between domains as much as possible [16  17  35  6  2  34  3  25  7  14  1  33]. These
works have mainly considered the problem of domain adaptation  where examples from the target

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

domain are entirely unlabeled. However  in many practical cases  there is a small amount of labeled
target data  which can be leveraged to derive more specialized measures of domain divergence.
To address this issue  we present the ﬁrst analysis for instance weighting transfer learning that
considers the presence of labeled target examples. The contribution of our work is two-fold. 1.
We address the question of how to measure the divergence between two domains given label
information for the target domain. Intuitively  if two domains are similar to each other  the model
trained on one domain should also perform well on the other domain  and vice versa. To formalize this
intuition  we propose the notion of performance gap between the source and target domains  and show
that the transfer learning model complexity can be upper bounded in terms of this performance gap.
In other words  it can be viewed as an algorithm-dependent regularizer  which leads to ﬁner and more
informative generalization bounds. This is  to the best of our knowledge  the ﬁrst generalization bound
for instance-based transfer learning that considers the presence of labeled target data. Moreover  our
deﬁnition of performance gap is intuitive and generally applicable to any form of transfer. Thus  our
analysis provides a deeper understanding of the general problem of transfer learning and new insight
into how to leverage the labeled target examples. 2. On the algorithmic side  instead of directly
minimizing the generalization bound  which is highly computationally expensive  we propose four
principled rules to follow when designing an instance weighting scheme for transfer learning. We
instantiate these rules with gapBoost  a novel and efﬁcient boosting algorithm for transfer learning 
which offers out-of-the-box usability and readily accommodates any algorithm for transfer learning.
Source code for gapBoost is available at https://github.com/bwang-ml/gapBoost.

2 Related Work

The large majority of transfer learning techniques can be categorized as instance  feature  or parameter
transfer [29  40  7]. In this paper  we consider the instance transfer approach  where the objective is
to correct the difference between the domains by weighting the instances. In this context  the authors
in [5  3] studied transfer learning algorithms that minimize a convex combination of the source and
target empirical risks  and proposed to use the H-divergence [4] to measure the distance between
the domains for 0-1 loss classiﬁcation. This study was generalized to arbitrary loss functions by
introducing the notion of discrepancy distance [25]. Since then  various measures have been proposed
in the literature [35  6  2  34  7  14  33]. Recently  instance weighting has been revisited in [21] based
on the notion of algorithmic stability. The authors revealed that the source domain features can be
interpreted as a regularization matrix  which beneﬁts the learning process of the target domain task.
Despite the wide applicability of the discrepancy measures deﬁned in these works  they fail to address
three problems. 1. These measures are designed for the setting of domain adaptation  where no label
information is available in the target domain. As a result  it is unclear how to leverage any labeling
information from the target domain in cases where it is available. Moreover  deriving generalization
bounds for domain adaptation requires additional assumptions. One common assumption is that there
exists an ideal hypothesis that performs well on both domains [3  25]  which cannot be empirically
veriﬁed due to the lack of labeled examples in the target domain. 2. These measures are either
algorithm-independent [16  35  6  2  34] or deﬁned over a hypothesis class [3  25  7  14  33]  and
so they ignore the speciﬁc algorithm used. An algorithm-speciﬁc notion of divergence measure
could lead to more informative generalization guarantees. 3. From the algorithmic perspective  most
methods are restricted to linear hypotheses (or nonlinear hypotheses deﬁned through a reproducing
kernel Hilbert space) and derive the instance weights by directly minimizing the generalization
bounds or divergence measures  which usually imposes a high computational burden [25  14  7].
Having access to labeled examples in the target domain enables us to derive more efﬁcient learning
algorithms. In [10]  an efﬁcient transfer boosting method was proposed to reweight the data for clas-
siﬁcation in the presence of labeled target data. Later  this approach was extended to regression [30]
and multi-source transfer [41  12]. In [36]  the authors proposed a two-stage instance weighting
approach for transfer learning  and analyzed its generalization bound by extending the result from [3].
While these algorithms are effective in practice  no theoretical results have been presented to show
why transfer learning succeeds when labeled information from the target domain is available.
In addition  most existing theoretical studies of transfer learning examine the convergence rate of
the Rademacher complexity or stability coefﬁcient  assuming that the model complexity (and hence
the loss function) of the transfer learning algorithm is upper bounded by a constant. One related

2

work we are aware of that relaxes this assumption is [42]  which proves the boundedness of the loss
functions in the setting of multitask learning. However  their analysis only relates the bounds with
regularization functions and requires the additional assumption that when the hypothesis outputs 0 
the loss function is upper bounded by another constant. Critically  these analyses do not provide
any insight into how the domain divergence affects the model complexity and the generalization
bound. More recently  this issue has been studied in [38]  showing that the model complexity of
parameter-sharing multitask learning algorithms is determined by the task similarities. However  this
theoretical result has not motivated any concrete algorithm.
In contrast to prior work  we derive an algorithm-speciﬁc generalization bound that considers the label
information from the target domain. Based on our newly developed theory  we design a principled
and efﬁcient instance weighting transfer learning algorithm.

3

Instance Weighting for Transfer Learning

In this section  we formalize the problem of instance weighting for transfer learning. We continue
by proposing four general rules to follow when developing new weighting schemes  along with the
theoretical grounds that support these rules. We then instantiate these new rules with gapBoost.
Let z = (x  y) ∈ X × Y be a training example drawn from some unknown distribution D  where x is
the data point  and y is its label  with Y = {−1  1} for binary classiﬁcation and Y ⊆ R for regression.
A hypothesis is a function h ∈ H that maps X to the set Y(cid:48) sometimes different from Y  where H is a
hypothesis class. For a convex  non-negative loss function (cid:96) : Y(cid:48) ×Y (cid:55)→ R+  we denote by (cid:96)(h(x)  y)
the loss of hypothesis h at point z = (x  y). Let S = {zi = (xi  yi)}N
(cid:80)N
i=1 be a set of N training
examples drawn independently from D. The empirical loss of h on S and its generalization loss
over D are deﬁned  respectively  by LS(h) = 1
i=1 (cid:96)(h(xi)  yi)  and LD(h) = Ez∼D[(cid:96)(h(x)  y)].
We consider the linear function class in a Euclidean space  but our analysis is also applicable to a
reproducing kernel Hilbert space. We also assume that (cid:107)x(cid:107)2 ≤ R ∀x ∈ X for some R ∈ R+  and
the loss function is ρ-Lipschitz continuous for some ρ ∈ R+.
In the setting of transfer learning  we have a training sample S = {ST   SS} of size N = NT + NS
composed of ST = {zT
i =
i=1 drawn from a source distribution DS. We analyze the transfer learning algorithms based
(xS
i   yS
on instance weighting  which aims to optimize the following objective function:

i=1 drawn from a target distribution DT and SS = {zS

i = (xT

i )}NT

i )}NS

i   yT

N

h∈HLΓ

min

S(h) + λR(h)  

(1)

S(h) = LΓT

ST (h) + LΓS

where LΓ
SS (h) is the weighted empirical loss over the source and tar-
get domains  R(h) is a regularization function to control the model complexity of h  and λ
(cid:80)NT
is a regularization parameter. The domain-speciﬁc weighted losses are given by LΓT
ST (h) =
i ). The instance weights Γ = [ΓT ; ΓS ]  with
i=1 γT
ΓT = [γT
i = 1 
and they can either be learned in a pre-processing step [17  8  28  15] or incorporated into learning
algorithms [10  23]. As we consider the linear function class  the hypothesis h has the form of an
inner product h(x) = (cid:104)h  x(cid:105)  and we study the regularization function R(h) = (cid:107)h(cid:107)2
2.

+   are such that(cid:80)NT

SS (h) =(cid:80)NS

i ) and LΓS
i )  yT
NT ](cid:62) ∈ RNT

i )  yS
NS ](cid:62) ∈ RNS

i +(cid:80)NS

i (cid:96)(h(xT
1   . . .   γT

i (cid:96)(h(xS

i=1 γS
1   . . .   γS

+ and ΓS = [γS

i=1 γT

i=1 γS

3.1 Principles for Instance Weighting

Leveraging problem (1) requires assigning appropriate values to Γ so that the solution to problem (1)
leads to effective transfer. There are a variety of weighting schemes developed in the literature. In
this paper  we summarize four general and intuitively reasonable rules as follows. As we will show
later  they are also theoretically grounded.

1. Minimize the weighted empirical loss over source and target domains  as suggested by (1).
2. Assign balanced weights to data points  as focusing too much on speciﬁc data points leads

to overﬁtting caused by perturbations in the training data [32].

3. Assign more weight to the target sample  since target data will be used for testing.
4. Assign weights such that the performance gap between the domains is small.

3

Our main contribution lies in Rule 4  for which we introduce the notion of performance gap. Although
intuitive  these rules are contradictory  so designing an algorithm based on them requires properly
trading them off. We explore one way to control this tradeoff via hyper-parameters in Section 3.3.

3.2 Theoretical Justiﬁcations

We now develop the theoretical foundations that justify the instance weighting rules. In contrast to
previous studies on domain adaptation  we propose a notion to measure the divergence between the
domains that leverages the label information  leading to Rule 4 in our instance weighting scheme.
Intuitively  if two domains are similar  the model trained on one domain should also perform well on
the other. To make this intuition precise  we deﬁne the notion of performance gap below.
Deﬁnition 1 (Performance gap). Let VS (h) = LΓS
ST (h) + ηλR(h) 
respectively  be the objective functions in the source and target domains  where η ∈ (0  1
2 )  and let
their minimizers  respectively  be hSS and hST . The performance gap between the source and target
domains is deﬁned as

SS (h) + ηλR(h) and VT (h) = LΓT

∇ = ∇T + ∇S  

ST (hST ).

ST (hSS ) − LΓT

SS (hST ) − LΓS

SS (hSS ) and ∇T = LΓT

where ∇S = LΓS
Note that the performance gap is both data and algorithm dependent  which is crucial for deriving
a more informative and ﬁner generalization bound. Moreover  note that  although we use the
performance gap to analyze the speciﬁc setting of instance weighting  it could be readily applied to
other transfer learning paradigms  such as feature-based transfer. We now present the deﬁnition of
Y-Discrepancy  which we require for our analysis.
Deﬁnition 2 (Y-Discrepancy [27]). Let H be a hypothesis class mapping X to Y and let (cid:96) : Y×Y (cid:55)→
R+ deﬁne a loss function over Y. The Y-discrepancy distance between two distributions D1 and D2
over X × Y is deﬁned as:

distY (D1 D2) = sup
h∈H

|LD1(h) − LD2(h)|

.

S  which justiﬁes our principles for instance weighting.

Our main theoretical contribution is the following theorem that bounds the difference between LDT
and LΓ
Theorem 1. Let hS be the optimal solution of the transfer learning problem (1). Assume that
(cid:107)x(cid:107)2 ≤ R ∀x ∈ X   and that the loss function is ρ-Lipschitz continuous and convex. Then  for any
δ ∈ (0  1)  with probability at least 1 − δ  we have

LDT (hS) ≤ LΓ

S(hS) + εΓ + (cid:107)ΓS(cid:107)1 distY (DT  DS )  

(cid:19)(cid:115)

(2)

(cid:41)

.

log 2
δ

2

where

εΓ = min

(cid:40)(cid:107)Γ(cid:107)∞ρ2R2

+

λ

(cid:18) ρ2R2((cid:107)Γ(cid:107)2

+ (cid:107)Γ(cid:107)∞B(Γ)

2 + (cid:107)Γ(cid:107)∞)
λ
2(cid:107)Γ(cid:107)∞(cid:107)Γ(cid:107)2ρ2R2

(cid:114)

N log 1
δ

2

 

(cid:115)

λ

2N log

+ (cid:107)Γ(cid:107)2B(Γ)

4
δ

Remark 1. Rule 1 is justiﬁed by LΓ
S  Rule 2 is justiﬁed by ||Γ||2 and ||Γ||∞  and Rule 3 is justiﬁed
by ||ΓS||1. B(Γ) is an upper bound of the loss function (cid:96)  such that (cid:96)(h(x)  y) ≤ B(Γ)  where h is
the output hypothesis of an algorithm solving the transfer learning problem (1). We emphasize that it
is a function of Γ and  as we show later  can be upper bounded in terms of ∇  which justiﬁes Rule 4.

Proof Sketch. (Details of the proof are available in the appendix)
Step 1: Bound LDT from LΓD. Let LΓD = LΓT
S. Then 
by linearity of the expectation and the deﬁnition of Y-discrepancy  we show that the following holds:
(3)

DS be the expected weighted loss of LΓ

LDT ≤ LΓD + (cid:107)ΓS(cid:107)1 distY (DT  DS ) .

DT +LΓS

4

Remark 2. Compared to the notion of discrepancy [25]  one advantage of Y-discrepancy is that
it does not require the assumption that the loss function obeys the triangle inequality [3  9]  which
does not hold for many loss functions (e.g.  hinge loss  squared loss)  to make (3) hold. In addition 
we can prove that for a binary classiﬁcation problem  distY (DT  DS ) can be upper bounded from a
ﬁnite sample by constructing a new classiﬁcation problem  where the positive target examples and
negative source examples are positively labeled  and the negative target examples and positive source
examples are negatively labeled. See Lemma A and Lemma B in the appendix for more details.
S. We present two schemas to upper bound LΓD: one is based on
Step 2: Bound LΓD from LΓ
algorithmic stability  and the other one is based on Rademacher complexity  which lead to the
deﬁnition of εΓ.
Algorithmic stability bound. We introduce the notion of weight-dependent uniform stability (see
Deﬁnition A in the appendix) and show that  for any δ ∈ (0  1)  with probability at least 1 − δ  the
expected loss LDT can be upper bounded by:

LΓD ≤ LΓ

S +

(cid:107)Γ(cid:107)∞ρ2R2

λ

+

2 + (cid:107)Γ(cid:107)∞)
λ

+ (cid:107)Γ(cid:107)∞B(Γ)

N log 1
δ

2

.

(4)

Rademacher complexity bound. We introduce the notion of weighted Rademacher complexity (see
Deﬁnition B in the appendix )  and relate it to the notion of uniform argument stability [20]. Then 
we prove that the learning algorithm (1) produces an algorithmic hypothesis class B  and  for any
δ ∈ (0  1)  with probability at least 1 − δ  the expected loss LDT can be upper bounded by:

(cid:18) ρ2R2((cid:107)Γ(cid:107)2

(cid:19)(cid:115)

(cid:115)

(cid:114)

LΓD ≤ LΓ

S + 2

(cid:107)Γ(cid:107)∞(cid:107)Γ(cid:107)2ρ2R2

λ

2N log

+ B(Γ)(cid:107)Γ(cid:107)2

4
δ

log 2
δ

2

.

(5)

Combining (3)  (4)  and (5)  we obtain the generalization bound (2).
N  ∀i ∈ {1  . . .   N}  we recover the standard argument stability bound from
Remark 3. If γi = 1
(2)  which suggests assigning equal weights to all instances to achieve a fast convergence rate  due
to (cid:107)Γ(cid:107)∞ and (cid:107)Γ(cid:107)2. In particular  if (cid:107)Γ(cid:107)∞ (and hence (cid:107)Γ(cid:107)2
N )  (2) leads to a convergence
rate of O( 1√
). However  in the setting of transfer learning  it is usually the case that NT (cid:28)
NS. Consequently  we may have (cid:107)Γ(cid:107)∞ (cid:28) 1
NT   which implies that transfer learning has a faster
convergence rate than single-task learning. On the other hand  as we will show in Step 3  the loss
bound B is also a function of Γ  which suggests a new criterion for instance weighting.

2) is O( 1

N

Step 3: Bound B(Γ). The following lemma shows that the model complexity of the transfer
learning algorithm (1) can be upper bounded in terms of the performance ∇.
Lemma 1. Let hS be the optimal solution of the instance weighting transfer learning problem (1).
Then  we have:

(cid:107)hS(cid:107)2 ≤

∇

2λ(1 − 2η)

+

(cid:107)hSS(cid:107)2

2 + (cid:107)hST (cid:107)2
2

2

.

(cid:115)

(cid:115)

(cid:115)

By bounding the model complexity  we obtain various upper bounds for different loss functions.

Corollary 1. The hinge loss function of the learning algorithm (1) can be upper bounded by:

For regression  if the response variable is bounded by |y| ≤ Y   the (cid:96)q loss of (1) can be bounded by:

B(Γ) ≤ 1 + R

∇

2λ(1 − 2η)

+

(cid:107)hS(cid:107)2

2 + (cid:107)hT (cid:107)2
2

2

.

(cid:32)

B(Γ) ≤

Y + R

(cid:33)q

.

(cid:107)hS(cid:107)2

2 + (cid:107)hT (cid:107)2
2

2

∇

2λ(1 − 2η)

+

5

Algorithm 1 gapBoost
gapBoost
gapBoost
Input: SS   ST   K  ρS ≤ ρT ≤ 0  γmax  a learning algorithm A
1: Initialize DS
1 (i) = DT
2: for k = 1  . . .   K do
3:
4:
5:

Call A to train a base learner hk using SS ∪ ST with distribution DS
Call A to train an auxiliary learner hS
k over source domain using SS with distribution DS
Call A to train an auxiliary learner hT
k over target domain using ST with distribution DT
DT
k (i)1

  αk = log 1−k

NS +NT for all i

k ∪ DT

NT(cid:80)

NS(cid:80)

1 (i) =

DS

+

k

k

k

1

i )(cid:54)=yS

i

i=1

i )(cid:54)=hT

k (xS

i ) + αk 1

hk(xS

i )(cid:54)=yS

i

i=1

k (i)1

hS
k (xS

hk(xS
k =
for i = 1  . . .   NS do

βS
i = ρS 1
end for
for i = 1  . . .   NT do
βT
i = ρT 1

Zk+1 =(cid:80)NS

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end for
Output: f (x) = sign

end for
k+1(i)  DT
if DS
DS
k+1(i)  DT
end if
Normalize DS

(cid:16)(cid:80)K

k (xT

hk(xT

i )(cid:54)=hT

hS
k (xT
i=1 DS

k+1(i) +(cid:80)NT

i ) + αk 1
i=1 DT
k+1(i) > γmaxZk+1 then
k+1(i) = γmaxZk+1
k+1 and DT

k+1 such that(cid:80)NS

k+1(i)

(cid:17)

k=1 αkhk(x)

hk(xT

i

i )(cid:54)=yT
  DS

k+1(i) = DS

  DT

k+1(i) = DT

i )(cid:54)=yT

i

k

k (i) exp(cid:0)βS
(cid:1)
k (i) exp(cid:0)βT

i

i

(cid:1)

k+1(i) +(cid:80)NT

i=1 DS

i=1 DT

k+1(i) = 1

Remark 4. Lemma 1 shows that  given ﬁxed weights  the model complexity (and hence the upper
bound of a loss function) is related to the performance gap between the source and target domains.
Lemma 1 reveals that transfer learning (1) can succeed when the hypotheses trained on their own
domains also work well on the other domains  which leads to a lower training loss and a faster
convergence to the best hypothesis in the class in terms of sample complexity.

By combining the Steps 1–3  we obtain Theorem 1.

By similar derivations  we obtain a PAC learning bound  which is also consistent with the instance
weighting rules.
Corollary 2. Let wS be the optimal solution of the transfer learning problem (1)  and h∗ =
arg minh LDT (h) be the minimizer in the target domain. Assume that (cid:107)x(cid:107)2 ≤ R ∀x ∈ X   and that
the loss function obeys the triangle inequality and is ρ-Lipschitz and convex. Then  for any δ ∈ (0  1) 
with probability at least 1 − δ  we have:

LDT (hS) ≤ LDT (h∗) + ε(cid:48)

where

ε(cid:48)
Γ = min

(cid:40)(cid:107)Γ(cid:107)∞ρ2R2

+

λ

(cid:18) ρ2R2((cid:107)Γ(cid:107)2

(6)

Γ + 2(cid:107)ΓS(cid:107)1 distY (DT  DS )  
(cid:19)

+ (cid:107)Γ(cid:107)∞

B(Γ)

(cid:18)||Γ||2√
(cid:114)

N

(cid:19)(cid:115)
(cid:115)

2N log

+ 2(cid:107)Γ(cid:107)2B(Γ)

8
δ

2 + (cid:107)Γ(cid:107)∞)
λ
2(cid:107)Γ(cid:107)∞(cid:107)Γ(cid:107)2ρ2R2

+

λ

N log 4
δ

 

(cid:41)

.

2

log 4
δ

2

gapBoost

3.3
As distY (DT  DS ) can be estimated from the training sample  it is possible to derive a weighting
scheme by minimizing the generalization bounds (2) as in previous works in the literature [25  7].
However  one common issue with this approach is that it leads to high computational cost for large
sample size and it is usually restricted to linear hypotheses. In contrast  our algorithmic goal is
to derive a computationally efﬁcient method that is applicable to large-scale data and also ﬂexible
enough to accommodate arbitrary learning algorithms for transfer learning.

6

Table 1: Comparison of boosting algorithms for transfer learning.

Rule 1 Rule 2 Rule 3 Rule 4

AdaBoost
TrAdaBoost
TransferBoost
gapBoost

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)


(cid:88)


(cid:88)
(cid:88)
(cid:88)




(cid:88)

To this end  we propose gapBoost in Algorithm 1  which explicitly exploits the rules from Section 3.1.
The algorithm trains a joint learner for source and target domains  as well as auxiliary source and
target learners (lines 3–5). Then  it up-weights incorrectly labeled instances as per traditional boosting
methods and down-weights instances for which the source and target learners disagree; the trade-off
for the two schemes is controlled separately for source and target instances via hyper-parameters ρS
and ρT (lines 6–12). Finally  the weights are clipped to a maximum value of γmax and normalized
(lines 13–17). 1. gapBoost follows Rule 1 by training the base learner hk at each iteration  which
aims to minimize the weighted empirical loss over the source and target domains. 2. By tuning γmax 
it explicitly controls (cid:107)Γ(cid:107)∞ and implicitly controls (cid:107)Γ(cid:107)2  as required by Rule 2. Additionally  as each
base learner hk is trained with a different set of weights  the ﬁnal classiﬁer f returned by gapBoost
is potentially trained over a balanced distribution. 3. Moreover  by setting ρT ≥ ρS  gapBoost
penalizes instances from the source domain more than from the target domain  implicitly assigning
more weight to the target domain sample than to the source domain sample  as suggested by Rule 3.
4. Finally  as ρS   ρT ≤ 0  the weight of any instance x will decrease if the learners disagree (i.e. 
k (x) (cid:54)= hT
k (x)). By doing so  gapBoost follows Rule 4 by minimizing the gap ∇. 5. The trade-off
hS
between the rules is balanced by the choice of the hyper-parameters ρT   ρS and γmax.
Table 1 compares various traditional boosting algorithms for transfer learning in terms of the instance
weighting rules. Conventional AdaBoost [13] treats source and target samples equally  and therefore
does not reduce (cid:107)ΓS(cid:107)1 or minimize the performance gap. On the other hand  TrAdaBoost [10] and
TransferBoost [12] explicitly exploit Rule 3 by assigning less weight to the source domain sample
at each iteration. However  they do not control (cid:107)Γ(cid:107)∞ or (cid:107)Γ(cid:107)2  so the weight of the target domain
sample can be large after a few iterations. Most critically  none of the previous algorithms minimize
the performance gap explicitly as we do  which can be crucial for transfer learning to succeed.
The generalization performance of gapBoost is upper bounded by the following proposition.

Proposition 1. Let f (x) = (cid:80)K
with each base learner trained by solving (1). For simplicity  we assume that(cid:80)K

k=1 αkhk(x) be the ensemble of classiﬁers returned by gapBoost 
k=1 αk = 1. Then 

for any δ ∈ (0  1)  with probability at least 1 − δ  we have

LDT (f ) ≤ LST (f ) + 2ρ2R2γT

.
∞ is the largest weight of the target sample over all boosting iterations.

δ + B(Γ)

δ
2NT

2 log 4

∞

λ

where γT
Remark 5. We observe that if γT
Proposition 1 suggests to set γmax = O(

∞ (cid:29)(cid:113) 1

as the loss function is convex  B(Γ) can be upper bounded by B(Γ) ≤(cid:80)K

NT   the bound will be dominated by the second term. Then 
1√
NT ) to achieve a fast convergence rate. On the other hand 
k=1 αkB(Γk)  where Γk
is the set of weights at the k-th boosting iteration. In other words  one should aim to minimize the
performance gap for every boosting iteration to achieve a tighter bound.

(cid:113)

(cid:113) log 2

4 Experiments

We evaluated gapBoost on two benchmark data sets.
20 Newsgroups This data set contains approximately 20 000 documents  grouped by seven top
categories and 20 subcategories. Each transfer learning task involved a top-level classiﬁcation
problem  while the source and target domains were chosen from different subcategories. The source
and target data sets were in the same way as in [10]  yielding 6 transfer learning problems.
Ofﬁce-Caltech This data set contains approximately 2 500 images from four distinct domains:
Amazon (A)  DSLR (D)  Webcam (W)  and Caltech (C)  which enabled us to construct 12 transfer

7

Table 2: Comparison of different methods on the 20 Newsgroups (top) and Ofﬁce-Caltech (bottom)
data sets in term of error rate (%). The row titles are standard names used in the literature to identify
the transfer problems. Our algorithm  gapBoost  outperforms all baselines in the majority of transfer
problems  and is competitive with the top performance in the remaining ones. Standard error is
reported after the ±.

comp vs sci
rec vs sci
comp vs talk
comp vs rec
rec vs talk
sci vs talk
A → C
A → D
A → W
C → A
C → D
C → W
D → A
D → C
D → W
W → A
W → C
W → D

AdaBoostT
12.45 ± 0.47
10.99 ± 0.37
11.83 ± 0.42
15.80 ± 0.53
12.08 ± 0.36
11.74 ± 0.49
43.87 ± 0.52
32.65 ± 1.35
37.23 ± 0.98
39.92 ± 0.74
27.88 ± 1.14
30.25 ± 1.05
44.30 ± 0.45
44.00 ± 0.56
50.63 ± 0.58
42.91 ± 0.46
44.12 ± 0.50
40.63 ± 1.45

AdaBoostT &S
13.45 ± 0.48
11.79 ± 0.35
14.57 ± 0.47
17.50 ± 0.64
9.40 ± 0.31
10.52 ± 0.37
27.76 ± 0.88
28.33 ± 1.33
26.94 ± 1.17
20.32 ± 0.80
25.69 ± 1.19
24.50 ± 1.30
40.86 ± 0.39
40.09 ± 0.46
49.64 ± 0.66
37.22 ± 0.56
37.93 ± 0.58
45.52 ± 1.58

TrAdaBoost
12.03 ± 0.41
10.03 ± 0.36
10.67 ± 0.37
14.86 ± 0.67
12.21 ± 0.40
10.13 ± 0.46
37.57 ± 0.68
34.93 ± 1.43
31.03 ± 0.95
29.13 ± 0.80
19.84 ± 1.09
22.86 ± 0.95
45.33 ± 0.48
43.72 ± 0.62
49.95 ± 0.65
44.24 ± 0.52
44.78 ± 0.65
40.00 ± 1.51

TransferBoost
8.83 ± 0.37
7.93 ± 0.30
6.45 ± 0.25
12.11 ± 0.43
6.26 ± 0.30
6.45 ± 0.26
27.86 ± 0.82
28.96 ± 1.38
26.95 ± 1.15
19.68 ± 0.80
23.44 ± 1.33
23.41 ± 1.30
40.50 ± 0.44
40.35 ± 0.46
49.63 ± 0.65
37.02 ± 0.53
37.79 ± 0.56
44.88 ± 1.58

gapBoost
7.68 ± 0.25
7.39 ± 0.21
7.10 ± 0.27
9.81 ± 0.29
5.66 ± 0.21
5.92 ± 0.24
27.06 ± 0.87
25.08 ± 1.37
24.34 ± 1.10
19.13 ± 0.83
21.03 ± 1.20
21.55 ± 1.20
40.66 ± 0.39
40.00 ± 0.46
50.24 ± 0.62
37.04 ± 0.52
37.48 ± 0.50
41.74 ± 1.40

Figure 1: Test error rates (%) with different sizes of target sample on different tasks and on average
across all tasks. gapBoost consistently outperforms the baselines on all regimes of target sample size.
Since gapBoost more effectively leverages the target instances  its improvement over the baselines is
more noticeable as the target sample size increases. Error bars represent standard error.

problems by alternately selecting each possible source-target pair. All four domains share the same
10 classes  so we constructed 5 binary classiﬁcation tasks for each transfer problem and the averaged
results are reported.

Performance comparison We evaluated gapBoost against four baseline algorithms: AdaBoostT
trained only on target data  AdaBoostT &S trained on both source and target data  TrAdaBoost  and
TransferBoost. Logistic regression is used as the base learner for all methods  and the number of
boosting iterations is set to 20. The hyper-parameters of gapBoost were set as γmax = 1√
NT as per
2.
Remark 5  ρT = 0  which corresponds to no punishment for the target data  and ρS = log 1
In both data sets we pre-processed the data using principal component analysis (PCA) to reduce the
the feature dimension to 100. For each data set  we used all source data and a small amount of target
data (10% on 20 Newsgroups and 10 points on Ofﬁce-Caltech) as training sample  and used the rest of
the target data for testing. We repeated all experiments over 20 different random train/test splits and
the average results are presented in Table 2  showing that our method is capable of outperforming all
the baselines in the majority of cases. In particular  gapBoost consistently outperforms AdaBoostT  
empirically indicating that it avoids negative transfer.

Learning with different number of target examples To further investigate the effectiveness of
gapBoost  we varied the fraction of target instances of the 20 Newsgroups data set used for training 

8

020406080Ratio of Training Examples (%)05101520253035Error Rate (%)comp vs sciAdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoost020406080Ratio of Training Examples (%)05101520253035Error Rate (%)comp vs recAdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoost020406080Ratio of Training Examples (%)05101520253035Error Rate (%)rec vs talkAdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoost020406080Ratio of Training Examples (%)05101520253035Error Rate (%)Average PerformanceAdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoostFigure 2: Test error rates (%) averaged across all tasks with respect to the values of the hyper-
parameter ρS for varying sample sizes. Rightmost graphic shows results averaged over all sample
sizes. gapBoost becomes less sensitive to the choice of ρS as the target sample grows larger. In all
cases  there is a range of ρS that outperforms all baselines. Error bars represent standard error.

Figure 3: Test error rates (%) with varying ρS and ρT . The valley curves correspond to ρT = 0 (i.e. 
the purple curves in Figure 2). Hence  regions below the curve indicate better hyper-parameters.

from 0.01 to 0.8. Figure 1 shows full learning curves on three example tasks  as well as the average
performance over all six tasks. The results reveal that gapBoost’s improvement over the baselines
increases as the number of target instances grows  indicating that it is able to leverage target data
more effectively than previous methods.

Parameter sensitivity Next  we empirically evaluated our algorithm’s sensitivity to the choice of
hyper-parameters. We ﬁrst ﬁxed ρT = 0 and varied exp(ρS ) in the range of [0.1  . . .   0.9]. Figure 2
shows the results averaged over all transfer problems on the 20 Newsgroups data set  showing that as
the size of the target sample increases  the inﬂuence of the hyper-parameter on performance decreases.
In particular  we see that we are able to obtain a range of hyper-parameters for which our method
outperforms all baselines in all sample size regimes.
Increase the weight of a target instance when hS
k (xT ) = hT
k (xT ) To further minimize the gap 
we can modify the weight update rule for target data: βT = ρT 1
hk(xT )(cid:54)=yT with
k (xT )=hT
hS
ρT ≥ 0. We vary ρS and ρT together  and the results are shown in Figure 3. It can be observed that
gapBoost can achieve even better performance by focusing more on performance gap minimization
(i.e.  choosing large ρS and ρT ). As the target data increase  the results are less sensitive to the
hyper-parameters.

k (xT ) + αk 1

5 Conclusions

We propose the notion of performance gap to measure the divergence between domains in transfer
learning by exploiting the label information in the target domain. Consequently  we propose a new
principle for transfer learning. In particular  our theoretical analysis justiﬁes four intuitively reasonable
rules for instance weighting  and provides new insight into transfer learning. We highlighted the role
of performance gap minimization and presented gapBoost  an algorithm that explicitly exploits the
rules for instance weighting. The empirical evaluation justiﬁes the effectiveness of our algorithm.
While the theoretical analysis is based on the convexity assumption  our principles are quite general 
and so would be applicable to a wide variety of algorithms (such as deep nets) for transfer learning.
In addition  the principle of performance gap minimization opens up several avenues for knowledge
transfer. For example  it could be used to analyze other forms of transfer learning like parameter
or feature transfer [37]. It could also help develop knowledge transfer strategies for other learning
paradigms such as meta-learning or lifelong learning [31]. We plan to explore these questions in
future work.

9

0.20.40.60.8exp(S)22242628303234Error Rate (%)1%AdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoost0.20.40.60.8exp(S)7891011121314Error Rate (%)10%AdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoost0.20.40.60.8exp(S)345678910Error Rate (%)50%AdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoost0.20.40.60.8exp(S)7891011121314Error Rate (%)Average PerformanceAdaBoostTAdaBoostT&STrAdaBoostTransferBoostgapBoostAcknowledgements

The research presented in this paper was supported by the Faculty of Science at the University of
Western Ontario and the Lifelong Learning Machines program from DARPA/MTO under grant
#FA8750-18-2-0117. We would like to thank the anonymous reviewers for their helpful feedback.

References
[1] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875  2017.

[2] K. Azizzadenesheli  A. Liu  F. Yang  and A. Anandkumar. Regularized learning for domain adaptation

under label shifts. In International Conference on Learning Representations  2019.

[3] S. Ben-David  J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. W. Vaughan. A theory of learning

from different domains. Machine Learning  79(1-2):151–175  2010.

[4] S. Ben-David  J. Blitzer  K. Crammer  and F. Pereira. Analysis of representations for domain adaptation.

In Advances in Neural Information Processing Systems  pages 137–144  2007.

[5] J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. Wortman. Learning bounds for domain adaptation.

In Advances in Neural Information Processing Systems  pages 129–136  2008.

[6] C. Cortes  Y. Mansour  and M. Mohri. Learning bounds for importance weighting. In Advances in Neural

Information Processing Systems  pages 442–450  2010.

[7] C. Cortes  M. Mohri  and A. M. Medina. Adaptation based on generalized discrepancy. The Journal of

Machine Learning Research  20(1):1–30  2019.

[8] C. Cortes  M. Mohri  M. Riley  and A. Rostamizadeh. Sample selection bias correction theory.

In
Proceedings of the International Conference on Algorithmic Learning Theory  pages 38–53. Springer 
2008.

[9] K. Crammer  M. Kearns  and J. Wortman. Learning from multiple sources. Journal of Machine Learning

Research  9:1757–1774  2008.

[10] W. Dai  Q. Yang  G.-R. Xue  and Y. Yu. Boosting for transfer learning. In Proceedings of the International

Conference on Machine Learning  pages 193–200  2007.

[11] S. S. Du  J. Koushik  A. Singh  and B. Póczos. Hypothesis transfer learning via transformation functions.

In Advances in Neural Information Processing Systems  pages 574–584  2017.

[12] E. Eaton and M. desJardins. Selective transfer between learning tasks using task-based boosting. In

Proceedings of the AAAI Conference on Artiﬁcial Intelligence  pages 337–342  2011.

[13] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to

boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

[14] P. Germain  A. Habrard  F. Laviolette  and E. Morvant. A PAC-Bayesian approach for domain adaptation
with specialization to linear classiﬁers. In International Conference on Machine Learning  pages 738–746 
2013.

[15] A. Gretton  K. M. Borgwardt  M. Rasch  B. Schölkopf  and A. J. Smola. A kernel method for the

two-sample-problem. In Advances in Neural Information Processing Systems  pages 513–520  2007.

[16] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Schölkopf  and A. Smola. A kernel two-sample test. Journal

of Machine Learning Research  13:723–773  2012.

[17] J. Huang  A. Gretton  K. M. Borgwardt  B. Schölkopf  and A. J. Smola. Correcting sample selection bias

by unlabeled data. In Advances in Neural Information Processing Systems  pages 601–608  2007.

[18] I. Kuzborskij and F. Orabona. Stability and hypothesis transfer learning. In Proceedings of the International

Conference on Machine Learning  pages 942–950  2013.

[19] I. Kuzborskij and F. Orabona. Fast rates by transferring from auxiliary hypotheses. Machine Learning 

106(2):171–195  2017.

[20] T. Liu  G. Lugosi  G. Neu  and D. Tao. Algorithmic stability and hypothesis complexity. In Proceedings of

the International Conference on Machine Learning  pages 2159–2167  2017.

10

[21] T. Liu  Q. Yang  and D. Tao. Understanding how feature structure transfers in transfer learning. In

Proceedings of the International Joint Conferences on Artiﬁcial Intelligence  pages 2365–2371  2017.

[22] M. Long  Y. Cao  J. Wang  and M. I. Jordan. Learning transferable features with deep adaptation networks.

In Proceedings of the International Conference on Machine Learning  pages 97–105  2015.

[23] M. Long  J. Wang  G. Ding  S. J. Pan  and S. Y. Philip. Adaptation regularization: A general framework
for transfer learning. IEEE Transactions on Knowledge and Data Engineering  26(5):1076–1089  2014.

[24] M. Long  H. Zhu  J. Wang  and M. I. Jordan. Deep transfer learning with joint adaptation networks. In

Proceedings of the International Conference on Machine Learning  pages 2208–2217  2017.

[25] Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In

Proceedings of the Conference on Learning Theory  2009.

[26] A. Maurer  M. Pontil  and B. Romera-Paredes. Sparse coding for multitask and transfer learning. In

Proceedings of the International Conference on Machine Learning  pages 343–351  2013.

[27] M. Mohri and A. M. Medina. New analysis and algorithm for learning with drifting distributions. In

International Conference on Algorithmic Learning Theory  pages 124–138. Springer  2012.

[28] S. J. Pan  I. W. Tsang  J. T. Kwok  and Q. Yang. Domain adaptation via transfer component analysis. IEEE

Transactions on Neural Networks  22(2):199–210  2011.

[29] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions Knowledge and Data Engineering 

22(10):1345–1359  2010.

[30] D. Pardoe and P. Stone. Boosting for regression transfer. In Proceedings of the International Conference

on Machine Learning  pages 863–870  2010.

[31] P. Ruvolo and E. Eaton. ELLA: An efﬁcient lifelong learning algorithm. In Proceedings of the International

Conference on Machine Learning  pages 507–515  2013.

[32] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.

Cambridge university press  2014.

[33] C. Shui  M. Abbasi  L.-É. Robitaille  B. Wang  and C. Gagné. A principled approach for learning task

similarity in multitask learning. arXiv preprint arXiv:1903.09109  2019.

[34] M. Sugiyama  T. Kanamori  T. Suzuki  M. C. d. Plessis  S. Liu  and I. Takeuchi. Density-difference

estimation. Neural Computation  25(10):2734–2775  2013.

[35] M. Sugiyama  S. Nakajima  H. Kashima  P. V. Buenau  and M. Kawanabe. Direct importance estimation
with model selection and its application to covariate shift adaptation. In Advances in Neural Information
Processing Systems  pages 1433–1440  2008.

[36] Q. Sun  R. Chattopadhyay  S. Panchanathan  and J. Ye. A two-stage weighting framework for multi-source

domain adaptation. In Advances in Neural Information Processing Systems  pages 505–513  2011.

[37] B. Wang and J. Pineau. Generalized dictionary for multitask learning with boosting. In Proceedings of the

International Joint Conferences on Artiﬁcial Intelligence  pages 2097–2103  2016.

[38] B. Wang  H. Zhang  P. Liu  Z. Shen  and J. Pineau. Multitask metric learning: Theory and algorithm. In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics  pages 3362–3371 
2019.

[39] X. Wang and J. Schneider. Flexible transfer learning under support and model shift. In Advances in Neural

Information Processing Systems  pages 1898–1906  2014.

[40] K. Weiss  T. M. Khoshgoftaar  and D. Wang. A survey of transfer learning. Journal of Big Data  3(1):9 

2016.

[41] Y. Yao and G. Doretto. Boosting for transfer learning with multiple sources. In Proceedings of the IEEE

Conference Computer Vision and Pattern Recognition  pages 1855–1862  2010.

[42] Y. Zhang. Multi-task learning and algorithmic stability. In Proceedings of the AAAI Conference on Artiﬁcial

Intelligence  pages 3181–3187  2015.

11

,Boyu Wang
Jorge Mendez
Mingbo Cai
Eric Eaton