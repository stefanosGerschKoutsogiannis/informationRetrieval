2018,An Information-Theoretic Analysis for Thompson Sampling with Many Actions,Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the dependence of regret on prior uncertainty. However  this dependence is through entropy  which can become arbitrarily large as the number of actions increases.  We establish new bounds that depend instead on a notion of rate-distortion.  Among other things  this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit.  We also offer a bound for the logistic bandit that dramatically improves on the best previously available  though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.,An Information-Theoretic Analysis for
Thompson Sampling with Many Actions

Shi Dong

Stanford University

sdong15@stanford.edu

Benjamin Van Roy
Stanford University
bvr@stanford.edu

Abstract

Information-theoretic Bayesian regret bounds of Russo and Van Roy [8] capture
the dependence of regret on prior uncertainty. However  this dependence is through
entropy  which can become arbitrarily large as the number of actions increases. We
establish new bounds that depend instead on a notion of rate-distortion. Among
other things  this allows us to recover through information-theoretic arguments a
near-optimal bound for the linear bandit. We also offer a bound for the logistic
bandit that dramatically improves on the best previously available  though this
bound depends on an information-theoretic statistic that we have only been able to
quantify via computation.

1

Introduction

(cid:113)

(cid:113)

Thompson sampling [11] has proved to be an effective heuristic across a broad range of online
decision problems [2  10]. Russo and Van Roy [8] provided an information-theoretic analysis that
ΓH(A∗)T on
yields insight into the algorithm’s broad applicability and establishes a bound of
cumulative expected regret over T time periods of any algorithm and online decision problem. The
information ratio Γ is a statistic that captures the manner in which an algorithm trades off between
immediate reward and information acquisition; Russo and Van Roy [8] bound the information ratio
of Thompson sampling for particular classes of problems. The entropy H(A∗) of the optimal action
quantiﬁes the agent’s initial uncertainty.
If the prior distribution of A∗ is uniform  the entropy H(A∗) is the logarithm of the number of actions.
ΓH(A∗)T grows arbitrarily large with the number of actions. On the other hand  even
As such 
for problems with inﬁnite action sets  like the linear bandit with a polytopic action set  Thompson
sampling is known to obey gracious regret bounds [6]. This suggests that the dependence on entropy
leaves room for improvement.
In this paper  we establish bounds that depend on a notion of rate-distortion instead of entropy. Our
new line of analysis is inspired by rate-distortion theory  which is a branch of information theory
that quantiﬁes the amount of information required to learn an approximation [3]. This concept was
also leveraged in recent work of Russo and Van Roy [9]  which develops an alternative to Thompson
sampling that aims to learn satisﬁcing actions. An important difference is that the results of this paper
apply to Thompson sampling itself.
We apply our analysis to linear and generalized linear bandits and establish Bayesian regret bounds
√
that remain sharp with large action spaces. For the d-dimensional linear bandit setting  our bound
T log T ) bound of [7]. Our bound also improves
is O(d

on the previous O((cid:112)dT H(A∗)) information-theoretic bound of [8] since it does not depend on

√
T log T )  which is tighter than the O(d

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

√
log T ) of the Ω(d

√
the number of actions. Our Bayesian regret bound is within a factor of O(
worst-case regret lower bound of [4].
For the logistic bandit  previous bounds for Thompson sampling [7] and upper-conﬁdence-bound
algorithms [5] scale linearly with supx φ(cid:48)(x)/ inf x φ(cid:48)(x)  where φ is the logistic function φ(x) =
eβx/(1 + eβx). These bounds explode as β → ∞ since limβ→∞ supx φ(cid:48)(x) = ∞. This does not
make sense because  as β grows  the reward of each action approaches a deterministic binary value 
which should simplify learning. Our analysis addresses this gap in understanding by establishing
a bound that decays as β becomes large  converging to 2d
T log 3 for any ﬁxed T . However  this
analysis relies on a conjecture about the information ratio of Thompson sampling for the logistic
bandit  which we only support through computational results.

√

T )

2 Problem Formulation

We consider an online decision problem in which over each time period t = 1  2  . . .  an agent selects
an action At from a ﬁnite action set A and observes an outcome YAt ∈ Y  where Y denotes the set of
possible outcomes. A ﬁxed and known system function g associates outcomes with actions according
to
where a ∈ A is the action  W is an exogenous noise term  and θ∗ is the “true” model unknown to
the agent. Here we adopt the Bayesian setting  in which θ∗ is a random variable taking value in a
space of parameters Θ. The randomness of θ∗ stems from the prior uncertainty of the agent. To make
notations succinct and avoid measure-theoretic issues  we assume that Θ = {θ1  . . .   θm} is a ﬁnite
set  whereas our analysis can be extended to the cases where both A and Θ are inﬁnite.
The reward function R : Y (cid:55)→ R assigns a real-valued reward to each outcome. As a shorthand we
deﬁne

Ya = g(a  θ∗  W ) 

µ(a  θ) = E(cid:2)R(Ya)(cid:12)(cid:12)θ∗ = θ(cid:3)  

∀a ∈ A  θ ∈ Θ.

Simply stated  µ(a  θ) is the expected reward of action a when the true model is θ. We assume that 
conditioned on the true model parameter and the selected action  the reward is bounded1  i.e.

R(y) − inf

y∈Y R(y) ≤ 1.

sup
y∈Y

In addition  for each parameter θ  let α(θ) be the optimal action under model θ  i.e.

α(θ) = argmax

a∈A

µ(a  θ).

Note that the ties induced by argmax can be circumvented by expanding Θ with identical elements.
Let A∗ = α(θ∗) be the “true” optimal action and let R∗ = µ(A∗  θ∗) be the corresponding maximum
reward.
Before making her decision at the beginning of period t  the agent has access to the history up to time
t − 1  which we denote by

Ht−1 =(cid:0)A1  YA1   . . .   At−1  YAt−1

(cid:1) .

A policy π = (π1  π2  . . .) is deﬁned as a sequence of functions mapping histories and exogenous
noise to actions  which can be written as

At = πt(Ht−1  ξt) 

t = 1  2  . . .  

where ξt is a random variable which characterizes the algorithmic randomness. The performance of
policy π is evaluated by the ﬁnite horizon Bayesian regret  deﬁned by

BayesRegret(T ; π) = E

(cid:34) T(cid:88)

(cid:0)R∗ − R(YAt)(cid:1)(cid:35)

 

1The boundedness assumption allows application of a basic version of Pinsker’s inequality. Since there exists
a version of Pinsker’s inequality that applies to sub-Gaussian random variables (see Lemma 3 of [8])  all of our
results hold without change for 1/4-sub-Gaussian rewards  i.e.

E(cid:2)exp(cid:8)λ [R(g(a  θ  W )) − µ(a  θ)](cid:9)(cid:3) ≤ exp(λ2/8) ∀λ ∈ R  a ∈ A  θ ∈ Θ.

t=1

2

where the actions are chosen by policy π  and the expectation is taken over the randomness in both
R∗ and (At)T

t=1.

3 Thompson Sampling and Information Ratio

The Thompson sampling policy πTS is deﬁned such that at each period  the agent samples the next
action according to her posterior belief of the optimal action  i.e.

P(cid:0)πTS

t

(Ht−1  ξt) = a(cid:12)(cid:12)Ht−1

An equivalent deﬁnition  which we use throughout our analysis  is that over period t the agent samples
a parameter θt from the posterior of the true parameter θ∗  and plays the action At = α(θt). The
history available to the agent is thus

a.s. ∀a ∈ A  t = 1  2  . . . .

(cid:1) = P(cid:0)A∗ = a(cid:12)(cid:12)Ht−1) 
˜Ht =(cid:0)θ1  Yα(θ1)  . . .   θt  Yα(θt)

(cid:1).

The information ratio  ﬁrst proposed in [8]  quantiﬁes the trade-off between exploration and exploita-
tion. Here we adopt the simpliﬁed deﬁnition in [9]  which integrates over all randomness. Let θ  θ(cid:48) be
two Θ-valued random variables. Over period t  the information ratio of θ(cid:48) with respect to θ is deﬁned
by

E(cid:2)R(Yα(θ)) − R(Yα(θ(cid:48)))(cid:3)2
I(cid:0)θ; (θ(cid:48)  Yα(θ(cid:48)))(cid:12)(cid:12) ˜Ht−1
(cid:1)  

Γt(θ; θ(cid:48)) =

(1)

where the denominator is the mutual information between θ and (θ(cid:48)  Yα(θ(cid:48)))  conditioned on the
σ-algebra generated by ˜Ht−1. We can interpret θ as a benchmark model parameter that the agent
wants to learn and θ(cid:48) as the model parameter that she selects. When Γt(θ; θ(cid:48)) is small  the agent
would only incur large regret over period t if she was expected to learn a lot of information about θ.
We restate a result proven in [6]  which proposes a bound for the regret of any policy in terms of the
worst-case information ratio.
t=1 be such that α(θt) = πt(Ht−1  ξt) for each
Proposition 1. For all T > 0 and policy π  let (θt)T
t = 1  2 . . .   T   then

BayesRegret(T ; π) ≤(cid:113)

ΓT · H(θ∗) · T  

where H(θ∗) is the entropy of θ∗ and

ΓT = max
1≤t≤T

Γt(θ∗; θt).

The bound given by Proposition 1 is loose in the sense that it depends implicitly on the cardinality
of Θ. When Θ is large  knowing exactly what θ∗ is requires a lot of information. Nevertheless 
because of the correlation between actions  it sufﬁces for the agent to learn a “blurry” version of θ∗ 
which conveys far less information  to achieve low regret. In the following section we concretize this
argument.

4 A Rate-Distortion Analysis of Thompson Sampling

In this section we develop a sharper bound for Thompson sampling. At a high level  the argument
relies on the existence of a statistic ψ of θ∗ such that:

i The statistic ψ is less informative than θ∗;
ii In each period  if the agent aims to learn ψ instead of θ∗  the regret incurred can be
bounded in terms of the information gained about ψ; we refer to this approximate learning
as “compressed Thompson sampling”;

iii The regret of Thompson sampling is close to that of the compressed Thompson sampling
based on the statistic ψ  and at the same time  compressed Thompson sampling yields no
more information about ψ than Thompson sampling.

3

Following the above line of analysis  we can bound the regret of Thompson sampling by the mutual
information between the statistic ψ and θ∗. When ψ can be chosen to be far less informative than θ∗ 
we obtain a signiﬁcantly tighter bound.
To develop the argument  we ﬁrst quantify the amount of distortion that we incur if we replace one
parameter with another. For two parameters θ  θ(cid:48) ∈ Θ  the distortion of θ with respect to θ(cid:48) is deﬁned
as

d(θ  θ(cid:48)) = µ(α(θ(cid:48))  θ(cid:48)) − µ(α(θ)  θ(cid:48)).

(2)

In other words  the distortion is the price we pay if we deem θ to be the true parameter while the
actual true parameter is θ(cid:48). Notice that from the deﬁnition of α  we always have d(θ  θ(cid:48)) ≥ 0. Let
{Θk}K

k=1 Θk = Θ and Θi ∩ Θj = ∅  ∀i (cid:54)= j  such that

k=1 be a partition of Θ  i.e.(cid:83)K

d(θ  θ(cid:48)) ≤  

∀θ  θ(cid:48) ∈ Θk  k = 1  . . .   K.

(3)

where  > 0 is a positive distortion tolerance. Let ψ be the random variable taking values in
{1  . . .   K} that records the index of the partition in which θ∗ lies  i.e.

ψ = k ⇔ θ∗ ∈ Θk.

(4)
Then we have H(ψ) ≤ log K. If the structure of Θ allows for a small number of partitions  ψ would
have much less information than θ∗. Let subscript t − 1 denote corresponding values under the
posterior measure Pt−1(·) = P(·| ˜Ht−1). In other words  Et−1[·] and It−1(·;·) are random variables
that are functions of ˜Ht−1. We claim the following.
Proposition 2. Let ψ be deﬁned as in (4). For each t = 1  2  . . .  there exists a Θ-valued random
variable ˜θ∗

(i) ˜θ∗
(ii) Et−1

t that satisﬁes the following:
t is independent of θ∗  conditioned on ψ.

(cid:2)R∗ − R(Yα(θt))(cid:3) − Et−1
(cid:0)ψ; (˜θt  Yα(˜θt))(cid:1) ≤ It−1

(cid:2)R(Yα(˜θ∗
(cid:0)ψ; (θt  Yα(θt))(cid:1)  a.s.

(iii) It−1

t )) − R(Yα(˜θt))(cid:3) ≤   a.s.

where in (ii) and (iii)  ˜θt is independent from and distributed identically with ˜θ∗
t .

According to Proposition 2  over period t if the agent deviated from her original Thompson sampling
scheme and applied a “one-step” compressed Thompson sampling to learn ˜θ∗
t by sampling ˜θt  the
extra regret that she would incur can be bounded (as is guaranteed by (ii)). Meanwhile  from (i)  (iii)
and the data-processing inequality  we have that

(cid:0)˜θ∗
t ; (˜θt  Yα(˜θt))(cid:1) ≤ It−1

(cid:0)ψ; (˜θt  Yα(˜θt))(cid:1) ≤ It−1

(cid:0)ψ; (θt  Yα(θt))(cid:1)  a.s.

It−1

(5)

which implies that the information gain of the compressed Thompson sampling will not exceed that of
the original Thompson sampling towards ψ. Therefore  the regret of the original Thompson sampling
can be bounded in terms of the total information gain towards ψ and the worst-case information ratio
of the one-step compressed Thompson sampling. Formally  we have the following.
Theorem 1. Let {Θk}K
d(θ  θ(cid:48)) ≤ . Let ψ be deﬁned as in (4) and let ˜θ∗
have

k=1 be any partition of Θ such that for any k = 1  . . .   K and θ  θ(cid:48) ∈ Θk 
t and ˜θt satisfy the conditions in Proposition 2. We

BayesRegret(T ; πTS) ≤(cid:113)

Γ · I(θ∗; ψ) · T +  · T 

(6)

where

Γ = max
1≤t≤T

Γt(˜θ∗

t ; ˜θt).

4

t )) − R(Yα(˜θt))

(cid:105)(cid:111)
(cid:16)˜θ∗
t ; (˜θt  Yα(˜θt))(cid:12)(cid:12) ˜Ht−1

(cid:17)

+  · T

+  · T

(cid:105)(cid:111)

R∗ − R(YAt)

R(Yα(˜θ∗

(cid:105)
R∗ − R(YAt)

t=1

t=1

t=1

t=1

(cid:104)
(cid:104)

Γt(˜θ∗

E(cid:104)
T(cid:88)
E(cid:110)Et−1
T(cid:88)
E(cid:110)Et−1
T(cid:88)
(cid:114)
T(cid:88)
t   ˜θt) · I
(cid:114)
(cid:16)
T(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)Γ · T · T(cid:88)
(cid:114)
(cid:16)
(cid:114)
(cid:16)
ψ; θ∗(cid:17)
(cid:17)

Γ · T · I

Γ · T · I

Γ · I

(cid:16)

= I

+ I

t=1

t=1

I

(cid:17)
ψ; (θt  Yα(θt))(cid:12)(cid:12) ˜Ht−1
(cid:16)
ψ; (θt  Yα(θt))(cid:12)(cid:12) ˜Ht−1
(cid:17)
(cid:17)
(cid:16)

+  · T 

+  · T

ψ; ˜HT−1

θ∗; ψ

(cid:12)(cid:12)θ∗(cid:17)

ψ; ˜HT

= I

+  · T

(7)

+  · T

(cid:17)

(cid:16)

ψ; θ∗(cid:17)

 

Proof. We have that

BayesRegret(T ; πTS) =

=

(a)≤

=

(b)≤

(c)≤

(d)
=

(e)≤

where (a) follows from Proposition 2 (ii); (b) follows from (5); (c) results from Cauchy-Schwartz
inequality; (d) is the chain rule for mutual information and (e) comes from that

(cid:16)

(cid:17) ≤ I

(cid:16)

ψ; ˜HT

I

ψ; (θ∗  ˜HT )

where we use the fact that ψ is independent of ˜HT   conditioned on θ∗. Thence we arrive at our
desired result.
Remark. The bound given in Theorem 1 dramatically improves the previous bound in Proposition 1
since I(θ∗; ψ) can be bounded by H(ψ)  which  when Θ is large  can be much smaller than H(θ∗).
The new bound also characterizes the tradeoff between the preserved information I(θ∗; ψ) and the
distortion tolerance   which is the essence of rate distortion theory. In fact  we can deﬁne the
distortion between θ∗ and ψ as
esssup

(cid:2)R∗ − R(Yα(θt))(cid:3) − Et−1

t )) − R(Yα(˜θt))(cid:3)(cid:111)

(cid:2)R(Yα(˜θ∗

(cid:110)Et−1

D(θ∗  ψ) = max
1≤t≤T

 

t and ˜θt depend on ψ through Proposition 2. By taking the inﬁmum over all possible choices

where ˜θ∗
of ψ  the bound (6) can be written as

BayesRegret(T ; πTS) ≤(cid:113)

Γ · ρ() · T +  · T 

∀ > 0 

(8)

where

ρ() = minψ

I(θ∗; ψ)

s.t. D(θ∗  ψ) ≤ 

is the rate-distortion function with respect to the distortion D.
To obtain explicit bounds for speciﬁc problem instances  we use the fact that I(θ∗; ψ) ≤ H(ψ) ≤
log K. In the following section we introduce a broad range of problems in which both K and Γ can
be effectively bounded.

5 Specializing to Structured Bandit Problems

We now apply the analysis in Section 2 to common bandit settings and show that our bounds are
signiﬁcantly sharper than the previous bounds. In these models  the observation of the agent is the
received reward. Hence we can let R be the identity function and use Ra as a shorthand for R(Ya).

5

5.1 Linear Bandits

Linear bandits are a class of problems in which each action is parametrized by a ﬁnite-dimensional
feature vector  and the mean reward of playing each action is the inner product between the feature
vector and the model parameter vector. Formally  let A  Θ ⊂ Rd  where d < ∞  and Y ⊆ [−1/2  1/2].
The reward of playing action a satisﬁes

E[Ra|θ∗ = θ] = µ(a  θ) =

a(cid:62)θ 

1
2

∀a ∈ A  θ ∈ Θ.

Note that we apply a normalizing factor 1/2 to make the setting consistent with our assumption that
supy R(y) − inf y R(y) ≤ 1.
A similar line of analysis as in [8] allows us to bound the information ratio of the one-step compressed
Thompson sampling.
Proposition 3. Under the linear bandit setting  for each t = 1  2  . . .  letting ˜θ∗
conditions in Proposition 2  we have

t and ˜θt satisfy the

Γt(˜θ∗

t ; ˜θt) ≤ d
2

.

At the same time  with the help of a covering argument  we can also bound the number of partitions
that is required to achieve distortion tolerance .
Proposition 4. Under the linear bandit setting  suppose that A  Θ ⊆ Bd(0  1)  where Bd(0  1) is
the d-dimensional closed Euclidean unit ball. Then for any  > 0 there exists a partition {Θk}K
k=1 of
Θ such that for all k = 1  . . .   K and θ  θ(cid:48) ∈ Θk  we have d(θ  θ(cid:48)) ≤  and

(cid:19)d

(cid:18) 1



K ≤

+ 1

.

Combining Theorem 1  Propositions 3 and 4  we arrive at the following bound.
Theorem 2. Under the linear bandit setting  if A  Θ ⊆ Bd(0  1)  then
√

BayesRegret(T ; πTS) ≤ d

(cid:118)(cid:117)(cid:117)(cid:116)T log

(cid:32)

(cid:33)

.

3 +

3

2T
d

tions. It signiﬁcantly improves the bound O(cid:0)(cid:112)dT · H(A∗)(cid:1) in [8] and the bound O(cid:0)(cid:112)|A|T log |A|(cid:1)

This bound is the ﬁrst information-theoretic bound that does not depend on the number of available ac-

in [1] in that it drops the dependence on the cardinality of the action set and imposes no assumption
√
on the reward distribution. Comparing with the conﬁdence-set-based analysis in [7]  which results in
the bound O(d
T log T )  our argument is much simpler and cleaner and yields a tighter bound. This
√
bound suggests that Thompson sampling is near-optimal in this context since it exceeds the minimax
lower bound Ω(d

T ) proposed in [4] by only a

log T factor.

√

5.2 Generalized Linear Bandits with iid Noise
In generalized linear models  there is a ﬁxed and strictly increasing link function φ : R (cid:55)→ [0  1]  such
that

E[Ra|θ∗ = θ] = µ(a  θ) = φ(a(cid:62)θ).

Let

L = inf

a∈A θ∈Θ

a(cid:62)θ  L = sup

a∈A θ∈Θ

a(cid:62)θ.

We make the following assumptions.
Assumption 1. The reward noise is iid  i.e.

Ra = µ(a  θ∗) + Wa = φ(a(cid:62)θ∗) + Wa 

∀a ∈ A 

where Wa is a zero-mean noise term with a ﬁxed and known distribution for all a ∈ A.

6

Assumption 2. The link function φ is continuously differentiable in [L  L]  with

C(φ) = sup

x∈[L L]

φ(cid:48)(x) < ∞.

Under these assumptions  both the information ratio of the compressed Thompson sampling and the
number of partitions can be bounded.
Proposition 5. Under the genearlized linear bandit setting and Assumptions 1 and 2  for each
t = 1  2  . . .  letting ˜θ∗

t and ˜θt satisfy the conditions in Proposition 2  we have

Γt(˜θ∗

t ; ˜θt) ≤ 2C(φ)2d.
Proposition 6. Under the generalized linear bandit setting and Assumption 2  suppose that A  Θ ⊆
Bd(0  1). Then for any  > 0 there exists a partition {Θk}K
k=1 of Θ such that for each k = 1  . . .   K
(cid:19)d
and θ  θ(cid:48) ∈ Θk we have d(θ  θ(cid:48)) ≤  and

(cid:18) 2C(φ)

K ≤

+ 1

.



Combining Theorem 1  Propositions 5 and 6  we have the following.
Theorem 3. Under the generalized linear bandit setting and Assumptions 1 and 2  if A  Θ ⊆
Bd(0  1)  then

BayesRegret(T ; πTS) ≤ 2C(φ) · d

(cid:118)(cid:117)(cid:117)(cid:116)T log

(cid:32)

(cid:33)

.

√
3

2T
d

3 +

√
√
Note that the optimism-based algorithm in [5] achieves O(rd
Thompson sampling given in [7] is O(rd
3 apparently yields a sharper bound.

T log T ) regret  and the bound of
T log3/2 T )  where r = supx φ(cid:48)(x)/ inf x φ(cid:48)(x). Theorem

5.3 Logistic Bandits

Logistic bandits are special cases of generalized linear bandits  in which the agent only observes
binary rewards  i.e. Y = {0  1}. The link function is given by φL(x) = eβx/(1 + eβx)  where
β ∈ (0 ∞) is a ﬁxed and known parameter. Conditioned on θ∗ = θ  the reward of playing action a is
Bernoulli distributed with parameter φL(a(cid:62)θ).
The preexisting upper bounds on logistic bandit problems all scale linearly with

r = sup

x

(φL)(cid:48)(x)/ inf

(φL)(cid:48)(x) 

x

which explodes when β → ∞. However  when β is large  the rewards of actions are clearly bifurcated
by a hyperplane and we expect Thompson sampling to perform better. The regret bound given by our
analysis addresses this point and has a ﬁnite limit as β increases. Since the logistic bandit setting
is incompatible with Assumption 1  we propose the following conjecture  which is supported with
numerical evidence.
Conjecture 1. Under the logistic bandit setting  let the link function be φL(x) = eβx/(1 + eβx)  and
t and ˜θt satisfy the conditions in Proposition 2. Then for all β ∈ (0 ∞) 
for each t = 1  2 . . .  let ˜θ∗

Γt(˜θ∗

t ; ˜θt) ≤ d
2

.

To provide evidence for Conjecture 1  for each β and d  we randomly generate 100 actions and
parameters and compute the exact information ratio under a randomly selected distribution over the
parameters. The result is given in Figure 1. As the ﬁgure shows  the simulated information ratio is
always smaller than the conjectured upper bound d/2. We suspect that for every link function φ 
there exists an upper bound for the information ratio that depends only on d and φ and is independent
of the cardinality of the parameter space. This opens an interesting topic for future research.
We further make the following assumption  which posits existence of a classiﬁcation margin that
applies uniformly over θ ∈ Θ

7

Figure 1: Simulated information ratio values for dimensions d = 2  3  . . .   20 and (a) β = 0.1  (b)
β = 1  (c) β = 10 and (d) β = 100. The diagonal black dashed line is the upper bound Γ = d/2.

Assumption 3. We have that inf θ∈Θ |µ(α(θ)  θ) − 1/2| > 0. Equivalently  we have that

(cid:12)(cid:12)α(θ)(cid:62)θ(cid:12)(cid:12) > 0.

inf
θ∈Θ

The following theorem introduces the bound for the logistic bandit.
Theorem 4. Under the logistic bandit setting where A  Θ ⊆ Bd(0  1)  for all β > 0  if the link
function is given by φL(x) = eβx/(1 + eβx)  Assumption 3 holds with inf θ∈Θ
and Conjecture 1 holds  then for all sufﬁciently large T  

BayesRegret(T ; πTS) ≤ 2d

(cid:118)(cid:117)(cid:117)(cid:116)T log
(cid:118)(cid:117)(cid:117)(cid:116)T log

(cid:32)
(cid:32)

3 +

3 +

√

2T
d
√

2T

6

3

2d

≤ 2d

(cid:12)(cid:12)α(θ)(cid:62)θ(cid:12)(cid:12) = δ > 0 
(cid:33)

(cid:33)

.

(9)

(10)

·

βeβδ

(1 + eβδ)2

· min{δ−1  β}

√
For ﬁxed d and T   when β → ∞ the right-hand side of (9) converges to 2d
substantially sharper than previous bounds when β is large.

T log 3. Thus (9) is

6 Conclusion

√

Through an analysis based on rate-distortion  we established a new information-theoretic regret
bound for Thompson sampling that scales gracefully to large action spaces. Our analysis yields an
T log T ) regret bound for the linear bandit problem  which strengthens state-of-the-art bounds.
O(d
The same regret bound applies also to the logistic bandit problem if a conjecture about the information
ratio that agrees with computational results holds. We expect that our new line of analysis applies to
a wide range of online decision algorithms.

8

Acknowledgments

This work was supported by a grant from the Boeing Corporation and the Herb and Jane Dwight
Stanford Graduate Fellowship. We would also like to thank Daniel Russo  David Tse and Xiuyuan
Lu for useful conversations.

References
[1] Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for Thompson sampling. Journal

of the ACM (JACM)  64(5):30  2017.

[2] Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances

in neural information processing systems  pages 2249–2257  2011.

[3] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons 

2012.

[4] Varsha Dani  Thomas P Hayes  and Sham M Kakade. Stochastic linear optimization under

bandit feedback. In 21st Annual Conference on Learning Theory  pages 355–366  2008.

[5] Lihong Li  Yu Lu  and Dengyong Zhou. Provably optimal algorithms for generalized linear
contextual bandits. In International Conference on Machine Learning  pages 2071–2080  2017.

[6] Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling.

In Advances in Neural Information Processing Systems  pages 1583–1591  2014.

[7] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics

of Operations Research  39(4):1221–1243  2014.

[8] Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling.

The Journal of Machine Learning Research  17(1):2442–2471  2016.

[9] Daniel Russo and Benjamin Van Roy. Satisﬁcing in time-sensitive bandit learning. arXiv

preprint arXiv:1803.02855  2018.

[10] Daniel J Russo  Benjamin Van Roy  Abbas Kazerouni  Ian Osband  and Zheng Wen. A tutorial
on Thompson sampling. Foundations and Trends R(cid:13) in Machine Learning  11(1):1–96  2018.
[11] William R Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika  25(3/4):285–294  1933.

9

,Shi Dong
Benjamin Van Roy