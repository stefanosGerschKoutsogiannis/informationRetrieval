2014,A Framework for Testing Identifiability of Bayesian Models of Perception,Bayesian observer models are very effective in describing human performance in perceptual tasks  so much so that they are trusted to faithfully recover hidden mental representations of priors  likelihoods  or loss functions from the data. However  the intrinsic degeneracy of the Bayesian framework  as multiple combinations of elements can yield empirically indistinguishable results  prompts the question of model identifiability. We propose a novel framework for a systematic testing of the identifiability of a significant class of Bayesian observer models  with practical applications for improving experimental design. We examine the theoretical identifiability of the inferred internal representations in two case studies. First  we show which experimental designs work better to remove the underlying degeneracy in a time interval estimation task. Second  we find that the reconstructed representations in a speed perception task under a slow-speed prior are fairly robust.,A Framework for Testing Identiﬁability

of Bayesian Models of Perception

Luigi Acerbi1 2

Wei Ji Ma2

Sethu Vijayakumar1

1 School of Informatics  University of Edinburgh  UK

2 Center for Neural Science & Department of Psychology  New York University  USA
{luigi.acerbi weijima}@nyu.edu
sethu.vijayakumar@ed.ac.uk

Abstract

Bayesian observer models are very effective in describing human performance in
perceptual tasks  so much so that they are trusted to faithfully recover hidden men-
tal representations of priors  likelihoods  or loss functions from the data. However 
the intrinsic degeneracy of the Bayesian framework  as multiple combinations of
elements can yield empirically indistinguishable results  prompts the question of
model identiﬁability. We propose a novel framework for a systematic testing of
the identiﬁability of a signiﬁcant class of Bayesian observer models  with practi-
cal applications for improving experimental design. We examine the theoretical
identiﬁability of the inferred internal representations in two case studies. First 
we show which experimental designs work better to remove the underlying de-
generacy in a time interval estimation task. Second  we ﬁnd that the reconstructed
representations in a speed perception task under a slow-speed prior are fairly ro-
bust.

1 Motivation

Bayesian Decision Theory (BDT) has been traditionally used as a benchmark of ideal perceptual
performance [1]  and a large body of work has established that humans behave close to Bayesian
observers in a variety of psychophysical tasks (see e.g. [2  3  4]). The efﬁcacy of the Bayesian
framework in explaining a huge set of diverse behavioral data suggests a stronger interpretation
of BDT as a process model of perception  according to which the formal elements of the decision
process (priors  likelihoods  loss functions) are independently represented in the brain and shared
across tasks [5  6]. Importantly  such mental representations  albeit not directly accessible to the
experimenter  can be tentatively recovered from the behavioral data by ‘inverting’ a model of the
decision process (e.g.  priors [7  8  9  10  11  12  13  14]  likelihood [9]  and loss functions [12  15]).
The ability to faithfully reconstruct the observer’s internal representations is key to the understanding
of several outstanding issues  such as the complexity of statistical learning [11  12  16]  the nature
of mental categories [10  13]  and linking behavioral to neural representations of uncertainty [4  6].
In spite of these successes  the validity of the conclusions reached by ﬁtting Bayesian observer
models to the data can be questioned [17  18]. A major issue is that the inverse mapping from
observed behavior to elements of the decision process is not unique [19]. To see this degeneracy 
consider a simple perceptual task in which the observer is exposed to stimulus s that induces a noisy
sensory measurement x. The Bayesian observer reports the optimal estimate s∗ that minimizes his
or her expected loss  where the loss function L (s  ˆs) encodes the loss (or cost) for choosing ˆs when
the real stimulus is s. The optimal estimate for a given measurement x is computed as follows [20]:

(1)
where qprior(s) is the observer’s prior density over stimuli and qmeas(x|s) the observer’s sensory
likelihood (as a function of s). Crucially  for a given x  the solution of Eq. 1 is the same for any

s∗(x) = arg min

ˆs (cid:90) qmeas(x|s)qprior(s)L (s  ˆs) ds

1

triplet of prior qprior(s)· φ1(s)  likelihood qmeas(x|s)· φ2(s)  and loss function L (ˆs  s)· φ3(s)  where
the φi(s) are three generic functions such that(cid:81)3
i=1 φi(s) = c  for a constant c > 0. This analysis
shows that the ‘inverse problem’ is ill-posed  as multiple combinations of priors  likelihoods and
loss functions yield identical behavior [19]  even before considering other confounding issues  such
as latent states. If uncontrolled  this redundancy of solutions may condemn the Bayesian models of
perception to a severe form of model non-identiﬁability that prevents the reliable recovery of model
components  and in particular the sought-after internal representations  from the data.
In practice  the degeneracy of Eq. 1 can be prevented by enforcing constraints on the shape that the
internal representations are allowed to take. Such constraints include: (a) theoretical considerations
(e.g.  that the likelihood emerges from a speciﬁc noise model [21]); (b) assumptions related to
the experimental layout (e.g.  that the observer will adopt the loss function imposed by the reward
system of the task [3]); (c) additional measurements obtained either in independent experiments or
in distinct conditions of the same experiment (e.g.  through Bayesian transfer [5]). Crucially  both
(b) and (c) are under partial control of the experimenter  as they depend on the experimental design
(e.g.  choice of reward system  number of conditions  separate control experiments). Although
several approaches have been used or proposed to suppress the degeneracy of Bayesian models of
perception [12  19]  there has been no systematic analysis – neither empirical nor theoretical – of
their effectiveness  nor a framework to perform such study a priori  before running an experiment.
This paper aims to ﬁll this gap for a large class of psychophysical tasks. Similar issues of model
non-identiﬁability are not new to psychology [22]  and generic techniques of analysis have been
proposed (e.g.  [23]). Here we present an efﬁcient method that exploits the common structure shared
by many Bayesian models of sensory estimation. First  we provide a general framework that allows a
modeller to perform a systematic  a priori investigation of identiﬁability  that is the ability to reliably
recover the parameters of interest  for a chosen Bayesian observer model. Second  we show how 
by comparing identiﬁability within distinct ideal experimental setups  our framework can be used
to improve experimental design. In Section 2 we introduce a novel class of observer models that is
both ﬂexible and efﬁcient  key requirements for the subsequent analysis. In Section 3 we describe
a method to efﬁciently explore identiﬁability of a given observer model within our framework. In
Section 4 we show an application of our technique to two well-known scenarios in time perception
[24] and speed perception [9]. We conclude with a few remarks in Section 5.

2 Bayesian observer model

Here we introduce a continuous class of Bayesian observer models parametrized by vector θ. Each
value of θ corresponds to a speciﬁc observer that can be used to model the psychophysical task of
interest. The current model (class) extends previous work [12  14] by encompassing any sensori-
motor estimation task in which a one-dimensional stimulus magnitude variable s  such as duration 
distance  speed  etc. is directly estimated by the observer. This is a fundamental experimental condi-
tion representative of several studies in the ﬁeld (e.g.  [7  9  12  24  14]). With minor modiﬁcations 
the model can also cover angular variables such as orientation (for small errors) [8  11] and multi-
dimensional variables when symmetries make the actual inference space one-dimensional [25]. The
main novel feature of the presented model is that it covers a large representational basis with a sin-
gle parametrization  while still allowing fast computation of the observer’s behavior  both necessary
requirements to permit an exploration of the complex model space  as described in Section 3.
The generic observer model is constructed in four steps (Figure 1 a & b): 1) the sensation stage
describes how the physical stimulus s determines the internal measurement x; 2) the perception stage
describes how the internal measurement x is combined with the prior to yield a posterior distribution;
3) the decision-making stage describes how the posterior distribution and loss function guide the
choice of an ‘optimal’ estimate s∗ (possibly corrupted by lapses); and ﬁnally 4) the response stage
describes how the optimal estimate leads to the observed response r.

2.1 Sensation stage
For computational convenience  we assume that the stimulus s ∈ R+ (the task space) comes from
a discrete experimental distribution of stimuli si with frequencies Pi  with Pi > 0 (cid:80)i Pi = 1
for 1 ≤ i ≤ Nexp. Discrete distributions of stimuli are common in psychophysics  and continu-

2

f (s) = A ln(cid:34)1 +(cid:18) s

s0(cid:19)d(cid:35) + B with inverse

f−1(t) = s0

t−B

A − 1

(2)

d(cid:113)e

Figure 1: Observer model. Graphical model of a sensorimotor estimation task  as seen from the
outside (a)  and from the subjective point of view of the observer (b). a: Objective generative
model of the task. Stimulus s induces a noisy sensory measurement x in the observer  who decides
for estimate s∗ (see b). The recorded response r is further perturbed by reporting noise. Shaded
nodes denote experimentally accessible variables. b: Observer’s internal model of the task. The
observer performs inference in an internal measurement space in which the unknown stimulus is
denoted by t (with t = f (s)). The observer either chooses the subjectively optimal value of t  given
internal measurement x  by minimizing the expected loss  or simply lapses with probability λ. The
observer’s chosen estimate t∗ is converted to task space through the inverse mapping s∗ = f−1(t∗).
The whole process in this panel is encoded in (a) by the estimate distribution pest (s∗|x).
ous distributions can be ‘binned’ and approximated up to the desired precision by increasing Nexp.
Due to noise in the sensory systems  stimulus s induces an internal measurement x ∈ R accord-
ing to measurement distribution pmeas(x|s) [20]. In general  the magnitude of sensory noise may be
stimulus-dependent in task space  in which case the shape of the likelihood would change from point
to point – which is unwieldy for subsequent computations. We want instead to ﬁnd a transformed
space in which the scale of the noise is stimulus-independent and the likelihood translationally in-
variant [9] (see Supplementary Material). We assume that such change of variables is performed by
a function f (s) : s → t that monotonically maps stimulus s from task space into t = f (s)  which
lives with x in an internal measurement space. We assume for f (s) the following parametric form:

where A and B are chosen  without loss of generality  such that the discrete distribution of stimuli
mapped in internal space  {f (si)} for 1 ≤ i ≤ Nexp  has range [−1  1]. The parametric form of the
sensory map in Eq. 2 can approximate both Weber-Fechner’s law and Steven’s law  for different
values of base noise magnitude s0 and power exponent d (see Supplementary Material).
We determine the shape of pmeas(x|s) with a maximum-entropy approach by ﬁxing the ﬁrst four
moments of the distribution  and under the rather general assumptions that the sensory measure-
ment is unimodal and centered on the stimulus in internal measurement space. For computational
convenience  we express pmeas(x|s) as a mixture of (two) Gaussians in internal measurement space:
(3)
where N(cid:0)x|µ  σ2(cid:1) is a normal distribution with mean µ and variance σ2 (in this paper we consider

a two-component mixture but derivations easily generalize to more components). The parameters in
Eq. 3 are partially determined by specifying the ﬁrst four central moments: E [x] = f (s)  Var[x] =
σ2  Skew[x] = γ  Kurt[x] = κ; where σ  γ  κ are free parameters. The remaining degrees of freedom
(one  for two Gaussians) are ﬁxed by picking a distribution that satisﬁes unimodality and locally
maximizes the differential entropy (see Supplementary Material). The sensation model represented
by Eqs. 2 and 3 allows to express a large class of sensory models in the psychophysics literature 
including for instance stimulus-dependent noise [9  12  24] and ‘robust’ mixture models [21  26].

pmeas(x|s) = πN(cid:0)x|f (s) + µ1  σ2

1(cid:1) + (1 − π)N(cid:0)x|f (s) + µ2  σ2
2(cid:1)

2.2 Perceptual stage

Without loss of generality  we represent the observer’s prior distribution qprior(t) as a mixture of M
dense  regularly spaced Gaussian distributions in internal measurement space:

qprior(t) =

M(cid:88)m=1

wmN(cid:0)t|µmin + (m − 1)a  a2(cid:1)

a ≡

µmax − µmin

M − 1

(4)

3

sxs∗rtxminimize(cid:10)L(ˆt−t)(cid:11)lapset∗1−λλa.Generativemodelb.InternalmodelSensationpmeas(x|s)Perception&Decision-makingpest(s∗|x)Responsepreport(r|s∗)qprior(t)PerceptionDecision-makingqmeas(x|t)where wm are the mixing weights  a the lattice spacing and [µmin  µmax] the range in internal space
over which the prior is deﬁned (chosen 50% wider than the true stimulus range). Eq. 4 allows
the modeller to approximate any observer’s prior  where M regulates the ﬁne-grainedness of the
representation and is determined by computational constraints (for all our analyses we ﬁx M = 15).
For simplicity  we assume that the observer’s internal representation of the likelihood  qmeas(x|t)  is
expressed in the same measurement space and takes again the form of a unimodal mixture of two
Gaussians  Eq. 3  although with possibly different variance  skewness and kurtosis (respectively 
˜σ2  ˜γ and ˜κ) than the true likelihood. We write the observer’s posterior distribution as: qpost(t|x) =
Z qprior(t)qmeas(x|t) with Z the normalization constant.
2.3 Decision-making stage

1

According to Bayesian Decision Theory (BDT)  the observer’s ‘optimal’ estimate corresponds to the
value of the stimulus that minimizes the expected loss  with respect to loss function L(t  ˆt)  where
t is the true value of the stimulus and ˆt its estimate. In general the loss could depend on t and ˆt in
different ways  but for now we assume a functional dependence only on the stimulus difference in
internal measurement space  ˆt − t. The (subjectively) optimal estimate is:

t∗(x) = arg min

ˆt (cid:90) qpost(t|x)L(cid:0)ˆt − t(cid:1) dt

where the integral on the r.h.s. represents the expected loss. We make the further assumption that
the loss function is well-behaved  that is smooth  with a unique minimum at zero (i.e.  the loss is
minimal when the estimate matches the true stimulus)  and with no other local minima. As before 
we adopt a maximum-entropy approach and we restrict ourselves to the class of loss functions that
can be described as mixtures of two (inverted) Gaussians:

L(ˆt − t) = −π(cid:96)N(cid:16)ˆt − t|µ(cid:96)

1  σ(cid:96)
1

2(cid:17) − (1 − π(cid:96))N(cid:16)ˆt − t|µ(cid:96)

2  σ(cid:96)
2

2(cid:17) .

Although the loss function is not a distribution  we ﬁnd convenient to parametrize it in terms
of statistics of a corresponding unimodal distribution obtained by ﬂipping Eq. 6 upside down:
Mode [t(cid:48)] = 0  Var [t(cid:48)] = σ2
(cid:96)   Skew [t(cid:48)] = γ(cid:96)  Kurt [t(cid:48)] = κ(cid:96); with t(cid:48) ≡ ˆt − t. Note that we ﬁx
the location of the mode of the mixture of Gaussians so that the global minimum of the loss is at
zero. As before  the remaining free parameter is ﬁxed by taking a local maximum-entropy solution.
A single inverted Gaussian already allows to express a large variety of losses  from a delta function

(MAP strategy) for σ(cid:96) → 0 to a quadratic loss for σ(cid:96) → ∞ (in practice  for σ(cid:96) (cid:38) 1)  and it has been

shown to capture human sensorimotor behavior quite well [15]. Eq. 6 further extends the range of
describable losses to asymmetric and more or less peaked functions. Crucially  Eqs. 3  4  5 and 6
combined yield an analytical expression for the expected loss that is a mixture of Gaussians (see
Supplementary Material) that allows for a fast numerical solution [14  27].
We allow the possibility that the observer may occasionally deviate from BDT due to lapses with
probability λ ≥ 0. In the case of lapse  the observer’s estimate t∗ is drawn randomly from the prior
[11  14]. The combined stochastic estimator with lapse in task space has distribution:
(7)

pest(s∗|x) = (1 − λ) · δ(cid:2)s∗ − f−1 (t∗(x))(cid:3) + λ · qprior(s∗)|f(cid:48)(s∗)|

where f(cid:48)(s∗) is the derivative of the mapping in Eq. 2 (see Supplementary Material).

(5)

(6)

(8)

2.4 Response stage

We assume that the observer’s response r is equal to the observer’s estimate corrupted by indepen-
dent normal noise in task space  due to motor error and other residual sources of variability:

preport(r|s∗) = N(cid:0)r|s∗  σ2

report(s∗)(cid:1)

where we choose a simple parameteric form for the variance: σ2
1s2  that is the sum
of two independent noise terms (constant noise plus some noise that grows with the magnitude of
the stimulus). In our current analysis we are interested in observer models of perception  so we do
not explicitly model details of the motor aspect of the task and we do not include the consequences
of response error into the decision making part of the model (Eq. 5).

report(s) = ρ2

0 + ρ2

4

Finally  the main observable that the experimenter can measure is the response probability density 
presp(r|s; θ)  of a response r for a given stimulus s and observer’s parameter vector θ [12]:

obtained by marginalizing over unobserved variables (see Figure 1 a)  and which we can compute
through Eqs. 3–8. An observer model is fully characterized by parameter vector θ:

presp(r|s; θ) =(cid:90) N(cid:0)r|s∗  σ2
θ =(cid:16)σ  γ  κ  s0  d  ˜σ  ˜γ  ˜κ  σ(cid:96)  γ(cid:96)  κ(cid:96) {wm}M

report(s∗)(cid:1) pest(s∗|x)pmeas(x|s) ds∗ dx 
m=1   ρ0  ρ1  λ(cid:17) .

(10)
An experimental design is speciﬁed by a reference observer model θ∗  an experimental distribution
of stimuli (a discrete set of Nexp stimuli si  each with relative frequency Pi)  and possibly a subset
of parameters that are assumed to be equal to some a priori or experimentally measured values
during the inference. For experiments with multiple conditions  an observer model typically shares
several parameters across conditions. The reference observer θ∗ represents a ‘typical’ observer for
the idealized task under examination; its parameters are determined from pilot experiments  the
literature  or educated guesses. We are ready now to tackle the problem of identiﬁability of the
parameters of θ∗ within our framework for a given experimental design.

(9)

3 Mapping a priori identiﬁability

Two observer models θ and θ∗ are a priori practically non-identiﬁable if they produce similar re-
sponse probability densities presp(r|si; θ) and presp(r|si; θ∗) for all stimuli si in the experiment.
Speciﬁcally  we assume that data are generated by the reference observer θ∗ and we ask what is
the chance that a randomly generated dataset D of a ﬁxed size Ntr will instead provide support for
observer θ. For one speciﬁc dataset D  a natural way to quantify support would be the posterior
probability of a model given the data  Pr(θ|D). However  randomly generating a large number of
datasets so as to approximate the expected value of Pr(θ|D) over all datasets  in the spirit of previous
work on model identiﬁability [23]  becomes intractable for complex models such as ours.
Instead  we deﬁne the support for observer model θ  given dataset D  as its log likelihood 
log Pr(D|θ). The log (marginal) likelihood is a widespread measure of evidence in model com-
parison  from sampling algorithms to metrics such as AIC  BIC and DIC [28]. Since we know the
generative model of the data  Pr(D|θ∗)  we can compute the expected support for model θ as:

(cid:104)log Pr(D|θ)(cid:105) =(cid:90)|D|=Ntr

log Pr (D|θ) Pr (D|θ∗) dD.

The formal integration over all possible datasets with ﬁxed number of trials Ntr yields:

(11)

(12)

Nexp(cid:88)i=1

(cid:104)log Pr(D|θ)(cid:105) = −Ntr

Pi · DKL (presp(r|si; θ∗)||presp(r|si; θ)) + const

where DKL(·||·) is the Kullback-Leibler (KL) divergence between two distributions  and the con-
stant is an entropy term that does not affect our subsequent analysis  not depending on θ (see Sup-
plementary Material for the derivation). Crucially  DKL is non-negative  and zero only when the
two distributions are identical. The asymmetry of the KL-divergence captures the different status of
θ∗ and θ (that is  we measure differences only on datasets generated by θ∗). Eq. 12 quantiﬁes the
average support for model θ given true model θ∗  which we use as a proxy to assess model iden-
tiﬁability. As an empirical tool to explore the identiﬁability landscape  we deﬁne the approximate
expected posterior density as:

(13)
and we sample from Eq. 13 via MCMC. Clearly  E (θ|θ∗) is maximal for θ = θ∗ and generally
high for regions of the parameter space empirically close to the predictions of θ∗. Moreover  the
peakedness of E(θ|θ∗) is modulated by the number of trials Ntr (the more the trials  the more
information to discriminate between models).

E (θ|θ∗) ∝ e(cid:104)log Pr(D|θ)(cid:105)

4 Results

We apply our framework to two case studies: the inference of priors in a time interval estimation
task (see [24]) and the reconstruction of prior and noise characteristics in speed perception [9].

5

Figure 2: Internal representations in interval timing (Short condition). Accuracy of the recon-
structed priors in the Short range; each row corresponds to a different experimental design. a: The
ﬁrst column shows the reference prior (thick red line) and the recovered mean prior ± 1 SD (black
line and shaded area). The other columns display the distributions of the recovered central moments
of the prior. Each panel shows the median (black line)  the interquartile range (dark-shaded area)
and the 95 % interval (light-shaded area). The green dashed line marks the true value. b: Box plots
of the symmetric KL-divergence between the reconstructed priors and the prior of the reference ob-
server. At top  the primacy probability P ∗ of each setup having less reconstruction error than all
the others (computed by bootstrap). c: Joint posterior density of sensory noise σ and motor noise
ρ1 in setup BSL (gray contour plot; colored plots are marginal distributions). The parameters are
anti-correlated  and discordant with the true value (star and dashed lines). d: Marginal posterior
density for loss width parameter σ(cid:96)  suitably rescaled.

4.1 Temporal context and interval timing

We consider a time interval estimation and reproduction task very similar to [24]. In each trial  the
stimulus s is a time interval (e.g.  the interval between two ﬂashes)  drawn from a ﬁxed experimental
distribution  and the response r is the reproduced duration (e.g.  the interval between the second
ﬂash and a mouse click). Subjects perform in one or two conditions  corresponding to two different
discrete uniform distributions of durations  either on a Short (494-847 ms) or a Long (847-1200
ms) range. Subjects are trained separately on each condition till they (roughly) learn the underlying
distribution  at which point their performance is measured in a test session; here we only simulate the
test sessions. We assume that the experimenter’s goal is to faithfully recover the observer’s priors 
and we analyze the effect of different experimental designs on the reconstruction error.
To cast the problem within our framework  we need ﬁrst to deﬁne the reference observer θ∗. We
make the following assumptions: (a) the observer’s priors (or prior  in only one condition) are
smoothed versions of the experimental uniform distributions; (b) the sensory noise is affected by
the scalar property of interval timing  so that the sensory mapping is logarithmic (s0 ≈ 0  d = 1);
(c) we take average sensorimotor noise parameters from [24]: σ = 0.10  γ = 0  κ = 0  and ρ0 ≈ 0 
ρ1 = 0.07; (d) for simplicity  the internal likelihood coincides with the measurement distribution;
(e) the loss function in internal measurement space is almost-quadratic  with σ(cid:96) = 0.5  γ(cid:96) = 0 
κ(cid:96) = 0; (f) we assume a small lapse probability λ = 0.03; (g) in case the observer performs in two
conditions  all observer’s parameters are shared across conditions (except for the priors). For the
inferred observer θ we allow all model parameters to change freely  keeping only assumptions (d)
and (g). We compare the following variations of the experimental setup:

1. BSL: The baseline version of the experiment  the observer performs in both the Short and

Long conditions (Ntr = 500 each);

2. SRT or LNG: The observer performs more trials (Ntr = 1000)  but only either in the Short

(SRT) or in the Long (LNG) condition;

6

Mean010010010010100.511.501SD040Skewness01Kurtosis0104001010400101−101201−202401ms600800010ms50100040SRT05MAP05MTRms49484705PriorBSL a.05BSLSRTMAPMTR0.010.1110KLP∗0.060.130.02 0.79 b.0.060.080.10.12BSL0.10σ0.060.02ρ1 c.σℓ d.013. MAP: As BSL  but we assume a difference in the performance feedback of the task such

that the reference observer adopts a narrower loss function  closer to MAP (σ(cid:96) = 0.1);

4. MTR: As BSL  but the observer’s motor noise parameters ρ0  ρ1 are assumed to be known

(e.g. measured in a separate experiment)  and therefore ﬁxed during the inference.

We sample from the approximate posterior density (Eq. 13)  obtaining a set of sampled priors for
each distinct experimental setup (see Supplementary Material for details). Figure 2 a shows the
reconstructed priors and their central moments for the Short condition (results are analogous for
the Long condition; see Supplementary Material). We summarize the reconstruction error of the
recovered priors in terms of symmetric KL-divergence from the reference prior (Figure 2 b). Our
analysis suggests that the baseline setup BSL does a relatively poor job at inferring the observers’
priors. Mean and skewness of the inferred prior are generally acceptable  but for example the SD
tends to be considerably lower than the true value. Examining the posterior density across various
dimensions  we ﬁnd that this mismatch emerges from a partial non-identiﬁability of the sensory
noise  σ  and the motor noise  w1 (Figure 2 c).1 Limiting the task to a single condition with double
number of trials (SRT) only slightly improves the quality of the inference. Surprisingly  we ﬁnd that
a design that encourages the observer to adopt a loss function closer to MAP considerably worsens
the quality of the reconstruction in our model. In fact  the loss width parameter σ(cid:96) is only weakly
identiﬁable (Figure 2 d)  with severe consequences for the recovery of the priors in the MAP case.
Finally  we ﬁnd that if we can independently measure the motor parameters of the observer (MTR) 
the degeneracy is mostly removed and the priors can be recovered quite reliably.
Our analysis suggests that the reconstruction of internal representations in interval timing requires
strong experimental constraints and validations [12]. This worked example also shows how our
framework can be used to rank experimental designs by the quality of the inferred features of interest
(here  the recovered priors)  and to identify parameters that may critically affect the inference. Some
ﬁndings align with our intuitions (e.g.  measuring the motor parameters) but others may be non-
obvious  such as the bad impact that a narrow loss function may have on the inferred priors within
our model. Incidentally  the low identiﬁability of σ(cid:96) that we found in this task suggests that claims
about the loss function adopted by observers in interval timing (see [24])  without independent
validation  might deserve additional investigation. Finally  note that the analysis we performed
is theoretical  as the effects of each experimental design are formulated in terms of changes in the
parameters of the ideal reference observer. Nevertheless  the framework allows to test the robustness
of our conclusions as we modify our assumptions about the reference observer.

4.2 Slow-speed prior in speed perception

As a further demonstration  we use our framework to re-examine a well-known ﬁnding in visual
speed perception  that observers have a heavy-tailed prior expectation for slow speeds [9  29]. The
original study uses a 2AFC paradigm [9]  that we convert for our analysis into an equivalent estima-
tion task (see e.g. [30]). In each trial  the stimulus magnitude s is speed of motion (e.g.  the speed
of a moving dot in deg/s)  and the response r is the perceived speed (e.g.  measured by interception
timing). Subjects perform in two conditions  with different contrast levels of the stimulus  either
High (cHigh = 0.5) or Low (cLow = 0.075)  corresponding to different levels of estimation noise.
Note that in a real speed estimation experiment subjects quickly develop a prior that depends on the
experimental distribution of speeds [30] – but here we assume no learning of that kind in agreement
with the underlying 2AFC task. Instead  we assume that observers use their ‘natural’ prior over
speeds. Our goal is to probe the reliability of the inference of the slow-speed prior and of the noise
characteristics of the reference observer (see [9]).
We deﬁne the reference observer θ∗ as follows: (a) the observer’s prior is deﬁned in task space by
prior)−kprior  with sprior = 1 deg/s and kprior = 2.4 [29];
a parametric formula: pprior(s) = (s2 + s2
(b) the sensory mapping has parameters s0 = 0.35 deg/s  d = 1 [29]; (c) the amount of sensory
noise depends on the contrast level  as per [9]: σHigh = 0.2  σLow = 0.4  and γ = 0  κ = 0; (d)
the internal likelihood coincides with the measurement distribution; (e) the loss function in internal
measurement space is almost-quadratic  with σ(cid:96) = 0.5  γ(cid:96) = 0  κ(cid:96) = 0; (f) we assume a consider-
1This degeneracy is not surprising  as both sensory and motor noise of the reference observer θ∗ are ap-
proximately Gaussian in internal measurement space (∼ log task space). This lack of identiﬁability also affects
the prior since the relative weight between prior and likelihood needs to remain roughly the same.

7

Internal representations in speed perception. Accuracy of the reconstructed internal
Figure 3:
representations (priors and likelihoods). Each row corresponds to different assumptions during the
inference. a: The ﬁrst column shows the reference log prior (thick red line) and the recovered mean
log prior ± 1 SD (black line and shaded area). The other two columns display the approximate
posteriors of kprior and sprior  obtained by ﬁtting the reconstructed ‘non-parametric’ priors with a
parametric formula (see text). Each panel shows the median (black line)  the interquartile range
(dark-shaded area) and the 95 % interval (light-shaded area). The green dashed line marks the
true value. b: Box plots of the symmetric KL-divergence between the reconstructed and reference
prior. c: Approximate posterior distributions for sensory mapping and sensory noise parameters. In
experimental design STD  the internal likelihood parameters (˜σHigh  ˜σLow) are equal to their objective
counterparts (σHigh  σLow).

able amount of reporting noise  with ρ0 = 0.3 deg/s  ρ1 = 0.21; (g) we assume a contrast-dependent
lapse probability (λHigh = 0.01  λLow = 0.05); (h) all parameters that are not contrast-dependent
are shared across the two conditions. For the inferred observer θ we allow all model parameters to
change freely  keeping only assumptions (d) and (h). We consider the standard experimental setup
described above (STD)  and an ‘uncoupled’ variant (UNC) in which we do not take the usual as-
sumption that the internal representation of the likelihoods is coupled to the experimental one (so 
˜σHigh  ˜σLow  ˜γ and ˜κ are free parameters). As a sanity check  we also consider an observer with a
uniformly ﬂat speed prior (FLA)  to show that in this case the algorithm can correctly infer back the
absence of a prior for slow speeds (see Supplementary Material).
Unlike the previous example  our analysis shows that here the reconstruction of both the prior and the
characteristics of sensory noise is relatively reliable (Figure 3 and Supplementary Material)  without
major biases  even when we decouple the internal representation of the noise from its objective
counterpart (except for underestimation of the noise lower bound s0  and of the internal noise ˜σHigh 
Figure 3 c). In particular  in all cases the exponent kprior of the prior over speeds can be recovered
with good accuracy. Our results provide theoretical validation  in addition to existing empirical
support  for previous work that inferred internal representations in speed perception [9  29].

5 Conclusions

We have proposed a framework for studying a priori identiﬁability of Bayesian models of perception.
We have built a fairly general class of observer models and presented an efﬁcient technique to
explore their vast identiﬁability landscape. In one case study  a time interval estimation task  we have
demonstrated how our framework could be used to rank candidate experimental designs depending
on their ability to resolve the underlying degeneracy of parameters of interest. The obtained ranking
is non-trivial: for example  it suggests that experimentally imposing a narrow loss function may
be detrimental  under certain assumptions. In a second case study  we have shown instead that the
inference of internal representations in speed perception  at least when cast as an estimation task in
the presence of a slow-speed prior  is generally robust and in theory not prone to major degeneracies.
Several modiﬁcations can be implemented to increase the scope of the psychophysical tasks covered
by the framework. For example  the observer model could include a generalization to arbitrary loss
spaces (see Supplementary Material)  the generative model could be extended to allow multiple
cues (to analyze cue-integration studies)  and a variant of the model could be developed for discrete-
choice paradigms  such as 2AFC  whose identiﬁability properties are largely unknown.

8

kprior01sprior01σHigh0510σLow05˜σHigh0510˜σLow050240100.20.405100.20.40.60500.20.405100.20.40.605deg/s01201deg/s0.010.1105deg/sUNC0.51248−100Log priorSTD a.−100STDUNC0.010.1110KL b.s0 c.05References
[1] Geisler  W. S. (2011) Contributions of ideal observer theory to vision research. Vision Res 51  771–781.
[2] Knill  D. C. & Richards  W. (1996) Perception as Bayesian inference. (Cambridge University Press).
[3] Trommersh¨auser  J.  Maloney  L.  & Landy  M. (2008) Decision making  movement planning and statis-

tical decision theory. Trends Cogn Sci 12  291–297.

[4] Pouget  A.  Beck  J. M.  Ma  W. J.  & Latham  P. E. (2013) Probabilistic brains: knowns and unknowns.

Nat Neurosci 16  1170–1178.

[5] Maloney  L.  Mamassian  P.  et al. (2009) Bayesian decision theory as a model of human visual perception:

testing Bayesian transfer. Vis Neurosci 26  147–155.

[6] Vilares  I.  Howard  J. D.  Fernandes  H. L.  Gottfried  J. A.  & K¨ording  K. P.

representations of prior and likelihood uncertainty in the human brain. Curr Biol 22  1641–1648.

(2012) Differential

[7] K¨ording  K. P. & Wolpert  D. M.

244–247.

(2004) Bayesian integration in sensorimotor learning. Nature 427 

[8] Girshick  A.  Landy  M.  & Simoncelli  E. (2011) Cardinal rules: visual orientation perception reﬂects

knowledge of environmental statistics. Nat Neurosci 14  926–932.

[9] Stocker  A. A. & Simoncelli  E. P. (2006) Noise characteristics and prior expectations in human visual

speed perception. Nat Neurosci 9  578–585.

[10] Sanborn  A. & Grifﬁths  T. L. (2008) Markov chain monte carlo with people. Adv Neural Inf Process Syst

20  1265–1272.

[11] Chalk  M.  Seitz  A.  & Seri`es  P. (2010) Rapidly learned stimulus expectations alter perception of motion.

J Vis 10  1–18.

[12] Acerbi  L.  Wolpert  D. M.  & Vijayakumar  S. (2012) Internal representations of temporal statistics and

feedback calibrate motor-sensory interval timing. PLoS Comput Biol 8  e1002771.

[13] Houlsby  N. M.  Husz´ar  F.  Ghassemi  M. M.  Orb´an  G.  Wolpert  D. M.  & Lengyel  M. (2013) Cognitive

tomography reveals complex  task-independent mental representations. Curr Biol 23  2169–2175.

[14] Acerbi  L.  Vijayakumar  S.  & Wolpert  D. M. (2014) On the origins of suboptimality in human proba-

bilistic inference. PLoS Comput Biol 10  e1003661.

[15] K¨ording  K. P. & Wolpert  D. M. (2004) The loss function of sensorimotor learning. Proc Natl Acad Sci

U S A 101  9839–9842.

[16] Gekas  N.  Chalk  M.  Seitz  A. R.  & Seri`es  P. (2013) Complexity and speciﬁcity of experimentally-

induced expectations in motion perception. J Vis 13  1–18.

[17] Jones  M. & Love  B. (2011) Bayesian Fundamentalism or Enlightenment? On the explanatory status and

theoretical contributions of Bayesian models of cognition. Behav Brain Sci 34  169–188.

[18] Bowers  J. S. & Davis  C. J. (2012) Bayesian just-so stories in psychology and neuroscience. Psychol

Bull 138  389.

[19] Mamassian  P. & Landy  M. S. (2010) It’s that time again. Nat Neurosci 13  914–916.
[20] Simoncelli  E. P. (2009) in The Cognitive Neurosciences  ed. M  G. (MIT Press)  pp. 525–535.
[21] Knill  D. C. (2003) Mixture models and the probabilistic structure of depth cues. Vision Res 43  831–854.
[22] Anderson  J. R. (1978) Arguments concerning representations for mental imagery. Psychol Rev 85  249.
[23] Navarro  D. J.  Pitt  M. A.  & Myung  I. J. (2004) Assessing the distinguishability of models and the

informativeness of data. Cognitive Psychol 49  47–84.

[24] Jazayeri  M. & Shadlen  M. N. (2010) Temporal context calibrates interval timing. Nat Neurosci 13 

1020–1026.

[25] Tassinari  H.  Hudson  T.  & Landy  M. (2006) Combining priors and noisy visual cues in a rapid pointing

task. J Neurosci 26  10154–10163.

[26] Natarajan  R.  Murray  I.  Shams  L.  & Zemel  R. S. (2009) Characterizing response behavior in multi-

sensory perception with conﬂicting cues. Adv Neural Inf Process Syst 21  1153–1160.

[27] Carreira-Perpi˜n´an  M. A. (2000) Mode-ﬁnding for mixtures of gaussian distributions. IEEE T Pattern

Anal 22  1318–1323.

[28] Spiegelhalter  D. J.  Best  N. G.  Carlin  B. P.  & Van Der Linde  A. (2002) Bayesian measures of model

complexity and ﬁt. J R Stat Soc B 64  583–639.

[29] Hedges  J. H.  Stocker  A. A.  & Simoncelli  E. P.

coherence of visual motion stimuli. J Vis 11  14  1–16.

(2011) Optimal inference explains the perceptual

[30] Kwon  O. S. & Knill  D. C. (2013) The brain uses adaptive internal models of scene statistics for senso-

rimotor estimation and planning. Proc Natl Acad Sci U S A 110  E1064–E1073.

9

,Luigi Acerbi
Wei Ji Ma
Sethu Vijayakumar
Eran Malach
Shai Shalev-Shwartz