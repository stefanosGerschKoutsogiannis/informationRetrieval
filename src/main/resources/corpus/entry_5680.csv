2019,Triad Constraints for Learning Causal Structure of Latent Variables,Learning causal structure from observational data has attracted much attention  and it is notoriously challenging to find the underlying structure in the presence of confounders (hidden direct common causes of two variables). In this paper  by properly leveraging the non-Gaussianity of the data  we propose to estimate the structure over latent variables with the so-called Triad constraints: we design a form of "pseudo-residual" from three variables  and show that when causal relations are linear and noise terms are non-Gaussian  the causal direction between the latent variables for the three observed variables is identifiable by checking a certain kind of independence relationship. In other words  the Triad constraints help us to locate latent confounders and determine the causal direction between them. This goes far beyond the Tetrad constraints and reveals more information about the underlying structure from non-Gaussian data. Finally  based on the Triad constraints  we develop a two-step algorithm to learn the causal structure corresponding to measurement models. Experimental results on both synthetic and real data demonstrate the effectiveness and reliability of our method.,Triad Constraints for Learning Causal Structure of

Latent Variables

Ruichu Cai∗ 1  Feng Xie ∗1  Clark Glymour 2  Zhifeng Hao 1 3  Kun Zhang 2

1 School of Computer Science  Guangdong University of Technology  Guangzhou  China

2 Department of Philosophy  Carnegie Mellon University  Pittsburgh  USA
3 School of Mathematics and Big Data  Foshan University  Foshan  China

cairuichu@gdut.edu.cn xiefeng009@gmail.com cg09@andrew.cmu.edu

zfhao@gdut.edu.cn kunz1@cmu.edu

Abstract

Learning causal structure from observational data has attracted much attention 
and it is notoriously challenging to ﬁnd the underlying structure in the presence
of confounders (hidden direct common causes of two variables). In this paper 
by properly leveraging the non-Gaussianity of the data  we propose to estimate
the structure over latent variables with the so-called Triad constraints: we design
a form of "pseudo-residual" from three variables  and show that when causal
relations are linear and noise terms are non-Gaussian  the causal direction between
the latent variables for the three observed variables is identiﬁable by checking a
certain kind of independence relationship. In other words  the Triad constraints
help us to locate latent confounders and determine the causal direction between
them. This goes far beyond the Tetrad constraints and reveals more information
about the underlying structure from non-Gaussian data. Finally  based on the
Triad constraints  we develop a two-step algorithm to learn the causal structure
corresponding to measurement models. Experimental results on both synthetic and
real data demonstrate the effectiveness and reliability of our method.

1

Introduction

Traditional methods for causal discovery  which aims to ﬁnd causal relations from (purely) observa-
tional data  can be roughly divided into two categories  namely constraint-based methods including
PC [Spirtes and Glymour  1991] and FCI [Spirtes et al.  1995; Colombo et al.  2012]  and score-based
ones such as GES [Chickering  2002] and GES with generalized scores [Huang et al.  2018]. A num-
ber of methods focus on estimating causal relationships between observed variables and fail to recover
the underlying causal structure of latent variables. For example  from large enough data generated by
the structure in Figure 1  where Li are latent variables and Xi are observed ones  we may only get a
complete graph using the PC algorithm [Spirtes and Glymour  1991]  a widely-used constraint-based

method  since there is no d-separation relation among the observed variables (although{X1} and
{X2  X3} are d-separated by L1  which is latent). Besides  in reality we can measure only a limited

number of variables and the causal inﬂuences may happen at the level of latent variables  so we are
often concerned about the causal structure of latent variables; see e.g.  Bartholomew et al. [2008].
There exist several methods for causal discovery in the case with confounders. Spirtes et al. [2000]
attempt to resolve this problem using the so-called Tetrad constraints [Spearman  1928]. Inspired
by Tetrad constraints  various contributions have been made towards estimating structure over latent

∗These authors contributed equally to this work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

variables. For instance  Silva and Scheines [2005] presented testable statistical conditions to identify
d-separations in linear latent variable models  Silva et al. [2006] propose the BPC algorithm using
Tetrad constraints to discovery causal structure of latent variables  and Shimizu et al. [2009] further
applied analysis based on the Linear  Non-Gaussian  Acyclic Model (LiNGAM) [Shimizu et al. 
2006] to the recovered latent variables to further improve the estimated causal relations between
them; Sullivant et al. [2010] showed that a sub-matrix of the covariance matrix with low rank
corresponds to conditional independence constraints on the collections of Gaussian data and proposed
a trek separation criterion to learn causal structure. Recently  Kummerfeld and Ramsey [2016]
used the extended t-separation [Spirtes  2013] to infer causal relations of latent variables  with the
FindOneFactorClusters (FOFC) algorithm. However  these methods fail to work when latent variables
have fewer than three pure measurement variables. Furthermore  even when this condition holds 
Tetrad and its variants may not be able to ﬁnd the causal direction between latent variables. Over-
complete independent component analysis offers another method [Hoyer et al.  2008]  as an extension
of the LiNGAM analysis; however  this analysis is generally hard to do  especially when there are
relatively many latent variables  and the method does not focus on the structure of latent variables.
More recently  Zhang et al. [2017] and Huang et al. [2015] deal with a speciﬁc type of confounders 
which can be written as functions of the time/domain index in nonstationary/heterogeneous data.
Overall  learning the structure of latent variables is a challenging problem; for instance  none of the
above methods is able to recover the causal structure as shown in Figure 1.
It is desirable to develop testable conditions on the observed data to estimate the structure of latent
variables. Interestingly  we ﬁnd that given three variables in the non-Gaussian case  the independence
condition between one of them and a certain linear combination of the remaining two variables gives
hints as to the causal structure even in the presence of latent confounders. In particular  given a set

of three distinct and dependent variables{Xi  Xj  Xk}  we deﬁne a particular type of "regression
residual " E(i jࢯ k) ࢼ= Xi − Cov(Xi Xk)
Cov(Xj  Xk) ⋅ Xj. Then whether E(i jࢯ k) is independent from Xk contains

information regarding where latent confounders might be and the causal relationships among them.
We term this condition the Triad constraint.
We further extend our Triad constraints to learn the
structure of a wide class of linear latent structure mod-
els from non-Gaussian data. Speciﬁcally  we propose
a two-phase algorithm to discover the causal relation-
ships of latent variables. It ﬁrst ﬁnds pure clusters
(clusters of variables having only one common latent
variable and no observed parent) from observed data
in phase I. Then in phase II it learns the causal order of
latent variables based on the clusters. Compared with
Tetrad constraints  Triad constraints can reveal more
information about the causal structure involving latent
variables for non-Gaussian data. For instance  Triad
constraints can be used to locate the latent variables Li  i = 1  ...  5  in Figure 1 and identify their
structure  including their causal direction  but Tetrad constraints cannot (see the details in Section 4).
Our main contributions include 1) proposing a novel constraint involving only three non-Gaussian
variables  namely the Triad constraint  and showing the connection between this constraint and the
underlying causal structure  which helps identify causal information of latent confounders  and 2)
developing a two-phase algorithm to learn the causal structure of latent variables  including causal
skeleton and causal directions  based on the Triad constraints.

Figure 1: A causal structure involving 5 la-
tent variables.

c d

X2 X3

θ

L5

X5

X7 X8

X1

L4

e

X6

a

λ

α

γ

L2

L3

f

β

L1

b

X4

2 Problem Deﬁnition

In this work  we focus on a particular type of linear latent structure model. Let X ={X1  X2  ...Xm}
denote the observed variable set  L ={L1  L2  ...Ln} denote the latent variable set  and V = X ∪ L
bikVk + εVi i = 1  2  ...  m + n  where P a(Vi) contains all the parent variables of Vi and
Vk∈P a(Vi) k≠i

denote the full variable set. In the linear latent structure model  the data generation process follows:
1) the structure of V can be represented by a Direct Acyclic Graph (DAG)  2) no observed variable
in X is an ancestor of any latent variable in L  3) the generation of V is assumed to follow Vi =

∑

2

bik is the causal strength from Vk to Vi; and 4) all εVi are noise (disturbance) variables which are
independent with each other.
BPC  FOFC  and their variants [Silva et al.  2006; Kummerfeld and Ramsey  2016] have been shown
to be able to recover a certain amount of causal information for some linear latent structure models
from observed data. These methods usually assume that each latent variable has at least three pure
measurement variables  which may not hold in practice  e.g.  for the example given in Figure 1;
furthermore  they cannot always recover the causal direction between latent variables. Here  pure
measurement variables are deﬁned as measured variables that have only one latent parent and no
observed parent.
Here  we greatly relax the structural assumption of Tetrad; we consider the case where each latent
variable has two or more pure variables as children  under the assumption of non-Gaussianity of the
noise terms. Here  pure variables are the variables that may be latent or observed but have only one
parent. The model is deﬁned as follows.
Deﬁnition 1 (Non-Gaussian Two-Pure Linear Latent Structure Model). A linear latent structure
model is called a Non-Gaussian Two-Pure (NG2P) linear latent structure model if it further satisﬁes
the following three assumptions:

1) [Purity Assumption] there is no direct edges between the observed variables;

2) [Two-Pure Child Variable Assumption] each latent variable has at least two pure variables

as children;

3) [Non-Gaussianity Assumption] the noise terms are non-Gaussian.

One may wonder how restrictive the above assumptions are and how to interpret the result produced
by our proposed method when the assumptions  especially assumption 1)  are violated. We will
discuss such issues in Section 5.

3 Triad Constraints: A Brief Formulation

We begin with the deﬁnition of Triad constraints  the independence relationship between the "pseudo-
residual" and the observed variables. It is worth noting that there is some related work that also exploits
similar concepts to "pseudo-residual"  e.g.  in the context of auxiliary variables (or instrumental
variables)[Chen et al.  2017] or pseudo-variable [Drton and Richardson  2004]  but to the best of our
knowledge  it has not been realized that the independence property involving such pseudo-residuals
reﬂects structural asymmetry of the latent variables.
Deﬁnition 2 (Triad constraints). Suppose Xi  Xj and Xk are distinct and correlated variables and

that all noise variables are non-Gaussian. Deﬁne the pseudo-residual of{Xi  Xj} relative to Xk 
E(i jࢯ k) ࢼ= Xi − Cov(Xi  Xk)
Cov(Xj  Xk) ⋅ Xj.
We say that{Xi  Xj} and Xk satisfy Triad constraint if and only if E(i jࢯ k)  Xk  i.e. {Xi  Xj} and
Xk violate the Triad constraint if and only if E(i jࢯ k)  Xk.

which is called a reference variable  as

(1)

The following two theorems show some interesting properties of the Triad constraints  which are
further explored to discover the causal structure among the latent variables. We ﬁrst aim at the
identiﬁcation of the causal direction of latent variables by analyzing the variables in the clusters. The
following theorem shows the asymmetry between the latent variables in light of the Triad condition
in the non-Gaussian case.
Theorem 1. Let La and Lb be two directed connected latent variables without confounders and

let{Xi} and{Xj  Xk} be their children  respectively. Then if{Xi  Xj} and Xk violate the Triad

constraint  La → Lb holds. In other words  if the Triad condition is violated and the latent variables
have no confounders  then the latent variable of the reference variable is a child of the other latent
variable.

The proof is given in the Supplementary Material  and it heavily relies on the Darmois-Skitovich
Theorem Kagan et al. [1973]  which essentially says that as long as two variables share any non-
Gaussian  independent component  they cannot be statistically independent. The following example

3

shows that Triad constraints help ﬁnd the causal direction between two latent variables from their
pure clusters.

Example 1. Consider the example in Figure 1  clusters{X1} and{X4  X5} have corresponding
with any child of L2 is violated  i.e.  E(1 4ࢯ 5)  X5  and E(1 5ࢯ 4)  X4  but E(4 5ࢯ 1)  X1. This

latent variables L1 and L2  respectively. Because L1 → L2 without a confounder  any Triad condition

shows the asymmetry between L1 and L2  implied by the three observed variables.

One might wonder whether we can make use of the Triad constraints in the Gaussian case to
infer the causal direction between L1 and L2 in the above example. Unfortunately  one can show

E(1 2ࢯ 3)  X3  E(1 3ࢯ 2)  X2 and E(2 3ࢯ 1)  X1 when the variables are jointly Gaussian  and thus

the asymmetry between L1 and L2 disappears.
The second theorem is about the property of the clusters in terms of the Triad constraints. Here we
say a set of observed variables is a cluster if these variables have the same latent variable as the
parent. Intuitively  if such variables are pure variables  they are equivalent under the Triad constraints.
For example  X2 and X3 in Figure 1 have the same constraints. Theorem 2 formalizes this property
of clusters and gives the criterion for ﬁnding clusters.

Theorem 2. Let S be a correlated variable set. If ∀Xi  Xj ∈ S and ∀Xk ∈ X\ S {Xi  Xj} and
Example 2. Consider the example in Figure 1  for{X4  X5}  one may ﬁnd{X4  X5} and Xi satisfy
Triad constraint  where i = 1  2  3  6  7  8  so{X4  X5} is a cluster. But for{X1  X4}  E(1 4ࢯ 5) is not
independent of X5  so{X1  X4} is not a cluster.

The proof is given in the Supplementary Material. The following example illuminates how the
theorem can be used to distinguish the cluster of the variables.

Xk satisfy the Triad constraints  then S is a cluster.

4 Triad Constraint-Based Causal Latent Structure Discovery

In this section  we extend the above results to estimate the NG2P linear latent structure. To this
end  we propose a two-phase algorithm to Learn the Structure of latent variables based on Triad
Constraints (LSTC). It ﬁrstly ﬁnds pure clusters from the observed data (phase I)  and then it learns
the structure of the latent variables behind these clusters (phase II).

4.1 Phase 1: Finding Clusters

Theorem 2 has paved the way to discover the clusters of the variables. It also enables us to use a
cluster fusion-like method to discover the clusters of observed variables and latent variables that
have already been found  i.e.  we recursively ﬁnd the clusters of variables and merge the overlapping
clusters. Here we consider two practical issues involved in such a recursive fusion algorithm. The
ﬁrst is what clusters are to be merged  and the second is how to check whether Triad constraints
involving latent variables hold given that they are hidden.
For the merge problem  we ﬁnd that the overlapping clusters can be directly merged into one cluster.
This is because the overlapping clusters have the same latent variable as the parent under the NG2P
linear latent structure. The validity of the merge step is guaranteed by Proposition 1.
Proposition 1. Let C1 and C2 be two clusters. If C1 and C2 are overlapping  C1 and C2 share the
same latent parent.

This proposition holds true because of the equivalence of the pure variables in terms of Triad
constraints. In particular  as shown in Theorem 2  all variables in a cluster have the same Triad
constraints.
After we ﬁnd and merge clusters  we associate each cluster with a latent variable and  in fact  replace
the variables in the cluster by the corresponding latent variable. We will then continue ﬁnding
clusters and merging clusters. Since we replace variables in the same cluster with the associated
latent variable  clearly subsequent Triad constraints to be checked may involve latent variables. How
can we check such constraints without knowing the values of the latent variables? Thanks to the
linearity assumption and the transitivity of linear causal relations  one can use its child to test the

Triad constraints. Consider the example in Figure 1. Suppose we already found the cluster{X2  X3}

4

Algorithm 1 FindClusters
Output: Partial causal structure G
1: Initialize C = ∅  G = ∅  V = X;
2: repeat
3:
4:
5:

Input: Data set X ={X1  ...  Xm}
for each{Vi  Vj} ∈ V do
V\{Vi  Vj} then
if E(i jࢯ k)  Vk holds for ∀Vk ∈
C = C ∪{{Vi  Vj}};

if Vi and Vj then

6:
7:
8:
9:

end if

end if
end for

10: Merge all the overlapping sets in C.
11:
12:

for each S ∈ C do

Introduce a latent variable L for S and
initialize L with the value of any vari-
able of S;

V =(V\ S) ∪{L};
G = G ∪{L → ViࢯVi ∈ S};

13:
14:
end for
15:
16: until V contains only latent variables.
17: Return: G

{X1  L4} and X5  holds true if and only if{X1  X2} and X5 holds because X3 is not in the variable

and associated it with a latent variable  say L4. Then one can see that if only one variable in this
cluster  say X2  is kept (i.e.  X3 is removed)  then any subsequent Triad constraint  e.g.  that of

set and L4 and its only child  X2  have the same Triad properties relative to any other remaining
variable. That means  we can just use the observed variables of X2 as the values of L4 and ignore all
the other variables in the same cluster for the purpose of checking Triad constraints.
Consideration of the above two issues directly leads to the following algorithm  which includes three
main steps: 1) ﬁnd the clusters according to Theorem 2; 2) merge the overlapping clusters according
to Proposition 1; 3) introduce a new latent variable to represent a newly discovered cluster and use
the values of an arbitrary variable in the cluster as the observed values of the latent variable for
subsequent Triad condition checking. This procedure is illustrated with the following example.

Example 3. Consider the example in Figure 1. First  we ﬁnd the clusters{X2  X3} {X4  X5} 
{X7  X8} based on the Theorem 2 (line 3-8). Second  introduce L4  L2 and L5 as the parents
for{X2  X3} {X4  X5} {X7  X8}  respectively  whose values are set to those of X2  X4 and X7 
respectively. Third  we ﬁnd the clusters{X1  L4} {X6  L5} on the updated V based on Theorem
2 (line 3-8). Fourth  introduce L1 and L3 as the parents of{X1  L4} and{X6  L5}  respectively.
Finally  we return the clusters of the variables in the form of partial graph as G ={L1 →{X1  L4} 
L4 →{X2  X3}  L2 →{X4  X5}  L3 →{X6  L5} and L5 →{X7  X8}}.

4.2 Phase 2: Learning the Structure of Latent Variables

Given the clusters discovered in the previous step  we aim to recover the structure among the root
latent variables of each cluster. Due to the availability of various independence test methods for the
latent variables  the causal order is the focus of this learning procedure. As an immediate extension
of Theorem 1  the root latent variable can be identiﬁed by checking the Triad constraints  as stated in
the following proposition.

Proposition 2. Given a latent variable Lr and its two children{Vi  Vj}  Lr is a root latent variable
if and only if E(k iࢯj)  Vj holds for each Vk  where Vk is a child of any other latent variables.

This proposition inspires us to use a recur-
sive approach to discover the causal order;
we recursively identify the root latent vari-
able and update the data by removing the
root variable’s effect  until the causal order
over all latent variables is determined. The
key concern of such recursive approach is
whether Proposition 2 still works on the
updated data.
Fortunately  we ﬁnd that there is still asym-
metry implied by the Triad constraints if

we update the data as follows: let{Vi  Vj}

be two pure variables of the root latent Lr 

′
L
2

β

′
L
3

X5

X7

c d

e f

θ

dαL1 + εX5

E(4 1ࢯ 2)
E(6 1ࢯ 2)
f θ(γ + αβ)L1 + ε
− e(γ+αβ)
fects of L1 through{X1  X2}  where L

Figure 2: Structure obtained after removing the ef-
3 =
+ εX7  and the inﬂuences of

eβεL2
noise terms are shown by dashed lines.

2 = εL2  L

+ εL3  ε

− cα
a

⋅ εX1

′
X7

= f εL5

εX6

εX1

a

′

′

′
X7

εX4

5

for any other remaining latent variable L  we update the value of Vk  which is a child of the value

of L  as Vk ࢼ= E(k iࢯj) and keep the value of the other children unchanged. On the updated data 
the property of the root  i.e.  E(k iࢯj) is independent of Xj still holds. Recall the example given
i.e.  E(4 1ࢯ2) and E(6 1ࢯ2) share a common noise εX1  as seen in Figure 2 {E(4 1ࢯ2) {E(6 1ࢯ2)} and X5
satisfy the Triad constraint  while{E(4 1ࢯ2) {E(6 1ࢯ2)} and X7 violate it. More detail is given in the

in Figure 1  although such a removal step introduces common effect into the updated variables 

Supplementary Material.
Given the causal order of the variables  we can ﬁnd the causal structure simply by removing redun-
dant edges from the full acyclic graph using the independence test methods. Here we adopt the
independence test method proposed in [Silva et al.  2006] (see Theorem 19 therein for the detail).
Finally  we present the following recursive algorithm for learning the structure over latent variables 
and give the following example for illustration.

Algorithm 2 LearnLatentStructure
Input: Partial causal structure G
Output: Complete causal structure G
1: Initialize L with the root variables of each
2: Select two pure child for each L ∈ L;
3: repeat
4:

subgraph in G and Lr = φ;

Find the root node Lr and it’s children
Lchild be the largest set satisﬁng Proposi-
tion 2 and add the Lr into Lr;
while L′ ≠ φ do

L = L\{Lr ∪ Lchild}  L′ ={Lr ∪ Lchild};
L′ = L′\{L
r};

′
Find the root node L
to Proposition 2.

r from L′ according

′
Let Vi  Vj be the children of L
r;

′

5:
6:
7:

8:
9:

10:
11:
12:

′};

for each L

′) as Vk =

′ ∈ L′ do
′
r → L

update Vk (a child of L

G = G ∪{L
E(k iࢯj);
end for
16: ifࢯLrࢯ > 1 then
13:
end while
14:
15: until L = φ
G = G ∪{L → Lr} for all Lr ∈ Lr;

Construct an new latent variable L;

17:
18:
19: end if
20: Remove the redundant edges of G using the
21: Return: G

method given in [Silva et al.  2006]);

Example 4. Continue to consider the example in Figure 1. Given the partial structure discovered

in previous phase  i.e.  L1 →{X1  L4}  L4 →{X2  X3}  L2 →{X4  X5}  L3 →{X6  L5} and L5 →
{X7  X8}  the algorithm proceeds is as follows. First  we ﬁnd three latent variables{L1  L2  L3} in
L1 is the root variable (Line 4). Third  we update data make use of{X1  X2} (Line 12) and the
{E(4 1ࢯ2) {E(6 1ࢯ2)} and X5 satisﬁes the Triad constraint  while{E(4 1ࢯ2) {E(6 1ࢯ2)} and X7 violates it.
Finally  the whole structure is L1 →{L4  L2  L3}  L2 → L3  and L3 → L4.

the partial graph G that cannot be further merged (Line 1). Second  we ﬁnd that the latent variable

results are given in Figure 2 . Fourth  we ﬁnd that L2 a root latent variable of L3 (Line 7)  because

5 Discussion of the Assumptions of Our Model

α

β

L3

γ

L2

L1

ab

To understand the applicability of our
model (Deﬁnition 1)  we discuss the plau-
sibility of the involved three assumptions
and what may happen if they are violated.
If Purity Assumption is violated  i.e.  there
are directed links between observed vari-
ables  there may exist pure models equiv-
alent to the underlying causal structure in
terms of Triad constraints. For example 
if we have enough data generated by the
non-pure structure given in Figure 3  the
estimated structure would be the one given
in Figure 1. In the result  one essentially
uses another latent variable (e.g.  L4) to replace the direct causal relation between the observed

Figure 3: An non-pure latent causal structure  which
can be transformed into the equivalent pure structure in
Figure 1  by simply using a latent variable to represent
the direct causal relation among the observed variables.

X1 X2 X3

X6 X7 X8

c d

ef

X4

X5

6

variables (e.g.  X2 and X3). It is challenging but desirable to give a characterization of the result
given by our procedure and its connection to the underlying causal structure in the general case.
For Two-Pure Children Variable Assumption  our assumption is much milder than that of Tetrad:
we only need two pure variables for each latent variable  while Tetrad needs three pure observed
variables for each latent variable. For Non-Gaussianity Assumption  we note that this assumption can
be easily tested from the observed data. Furthermore  non-Gaussian distributions  unlike Gaussian
ones  are expected to be ubiquitous  due to Cramér Decomposition Theorem [Cramér  1962]  as
argued in Spirtes and Zhang [2016]. In fact  for our algorithm  this assumption can be relaxed to at
most one noise term is Gaussian for observed variables  but not for latent confounders.

6 Simulation

For fair comparison  we simulate data following the linear latent structure model. There are four
typical cases: Cases 1 and 2 have two latent variables L1 and L2  with L1 → L2  and Cases 3 and
4 have three latent variables L1  L2  and L3  with L2 ← L1 → L3  and L2 → L3  respectively.
Note that the simulated structure does not necessarily follow the pure assumption of our model (e.g.
X2 → X5 violates the purity assumption of our model)  we simply recover the equivalent pure latent
variable model for such structure as discussed in Section 5. In all four cases  the causal strength b is

sampled from a uniform distribution between[−2  −0.5] ∪[0.5  2]  noise terms are generated as the
ﬁfth power of uniform(-1 1) variables  and the sample size is selected from{500  1000  2000}. The
• Case 1: L1 and L2 both have two pure measurement variables  i.e.  L1 →{X1  X2} and
L2 →{X3  X4}.
and add edges{X2 → X5  X4 → X6}.
• Case 3: each latent variable has two measurement variables  i.e.  L1 →{X1  X2}  L2 →
{X3  X4}  L3 →{X5  X6}.
edges{X9 → X10  X11 → X12}.

• Case 2: adding impure variables to Case 1. We add X5 and X6 to L1 and L2 respectively 

details of these networks are as follow.

• Case 4: adding impurities to Case 3. In detail  we add two measurement variables to each
latent variable  i.e.  add X7  X8 to L1  X9  X10 to L2  and X11  X12 to L3. Further add

Considering the data with non-Gaussian noise variables  we choose the Hilbert-Schmidt Independence
Criterion (HSIC) test [Gretton et al.  2008] as the independence test. We compared the proposed
algorithm with the BPC [Silva et al.  2006] and FOFC [Kummerfeld and Ramsey  2016] algorithms2.
The method by Shimizu et al. [2009] exploits BPC as its ﬁrst step  so it is not used for comparison 
given that BPC is included. All the following experimental results are based on 10 runs of the
algorithms over randomly generated data.
In the experiment  the discovered measurement model and the reconstructed structure model are
compared with ground truth to evaluate the performance of the algorithms. To evaluate the quality
of the measurement model  we use Latent omission= OL
T L   and Mismeasure-
ment= M O
T O as the evaluation metrics  where OL is the number of omission latent variables  F L is the
number of false latent variables  and T L is the total number of latent variables in ground truth graph
(See the details in [Silva et al.  2006]) . To evaluate the quality of the reconstructed structure model 
we further use the F 1 = 2P ×R
P +R as our metric. Here P and R are the precision and recall  respectively.

T L   Latent commission= F L

As shown in Table 1  our algorithm  LSTC  achieves the best performance (the lowest errors) on all
cases of the measurement model. Notably  when the sample size reaches 2000  the latent omission 
latent commission  and mismeasurements of our method all reach 0. The BPC and FOFC algorithms
(with the Delta test  a distribution-free test) do not perform well. These ﬁndings demonstrate that our
algorithm requires only two pure variables in the measurement model  which is a clear advantage
over the compared methods. Because of the clear performance gap  we only report the results of our
methods on structure learning in Figure 4.

2We used these implementations in the TETRAD package  which can be downloaded at http://www.phil.cmu.

edu/tetrad/.

7

Table 1: Evaluation of output latent variables

Algorithm

Latent omission

BPC

FOFC

Latent commission

BPC

FOFC

Mismeasurements

BPC

FOFC

Case 1

Case 2

Case 3

Case 4

LSTC
0.00(0)
0.00(0)
0.00(0)
0.10(0)
0.05(0)
0.00(0)
0.20(0)
0.13(0)
0.00(0)
0.00(0)
0.00(0)
0.00(0)

500
1000
2000
500
1000
2000
500
1000
2000
500
1000
2000

-
-
-

-

-

0.50(2)
0.65(3)

0.86(6)
0.93(8)

0.10(0)
0.00(0)
0.00(0)

-
-
-

-
-

-
-

0.90(8)

0.96(9)

0.13(0)
0.16(0)
0.50(0)

LSTC
0.00(0)
0.00(0)
0.00(0)
0.05(0)
0.00(0)
0.00(0)
0.03(0)
0.00(0)
0.00(0)
0.00(0)
0.26(0)
0.00(0)

-
-
-

-

-

0.00(2)
0.00(3)

0.00(6)
0.00(8)

0.00(0)
0.00(0)
0.00(0)

-
-
-

-
-

-
-

0.00(8)

0.00(9)

0.00(0)
0.00(0)
0.00(0)

LSTC
0.00(0)
0.00(0)
0.00(0)
0.03(0)
0.00(0)
0.00(0)
0.17(0)
0.00(0)
0.00(0)
0.00(0)
0.00(0)
0.00(0)

-
-
-

-

-

0.06(2)
0.05(3)

0.71(6)
0.85(8)

0.04(0)
0.00(0)
0.00(0)

-
-
-

-
-

-
-

0.03(8)

0.93(9)

0.04(0)
0.00(0)
0.01(0)

Note: The number in parentheses indicates the number of occurrences that the current algorithm cannot
correctly solve the problem. If the result of a method is always wrong  we use the symbol ’-’ to indicate it.

As shown in Figure 4  the F1 score gradually increases to 1 as the sample size increases in all the four
cases  which illustrates that our algorithm can recover the complete structure of the latent variables 
including their causal directions.

7 Application to Stock Market Data

We now apply our algorithm to discover the causal
network behind the Hong Kong stock market. The
data set contains 1331 daily returns of 14 major
stocks. Although some interesting results have been
discovered on the data [Zhang and Chan  2008]  the
latent variables behind the stocks are still unexplored.
The kernel width in the HSIC test [Gretton et al. 
2008] is set to 0.1. Note that the condition for
ﬁnding clusters (Theorem 2) might be partially vi-
olated in the real world; we choose the candi-
date clusters with the highest number of satisﬁed
Triad constraints in the algorithm  which proceeds

as follows. First  {X4  X7  X12}  {X2  X3  X6} 
{X1  X10  X11} {X5  X8  X13}  and{X9  X14} are

1

1

1

1

1

1

1

e
r
o
c
o
s

1
F

0.9

0.8

0.7

0.9

0.84

0.9

0.77

0.8

0.7

Case 1

Case 2

Case 3

Case 4

500

1000

2000

Figure 4: The F1 scores of LSTC algorithm.

identiﬁed as clusters by running the FindClusters al-
gorithm. These ﬁve clusters are set to L2  L3  L4  L5
and L6  respectively. We then run algorithm 2 over the ﬁve clusters and obtain the ﬁnal result  shown
in Figure 5.
We have a number of observations from the discovered structure  which are consistent with our
understanding of the stock market. 1) All stocks are affected by a major latent variable (L1)  which
may be related to government policy  the total risk in the market  etc. 2) Companies in the same sub-

index tend to gather under a common latent variable. For example  the cluster{X5  X8  X13} is in the
Finance Sub-index; the cluster{X2  X3  X6} is in the Utilities Sub-index; the cluster{X1  X10  X11}

is in the Properties Sub-index. 3) Ownership relations tend to have one common latent variable  i.e. 
X1 holds about 50% of X10  and they have one common cause L4. Similarly  X5 holds about 60%
of X8  and they have one common cause L5.

8 Conclusion

In this paper  we proposed the so-called Triad constraints for estimating a particular type of linear
non-Gaussian latent variable model. The constraints help locate latent variables and identify their
causal structure. Then we apply these constraints to discover the whole structure of latent variables
with a two-phase algorithm. Theoretical analysis showed asymptotic correctness of the proposed

8

Figure 5: Causal diagram of the stocks

approach under our assumptions. Experimental results further veriﬁed the usefulness of our algorithm.
Our future work is to 1) characterize properties of the results of our procedure for general causal
structures with latent variables and 2) further relax our assumptions for better applicability of the
method.

Acknowledgments

This research was supported in part by NSFC-Guangdong Joint Found (U1501254)  Natural Science
Foundation of China (61876043)  Natural Science Foundation of Guangdong (2014A030306004 
2014A030308008)  Guangdong High-level Personnel of Special Support Program (2015TQ01X140) 
Science and Technology Planning Project of Guangzhou(201902010058) and Outstanding Young
Scientiﬁc Research Talents International Cultivation Project Fund of Department of Education of
Guangdong Province(40190001). KZ would like to acknowledge the support by NIH under Contract
No. NIH-1R01EB022858-01  FAINR01EB022858  NIH-1R01LM012087  NIH-5U54HG008540-02 
and FAINU54HG008540  by the United States Air Force under Contract No. FA8650-17-C-7715 
and by NSF EAGER Grant No. IIS-1829681. The NIH  the U.S. Air Force  and the NSF are not
responsible for the views reported here. KZ also beneﬁted from funding from Living Analytics
Research Center and Singapore Management University. Feng would like to thank Shohei Shimizu
for his insightful discussions and suggestions on the original draft. We appreciate the comments from
anonymous reviewers  which greatly helped to improve the paper.

References
David Bartholomew  Fiona Steele  Ir Moustaki  and Jane Galbraith. The analysis and interpretation of multivari-

ate data for social scientists. Routledge (2 edition)  2008.

Bryant Chen  Daniel Kumor  and Elias Bareinboim.

Identiﬁcation and model testing in linear structural
equation models using auxiliary variables. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70  pages 757–766. JMLR. org  2017.

David Maxwell Chickering. Optimal structure identiﬁcation with greedy search. Journal of machine learning

research  3(Nov):507–554  2002.

Diego Colombo  Marloes H Maathuis  Markus Kalisch  and Thomas S Richardson. Learning high-dimensional

directed acyclic graphs with latent and selection variables. The Annals of Statistics  pages 294–321  2012.

H. Cramér. Random variables and probability distributions. Cambridge University Press  Cambridge  2nd

edition  1962.

Mathias Drton and Thomas S Richardson. Iterative conditional ﬁtting for gaussian ancestral graph models. In
Proceedings of the 20th conference on Uncertainty in artiﬁcial intelligence  pages 130–137. AUAI Press 
2004.

Arthur Gretton  Kenji Fukumizu  Choon H Teo  Le Song  Bernhard Schölkopf  and Alex J Smola. A kernel
statistical test of independence. In Advances in neural information processing systems  pages 585–592  2008.
Patrik O Hoyer  Shohei Shimizu  Antti J Kerminen  and Markus Palviainen. Estimation of causal effects using
linear non-gaussian causal models with hidden variables. International Journal of Approximate Reasoning 
49(2):362–378  2008.

Biwei Huang  Kun Zhang  and Bernhard Schölkopf. Identiﬁcation of time-dependent causal model: A gaussian
process treatment. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence  pages 3561–
3568  2015.

9

3L13X2L5L1L8X5X12X7X4X6X3X2X1X4L11X10X6L9X14XSun Hung Kai Prop (0016.hk)11XCheung Kong (0001.hk)1XCLP Hldgs (0002.hk)2XHK & China Gas (0003.hk)3XWharf (Hldgs) (0004.hk)4XHSBC Hldg (0005.hk)5XHK Electric (0006.hk)6XHang Lung Dev (0010.hk)7XHang Seng Bank (0011.hk)8XHenderson Land (0012.hk)9XHutchison (0013.hk)10XSwire Pacific 'A' (0019.hk)12XBank of East Asia (0023.hk)13XCathay Pacific Air (0293.hk)14XBiwei Huang  Kun Zhang  Yizhu Lin  Bernhard Schölkopf  and Clark Glymour. Generalized score functions
for causal discovery. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining  pages 1551–1560. ACM  2018.

Abram M Kagan  Calyampudi Radhakrishna Rao  and Yurij Vladimirovich Linnik. Characterization problems

in mathematical statistics. 1973.

Erich Kummerfeld and Joseph Ramsey. Causal clustering for 1-factor measurement models. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages
1655–1664. ACM  2016.

Shohei Shimizu  Patrik O Hoyer  Aapo Hyvärinen  and Antti Kerminen. A linear non-Gaussian acyclic model

for causal discovery. Journal of Machine Learning Research  7(Oct):2003–2030  2006.

Shohei Shimizu  Patrik O Hoyer  and Aapo Hyvärinen. Estimation of linear non-gaussian acyclic models for

latent factors. Neurocomputing  72(7-9):2024–2027  2009.

Ricardo Silva and Richard Scheines. New d-separation identiﬁcation results for learning continuous latent
variable models. In Proceedings of the 22nd international conference on Machine learning  pages 808–815.
ACM  2005.

Ricardo Silva  Richard Scheine  Clark Glymour  and Peter Spirtes. Learning the structure of linear latent variable

models. Journal of Machine Learning Research  7(Feb):191–246  2006.

Charles Spearman. Pearson’s contribution to the theory of two factors. British Journal of Psychology. General

Section  19(1):95–101  1928.

Peter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. Social science

computer review  9(1):62–72  1991.

Peter Spirtes and Kun Zhang. Causal discovery and inference: concepts and recent methodological advances. In

Applied informatics  volume 3  page 3. SpringerOpen  2016.

Peter Spirtes  Christopher Meek  and Thomas Richardson. Causal inference in the presence of latent variables
and selection bias. In Proceedings of the Eleventh conference on Uncertainty in artiﬁcial intelligence  pages
499–506. Morgan Kaufmann Publishers Inc.  1995.

Peter Spirtes  Clark N Glymour  and Richard Scheines. Causation  Prediction  and Search. MIT press  2000.
Peter Spirtes. Calculation of entailed rank constraints in partially non-linear and cyclic models. In Proceedings
of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence  pages 606–615. AUAI Press  2013.
Seth Sullivant  Kelli Talaska  Jan Draisma  et al. Trek separation for gaussian graphical models. The Annals of

Statistics  38(3):1665–1685  2010.

Kun Zhang and Laiwan Chan. Minimal nonlinear distortion principle for nonlinear independent component

analysis. Journal of Machine Learning Research  9(Nov):2455–2487  2008.

Kun Zhang  Biwei Huang  Jiji Zhang  Clark Glymour  and Bernhard Schölkopf. Causal discovery from
nonstationary/heterogeneous data: Skeleton estimation and orientation determination. In IJCAI: Proceedings
of the Conference  pages 1347–1353  2017.

10

,Ruichu Cai
Feng Xie
Clark Glymour
Zhifeng Hao
Kun Zhang