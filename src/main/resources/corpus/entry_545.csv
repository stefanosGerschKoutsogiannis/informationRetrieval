2014,Analysis of Learning from Positive and Unlabeled Data,Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper  we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias  but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data  and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally  we provide generalization error bounds and show that  for an equal number of labeled and unlabeled samples  the generalization error of learning only from positive and unlabeled samples is no worse than $2\sqrt{2}$ times the fully supervised case. These theoretical findings are also validated through experiments.,Analysis of Learning from
Positive and Unlabeled Data

Marthinus C. du Plessis
The University of Tokyo
Tokyo  113-0033  Japan

christo@ms.k.u-tokyo.ac.jp

Gang Niu
Baidu Inc.

Beijing  100085  China
niugang@baidu.com

Masashi Sugiyama

The University of Tokyo
Tokyo  113-0033  Japan

sugi@k.u-tokyo.ac.jp

Abstract

Learning a classiﬁer from positive and unlabeled data is an important class of
classiﬁcation problems that are conceivable in many practical applications. In this
paper  we ﬁrst show that this problem can be solved by cost-sensitive learning
between positive and unlabeled data. We then show that convex surrogate loss
functions such as the hinge loss may lead to a wrong classiﬁcation boundary due
to an intrinsic bias  but the problem can be avoided by using non-convex loss func-
tions such as the ramp loss. We next analyze the excess risk when the class prior
is estimated from data  and show that the classiﬁcation accuracy is not sensitive to
class prior estimation if the unlabeled data is dominated by the positive data (this
is naturally satisﬁed in inlier-based outlier detection because inliers are dominant
in the unlabeled dataset). Finally  we provide generalization error bounds and
show that  for an equal number of labeled and unlabeled samples  the generaliza-
p
tion error of learning only from positive and unlabeled samples is no worse than
2 times the fully supervised case. These theoretical ﬁndings are also validated
2
through experiments.

1 Introduction

Let us consider the problem of learning a classiﬁer from positive and unlabeled data (PU classiﬁca-
tion)  which is aimed at assigning labels to the unlabeled dataset [1]. PU classiﬁcation is conceivable
in various applications such as land-cover classiﬁcation [2]  where positive samples (built-up urban
areas) can be easily obtained  but negative samples (rural areas) are too diverse to be labeled. Outlier
detection in unlabeled data based on inlier data can also be regarded as PU classiﬁcation [3  4].
In this paper  we ﬁrst explain that  if the class prior in the unlabeled dataset is known  PU classiﬁca-
tion can be reduced to the problem of cost-sensitive classiﬁcation [5] between positive and unlabeled
data. Thus  in principle  the PU classiﬁcation problem can be solved by a standard cost-sensitive
classiﬁer such as the weighted support vector machine [6]. The goal of this paper is to give new
insight into this PU classiﬁcation algorithm. Our contributions are three folds:

(cid:15) The use of convex surrogate loss functions such as the hinge loss may potentially lead
to a wrong classiﬁcation boundary being selected  even when the underlying classes are
completely separable. To obtain the correct classiﬁcation boundary  the use of non-convex
loss functions such as the ramp loss is essential.

1

(cid:15) When the class prior in the unlabeled dataset is estimated from data  the classiﬁcation error
is governed by what we call the effective class prior that depends both on the true class prior
and the estimated class prior. In addition to gaining intuition behind the classiﬁcation error
incurred in PU classiﬁcation  a practical outcome of this analysis is that the classiﬁcation
error is not sensitive to class-prior estimation error if the unlabeled data is dominated by
positive data. This would be useful in  e.g.  inlier-based outlier detection scenarios where
inlier samples are dominant in the unlabeled dataset [3  4]. This analysis can be regarded as
an extension of traditional analysis of class priors in ordinary classiﬁcation scenarios [7  8]
to PU classiﬁcation.
(cid:15) We establish generalization error bounds for PU classiﬁcation. For an equal number of
p
positive and unlabeled samples  the convergence rate is no worse than 2
2 times the fully
supervised case.

Finally  we numerically illustrate the above theoretical ﬁndings through experiments.

2 PU classiﬁcation as cost-sensitive classiﬁcation

In this section  we show that the problem of PU classiﬁcation can be cast as cost-sensitive classiﬁ-
cation.

Ordinary classiﬁcation: The Bayes optimal classiﬁer corresponds to the decision function
f (X) 2 f1;(cid:0)1g that minimizes the expected misclassiﬁcation rate w.r.t. a class prior of (cid:25):

R(f ) := (cid:25)R1(f ) + (1 (cid:0) (cid:25))R(cid:0)1(f );

where R(cid:0)1(f ) and R1(f ) denote the expected false positive rate and expected false negative rate:

R(cid:0)1(f ) = P(cid:0)1(f (X) ̸= (cid:0)1)

and R1(f ) = P1(f (X) ̸= 1);
and P1 and P(cid:0)1 denote the marginal probabilities of positive and negative samples.
In the empirical risk minimization framework  the above risk is replaced with their empirical ver-
sions obtained from fully labeled data  leading to practical classiﬁers [9].
Cost-sensitive classiﬁcation: A cost-sensitive classiﬁer selects a function f (X) 2 f1;(cid:0)1g in
order to minimize the weighted expected misclassiﬁcation rate:

R(f ) := (cid:25)c1R1(f ) + (1 (cid:0) (cid:25))c(cid:0)1R(cid:0)1(f );

(1)

where c1 and c(cid:0)1 are the per-class costs [5].
Since scaling does not matter in (1)  it is often useful to interpret the per-class costs as reweighting
the problem according to new class priors proportional to (cid:25)c1 and (1 (cid:0) (cid:25))c(cid:0)1.

PU classiﬁcation:
In PU classiﬁcation  a classiﬁer is learned using labeled data drawn from the
positive class P1 and unlabeled data that is a mixture of positive and negative samples with unknown
class prior (cid:25):

PX = (cid:25)P1 + (1 (cid:0) (cid:25))P(cid:0)1:

Since negative samples are not available  let us train a classiﬁer to minimize the expected misclassi-
ﬁcation rate between positive and unlabeled samples. Since we do not have negative samples in the
PU classiﬁcation setup  we cannot directly estimate R(cid:0)1(f ) and thus we rewrite the risk R(f ) not
to include R(cid:0)1(f ). More speciﬁcally  let RX (f ) be the probability that the function f (X) gives the
positive label over PX [10]:

RX (f ) = PX (f (X) = 1)

= (cid:25)P1(f (X) = 1) + (1 (cid:0) (cid:25))P(cid:0)1(f (X) = 1)
= (cid:25)(1 (cid:0) R1(f )) + (1 (cid:0) (cid:25))R(cid:0)1(f ):

(2)

2

Then the risk R(f ) can be written as

R(f ) = (cid:25)R1(f ) + (1 (cid:0) (cid:25))R(cid:0)1(f )
= (cid:25)R1(f ) (cid:0) (cid:25)(1 (cid:0) R1(f )) + RX (f )
= 2(cid:25)R1(f ) + RX (f ) (cid:0) (cid:25):

(3)

n

Let (cid:17) be the proportion of samples from P1 compared to PX  which is empirically estimated by
′ denote the numbers of positive and unlabeled samples  respectively. The risk
n+n′ where n and n
R(f ) can then be expressed as

R(f ) = c1(cid:17)R1(f ) + cX (1 (cid:0) (cid:17))RX (f ) (cid:0) (cid:25); where

c1 =

2(cid:25)
(cid:17)

and

cX =

1
1 (cid:0) (cid:17)

:

Comparing this expression with (1)  we can conﬁrm that the PU classiﬁcation problem is solved
by cost-sensitive classiﬁcation between positive and unlabeled data with costs c1 and cX. Some
implementations of support vector machines  such as libsvm [6]  allow for assigning weights
to classes. In practice  the unknown class prior (cid:25) may be estimated by the methods proposed in
[10  1  11].
In the following sections  we analyze this algorithm.

3 Necessity of non-convex loss functions in PU classiﬁcation

In this section  we show that solving the PU classiﬁcation problem with a convex loss function may
lead to a biased solution  and the use of a non-convex loss function is essential to avoid this problem.

Loss functions in ordinary classiﬁcation: We ﬁrst consider ordinary classiﬁcation problems
Instead of a binary decision function f (X) 2
where samples from both classes are available.
f(cid:0)1; 1g  a continuous decision function g(X) 2 R such that sign(g(X)) = f (X) is learned. The
loss function then becomes

J0-1(g) = (cid:25)E1 [ℓ0-1(g(X))] + (1 (cid:0) (cid:25))E(cid:0)1 [ℓ0-1((cid:0)g(X))] ;

where Ey is the expectation over Py and ℓ0-1(z) is the zero-one loss:

{
0 z > 0;
1 z (cid:20) 0:

ℓ0-1(z) =

Since the zero-one loss is hard to optimize in practice due to its discontinuous nature  it may be
replaced with a ramp loss (as illustrated in Figure 1):

ℓR(z) =

1
2

max(0; min(2; 1 (cid:0) z));

giving an objective function of

JR(g) = (cid:25)E1 [ℓR (g(X))] + (1 (cid:0) (cid:25))E(cid:0)1 [ℓR((cid:0)g(X))] :

(4)

To avoid the non-convexity of the ramp loss  the hinge loss is often preferred in practice:

ℓH(z) =

max(1 (cid:0) z; 0);

1
2

giving an objective of

JH(g) = (cid:25)E1 [ℓH (g(X))] + (1 (cid:0) (cid:25))E(cid:0)1 [ℓH((cid:0)g(X))] :

(5)
One practical motivation to use the convex hinge loss instead of the non-convex ramp loss is that
separability (i.e.  ming JR(g) = 0) implies ℓR(z) = 0 everywhere  and for all values of z for which
ℓR(z) = 0  we have ℓH(z) = 0. Therefore  the convex hinge loss will give the same decision
boundary as the non-convex ramp loss in the ordinary classiﬁcation setup  under the assumption that
the positive and negative samples are non-overlapping.

3

ℓR(z) = 1

ℓH (z) = 1

2 max(0; min(2; 1(cid:0)z))
2 max(0; 1(cid:0)z)

ℓH(z)

ℓR(z)

1

1
2

ℓH(z) + ℓH((cid:0)z)

1

ℓR(z) + ℓR((cid:0)z)

(cid:0)1

1

(a) Loss functions

(cid:0)1

1

(b) Resulting penalties

Figure 1: ℓR(z) denotes the ramp loss  and ℓH(z) denotes the hinge loss. ℓR(z)+ℓR((cid:0)z) is constant
but ℓH (z) + ℓH ((cid:0)z) is not and therefore causes a superﬂuous penalty.

Ramp loss function in PU classiﬁcation: An important question is whether the same interpreta-
tion will hold for PU classiﬁcation: can the PU classiﬁcation problem be solved by using the convex
hinge loss? As we show below  the answer to this question is unfortunately “no”.
In PU classiﬁcation  the risk is given by (3)  and its ramp-loss version is given by

JPU-R(g) = 2(cid:25)R1(f ) + RX (f ) (cid:0) (cid:25)

= 2(cid:25)E1 [ℓR(g(X))] + [(cid:25)E1 [ℓR((cid:0)g(X))] + (1 (cid:0) (cid:25))E(cid:0)1 [ℓR((cid:0)g(X))]] (cid:0) (cid:25)
= (cid:25)E1 [ℓR(g(X))] + (cid:25)E1 [ℓR(g(X)) + ℓR((cid:0)g(X))]
+ (1 (cid:0) (cid:25))E(cid:0)1 [ℓR((cid:0)g(X))] (cid:0) (cid:25);

(8)
where (6) comes from (3) and (7) is due to the substitution of (2). Since the ramp loss is symmetric
in the sense of

(6)
(7)

ℓR((cid:0)z) + ℓR(z) = 1;

(8) yields

JPU-R(g) = (cid:25)E1 [ℓR(g(X))] + (1 (cid:0) (cid:25))E(cid:0)1 [ℓR((cid:0)g(X))] :

(9)
(9) is essentially the same as (4)  meaning that learning with the ramp loss in the PU classiﬁcation
setting will give the same classiﬁcation boundary as in the ordinary classiﬁcation setting.
For non-convex optimization with the ramp loss  see [12  13].

Hinge loss function in PU classiﬁcation: On the other hand  using the hinge loss to minimize (3)
for PU learning gives
JPU-H(g) = 2(cid:25)E1 [ℓH (g(X))] + [(cid:25)E1 [ℓH ((cid:0)g(X))] + (1 (cid:0) (cid:25))E(cid:0)1 [ℓH ((cid:0)g(X))]] (cid:0) (cid:25);

}
|
= (cid:25)E1 [ℓH (g(X))] + (1 (cid:0) (cid:25))E(cid:0)1 [ℓH ((cid:0)g(X))]

{z

|
}
+ (cid:25)E1 [ℓH (g(X)) + ℓH ((cid:0)g(X))]

{z

(10)
(cid:0)(cid:25):

Ordinary error term  cf. (5)

Superﬂuous penalty

We see that the hinge loss has a term that corresponds to (5)  but it also has a superﬂuous penalty
term (see also Figure 1). This penalty term may cause an incorrect classiﬁcation boundary to be
selected. Indeed  even if g(X) perfectly separates the data  it may not minimize JPU-H(g) due to the
superﬂuous penalty. To obtain the correct decision boundary  the loss function should be symmetric
(and therefore non-convex). Alternatively  since the superﬂuous penalty term can be evaluated  it
can be subtracted from the objective function. Note that  for the problem of label noise  an identical
symmetry condition has been obtained [14].

Illustration: We illustrate the failure of the hinge loss on a toy PU classiﬁcation problem with
class conditional densities of:

p(xjy = 1) = N((cid:0)3; 12

)

and p(xjy = 1) = N(

)

3; 12

;

where N ((cid:22); (cid:27)2) is a normal distribution with mean (cid:22) and variance (cid:27)2. The hinge-loss objective
function for PU classiﬁcation  JPU-H(g)  is minimized with a model of g(x) = wx + b (the ex-
pectations in the objective function is computed via numerical integration). The optimal decision

4

(a) Class-conditional densities of
the problem

(b) Optimal threshold and threshold
using the hinge loss

(c) The misclassiﬁcation rate for
the optimal and hinge loss case

Figure 2: Illustration of the failure of the hinge loss for PU classiﬁcation. The optimal threshold
and the threshold estimated by the hinge loss differ signiﬁcantly (Figure 2(b))  causing a difference
in the misclassiﬁcation rates (Figure 2(c)). The threshold for the ramp loss agrees with the optimal
threshold.

threshold and the threshold for the hinge loss is plotted in Figure 2(b) for a range of class priors.
Note that the threshold for the ramp loss will correspond to the optimal threshold. From this ﬁgure 
we note that the hinge-loss threshold differs from the optimal threshold. The difference is especially
severe for larger class priors  due to the fact that the superﬂuous penalty is weighted by the class
prior. When the class-prior is large enough  the large hinge-loss threshold causes all samples to be
positively labeled. In such a case  the false negative rate is R1 = 0 but the false positive rate is
R(cid:0)1 = 1. Therefore  the overall misclassiﬁcation rate for the hinge loss will be 1 (cid:0) (cid:25).

4 Effect of inaccurate class-prior estimation

To solve the PU classiﬁcation problem by cost-sensitive learning described in Section 2  the true
class prior (cid:25) is needed. However  since it is often unknown in practice  it needs to be estimated  e.g. 
by the methods proposed in [10  1  11]. Since many of the estimation methods are biased [1  11] 
it is important to understand the inﬂuence of inaccurate class-prior estimation on the classiﬁcation

performance. In this section  we elucidate how the error in the estimated class priorb(cid:25) affects the

classiﬁcation accuracy in the PU classiﬁcation setting.

Risk with true class prior in ordinary classiﬁcation:
In the ordinary classiﬁcation scenarios
with positive and negative samples  the risk for a classiﬁer f on a dataset with class prior (cid:25) is given
as follows ([8  pp. 26–29] and [7]):

R(f; (cid:25)) = (cid:25)R1(f ) + (1 (cid:0) (cid:25))R(cid:0)1(f ):

The risk for the optimal classiﬁer according to the class prior (cid:25) is therefore 

(cid:3)

R

((cid:25)) = min

f2F R(f; (cid:25))

(cid:3)

Note that R
is illustrated in Figure 3(a).

((cid:25)) is concave  since it is the minimum of a set of functions that are linear w.r.t. (cid:25). This

Excess risk with class prior estimation in ordinary classiﬁcation: Suppose we have a classiﬁer

bf that minimizes the risk for an estimated class priorb(cid:25):
bf := arg min
f2F R(f;b(cid:25)):
The risk when applying the classiﬁer bf on a dataset with true class prior (cid:25) is then on the line tangent
((cid:25)) at (cid:25) =b(cid:25)  as illustrated in Figure 3(a):
bR((cid:25)) = (cid:25)R1(bf ) + (1 (cid:0) (cid:25))R(cid:0)1(bf ):
The function bf is suboptimal at (cid:25)  and results in the excess risk [8]:

to the concave function R

(cid:3)

E(cid:25) = bR((cid:25)) (cid:0) R((cid:25)):

5

−6−303600.20.4xp(x) p(x|y=1)p(x|y=−1)0.10.30.50.70.9−1−0.500.51πThreshold OptimalHinge Loss0.10.30.50.70.900.0020.0040.0060.0080.01πMisclassification rate OptimalHinge((cid:25)) = bR((cid:25))

(cid:3)

R

bR((cid:25))

0:2

0:1

k
s
i
R

E(cid:25)

e(cid:25)

1

(cid:25)

Class prior
(a) Selecting a classiﬁer to minimize (11) and apply-
ing it to a dataset with class prior (cid:25) leads to an excess
risk of E(cid:25).

(b) The effective class priore(cid:25) vs. the estimated class
priorb(cid:25) for different true class priors (cid:25).
Figure 3: Learning in the PU framework with an estimated class priorb(cid:25) is equivalent to selecting
a classiﬁer which minimizes the risk according to an effective class prior e(cid:25).
between the effective class prior e(cid:25) and the true class prior (cid:25) causes an excess risk E(cid:25). (b) The
effective class priore(cid:25) depends on the true class prior (cid:25) and the estimated class priorb(cid:25).
minimizes the risk in (3). In practice  however  we only know an estimated class priorb(cid:25). Therefore 

Excess risk with class prior estimation in PU classiﬁcation: We wish to select a classiﬁer that

(a) The difference

a classiﬁer is selected to minimize

R(f ) = 2b(cid:25)R1(f ) + RX (f ) (cid:0)b(cid:25):

(11)

2 (cid:25).

Expanding the above risk based on (2) gives

R(f ) = 2b(cid:25)R1(f ) + (cid:25)(1 (cid:0) R1(f )) + (1 (cid:0) (cid:25))R(cid:0)1(f ) (cid:0)b(cid:25)
= (2b(cid:25) (cid:0) (cid:25)) R1(f ) + (1 (cid:0) (cid:25))R(cid:0)1(f ) + (cid:25) (cid:0)b(cid:25):

immediately shows that PU classiﬁcation cannot be performed when the estimated class prior is

Thus  the estimated class prior affects the risk with respect to 2b(cid:25) (cid:0) (cid:25) and 1 (cid:0) (cid:25). This result
less than half of the true class prior:b(cid:25) (cid:20) 1
We deﬁne the effective class priore(cid:25) so that 2b(cid:25) (cid:0) (cid:25) and 1 (cid:0) (cid:25) are normalized to sum to one:
2b(cid:25) (cid:0) (cid:25)
2b(cid:25) (cid:0) (cid:25) + 1 (cid:0) (cid:25)
Figure 3(b) shows the proﬁle of the effective class priore(cid:25) for different (cid:25). The graph shows that
when the true class prior (cid:25) is large e(cid:25) tends to be ﬂat around (cid:25). When the true class prior is known

2b(cid:25) (cid:0) (cid:25)
2b(cid:25) (cid:0) 2(cid:25) + 1

e(cid:25) =

to be large (such as the proportion of inliers in inlier-based outlier detection)  a rough class-prior
estimator is sufﬁcient to have a good classiﬁcation performance. On the other hand  if the true class
prior is small  PU classiﬁcation tends to be hard and an accurate class-prior estimator is necessary.
We also see that when the true class prior is large  overestimation of the class prior is more attenu-
ated. This may explain why some class-prior estimation methods [1  11] still give a good practical
performance in spite of having a positive bias.

=

:

5 Generalization error bounds for PU classiﬁcation

In this section  we analyze the generalization error for PU classiﬁcation  when training samples are
clearly not identically distributed.
More speciﬁcally  we derive error bounds for the classiﬁcation function f (x) of form

n∑

i=1

′∑

n

f (x) =

(cid:11)ik(xi; x) +

′
jk(x

(cid:11)

′
j; x);

where x1; : : : ; xn are positive training data and x

A = f((cid:11)1; : : : ; (cid:11)n; (cid:11)

′
1; : : : ; (cid:11)

n′) j x1; : : : ; xn (cid:24) p(x j y = +1); x
′
′
1; : : : ; x

n′ (cid:24) p(x)g
′

′
1; : : : ; x

j=1
′
n′ are positive and negative test data. Let

6

0.20.30.40.50.60.70.80.90.95100.10.20.30.40.50.60.70.80.90.951EstimatedpriorbπEﬀectiveprioreπ π = 0.95π = 0.9π = 0.7π = 0.5be the set of all possible optimal solutions returned by the algorithm given some training data and
test data according to p(x j y = +1) and p(x). Then deﬁne the constants
∑
C(cid:11) = sup(cid:11)2A;x1;:::;xn(cid:24)p(xjy=+1);x
n′(cid:24)p(x)
′
′
1;:::;x

)1=2

(∑

∑

′
n
j=1 (cid:11)i(cid:11)

′
′
j) +
jk(xi; x

′
n
j;j′=1 (cid:11)

′
j(cid:11)

′
′
′
j′ )
j; x
j′k(x

;

n
i;i′=1 (cid:11)i(cid:11)i′k(xi; xi′ ) + 2

n
i=1

√

∑
F = ff : x 7! n∑

Ck = supx2Rd

k(x; x);
and deﬁne the function class

Let ℓ(cid:17)(z) be a surrogate loss for the zero-one loss

′∑
x1; : : : ; xn (cid:24) p(x j y = +1); x

(cid:11)ik(xi; x) +

i=1

n

(cid:11)

j; x) j (cid:11) 2 A;
′
′
jk(x
j=1
n′ (cid:24) p(x)g:
′
′
1; : : : ; x

8<:0

ℓ(cid:17)(z) =

1 (cid:0) z=(cid:17)
1

if z > (cid:17);
if 0 < z (cid:20) (cid:17);
if z (cid:20) 0:

(12)

For any (cid:17) > 0  ℓ(cid:17)(z) is lower bounded by ℓ0-1(z) and approaches ℓ0-1(z) as (cid:17) approaches zero.

Moreover  leteℓ(yf (x)) =

2

y + 3

ℓ0-1(yf (x))

Then we have the following theorems (proofs are provided in Appendix A). Our key idea is to
decompose the generalization error as

2

y + 3

ℓ(cid:17)(yf (x)):

and eℓ(cid:17)(yf (x)) =
[eℓ(f (x))
]
n∑
n′)g for evaluating the empirical error 1
′
eℓ(f (xi)) +

[eℓ(yf (x))
)√

+ Ep(x;y)

(

]

+

;

Ep(x;y)[ℓ0-1(yf (x))] = (cid:25)

(cid:3)

Ep(xjy=+1)

(cid:3)

:= p(y = 1) is the true class prior of the positive class.

where (cid:25)
Theorem 1. Fix f 2 F  then  for any 0 < (cid:14) < 1  with probability at least 1 (cid:0) (cid:14) over the repeated
′∑
sampling of fx1; : : : ; xng and f(x
Ep(x;y)[ℓ0-1(yf (x))] (cid:0) 1
n′

′
′
1); : : : ; (x
n′; y
(cid:3)
j)) (cid:20) (cid:25)
′

(cid:3)
p
(cid:25)
n
2

eℓ(y

′
jf (x

ln(2=(cid:14))

′
1; y

n

2

n

:

1p
n′

j=1

i=1

n

(cid:3)

(cid:25)

n

j=1

′
1; y

)

eℓ(cid:17)(y

j)) (cid:20) (cid:25)
′
′
jf (x

′
′
1); : : : ; (x
n′; y

(13)
Theorem 2. Fix (cid:17) > 0  then  for any 0 < (cid:14) < 1 with probability at least 1 (cid:0) (cid:14) over the repeated
n′)g for evaluating the empirical error  every
sampling of fx1; : : : ; xng and f(x
′
′∑
n∑
f 2 F satisﬁes
(

(
eℓ(cid:17)(f (xi)) +
(cid:3)p
)√

Ep(x;y)[ℓ0-1(yf (x))] (cid:0) 1
n′

1p
n′
2
p
p
n′). This order is
In both theorems  the generalization error bounds are of order O(1=
n + 1=
′ i.i.d. data from
optimal for PU classiﬁcation where we have n i.i.d. data from a distribution and n
′
p
another distribution. The error bounds for fully supervised classiﬁcation  by assuming these n + n
n + n′). However  this assumption is unreasonable
data are all i.i.d.  would be of order O(1=
′ samples.
p
p
for PU classiﬁcation  and we cannot train fully supervised classiﬁers using these n + n
n′) for PU classiﬁcation is no
p
p
Although the orders (and the losses) differ slightly  O(1=
n + 1=
′ are
n + n′) for fully supervised classiﬁcation (assuming n and n
2 times O(1=
worse than 2
equal). To the best of our knowledge  no previous work has provided such generalization error
bounds for PU classiﬁcation.

(cid:3)
p
(cid:25)
n
2

2p
n′

ln(2=(cid:14))

C(cid:11)Ck

i=1

+

+

+

n

(cid:17)

:

1The empirical error that we cannot evaluate in practice is in the left-hand side of (13)  and the empirical

error and conﬁdence terms that we can evaluate in practice are in the right-hand side of (13).

7

Table 1: Misclassiﬁcation rate (in percent) for PU classiﬁcation on the USPS dataset. The best  and
equivalent by 95% t-test  is indicated in bold.
(cid:25)

0.95

0.4

0.2

0.8

0.6

0.9

Ramp Hinge Ramp Hinge Ramp Hinge Ramp Hinge Ramp Hinge Ramp Hinge
3.36
4.94
5.15
4.94
3.49
4.94
1.68
4.94
5.21
4.94
11.47
4.94
1.89
4.94
3.98
4.94
1.22
4.94

4.00
14.60
16.51
3.03
19.78
19.83
2.49
11.34
2.24

4.78
8.67
8.08
4.00
11.16
19.59
4.61
7.00
3.86

5.18
8.79
8.52
3.99
12.04
22.94
3.70
6.85
3.56

4.85
6.96
4.72
2.05
7.22
19.87
2.55
4.81
1.60

4.40
6.20
5.52
2.83
7.42
11.61
3.55
5.09
2.76

4.16
5.90
4.06
2.00
6.16
15.13
2.31
3.74
1.61

5.48
7.22
5.02
2.21
7.46
22.58
2.64
4.75
1.73

1.71
2.80
2.12
1.42
3.21
5.29
1.39
2.11
1.13

0 vs 1
0 vs 2
0 vs 3
0 vs 4
0 vs 5
0 vs 6
0 vs 7
0 vs 8
0 vs 9

2.68
4.12
2.89
1.70
4.36
8.86
1.78
2.79
1.38

9.86
9.92
9.92
9.92
9.92
9.92
9.92
9.92
9.92

.

(a) Loss functions

(b) Class prior is (cid:25) = 0:2 (c) Class prior is (cid:25) = 0:6 (d) Class prior is (cid:25) = 0:9.

Figure 4: Examples of the classiﬁcation boundary for the “0” vs. “7” digits  obtained by PU learning.
The unlabeled dataset and the underlying (latent) class labels are given. Since discriminant function
for the hinge loss case is constant 1 when (cid:25) = 0:9  no decision boundary can be drawn and all
negative samples are misclassiﬁed.
6 Experiments
In this section  the experimentally compare the performance of the ramp loss and the hinge loss in PU
classiﬁcation (weighting was performed w.r.t. the true class prior and the ramp loss was optimized
with [12]). We used the USPS dataset  with the dimensionality reduced to 2 via principal component
analysis to enable illustration. 550 samples were used for the positive and mixture datasets. From the
results in Table 1  it is clear that the ramp loss gives a much higher classiﬁcation accuracy than the
hinge loss  especially for large class priors. This is due to the fact that the effect of the superﬂuous
penalty term in (10) becomes larger since it scales with (cid:25).
When the class prior is large  the classiﬁcation accuracy for the hinge loss is often close to 1 (cid:0) (cid:25).
This can be explained by (10): collecting the terms for the positive expectation  we get an effective
loss function for the positive samples (illustrated in Figure 4(a)). When (cid:25) is large enough  the
positive loss is minimized  giving a constant 1. The misclassiﬁcation rate becomes 1 (cid:0) (cid:25) since it is
a combination of the false negative rate and the false positive rate according to the class prior.
Examples of the discrimination boundary for digits “0” vs. “7” are given in Figure 4. When the
class-prior is low (Figure 4(b) and Figure 4(c)) the misclassiﬁcation rate of the hinge loss is slightly
higher. For large class-priors (Figure 4(d))  the hinge loss causes all samples to be classiﬁed as
positive (inspection showed that w = 0 and b = 1).
7 Conclusion
In this paper we discussed the problem of learning a classiﬁer from positive and unlabeled data.
We showed that PU learning can be solved using a cost-sensitive classiﬁer if the class prior of the
unlabeled dataset is known. We showed  however  that a non-convex loss must be used in order to
prevent a superﬂuous penalty term in the objective function.
In practice  the class prior is unknown and estimated from data. We showed that the excess risk is
actually controlled by an effective class prior which depends on both the estimated class prior and
the true class prior. Finally  generalization error bounds for the problem were provided.
Acknowledgments
MCdP is supported by the JST CREST program  GN was supported by the 973 Program No.
2014CB340505 and MS is supported by KAKENHI 23120004.

8

−2−10120123zLoss Positive lossNegative loss−6−4−20246−505x1x2 PositiveNegativeHingeRamp−6−4−20246−505x1x2−6−4−20246−505x1x2References
[1] C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. In Proceedings of the
14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD2008) 
pages 213–220  2008.

[2] W. Li  Q. Guo  and C. Elkan. A positive and unlabeled learning algorithm for one-class classiﬁcation of

remote-sensing data. IEEE Transactions on Geoscience and Remote Sensing  49(2):717–725  2011.

[3] S. Hido  Y. Tsuboi  H. Kashima  M. Sugiyama  and T. Kanamori. Inlier-based outlier detection via direct
density ratio estimation.
In F. Giannotti  D. Gunopulos  F. Turini  C. Zaniolo  N. Ramakrishnan  and
X. Wu  editors  Proceedings of IEEE International Conference on Data Mining (ICDM2008)  pages 223–
232  Pisa  Italy  Dec. 15–19 2008.

[4] C. Scott and G. Blanchard. Novelty detection: Unlabeled data deﬁnitely help.

In Proceedings of the
Twelfth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS2009)  pages 464–471 
Clearwater Beach  Florida USA  Apr. 16-18 2009.

[5] C. Elkan. The foundations of cost-sensitive learning. In Proceedings of the Seventeenth International

Joint Conference on Artiﬁcial Intelligence (IJCAI2001)  pages 973–978  2001.

[6] C.C. Chang and C.J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intel-

ligent Systems and Technology  2:27:1–27:27  2011.

[7] H.L. Van Trees. Detection  Estimation  and Modulation Theory  Part I. Detection  Estimation  and

Modulation Theory. John Wiley and Sons  New York  NY  USA  1968.

[8] R. Duda  P. Hart  and D. Stork. Pattern Classiﬁcation. John Wiley & Sons  2nd edition  2001.
[9] V. Vapnik. The Nature of Statistical Learning Theory. Springer  2000.
[10] G. Blanchard  G. Lee  and C. Scott. Semi-supervised novelty detection. The Journal of Machine Learning

Research  11:2973–3009  2010.

[11] M. C. du Plessis and M. Sugiyama. Class prior estimation from positive and unlabeled data.

Transactions on Information and Systems  E97-D:1358–1362  2014.

IEICE

[12] R. Collobert  F.H. Sinz  J. Weston  and L. Bottou. Trading convexity for scalability. In Proceedings of the

23rd International Conference on Machine learning (ICML2006)  pages 201–208  2006.

[13] S. Suzumura  K. Ogawa  M. Sugiyama  and I. Takeuchi. Outlier path: A homotopy algorithm for robust
SVM. In Proceedings of 31st International Conference on Machine Learning (ICML2014)  pages 1098–
1106  Beijing  China  Jun. 21–26 2014.

[14] A. Ghosh  N. Manwani  and P. S. Sastry. Making risk minimization tolerant to label noise. CoRR 

abs/1403.3610  2014.

[15] M. Mohri  A. Rostamizadeh  and A. Talwalkar. Foundations of Machine Learning. MIT Press  2012.

9

,Marthinus du Plessis
Gang Niu
Masashi Sugiyama
Kush Bhatia
Himanshu Jain
Purushottam Kar
Manik Varma
Prateek Jain