2016,Testing for Differences in Gaussian Graphical Models:  Applications to Brain Connectivity,Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs)  e.g.\ using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two populations calls for comparing these estimated GGMs. Our goal is to identify differences in GGMs known to have similar structure. We characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings. Characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator. Recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso.  These distributions can be used to give confidence intervals on edges in GGMs  and by extension their differences. However  in the case of comparing GGMs  these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso  whose distribution can be characterized in an efficient manner. We then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in GGMs. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.,Testing for Differences in Gaussian Graphical Models:

Applications to Brain Connectivity

Eugene Belilovsky 1 2 3  Gael Varoquaux2  Matthew Blaschko3

matthew.blaschko@esat.kuleuven.be

1University of Paris-Saclay  2INRIA  3KU Leuven

{eugene.belilovsky  gael.varoquaux } @inria.fr

Abstract

Functional brain networks are well described and estimated from data with Gaus-
sian Graphical Models (GGMs)  e.g. using sparse inverse covariance estimators.
Comparing functional connectivity of subjects in two populations calls for compar-
ing these estimated GGMs. Our goal is to identify differences in GGMs known
to have similar structure. We characterize the uncertainty of differences with
conﬁdence intervals obtained using a parametric distribution on parameters of a
sparse estimator. Sparse penalties enable statistical guarantees and interpretable
models even in high-dimensional and low-sample settings. Characterizing the
distributions of sparse models is inherently challenging as the penalties produce
a biased estimator. Recent work invokes the sparsity assumptions to effectively
remove the bias from a sparse estimator such as the lasso. These distributions can
be used to give conﬁdence intervals on edges in GGMs  and by extension their
differences. However  in the case of comparing GGMs  these estimators do not
make use of any assumed joint structure among the GGMs. Inspired by priors from
brain functional connectivity we derive the distribution of parameter differences
under a joint penalty when parameters are known to be sparse in the difference.
This leads us to introduce the debiased multi-task fused lasso  whose distribution
can be characterized in an efﬁcient manner. We then show how the debiased lasso
and multi-task fused lasso can be used to obtain conﬁdence intervals on edge
differences in GGMs. We validate the techniques proposed on a set of synthetic
examples as well as neuro-imaging dataset created for the study of autism.

Introduction

1
Gaussian graphical models describe well interactions in many real-world systems. For instance 
correlations in brain activity reveal brain interactions between distant regions  a process know as
functional connectivity. Functional connectivity is an interesting probe on brain mechanisms as
it persists in the absence of tasks (the so-called “resting-state”) and is thus applicable to study
populations of impaired subjects  as in neurologic or psychiatric diseases [3]. From a formal
standpoint  Gaussian graphical models are well suited to estimate brain connections from functional
Magnetic Resonance Imaging (fMRI) signals [28  33]. A set of brain regions and related functional
connections is then called a functional connectome [31  3]. Its variation across subjects can capture
cognition [26  27] or pathology [17  3]. However  the effects of pathologies are often very small  as
resting-state fMRI is a weakly-constrained and noisy imaging modality  and the number of subjects
in a study is often small given the cost of imaging. Statistical power is then a major concern [2]. The
statistical challenge is to increase the power to detect differences between Gaussian graphical models
in the small-sample regime.
In these settings  estimation and comparison of Gaussian graphical models fall in the range of
high-dimensional statistics: the number of degrees of freedom in the data is small compared to

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

the dimensionality of the model. In this regime  sparsity-promoting (cid:96)1-based penalties can make
estimation well-posed and recover good estimation performance despite the scarcity of data [29  10 
22  6  1]. These encompass sparse regression methods such as the lasso or recovery methods such as
basis pursuit  and can be applied to estimation of Gaussian graphical models with approaches such as
the graphical lasso[10]. There is now a wide body of literature which demonstrates the statistical
properties of these methods [1]. Crucial to applications in medicine or neuroscience  recent work
characterizes the uncertainty  with conﬁdence intervals and p-values  of the parameters selected by
these methods [15  16  19  12]. These works focus primarily on the lasso and graphical lasso.
Approaches to estimate statistical signiﬁcance on sparse models fall into several general categories:
(a) non-parameteric sampling based methods which are inherently expensive and have difﬁcult
limiting distributions [1  24  5]  (b) characterizations of the distribution of new parameters that enter a
model along a regularization path [19  12]  or (c) for a particular regularization parameter  debiasing
the solution to obtain a new consistent estimator with known distribution [16  15  30]. While some of
the latter work has been used to characterize conﬁdence intervals on network edge selection  there is
no result  to our knowledge  on the important problem of identifying differences in networks. Here
the conﬁdence on the result is even more critical  as the differences are the direct outcome used for
neuroscience research or medical practice  and it is important to provide the practitioner a measure of
the uncertainty.
Here  we consider the setting of two datasets known to have very similar underlying signals  but
which individually may not be very sparse. A motivating example is determining the difference
in brain networks of subjects from different groups: population analysis of connectomes [31  17].
Recent literature in neuroscience [20] has suggested functional networks are not sparse. On the
other hand  differences in connections across subjects should be sparse. Indeed the link between
functional and anatomical brain networks [13] suggests they should not differ drastically from one
subject to another. From a neuroscientiﬁc standpoint we are interested in determining which edges
between two populations (e.g. autistic and non-autistic) are different. Furthermore we want to provide
conﬁdence-intervals on our results. We particularly focus on the setting where one dataset is larger
than the other. In many applications it is more difﬁcult to collect one group (e.g. individuals with
speciﬁc pathologies) than another.
We introduce an estimator tailored to this goal: the debiased multi-task fused lasso. We show that 
when the underlying parameter differences are indeed sparse  we can obtain a tractable Gaussian
distribution for the parameter difference. This closed-form distribution underpins accurate hypothesis
testing and conﬁdence intervals. We then use the relationship between nodewise regression and the
inverse covariance matrix to apply our estimator to learning differences of Gaussian graphical models.
The paper is organized as follows. In Section 2 we review previous work on learning of GGMs and
the debiased lasso. Section 3 discusses a joint debiasing procedure that speciﬁcally debiases the
difference estimator. In Section 3.1 we introduce the debiased multi-task fused lasso and show how
it can be used to learn parameter differences in linear models. In Section 3.2  we show how these
results can be used for GGMs. In Section 4 we validate our approach on synthetic and fMRI data.
2 Background and Related Work
Debiased Lasso A central starting point for our work is the debiased lasso [30  16]. Here one
considers the linear regression model  Y = Xβ +   with data matrix X and output Y   corrupted by
 ∼ N (0  σ2

 I) noise. The lasso estimator is formulated as follows:
(cid:107)Y − Xβ(cid:107)2 + λ(cid:107)β(cid:107)1

ˆβλ = arg min
β

1
n

(1)

n X T (Y − Xβ)  where ˆk is the subgradient of λ(cid:107)β(cid:107)1. The debiased
The KKT conditions give ˆkλ = 1
u = ˆβλ +M ˆkλ for some M that is constructed to give
lasso estimator [30  16] is then formulated as ˆβλ
guarantees on the asymptotic distribution of ˆβλ
u. Note that this estimator is not strictly unbiased in the
ﬁnite sample case  but has a bias that rapidly approaches zero (w.r.t. n) if M is chosen appropriately 
the true regressor β is indeed sparse  and the design matrix satistifes a certain restricted eigenvalue
property [30  16]. We decompose the difference of this debiased estimator and the truth as follows:

u − β =
ˆβλ

1
n

M X T  − (M ˆΣ − I)( ˆβ − β)

(2)

2

The ﬁrst term is Gaussian and the second term is responsible for the bias. Using Holder’s inequality
the second term can be bounded by (cid:107)M ˆΣ−I(cid:107)∞(cid:107) ˆβ−β(cid:107)1. The ﬁrst part of which we can bound using
an appropriate selection of M while the second part is bounded by our implicit sparsity assumptions
coming from lasso theory [1]. Two approaches from the recent literature discuss how one can select
M to appropriately debias this estimate. In [30] it sufﬁces to use nodewise regression to learn an
inverse covariance matrix which guarantees constraints on (cid:107)M ˆΣ − I(cid:107)∞. A second approach by [16]
proposes to solve a quadratic program to directly minimize the variance of the debiased estimator
while constraining (cid:107)M ˆΣ − I(cid:107)∞ to induce sufﬁciently small bias.
Intuitively the construction of ˆβλ
u allows us to trade variance and bias via the M matrix. This allows
us to overcome a naive bias-variance tradeoff by leveraging the sparsity assumptions that bound
(cid:107) ˆβ − β(cid:107)1. In the sequel we expand this idea to the case of debiased parameter difference estimates
and sparsity assumptions on the parameter differences.
In the context of GGMs  the debiased lasso can gives us an estimator that asymptotically converges to
the partial correlations. As highlighted by [34] we can thus use the debiased lasso to obtain difference
estimators with known distributions. This allows us to obtain conﬁdence intervals on edge differences
between Gaussian graphical models. We discuss this further in the sequel.

Gaussian Graphical Model Structure Learning A standard approach to estimating Gaussian
graphical models in high dimensions is to assume sparsity of the precision matrix and have a
constraint which limits the number of non-zero entries of the precision matrix. This constraint can
be achieved with a (cid:96)1-norm regularizer as in the popular graphical lasso [10]. Many variants of this
approach that incorporate further structural assumptions have been proposed [14  6  23].
An alternative solution to inducing sparsity on the precision matrix indirectly is neighborhood (cid:96)1
regression from [22]. Here the authors make use of a long known property that connects the entries
of the precision matrix to the problem of regression of one variable on all the others [21]. This
property is critical to our proposed estimation as it allows relating regression models to ﬁnding edges
connected to speciﬁc nodes in the GGM.
GGMs have been found to be good at recovering the main brain networks from fMRI data [28  33].
Yet  recent work in neuroscience has showed that the structural wiring of the brain did not correspond
to a very sparse network [20]  thus questioning the underlying assumption of sparsity often used
to estimate brain network connectivity. On the other hand  for the problem of ﬁnding differences
between networks in two populations  sparsity may be a valid assumption. It is well known that
anatomical brain connections tend to closely follow functional ones [13]. Since anatomical networks
do not differ drastically we can surmise that two brain networks should not differ much even in the
presence of pathologies. The statistical method we present here leverages sparsity in the difference of
two networks  to yield well-behaved estimation and hypothesis testing in the low-sample regime. Most
closely related to our work  [35  9] recently consider a different approach to estimating difference
networks  but does not consider assigning signiﬁcance to the detection of edges.
3 Debiased Difference Estimation
In many applications one may be interested in learning multiple linear models from data that share
many parameters. Situations such as this arise often in neuroimaging and bioinformatics applications.
We can often improve the learning procedure of such models by incorporating fused penalties that
penalize the (cid:107)·(cid:107)1 norm of the parameter differences or (cid:107)·(cid:107)1 2 which encourages groups of parameters
to shrink together. These methods have been shown to substantially improve the learning of the
joint models. However  the differences between model parameters  which can have a high sample
complexity when there are few of them  are often pointed out only in passing [4  6  14]. On the
other hand  in many situations we might be interested in actually understanding and identifying
the differences between elements of the support. For example when considering brain networks of
patients suffering from a pathology and healthy control subjects  the difference in brain connectivity
may be of great interest. Here we focus speciﬁcally on accurately identifying differences with
signiﬁcance.
We consider the case of two tasks (e.g. two groups of subjects)  but the analysis can be easily extended
to general multi-task settings. Consider the problem setting of data matrices X1 and X2  which
are n1 × p and n2 × p  respectively. We model them as producing outputs Y1 and Y2  corrupted by

3

diagonal gaussian noise 1 and 2 as follows

Y1 = X1β1 + 1  Y2 = X2β2 + 2

(3)
Let S1 and S2 index the elements of the support of β1 and β2  respectively. Furthermore the support
of β1 − β2 is indexed by Sd and ﬁnally the union of S1 and S2 is denoted Sa. Using a squared loss
estimator producing independent estimates ˆβ1  ˆβ2 we can obtain a difference estimate ˆβd = ˆβ1 − ˆβ2.
In general if Sd is very small relative to Sa then we will have a difﬁcult time to identify the support
Sd. This can be seen if we consider each of the individual components of the prediction errors. The
larger the true support Sa the more it will drown out the subset which corresponds to the difference
support. This can be true even if one uses (cid:96)1 regularizers over the parameter vectors. Consequently 
one cannot rely on the straightforward strategy of learning two independent estimates and taking their
difference. The problem is particularly pronounced in the common setting where one group has fewer
samples than the other. Thus here we consider the setting where n1 > n2 and possibly n1 (cid:29) n2.
Let ˆβ1 and ˆβ2 be regularized least squares estimates. In our problem setting we wish to obtain
conﬁdence intervals on debiased versions of the difference ˆβd = ˆβ1 − ˆβ2 in a high-dimensional
setting (in the sense that n2 < p)  we aim to leverage assumptions about the form of the true βd 
primarily that it is sparse  while the independent ˆβ1 and ˆβ2 are weakly sparse or not sparse. We
consider a general case of a joint regularized least squares estimation of ˆβ1 and ˆβ2

min
β1 β2

1
n1

(cid:107)Y1 − X1β1(cid:107)2 +

(cid:107)Y2 − X2β2(cid:107)2 + R(β1  β2)

1
n2

(cid:21)
We note that the differentiating and using the KKT conditions gives
1 (Y − X1β1)
X T
2 (Y − X2β2)
X T

(cid:20)ˆk1

(cid:20) 1

ˆkλ =

(cid:21)

ˆk2

=

n1
1
n2

(4)

(5)

where ˆkλ is the (sub)gradient of R(β1  β2). Substituting Equation (3) we can now write

1
n1

ˆΣ1( ˆβ1 − β1) + ˆk1 =

1 1 and ˆΣ2( ˆβ2 − β2) + ˆk2 =
X T

(6)
We would like to solve for the difference ˆβ1 − ˆβ2 but the covariance matrices may not be invertible.
We introduce matrices M1 and M2  which will allow us to isolate the relevant term. We will see that
in addition these matrices will allow us to decouple the bias and variance of the estimators.
M1 ˆΣ1( ˆβ1 − β1) + M1
subtracting these and rearranging we can now isolate the difference estimator plus a term we add
back controlled by M1 and M2

1 1 and M2 ˆΣ2( ˆβ2 − β2) + M2

2 2 (7)

M2X T

M1X T

ˆk1 =

ˆk2 =

X T

2 2

1
n2

1
n1

1
n2

( ˆβ1 − ˆβ2) − (β1 − β2) + M1

ˆk1 − M2

ˆk2 =

1
n1

M1X T

1 1 − 1
n2

M2X T

2 2 − ∆

∆ = (M1 ˆΣ1 − I)( ˆβ1 − β1) − (M2 ˆΣ2 − I)( ˆβ2 − β2)

Denoting βd := β1 − β2 and βa := β1 + β2  we can reformulate ∆:

(M1 ˆΣ1 − I + M2 ˆΣ2 − I)

∆ =

2

( ˆβd − βd) +

(M1 ˆΣ1 − M2 ˆΣ2)

( ˆβa − βa)

2

Here  ∆ will control the bias of our estimator. Additionally  we want to minimize its variance 

1
n1

M1 ˆΣ1M1 ˆσ2

1 +

1
n2

M2 ˆΣ2M2 ˆσ2
2.

We can now overcome the limitations of simple bias variance trade-off by using an appropriate
regularizer coupled with an assumption on the underlying signal β1 and β2. This will in turn make ∆
asymptotically vanish while maximizing the variance.
Since we are interested in pointwise estimates  we can focus on bounding the inﬁnity norm of ∆.

(8)

(9)

(10)

(11)

(cid:107)∆(cid:107)∞ ≤ 1
2

(cid:124)

(cid:123)(cid:122)

µ1

(cid:107)M1 ˆΣ1 + M2 ˆΣ2 − 2I(cid:107)∞

(cid:107) ˆβd − βd(cid:107)1

(cid:107)M1 ˆΣ1 − M2 ˆΣ2(cid:107)∞

(cid:107) ˆβa − βa(cid:107)1

(12)

(cid:125)

(cid:124)

(cid:125)

+

1
2

(cid:124)

(cid:123)(cid:122)

µ2

(cid:125)

(cid:124)

(cid:123)(cid:122)

la

(cid:125)

(cid:123)(cid:122)

ld

4

We can control the maximum bias by selecting M1 and M2 appropriately. If we use an appropriate
regularizer coupled with sparsity assumptions we can bound the terms la and ld and use this knowledge
to appropriately select M1 and M2 such that the bias becomes neglibile.
If we had only the
independent parameter sparsity assumption we can apply the results of the debiased lasso and
estimate M1 and M2 independently as in [16]. In the case of interest where β1 and β2 share many
weights we can do better by taking this as an assumption and applying a sparsity regularization on the
difference by adding the term λ2(cid:107)β1 − β2(cid:107)1. Comparing the decoupled penalty to the fused penalty
proposed we see that ld would decrease at a given sample size. We now show how to jointly estimate
M1 and M2 so that (cid:107)∆(cid:107)∞ becomes negligible for a given n  p and sparsity assumption.
3.1 Debiasing the Multi-Task Fused Lasso
Motivated by the inductive hypothesis from neuroscience described above we introduce a consistent
low-variance estimator  the debiased multi-task fused lasso. We propose to use the following
regularizer R(β1  β2) = λ1(cid:107)β1(cid:107)1 + λ1(cid:107)β2(cid:107)1 + λ2(cid:107)β1 − β2(cid:107)1. This penalty has been referred to in
some literature as the multi-task fused lasso [4]. We propose to then debias this estimate as shown in
(8). We estimate the M1 and M2 matrices by solving the following QP for each row m1 and m2 of
the matrices M1 and M2.

min
m1 m2

1
n1

mT
1

ˆΣ1m1 +

mT
2

ˆΣ2m2

(13)

1
n2

s.t. (cid:107)M1 ˆΣ1 + M2 ˆΣ2 − 2I(cid:107)∞ ≤ µ1  (cid:107)M1 ˆΣ1 − M2 ˆΣ2(cid:107)∞ ≤ µ2

(cid:113) log p

This directly minimizes the variance  while bounding the bias in the constraint. We now show how to
set the bounds:
Proposition 1. Take λ1 > 2
and λ2 = O(λ1). Denote sd the difference sparsity  s1 2 the
parameter sparsity |S1| + |S2|  c > 1 a > 1  and 0 < m (cid:28) 1. When the compatibility condition
[1  11] holds the following bounds gives lau2 = o(1) and ldu1 = o(1) and thus (cid:107)∆(cid:107)∞ = o(1) with
high probability.

n2

µ1 ≤

1

cλ2sdnm
2

and µ2 ≤

1

a(λ1s1 2 + λ2sd)nm
2

(14)

The proof is given in the supplementary material. Using the prescribed Ms obtained with (13) and
14 we obtain an unbiased estimator given by (8) with variance (11)
3.2 GGM Difference Structure Discovery with Signiﬁcance
The debiased lasso and the debiased multi-task fused lasso  proposed in the previous section  can be
used to learn the structure of a difference of Gaussian graphical models and to provide signiﬁcance
results on the presence of edges within the difference graph. We refer to these two procedures as
Difference of Neighborhoods Debiased Lasso Selection and Difference of Neighborhoods Debiased
Fused Lasso Selection.
We recall that the conditional independence properties of a GGM are given by the zeros of the
precision matrix and these zeros correspond to the zeros of regression parameters when regressing
one variable on all the other. By obtaining a debiased lasso estimate for each node in the graph [34]
notes this leads to a sparse unbiased precision matrix estimate with a known asymptotic distribution.
Subtracting these estimates for two different datasets gives us a difference estimate whose zeros
correspond to no difference of graph edges in two GGMs. We can similarly use the debiased multi-
task fused lasso described above and the joint debiasing procedure to obtain a test statistic for the
difference of networks. We now formalize this procedure.

Notation Given GGMs j = 1  2. Let Xj denote the random variable in Rp associated with GGM
j. We denote Xj v the random variable associated with a node  v of the GGM and Xj vc all other
nodes in the graph. We denote ˆβj v the lasso or multi-task fused lasso estimate of Xj vc onto
Xj v  then ˆβj dL v is the debiased version of ˆβj v. Finally let βj v denote the unknown regression 
Xj v = Xj vcβj v + j where j ∼ N(0  σjI). Deﬁne βi
2 dL v the test statistic
associated with the edge v  i in the difference of GGMs j = 1  2.

1 dL v − ˆβi

D v = ˆβi

5

Algorithm 1 Difference Network Selection with
Neighborhood Debiased Lasso

Algorithm 2 Difference Network Selection with
Neighborhood Debiased Fused Lasso

V = {1  ...  P}
NxP Data Matrices  X1 and X2
Px(P-1) Output Matrix B of test statistics
for v ∈ V do

V = {1  ...  P}
NxP Data Matrices  X1 and X2
Px(P-1) Output Matrix B of test statistics
for v ∈ V do

Estimate unbiased ˆσ1  ˆσ2 from X1 v  X2 v
for j ∈ {1  2} do

βj ← SolveLasso(Xj vc   Xj v)
Mj ← M Estimator(Xj vc )
βj U ← βj+MjX T
end for
d ← diag( ˆσ2
σ2
for j ∈ vc do
Bv j = (β1 U j − β2 U j)/

j vc (Xj v − Xj vc βj)
ˆΣ1M1 + ˆσ2
2
n2

(cid:113)

M T
2

M T
1

1
n1

σ2

d j

ˆΣ2M2)

Estimate unbiased ˆσ1  ˆσ2 from X1 v  X2 v
β1 β2 ← F usedLasso(X1 vc   X1 v  X2 vc   X2 v)
M1  M2 ← M Estimator(X1 vc   X2 vc )
for j ∈ {1  2} do
j vc (Xj v − Xj vc βj)
ˆΣ1M1 + ˆσ2
2
n2

βj U ← βj+MjX T
end for
d ← diag( ˆσ2
σ2
for j ∈ vc do
Bv j = (β1 U j − β2 U j)/

ˆΣ2M2)

(cid:113)

M T
2

M T
1

1
n1

σ2

d j

end for

end for

end for

end for

Proposition 2. Given the ˆβi
D v  M1 and M2 computed as in [16] for the debiased lasso or as
in Section 3.1 for the debiased multi-task fused lasso. When the respective assumptions of these
estimators are satisﬁed the following holds w.h.p.

D v − βi
ˆβi

D v = W + o(1) where W ∼ N(0  [σ2

1M1 ˆΣ1M T

1 + σ2

2M2 ˆΣ2M T

2 ]i i)

(15)

j dL v for the debiased

This follows directly from the asymptotic consistency of each individual ˆβi
lasso and multi-task fused lasso.
We can now deﬁne the the null hypothesis of interest as H0 : Θ1 (i j) = Θ2 (i j). Obtaining a test
statistic for each element βi
D v allows us to perform hypothesis testing on individual edges  all the
edges  or groups of edges (controlling for the FWER). We summarize the Neighbourhood Debiased
Lasso Selection process in Algorithm 1 and the Neighbourhood Debiased Multi-Task Fused Lasso
Selection in Algorithm 2 which can be used to obtain a matrix of all the relevant test statistics.
4 Experiments
4.1 Simulations
We generate synthetic data based on two Gaussian graphical models with 75 vertices. Each of the
individual graphs have a sparsity of 19% and their difference sparsity is 3%. We construct the
models by taking two identical precision matrices and randomly removing some edges from both.
We generate synthetic data using both precision matrices. We use n1 = 800 samples for the ﬁrst
dataset and vary the second dataset n2 = 20  30  ...150.
We perform a regression using the debiased lasso and the debiased multi-task fused lasso on each
node of the graphs. As an extra baseline we consider the projected ridge method from the R package
fold cross validation k = {0.1  ..100} and M as prescribed in [16] which we obtain by solving a
quadratic program. ˆσ is an unbiased estimator of the noise variance. For the debiased lasso we let
both λ1 = k1 ˆσ2
from the same range as k. M1 and M2 are obtained as in Equation (13) with the bounds (14) being
set with c = a = 2  sd = 2  s1 2 = 15  m = 0.01  and the cross validated λ1 and λ2. In both
debiased lasso and fused multi-task lasso cases we utilize the Mosek QP solver package to obtain M.
For the projected ridge method we use the hdi package to obtain two estimates of β1 and β2 along
with their upper bounded biases which are then used to obtain p-values for the difference.
We report the false positive rate  the power  the coverage and interval length as per [30] for the
difference of graphs. In these experiments we aggregate statistics to demonstrate power of the test
statistic  as such we consider each edge as a separate test and do not perform corrections. Table
1 gives the numerical results for n2 = 60: the power and coverage is substantially better for the
debiased fused multi-task lasso  while at the same time the conﬁdence interval smaller.

“hdi” [7]. We use the debiased lasso of [16] where we set λ = kˆσ(cid:112)log p/n. We select c by 3-
(cid:112)log p/n2  and select based on 3-fold cross-validation

(cid:112)log p/n2 and λ2 = k2 ˆσ2

6

Method
Deb. Lasso
Deb. Fused Lasso
Ridge Projection

FP
3.7%
0.0%
0.0%

TP(Power) Cov S Cov Sc
d
92%

len S
96.2%
2.199
100% 98.6% 2.191
100%
100% 5.544

80.6%
93.3%
18.6%

len Sc
d
2.195
2.041
5.544

Table 1: Comparison of Debiased Lasso  Debiased
Fused Lasso  and Projected Ridge Regression for edge
selection in difference of GGM. The signiﬁcance level
is 5%  n1 = 800 and n2 = 60. All methods have
false positive below the signiﬁcance level and the de-
biased fused lasso dominates in terms of power. The
coverage of the difference support and non-difference
support is also best for the debiased fused lasso  which
simultaneously has smaller conﬁdence intervals on av-
erage.

Figure 1: Power of the test for different number of
samples in the second simulation  with n1 = 800. The
debiased fused lasso has highest statistical power.
Figure 1 shows the power of the test for different values of n2. The fusedlasso outperforms the other
methods substantially. Projected ridge regression is particularly weak  in this scenario  as it uses a
worst case p-value obtained using an estimate of an upper bound on the bias [7].
4.2 Autism Dataset
Correlations in brain activity measured via fMRI reveal functional interactions between remote brain
regions [18]. In population analysis  they are used to measure how connectivity varies between
different groups. Such analysis of brain function is particularly important in psychiatric diseases 
that have no known anatomical support: the brain functions in a pathological aspect  but nothing
abnormal is clearly visible in the brain tissues. Autism spectrum disorder is a typical example of such
ill-understood psychiatric disease. Resting-state fMRI is accumulated in an effort to shed light on
this diseases mechanisms: comparing the connectivity of autism patients versus control subjects. The
ABIDE (Autism Brain Imaging Data Exchange) dataset [8] gathers rest-fMRI from 1 112 subjects
across  with 539 individuals suffering from autism spectrum disorder and 573 typical controls. We
use the preprocessed and curated data1.
In a connectome analysis [31  26]  each subject is described by a GGM measuring functional
connectivity between a set of regions. We build a connectome from brain regions of interest based on
a multi-subject atlas2 of 39 functional regions derived from resting-state fMRI [32] (see. Fig. 4).
We are interested in determining edge differences between the autism group and the control group.
We use this data to show how our parametric hypothesis test can be used to determine differences in
brain networks. Since no ground truth exists for this problem  we use permutation testing to evaluate
the statistical procedures [25  5]. Here we permute the two conditions (e.g. autism and control group)
to compute a p-value and compare it to our test statistics. This provides us with a ﬁnite sample strict
control on the error rate: a non-parametric validation of our parametric test.
For our experiments we take 2000 randomly chosen volumes from the control group subjects and
100 volumes from the autism group subjects. We perform permutation testing using the de-biased
lasso  de-biased multi-task fused lasso  and projected ridge regression. Parameters for the de-biased
fused lasso are chosen as in the previous section. For the de-biased lasso we use the exact settings for
λ and constraints on M provided in the experimental section of [16]. Projected ridge regression is
evaluated as in the previous section.
Figure 2 shows a comparison of three parametric approaches versus their analogue obtained with
a permutation test. The chart plots the permutation p-values of each entry in the 38 × 39 B matrix
against the expected parametric p-value. For all the methods the points are above the line indicating
the tests are not breaching the expected false positive rates. However the de-biased lasso and ridge
projecting are very conservative and lead to few detections. The de-biased multi-task fused lasso
yields far more detections on the same dataset  within the expected false positive rate or near it.
We now analyse the reproducibility of the results by repeatedly sampling 100 subsets of the data (with
the same proportions n1 = 2000 and n2 = 100)  obtaining the matrix of test statistics  selecting edges
that fall below the 5% signiﬁcance level. Figure 3 shows how often edges are selected multiple times
across subsamples. We report results with a threshold on uncorrected p-values as the lasso procedure

1http://preprocessed-connectomes-project.github.io/abide/
2https://team.inria.fr/parietal/research/spatial_patterns/

spatial-patterns-in-resting-state/

7

30405060708090100110120130140150n20.00.20.40.60.81.0PowerPowerridgelassofusedlassoFigure 2: Permutation testing comparing debiased fused lasso  debiased lasso  and projected ridge regression
on the ABIDE dataset. The chart plots the permutation p-values of each method on each possible edge against
the expected parametric p-value. The debiased lasso and ridge projection are very conservative and lead to few
detections. The fused lasso yields far more detections on the same dataset  almost all within the expected false
positive rate.

Figure 4: Outlines of the regions of the MSDL atlas.

Figure 3: Reproducibility of results from sub-sampling
using uncorrected error rate. The fused lasso is much
more likely to detect edges and produce stable results.
Using corrected p-values no detections are made by
lasso (ﬁgure in supplementary material).

Figure 5: Connectome of repeatedly picked up edges
in 100 trials. We only show edges selected more than
once. Darker red indicates more frequent selection.

selects no edges with multiple comparison correction (supplementary materials give FDR-corrected
results for the de-biased fused multi-task lasso selection). Figure 5 shows a connectome of the edges
frequently selected by the de-biased fused multi-task lasso (with FDR correction).

5 Conclusions

We have shown how to characterize the distribution of differences of sparse estimators and how to use
this distribution for conﬁdence intervals and p-values on GGM network differences. For this purpose 
we have introduced the de-biased multi-task fused lasso. We have demonstrated on synthetic and
real data that this approach can provide accurate p-values and a sizable increase of statistical power
compared to standard procedures. The settings match those of population analysis for functional
brain connectivity  and the gain in statistical power is direly needed to tackle the low sample sizes [2].
Future work calls for expanding the analysis to cases with more than two groups as well as considering
a (cid:96)1 2 penalty sometimes used at the group level [33]. Additionally the squared loss objective
optimizes excessively the prediction and could be modiﬁed to lower further the sample complexity in
terms of parameter estimation.

8

0.00.20.40.60.81.0permutationp-values0.00.20.40.60.81.0parametricp-valuesfusedlasso0.000.050.100.150.20permutationp-values0.000.020.040.060.080.10parametricp-valuesfusedlassopvaluesattail0.00.20.40.60.81.0permutationp-values0.00.20.40.60.81.0parametricp-valueslasso0.000.050.100.150.20permutationp-values0.000.020.040.060.080.10parametricp-valueslassopvaluesattail0.00.20.40.60.81.0permutationp-values0.00.20.40.60.81.0parametricp-valuesridge0.000.050.100.150.20permutationp-values0.000.020.040.060.080.10parametricp-valuesridgepvaluesattail123456789Numberofoccurences(t)0.00.20.40.60.81.0FractionofconnectionsoccuringatleastttimesReproducibilityacrosssubsampleslassofusedlassox=2LRz=20Acknowledgements
This work is partially funded by Internal Funds KU Leuven  ERC Grant 259112  FP7-MC-CIG
334380  and DIGITEO 2013-0788D - SOPRANO  and ANR-11-BINF-0004 NiConnect.
References
[1] P. Bühlmann and S. van de Geer. Statistics for High-Dimensional Data. Springer  2011.
[2] K. Button et al. Power failure: Why small sample size undermines the reliability of neuroscience. Nature

Reviews Neuroscience  14:365  2013.

[3] F. X. Castellanos et al. Clinical applications of the functional connectome. Neuroimage  80:527  2013.
[4] X. Chen et al. Smoothing proximal gradient method for general structured sparse learning. In UAI  2011.
[5] B. Da Mota et al. Randomized parcellation based inference. NeuroImage  89:203–215  2014.
[6] P. Danaher  P. Wang  and D. Witten. The joint graphical lasso for inverse covariance estimation across

multiple classes. Journal of the Royal Statistical Society (B)  76(2):373–397  2014.

[7] R. Dezeure  P. Bühlmann  L. Meier  and N. Meinshausen. High-dimensional inference: Conﬁdence

intervals  p-values and R-software hdi. Statist. Sci.  30(4):533–558  11 2015.

[8] A. Di Martino et al. The autism brain imaging data exchange: Towards a large-scale evaluation of the

intrinsic brain architecture in autism. Mol. Psychiatry  19:659  2014.

[9] F. Fazayeli and A. Banerjee. Generalized direct change estimation in ising model structure. In ICML  2016.
[10] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.

[11] A. Ganguly and W. Polonik. Local neighborhood fusion in locally constant Gaussian graphical models.

Biostatistics  9(3):432–441  2008.

arXiv:1410.8766  2014.

[12] M. G. G’Sell  J. Taylor  and R. Tibshirani. Adaptive testing for the graphical lasso. arXiv:1307.4765  2013.
[13] C. Honey  O. Sporns  L. Cammoun  X. Gigandet  et al. Predicting human resting-state functional connec-

tivity from structural connectivity. Proc. Nat. Acad. Sciences  106:2035  2009.

[14] J. Honorio and D. Samaras. Multi-task learning of Gaussian graphical models. In ICML  2010.
[15] J. Janková and S. van de Geer. Conﬁdence intervals for high-dimensional inverse covariance estimation.

Electron. J. Statist.  9(1):1205–1229  2015.

[16] A. Javanmard and A. Montanari. Conﬁdence intervals and hypothesis testing for high-dimensional

regression. The Journal of Machine Learning Research  15(1):2869–2909  2014.

[17] C. Kelly  B. B. Biswal  R. C. Craddock  F. X. Castellanos  and M. P. Milham. Characterizing variation in

the functional connectome: Promise and pitfalls. Trends in Cog. Sci.  16:181  2012.

[18] M. A. Lindquist et al. The statistical analysis of fMRI data. Stat. Sci.  23(4):439–464  2008.
[19] R. Lockhart et al. A signiﬁcance test for the lasso. Ann. Stat.  42:413  2014.
[20] N. T. Markov  M. Ercsey-Ravasz  D. C. Van Essen  K. Knoblauch  Z. Toroczkai  and H. Kennedy. Cortical

high-density counterstream architectures. Science  342(6158):1238406  2013.

[21] G. Marsaglia. Conditional means and covariances of normal variables with singular covariance matrix.

Journal of the American Statistical Association  59(308):1203–1204  1964.

[22] N. Meinshausen and P. Bühlmann. High-dimensional graphs and variable selection with the lasso. Ann.

[23] K. Mohan et al. Structured learning of Gaussian graphical models. In NIPS  pages 620–628  2012.
[24] M. Narayan and G. I. Allen. Mixed effects models to ﬁnd differences in multi-subject functional connectiv-

Stat.  pages 1436–1462  2006.

ity. bioRxiv:027516  2015.

[25] T. E. Nichols and A. P. Holmes. Nonparametric permutation tests for functional neuroimaging: A primer

with examples. Human Brain Mapping  15(1):1–25  2002.

[26] J. Richiardi  H. Eryilmaz  S. Schwartz  P. Vuilleumier  and D. Van De Ville. Decoding brain states from

fMRI connectivity graphs. NeuroImage  56:616–626  2011.

[27] W. R. Shirer  S. Ryali  E. Rykhlevskaia  V. Menon  and M. D. Greicius. Decoding subject-driven cognitive

states with whole-brain connectivity patterns. Cerebral Cortex  22(1):158–165  2012.
[28] S. M. Smith et al. Network modelling methods for fMRI. NeuroImage  54:875  2011.
[29] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B  pages 267–288  1996.

[30] S. Van de Geer  P. Bühlmann  Y. Ritov  and R. Dezeure. On asymptotically optimal conﬁdence regions and

tests for high-dimensional models. Ann. Stat.  42(3):1166–1202  2014.

[31] G. Varoquaux and R. C. Craddock. Learning and comparing functional connectomes across subjects.

NeuroImage  80:405–415  2013.

[32] G. Varoquaux  A. Gramfort  F. Pedregosa  V. Michel  and B. Thirion. Multi-subject dictionary learning to

segment an atlas of brain spontaneous activity. In IPMI  2011.

[33] G. Varoquaux  A. Gramfort  J.-B. Poline  and B. Thirion. Brain covariance selection: Better individual

functional connectivity models using population prior. In NIPS  2010.

[34] L. Waldorp. Testing for graph differences using the desparsiﬁed lasso in high-dimensional data. Statistics

Survey  2014.

[35] S. D. Zhao et al. Direct estimation of differential networks. Biometrika  101(2):253–268  2014.

9

,Carl Doersch
Abhinav Gupta
Alexei Efros
Eugene Belilovsky
Gaël Varoquaux
Matthew Blaschko