2013,Learning word embeddings efficiently with noise-contrastive estimation,Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well  setting performance records on several word similarity tasks.  The best results are obtained by learning high-dimensional embeddings from very large quantities of data  which makes scalability of the training method a critical factor.  We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation.  Our approach is simpler  faster  and produces better results than the current state-of-the art method of Mikolov et al. (2013a). We achieve results comparable to the best ones reported  which were obtained on a cluster  using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.,Learning word embeddings efﬁciently with

noise-contrastive estimation

Andriy Mnih

DeepMind Technologies

andriy@deepmind.com

Koray Kavukcuoglu
DeepMind Technologies
koray@deepmind.com

Abstract

Continuous-valued word embeddings learned by neural language models have re-
cently been shown to capture semantic and syntactic information about words very
well  setting performance records on several word similarity tasks. The best results
are obtained by learning high-dimensional embeddings from very large quantities
of data  which makes scalability of the training method a critical factor.
We propose a simple and scalable new approach to learning word embeddings
based on training log-bilinear models with noise-contrastive estimation. Our ap-
proach is simpler  faster  and produces better results than the current state-of-the-
art method. We achieve results comparable to the best ones reported  which were
obtained on a cluster  using four times less data and more than an order of mag-
nitude less computing time. We also investigate several model types and ﬁnd that
the embeddings learned by the simpler models perform at least as well as those
learned by the more complex ones.

1

Introduction

Natural language processing and information retrieval systems can often beneﬁt from incorporating
accurate word similarity information. Learning word representations from large collections of un-
structured text is an effective way of capturing such information. The classic approach to this task
is to use the word space model  representing each word with a vector of co-occurrence counts with
other words [16]. Representations of this type suffer from data sparsity problems due to the ex-
treme dimensionality of the word count vectors. To address this  Latent Semantic Analysis performs
dimensionality reduction on such vectors  producing lower-dimensional real-valued word embed-
dings.
Better real-valued representations  however  are learned by neural language models which are trained
to predict the next word in the sentence given the preceding words. Such representations have been
used to achieve excellent performance on classic NLP tasks [4  18  17]. Unfortunately  few neural
language models scale well to large datasets and vocabularies due to use of hidden layers and the
cost of computing normalized probabilities.
Recently  a scalable method for learning word embeddings using light-weight tree-structured neural
language models was proposed in [10]. Although tree-structured models can be trained quickly  they
are considerably more complex than the traditional (ﬂat) models and their performance is sensitive
to the choice of the tree over words [13]. Inspired by the excellent results of [10]  we investigate
a simpler approach based on noise-contrastive estimation (NCE) [6]  which enables fast training
without the complexity of working with tree-structured models. We compound the speedup obtained
by using NCE to eliminate the normalization costs during training  by using very simple variants of
the log-bilinear model [14]  resulting in parameter update complexity linear in the word embedding
dimensionality.

1

We evaluate our approach on two analogy-based word similarity tasks [11  10] and show that de-
spite the considerably shorter training times our models outperform the Skip-gram model from [10]
trained on the same 1.5B-word Wikipedia dataset. Furthermore  we can obtain performance com-
parable to that of the huge Skip-gram and CBOW models trained on a 125-CPU-core cluster after
training for only four days on a single core using four times less training data. Finally  we explore
several model architectures and discover that the simplest architectures learn embeddings that are at
least as good as those learned by the more complex ones.

2 Neural probabilistic language models

Neural probabilistic language models (NPLMs) specify the distribution for the target word w  given
a sequence of words h  called the context. In statistical language modelling  w is typically the next
word in the sentence  while the context h is the sequence of words that precede w. Though some
models such as recurrent neural language models [9] can handle arbitrarily long contexts  in this
paper  we will restrict our attention to ﬁxed-length contexts. Since we are interested in learning
word representations as opposed to assigning probabilities to sentences  we do not need to restrict
our models to predicting the next word  and can  for example  predict w from the words surrounding
it as was done in [4].
Given a context h  an NPLM deﬁnes the distribution for the word to be predicted using the scoring
function sθ(w  h) that quantiﬁes the compatibility between the context and the candidate target
word. Here θ are model parameters  which include the word embeddings. The scores are converted
to probabilities by exponentiating and normalizing:

(cid:80)

P h

θ (w) =

exp(sθ(w  h))
w(cid:48) exp(sθ(w(cid:48)  h))

.

(1)

Unfortunately both evaluating P h
θ (w) and computing the corresponding likelihood gradient requires
normalizing over the entire vocabulary  which means that maximum likelihood training of such
models takes time linear in the vocabulary size  and thus is prohibitively expensive for all but the
smallest vocabularies.
There are two main approaches to scaling up NPLMs to large vocabularies. The ﬁrst one involves
using a tree-structured vocabulary with words at the leaves  resulting in training time logarithmic
in the vocabulary size [15]. Unfortunately  this approach is considerably more involved than ML
training and ﬁnding well-performing trees is non-trivial [13]. The alternative is to keep the model but
use a different training strategy. Using importance sampling to approximate the likelihood gradient
was the ﬁrst such method to be proposed [2  3]  and though it could produce substantial speedups  it
suffered from stability problems. Recently  a method for training unnormalized probabilistic models 
called noise-contrastive estimation (NCE) [6]  has been shown to be a stable and efﬁcient way of
training NPLMs [14]. As it is also considerably simpler than the tree-based prediction approach  we
use NCE for training models in this paper. We will describe NCE in detail in Section 3.1.

3 Scalable log-bilinear models

We are interested in highly scalable models that can be trained on billion-word datasets with vocab-
ularies of hundreds of thousands of words within a few days on a single core  which rules out most
traditional neural language models such as those from [1] and [4]. We will use the log-bilinear lan-
guage model (LBL) [12] as our starting point  which unlike traditional NPLMs  does not have a hid-
den layer and works by performing linear prediction in the word feature vector space. In particular 
we will use a more scalable version of LBL [14] that uses vectors instead of matrices for its context
weights to avoid the high cost of matrix-vector multiplication. This model  like all other models
we will describe  has two sets of word representations: one for the target words (i.e.
the words
being predicted) and one for the context words. We denote the target and the context representations
for word w with qw and rw respectively. Given a sequence of context words h = w1  ..  wn  the
model computes the predicted representation for the target word by taking a linear combination of
the context word feature vectors:

ˆq(h) =

ci (cid:12) rwi 

(2)

n(cid:88)

i=1

2

where ci is the weight vector for the context word in position i and (cid:12) denotes element-wise mul-
tiplication. The context can consist of words preceding  following  or surrounding the word being
predicted. The scoring function then computes the similarity between the predicted feature vector
and one for word w:

sθ(w  h) = ˆq(h)(cid:62)qw + bw 

(cid:80)n

(3)
where bw is a bias that captures the context-independent frequency of word w. We will refer to this
model as vLBL  for vector LBL.
vLBL can be made even simpler by eliminating the position-dependent weights and computing the
predicted feature vector simply by averaging the context word feature vectors: ˆq(h) = 1
i=1 rwi.
n
The result is something like a local topic model  which ignores the order of context words  potentially
forcing it to capture more semantic information  perhaps at the expense of syntax. The idea of simply
averaging context word feature vectors was introduced in [8]  where it was used to condition on large
contexts such as entire documents. The resulting model can be seen as a non-hierarchical version of
the CBOW model of [10].
As our primary concern is learning word representations as opposed to creating useful language
models  we are free to move away from the paradigm of predicting the target word from its context
and  for example  do the reverse. This approach is motivated by the distributional hypothesis  which
states that words with similar meanings often occur in the same contexts [7] and thus suggests look-
ing for word representations that capture their context distributions. The inverse language modelling
approach of learning to predict the context from the word is a natural way to do that. Some classic
word-space models such as HAL and COALS [16] follow this approach by representing the context
distribution using a bag-of-words but they do not learn embeddings from this information.
Unfortunately  predicting an n-word context requires modelling the joint distribution of n words 
which is considerably harder than modelling the distribution of a single word. We make the task
tractable by assuming that the words in different context positions are conditionally independent
given the current word w:

n(cid:89)

P w

θ (h) =

P w

i θ(wi).

(4)

i=1

Though this assumption can be easily relaxed without giving up tractability by introducing some
Markov structure into the context distribution  we leave investigating this direction as future work.
The context word distributions P w
i θ(wi) are simply vLBL models that condition on the current word
and are deﬁned by the scoring function

si θ(wi  w) = (ci (cid:12) rw)(cid:62)qwi + bwi.

(5)
The resulting model can be seen as a Naive Bayes classiﬁer parameterized in terms of word embed-
dings. As this model performs inverse language modelling  we will refer to it as ivLBL.
As with our traditional language model  we also consider the simpler version of this model without
position-dependent weights  deﬁned by the scoring function

si θ(wi  w) = r(cid:62)

(6)
The resulting model is the non-hierarchical counterpart of the Skip-gram model [10]. Note that
unlike the tree-based models  such as those in the above paper  which only learn conditional embed-
dings for words  in our models each word has both a conditional and a target embedding which can
potentially capture complementary information. Tree-based models replace target embeddings with
parameters vectors associated with the tree nodes  as opposed to individual words.

w qwi + bwi.

3.1 Noise-contrastive estimation

We train our models using noise-contrastive estimation  a method for ﬁtting unnormalized models
[6]  adapted to neural language modelling in [14]. NCE is based on the reduction of density estima-
tion to probabilistic binary classiﬁcation. The basic idea is to train a logistic regression classiﬁer to
discriminate between samples from the data distribution and samples from some “noise” distribu-
tion  based on the ratio of probabilities of the sample under the model and the noise distribution. The

3

main advantage of NCE is that it allows us to ﬁt models that are not explicitly normalized making
the training time effectively independent of the vocabulary size. Thus  we will be able to drop the
normalizing factor from Eq. 1  and simply use exp(sθ(w  h)) in place of P h
θ (w) during training. The
perplexity of NPLMs trained using this approach has been shown to be on par with those trained
with maximum likelihood learning  but at a fraction of the computational cost.
Suppose we would like to learn the distribution of words for some speciﬁc context h  denoted by
P h(w). To do that  we create an auxiliary binary classiﬁcation problem  treating the training data as
positive examples and samples from a noise distribution Pn(w) as negative examples. We are free
to choose any noise distribution that is easy to sample from and compute probabilities under  and
that does not assign zero probability to any word. We will use the (global) unigram distribution of
the training data as the noise distribution  a choice that is known to work well for training language
models. If we assume that noise samples are k times more frequent than data samples  the probability
that the given sample came from the data is P h(D = 1|w) =
d (w)+kPn(w). Our estimate of this
P h
probability is obtained by using our model distribution in place P h
d :

d (w)

P h

P h(D = 1|w  θ) =

P h

θ (w)

P h

= σ (∆sθ(w  h))  

θ (w) + kPn(w)

(7)
where σ(x) is the logistic function and ∆sθ(w  h) = sθ(w  h) − log(kPn(w)) is the difference in
the scores of word w under the model and the (scaled) noise distribution. The scaling factor k in
front of Pn(w) accounts for the fact that noise samples are k times more frequent than data samples.
Note that in the above equation we used sθ(w  h) in place of log P h
θ (w)  ignoring the normalization
term  because we are working with an unnormalized model. We can do this because the NCE
objective encourages the model to be approximately normalized and recovers a perfectly normalized
model if the model class contains the data distribution [6].
We ﬁt the model by maximizing the log-posterior probability of the correct labels D averaged over
the data and noise samples:
J h(θ) =EP h
=EP h

(cid:2)log P h(D = 1|w  θ)(cid:3) + kEPn

[log σ (∆sθ(w  h))] + kEPn [log (1 − σ (∆sθ(w  h)))]  

(cid:2)log P h(D = 0|w  θ)(cid:3)

(8)

d

d

In practice  the expectation over the noise distribution is approximated by sampling. Thus  we
estimate the contribution of a word / context pair w  h to the gradient of Eq. 8 by generating k noise
samples {xi} and computing

(cid:21)

(cid:20)

θ (w) − k(cid:88)

i=1

J h w(θ) = (1 − σ (∆sθ(w  h)))

∂
∂θ

∂
∂θ

log P h

σ (∆sθ(xi  h))

log P h

θ (xi)

.

(9)

∂
∂θ

Note that the gradient in Eq. 9 involves a sum over k noise samples instead of a sum over the entire
vocabulary  making the NCE training time linear in the number of noise samples and independent
of the vocabulary size. As we increase the number of noise samples k  this estimate approaches
the likelihood gradient of the normalized model  allowing us to trade off computation cost against
estimation accuracy [6].
NCE shares some similarities with a training method for non-probabilistic neural language models
that involves optimizing a margin-based ranking objective [4]. As that approach is non-probabilistic 
it is outside the scope of this paper  though it would be interesting to see whether it can be used to
learn competitive word embeddings.

4 Evaluating word embeddings

Using word embeddings learned by neural language models outside of the language modelling con-
text is a relatively recent development. An early example of this is the multi-layer neural network
of [4] trained to perform several NLP tasks which represented words exclusively in terms of learned
word embeddings. [18] provided the ﬁrst comparison of several word embeddings learned with dif-
ferent methods and showed that incorporating them into established NLP pipelines can boost their
performance.

4

Recently the focus has shifted towards evaluating such representations more directly  instead of mea-
suring their effect on the performance of larger systems. Microsoft Research (MSR) has released
two challenge sets: a set of sentences each with a missing word to be ﬁlled in [20] and a set of
analogy questions [11]  designed to evaluate semantic and syntactic content of word representa-
tions respectively. Another dataset  consisting of semantic and syntactic analogy questions has been
released by Google [10].
In this paper we will concentrate on the two analogy-based challenge sets  which consist of questions
”  denoted as a : b → c : ? . The task is to identify the held-out
of the form “a is to b is as c is to
fourth word  with only exact word matches deemed correct. Word embeddings learned by neural
language models have been shown to perform very well on these datasets when using the following
vector-similarity-based protocol for answering the questions. Suppose (cid:126)w is the representation vector
for word w normalized to unit norm. Then  following [11]  we answer a : b → c : ?   by ﬁnding the
word d∗ with the representation closest to (cid:126)b − (cid:126)a + (cid:126)c according to cosine similarity:

d∗ = arg max

x

((cid:126)b − (cid:126)a + (cid:126)c)(cid:62)(cid:126)x
(cid:107)(cid:126)b − (cid:126)a + (cid:126)c(cid:107) .

(10)

We discovered that reproducing the results reported in [10] and [11] for publicly available word
embeddings required excluding b and c from the vocabulary when looking for d∗ using Eq. 10 
though that was not clear from the papers. To see why this is necessary  we can rewrite Eq. 10 as

d∗ = arg max

(cid:126)b(cid:62)(cid:126)x − (cid:126)a(cid:62)(cid:126)x + (cid:126)c(cid:62)(cid:126)x

x

(11)

and notice that setting x to b or c maximizes the ﬁrst or third term respectively (since the vectors are
normalized)  resulting in a high similarity score. This equation suggests the following interpretation
of d∗: it is simply the word with the representation most similar to (cid:126)b and (cid:126)c and dissimilar to (cid:126)a  which
makes it quite natural to exclude b and c themselves from consideration.

5 Experimental evaluation

5.1 Datasets

We evaluated our word embeddings on two analogy-based word similarity tasks released recently
by Google and Microsoft Research that we described in Section 4. We could not train on the data
used for learning the embeddings in the original papers as it was not readily available. [10] used the
proprietary Google News corpus consisting of 6 billion words  while the 320-million-word training
set used in [11] is a compilation of several Linguistic Data Consortium corpora  some of which
available only to their subscribers.
Instead  we decided to use two freely-available datasets: the April 2013 dump of English Wikipedia
and the collection of about 500 Project Gutenberg texts that form the canonical training data for
the MSR Sentence Completion Challenge [19]. We preprocessed Wikipedia by stripping out the
XML formatting  mapping all words to lowercase  and replacing all digits with 7  leaving us with
1.5 billion words. Keeping all words that occurred at least 10 times resulted in a vocabulary of
about 872 thousand words. Such a large vocabulary was used to demonstrate the scalability of our
method as well as to ensure that the models will have seen almost all the words they will be tested
on. When preprocessing the 47M-word Gutenberg dataset  we kept all words that occurred 5 or
more times  resulting in an 80-thousand-word vocabulary. Note that many words used for testing
the representations are missing from this dataset  which greatly limits the accuracy achievable when
using it. To make our results directly comparable to those in other papers  we report accuracy scores
computed using Eq. 10  excluding the second and the third word in the question from consideration 
as explained in Section 4.

5.2 Details of training

All models were trained on a single core  using minibatches of size 100 and the initial learning
rate of 3 × 10−2. No regularization was used. Initially we used a validation-set based learning
rate adaptation scheme described in [14]  which halves the learning rate whenever the validation set

5

Table 1: Accuracy in percent on word similarity tasks. The models had 100D word embeddings
and were trained to predict 5 words on both sides of the current word on the 1.5B-word Wikipedia
dataset. Skip-gram(*) is our implementation of the model from [10]. ivLBL is the inverse language
model without position-dependent weights. NCEk denotes NCE training using k noise samples.

GOOGLE
SYNTACTIC OVERALL

MODEL
SKIP-GRAM(*)
IVLBL+NCE1
IVLBL+NCE2
IVLBL+NCE3
IVLBL+NCE5
IVLBL+NCE10
IVLBL+NCE25

SEMANTIC

28.0
28.4
30.8
34.2
37.2
38.9
40.0

36.4
42.1
44.1
43.6
44.7
45.0
46.1

MSR

31.7
34.9
36.2
36.3
36.7
36.0
36.7

TIME
(HOURS)
12.3
3.1
4.0
5.1
7.3
12.2
26.8

32.6
35.9
38.0
39.4
41.3
42.2
43.3

Table 2: Accuracy in percent on word similarity tasks for large models. The Skip-gram† and
CBOW† results are from [10].
ivLBL models predict 5 words before and after the current word.
vLBL models predict the current word from the 5 preceding and 5 following words.

MODEL
SKIP-GRAM†
SKIP-GRAM†
SKIP-GRAM†
IVLBL+NCE25
IVLBL+NCE25
IVLBL+NCE25
IVLBL+NCE25
IVLBL+NCE25
IVLBL+NCE25
CBOW†
CBOW†
VLBL+NCE5
VLBL+NCE5
VLBL+NCE5
VLBL+NCE5
VLBL+NCE5

EMBED.

DIM.
300
300
1000
300
300
300×2
100
100
100×2
300
1000
300
100
300
600
600×2

TRAINING
SET SIZE

1.6B
785M

6B
1.5B
1.5B
1.5B
1.5B
1.5B
1.5B
1.6B
6B
1.5B
1.5B
1.5B
1.5B
1.5B

SEM.
52.2
56.7
66.1
61.2
63.6
65.2
52.6
55.9
59.3
16.1
57.3
40.3
45.0
54.2
57.3
60.5

GOOGLE
SYN. OVERALL
55.1
52.2
65.1
58.4
61.8
63.0
48.5
50.1
54.2
52.6
68.9
55.4
56.8
64.8
66.0
67.1

53.8
55.5
65.6
59.7
62.6
64.0
50.3
53.2
56.5
36.1
63.7
48.5
51.5
60.0
62.1
64.1

MSR

48.8
52.4
54.2
39.2
42.3
44.6

48.7
52.3
58.1
59.1
60.8

TIME
(DAYS)
2.0
2.5
2.5×125
1.2
4.1
4.1
1.2
2.9
2.9
0.6
2×140
0.3
2.0
2.0
2.0
3.0

perplexity failed to improve after some time  but found that it led to poor representations despite
achieving low perplexity scores  which was likely due to undertraining. The linear learning rate
schedule described in [10] produced better results. Unfortunately  using it requires knowing in
advance how many passes through the data will be performed  which is not always possible or
convenient. Perhaps more seriously  this approach might result in undertraining of representations
for rare words because all representation share the same learning rate.
AdaGrad [5] provides an automatic way of dealing with this issue. Though AdaGrad has already
been used to train neural language models in a distributed setting [10]  we found that it helped
to learn better word representations even using a single CPU core. We reduced the potentially
prohibitive memory requirements of AdaGrad  which requires storing a running sum of squared
gradient values for each parameter  by using the same learning rate for all dimensions of a word
embedding. Thus we store only one extra number per embedding vector  which is helpful when
training models with hundreds of millions of parameters.

5.3 Results

Inspired by the excellent performance of tree-based models of [10]  we started by comparing the
best-performing model from that paper  the Skip-gram  to its non-hierarchical counterpart  ivLBL
without position-dependent weights  proposed in Section 3  trained using NCE. As there is no pub-
licly available Skip-gram implementation  we wrote our own. Our implementation is faithful to the
description in the paper  with one exception. To speed up training  instead of predicting all context
words around the current word  we predict only one context word  sampled at random using the

6

Table 3: Results for various models trained for 20 epochs on the 47M-word Gutenberg dataset
using NCE5 with AdaGrad. (D) and (I) denote models with and without position-dependent weights
respectively. For each task  the left (right) column give the accuracy obtained using the conditional
(target) word embeddings. nL (nR) denotes n words on the left (right) of the current word.

MODEL
VLBL(D)
VLBL(D)
VLBL(D)
VLBL(I)
VLBL(I)
VLBL(I)
IVLBL(D)
IVLBL(I)

CONTEXT

SIZE

5L + 5R

5L + 5R
5L + 5R

5L + 5R

10L
10R

10L
10R

SEMANTIC
2.6
2.4
1.9
2.8
2.4
2.7
2.9
3.0
2.8
2.5
2.6
2.3
2.8
2.3
2.6
2.8

GOOGLE
SYNTACTIC
23.8
24.7
22.1
14.8
24.1
13.1
29.6
27.5
16.1
23.5
24.6
16.2
15.1
13.0
26.8
26.8

OVERALL
14.2
14.6
12.9
9.3
14.2
8.4
17.5
16.4
10.1
14.0
14.6
9.9
9.5
8.1
15.8
15.9

MSR

23.4
20.9
8.8
22.9
19.8
10.0
14.5
21.4

23.1
9.0
23.0
24.2
10.1
20.3
14.0
21.0

TIME

(HOURS)

2.6
2.6
2.6
2.3
2.3
2.1
1.2
1.2

non-uniform weighting scheme from the paper. Note that our models are also trained using the same
context-word sampling approach. To make the comparison fair  we did not use AdaGrad for our
models in these experiments  using the linear learning rate schedule as in [10] instead.
Table 1 shows the results on the word similarity tasks for the two models trained on the Wikipedia
dataset. We ran NCE training several times with different numbers of noise samples to investigate the
effect of this parameter on the representation quality and training time. The models were trained for
three epochs  which in our experience provided a reasonable compromise between training time and
representation quality.1 All NCE-trained models outperformed the Skip-gram. Accuracy steadily
increased with the number of noise samples used  as did the training time. The best compromise
between running time and performance seems to be achieved with 5 or 10 noise samples.
We then experimented with training models using AdaGrad and found that it signiﬁcantly improved
the quality of embeddings obtained when training with 10 or 25 noise samples  increasing the se-
mantic score for the NCE25 model by over 10 percentage points. Encouraged by this  we trained
two ivLBL models with position-independent weights and different embedding dimensionalities
for several days using this approach. As some of the best results in [10] were obtained with the
CBOW model  we also trained its non-hierarchical counterpart from Section 3  vLBL with position-
independent weights  using 100/300/600-dimensional embeddings and NCE with 5 noise samples 
for shorter training times. Note that due to the unavailability of the Google News dataset used in that
paper  we trained on Wikipedia. The scores for ivLBL and vLBL models were obtained using the
conditional word and target word representations respectively  while the scores marked with d × 2
were obtained by concatenating the two word representations  after normalizing them.
The results  reported in Table 2  show that our models substantially outperform their hierarchical
counterparts when trained using comparable amounts of time and data. For example  the 300D
ivLBL model trained for just over a day  achieves accuracy scores 3-9 percentage points better than
the 300D Skip-gram trained on the same amount of data for almost twice as long. The same model
trained for four days achieves accuracy scores that are only 2-4 percentage points lower than those
of the 1000D Skip-gram trained on four times as much data using 75 times as many CPU cycles.
By computing word similarity scores using the conditional and the target word representations con-
catenated together  we can bring the accuracy gap down to 2 percentage points at no additional
computational cost. The accuracy achieved by vLBL models as compared to that of CBOW models
follows a similar pattern. Once again our models achieve better accuracy scores faster and we can
get within 3 percentage points of the result obtained on a cluster using much less data and far less
computation.
To determine whether we were crippling our models by using position-independent weight  we
evaluated all model architectures described in Section 3 on the Gutenberg corpus. The models were
trained for 20 epochs using NCE5 and AdaGrad. We report the accuracy obtained with both condi-
tional and target representation (left and right columns respectively) for each of the models in Ta-

1We checked this by training the Skip-gram model for 10 epochs  which did not result in a substantial

increase in accuracy.

7

Table 4: Accuracy on the MSR Sentence Completion Challenge dataset.

MODEL

CONTEXT

LATENT

LSA [19]

SKIP-GRAM [10]

LBL [14]
IVLBL
IVLBL
IVLBL

SIZE

SENTENCE
10L+10R

10L

5L+5R
5L+5R
5L+5R

DIM
300
640
300
100
300
600

PERCENT
CORRECT

49
48.0
54.7
51.0
55.2
55.5

ble 3. Perhaps surprisingly  the results show that representations learned with position-independent
weights  designated with (I)  tend to perform better than the ones learned with position-dependent
weights. The difference is small for traditional language models (vLBL)  but is quite pronounced
for the inverse language model (ivLBL). The best-performing representations were learned by the
traditional language model with the context surrounding the word and position-independent weights.
Sentence completion: We also applied our approach to the MSR Sentence Completion Challenge
[19]  where the task is to complete each of the 1 040 test sentences by picking the missing word
from the list of ﬁve candidate words. Using the 47M-word Gutenberg dataset  preprocessed as in
[14]  as the training set  we trained several ivLBL models with NCE5 to predict 5 words preceding
and 5 following the current word. To complete a sentence  we compute the probability of the 10
words around the missing word (using Eq. 4) for each of the candidate words and pick the one
producing the highest value. The resulting accuracy scores  given in Table 4 along with those of
several baselines  show that ivLBL models perform very well. Even the model with the lowest
embedding dimensionality of 100  achieves 51.0% correct  compared to 48.0% correct reported in
[10] for the Skip-gram model with 640D embeddings. The 55.5% correct achieved by the model
with 600D embeddings is also better than the best single-model score on this dataset in the literature
(54.7% in [14]).

6 Discussion

We have proposed a new highly scalable approach to learning word embeddings which involves
training lightweight log-bilinear language models with noise-contrastive estimation. It is simpler
than the tree-based language modelling approach of [10] and produces better-performing embed-
dings faster. Embeddings learned using a simple single-core implementation of our method achieve
accuracy scores comparable to the best reported ones  which were obtained on a large cluster using
four times as much data and almost two orders of magnitude as many CPU cycles. The scores we
report in this paper are also easy to compare to  because we trained our models only on publicly
available data.
Several promising directions remain to be explored. [8] have recently proposed a way of learning
multiple representations for each word by clustering the contexts the word occurs in and allocating
a different representation for each cluster  prior to training the model. As ivLBL predicts the context
from the word  it naturally allows using multiple context representations per current word  resulting
in a more principled approach to the problem based on mixture modeling. Sharing representations
between the context and the target words is also worth investigating as it might result in better-
estimated rare word representations.

Acknowledgments

We thank Volodymyr Mnih for his helpful comments.

References
[1] Yoshua Bengio  Rejean Ducharme  Pascal Vincent  and Christian Jauvin. A neural probabilistic language

model. Journal of Machine Learning Research  3:1137–1155  2003.

[2] Yoshua Bengio and Jean-S´ebastien Sen´ecal. Quick training of probabilistic neural nets by importance

sampling. In AISTATS’03  2003.

8

[3] Yoshua Bengio and Jean-S´ebastien Sen´ecal. Adaptive importance sampling to accelerate training of a

neural probabilistic language model. IEEE Transactions on Neural Networks  19(4):713–722  2008.

[4] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th International Conference on Machine Learning  2008.
[5] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12:2121–2159  2010.

[6] M.U. Gutmann and A. Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical models  with

applications to natural image statistics. Journal of Machine Learning Research  13:307–361  2012.

[7] Zellig S Harris. Distributional structure. Word  1954.
[8] Eric H Huang  Richard Socher  Christopher D Manning  and Andrew Y Ng. Improving word representa-
tions via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics  pages 873–882  2012.

[9] T. Mikolov  M. Karaﬁ´at  L. Burget  J. ˇCernock`y  and S. Khudanpur. Recurrent neural network based
language model. In Eleventh Annual Conference of the International Speech Communication Association 
2010.

[10] Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean. Efﬁcient estimation of word representations

in vector space. International Conference on Learning Representations 2013  2013.

[11] Tomas Mikolov  Wen-tau Yih  and Geoffrey Zweig. Linguistic regularities in continuous space word

representations. Proceedings of NAACL-HLT  2013.

[12] A. Mnih and G. Hinton. Three new graphical models for statistical language modelling. Proceedings of

the 24th International Conference on Machine Learning  pages 641–648  2007.

[13] Andriy Mnih and Geoffrey Hinton. A scalable hierarchical distributed language model. In Advances in

Neural Information Processing Systems  volume 21  2009.

[14] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language
models. In Proceedings of the 29th International Conference on Machine Learning  pages 1751–1758 
2012.

[15] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In AIS-

TATS’05  pages 246–252  2005.

[16] Magnus Sahlgren. The Word-Space Model: Using distributional analysis to represent syntagmatic and
paradigmatic relations between words in high-dimensional vector spaces. PhD thesis  Stockholm  2006.
[17] R. Socher  C.C. Lin  A.Y. Ng  and C.D. Manning. Parsing natural scenes and natural language with

recursive neural networks. In International Conference on Machine Learning (ICML)  2011.

[18] J. Turian  L. Ratinov  and Y. Bengio. Word representations: A simple and general method for semi-
supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics  pages 384–394  2010.

[19] G. Zweig and C.J.C. Burges. The Microsoft Research Sentence Completion Challenge. Technical Report

MSR-TR-2011-129  Microsoft Research  2011.

[20] Geoffrey Zweig and Chris J.C. Burges. A challenge set for advancing language modeling. In Proceedings
of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of
Language Modeling for HLT  pages 29–36  2012.

9

,Andriy Mnih
Koray Kavukcuoglu