2019,Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks,We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime  where the network  width (i.e.  number of hidden nodes per layer) is much larger than the number of training data points. We show that  the expected $0$-$1$ loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization  which we call a \textit{neural tangent random feature} (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error  our result yields a generalization error bound in the order of $\tilde{\mathcal{O}}(n^{-1/2})$ that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition  we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.,Generalization Bounds of Stochastic Gradient
Descent for Wide and Deep Neural Networks

Yuan Cao

Quanquan Gu

Department of Computer Science

University of California  Los Angeles

Department of Computer Science

University of California  Los Angeles

CA 90095  USA

yuancao@cs.ucla.edu

CA 90095  USA

qgu@cs.ucla.edu

Abstract

We study the training and generalization of deep neural networks (DNNs) in the
over-parameterized regime  where the network width (i.e.  number of hidden nodes
per layer) is much larger than the number of training data points. We show that  the
expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient
descent (SGD) and random initialization can be bounded by the training loss of
a random feature model induced by the network gradient at initialization  which
we call a neural tangent random feature (NTRF) model. For data distributions
that can be classiﬁed by NTRF model with sufﬁciently small error  our result

yields a generalization error bound in the order of rOpn´1{2q that is independent

of the network width. Our result is more general and sharper than many existing
generalization error bounds for over-parameterized neural networks. In addition 
we establish a strong connection between our generalization error bound and the
neural tangent kernel (NTK) proposed in recent work.

1

Introduction

Deep learning has achieved great success in a wide range of applications including image processing
[20]  natural language processing [17] and reinforcement learning [34]. Most of the deep neural
networks used in practice are highly over-parameterized  such that the number of parameters is much
larger than the number of training data. One of the mysteries in deep learning is that  even in an
over-parameterized regime  neural networks trained with stochastic gradient descent can still give
small test error and do not overﬁt. In fact  a famous empirical study by Zhang et al. [38] shows the
following phenomena:
• Even if one replaces the real labels of a training data set with purely random labels  an over-
parameterized neural network can still ﬁt the training data perfectly. However since the labels are
independent of the input  the resulting neural network does not generalize to the test dataset.
• If the same over-parameterized network is trained with real labels  it not only achieves small

training loss  but also generalizes well to the test dataset.

While a series of recent work has theoretically shown that a sufﬁciently over-parameterized (i.e. 
sufﬁciently wide) neural network can ﬁt random labels [12  2  11  39]  the reason why it can generalize
well when trained with real labels is less understood. Existing generalization bounds for deep neural
networks [29  6  27  15  13  5  24  35  28] based on uniform convergence usually cannot provide
non-vacuous bounds [21  13] in the over-parameterized regime. In fact  the empirical observation by
Zhang et al. [38] indicates that in order to understand deep learning  it is important to distinguish the
true data labels from random labels when studying generalization. In other words  it is essential to
quantify the “classiﬁability” of the underlying data distribution  i.e.  how difﬁcult it can be classiﬁed.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Certain effort has been made to take the “classiﬁability” of the data distribution into account for
generalization analysis of neural networks. Brutzkus et al. [7] showed that stochastic gradient descent
(SGD) can learn an over-parameterized two-layer neural network with good generalization for linearly
separable data. Li and Liang [25] proved that  if the data satisfy certain structural assumption  SGD
can learn an over-parameterized two-layer network with ﬁxed second layer weights and achieve a
small generalization error. Allen-Zhu et al. [1] studied the generalization performance of SGD and
its variants for learning two-layer and three-layer networks  and used the risk of smaller two-layer or
three-layer networks with smooth activation functions to characterize the classiﬁability of the data
distribution. There is another line of studies on the algorithm-dependent generalization bounds of
neural networks in the over-parameterized regime [10  4  8  37  14]  which quantiﬁes the classiﬁability
of the data with a reference function class deﬁned by random features [31  32] or kernels1. Speciﬁcally 
Daniely [10] showed that a neural network of large enough size is competitive with the best function
in the conjugate kernel class of the network. Arora et al. [4] gave a generalization error bound for two-
layer ReLU networks with ﬁxed second layer weights based on a ReLU kernel function. Cao and Gu
[8] showed that deep ReLU networks trained with gradient descent can achieve small generalization
error if the data can be separated by certain random feature model [32] with a margin. Yehudai and
Shamir [37] used the expected loss of a similar random feature model to quantify the generalization
error of two-layer neural networks with smooth activation functions. A similar generalization error
bound was also given by E et al. [14]  where the authors studied the optimization and generalization
of two-layer networks trained with gradient descent. However  all the aforementioned results are still
far from satisfactory: they are either limited to two-layer networks  or restricted to very simple and
special reference function classes.
In this paper  we aim at providing a sharper and generic analysis on the generalization of deep ReLU
networks trained by SGD. In detail  we base our analysis upon the key observations that near random
initialization  the neural network function is almost a linear function of its parameters and the loss
function is locally almost convex. This enables us to prove a cumulative loss bound of SGD  which
further leads to a generalization bound by online-to-batch conversion [9]. The main contributions of
our work are summarized as follows:
• We give a bound on the expected 0-1 error of deep ReLU networks trained by SGD with random
initialization. Our result relates the generalization bound of an over-parameterized ReLU network
with a random feature model deﬁned by the network gradients  which we call neural tangent
random feature (NTRF) model. It also suggests an algorithm-dependent generalization error bound

of order rOpn´1{2q  which is independent of network width  if the data can be classiﬁed by the
when reduced to their setting  i.e.  rOp1{2q versus rOp1{4q where  is the target generalization

• Our analysis is general enough to cover recent generalization error bounds for neural networks
with random feature based reference function classes  and provides better bounds. Our expected
0-1 error bound directly covers the result by Cao and Gu [8]  and gives a tighter sample complexity

NTRF model with small enough error.

error. Compared with recent results by Yehudai and Shamir [37]  E et al. [14] who only studied
two-layer networks  our bound not only works for deep networks  but also uses a larger reference
function class when reduced to the two-layer setting  and therefore is sharper.

rOpL ¨

• Our result has a direct connection to the neural tangent kernel studied in Jacot et al. [18]. When
interpreted in the language of kernel method  our result gives a generalization bound in the form of
yJpΘpLqq´1y{nq  where y is the training label vector  and ΘpLq is the neural tangent
kernel matrix deﬁned on the training input data. This form of generalization bound is similar to 
but more general and tighter than the bound given by Arora et al. [4].

ř
Notation We use lower case  lower case bold face  and upper case bold face letters to denote scalars 
ř
vectors and matrices respectively. For a vector v “ pv1  . . .   vdqT P Rd and a number 1 ď p ă 8 
i“1 |vi|pq1{p. We also deﬁne }v}8 “ maxi |vi|. For a matrix A “ pAi jqmˆn  we
let }v}p “ p
i jq1{2
use }A}0 to denote the number of non-zero entries of A  and denote }A}F “ p
and }A}p “ max}v}p“1 }Av}p for p ě 1. For two matrices A  B P Rmˆn  we deﬁne xA  By “
TrpAJBq. We denote by A ľ B if A ´ B is positive semideﬁnite. In addition  we deﬁne the
1Since random feature models and kernel methods are highly related [31  32]  we group them into the same

d

i j“1 A2

a

d

category. More details are discussed in Section 3.2.

2

asymptotic notations Op¨q  rOp¨q  Ωp¨q andrΩp¨q as follows. Suppose that an and bn be two sequences.
We use rOp¨q andrΩp¨q to hide the logarithmic factors in Op¨q and Ωp¨q.

We write an “ Opbnq if lim supnÑ8 |an{bn| ă 8  and an “ Ωpbnq if lim inf nÑ8 |an{bn| ą 0.

2 Problem Setup

fWpxq “ ?

In this section we introduce the basic problem setup. Following the same standard setup implemented
in the line of recent work [2  11  39  8]  we consider fully connected neural networks with width m 
depth L and input dimension d. Such a network is deﬁned by its weight matrices at each layer: for
L ě 2  let W1 P Rmˆd  Wl P Rmˆm  l “ 2  . . .   L ´ 1 and WL P R1ˆm be the weight matrices
of the network. Then the neural network with input x P Rd is deﬁned as

m ¨ WLσpWL´1σpWL´2 ¨¨¨ σpW1xq¨¨¨qq 

(2.1)
where σp¨q is the entry-wise activation function. In this paper  we only consider the ReLU activation
function σpzq “ maxt0  zu  which is the most commonly used activation function in applications. It
is also arguably one of the most difﬁcult activation functions to analyze  due to its non-smoothess. We
remark that our result can be generalized to many other Lipschitz continuous and smooth activation
functions. For simplicity  we follow Allen-Zhu et al. [2]  Du et al. [11] and assume that the widths of
each hidden layer are the same. Our result can be easily extended to the setting that the widths of
each layer are not equal but in the same order  as discussed in Zou et al. [39]  Cao and Gu [8].
When L “ 1  the neural network reduces to a linear function  which has been well-studied. Therefore 
for notational simplicity we focus on the case L ě 2  where the parameter space is deﬁned as

We also use W “ pW1  . . .   WLq P W to denote the collection of weight matrices for all layers.
For W  W1 P W  we deﬁne their inner product as xW  W1y :“
The goal of neural network learning is to minimize the expected risk  i.e. 

W :“ Rmˆd ˆ pRmˆmqL´2 ˆ R1ˆm.
ř
l“1 TrpWJ

lq.
l W1

L

LDpWq :“ Epx yq„DLpx yqpWq 

min
W

(2.2)
where Lpx yqpWq “ (cid:96)ry ¨ fWpxqs is the loss deﬁned on any example px  yq  and (cid:96)pzq is the loss
function. Without loss of generality  we consider the cross-entropy loss in this paper  which is deﬁned
as (cid:96)pzq “ logr1 ` expp´zqs. We would like to emphasize that our results also hold for most convex
and Lipschitz continuous loss functions such as hinge loss. We now introduce stochastic gradient
descent based training algorithm for minimizing the expected risk in (2.2). The detailed algorithm is
given in Algorithm 1.

Algorithm 1 SGD for DNNs starting at Gaussian initialization

independently from Np0  2{mq  l P rL ´ 1s.

p0q
l
p0q
L independently from Np0  1{mq.

Input: Number of iterations n  step size η.
Generate each entry of W
Generate each entry of W
for i “ 1  2  . . .   n do
Draw pxi  yiq from D.
Update Wpiq “ Wpi´1q ´ η ¨ ∇WLpxi yiqpWpiqq.
end for

Output: Randomly choosexW uniformly from tWp0q  . . .   Wpn´1qu.

The initialization scheme for Wp0q given in Algorithm 1 generates each entry of the weight matrices
from a zero-mean independent Gaussian distribution  whose variance is determined by the rule that
the expected length of the output vector in each hidden layer is equal to the length of the input.
This initialization method is also known as He initialization [16]. Here the last layer parameter is
initialized with variance 1{m instead of 2{m since the last layer is not associated with the ReLU
activation function.

3

3 Main Results

In this section we present the main results of this paper. In Section 3.1 we give an expected 0-1 error
bound against a neural tangent random feature reference function class. In Section 3.2  we discuss
the connection between our result and the neural tangent kernel proposed in Jacot et al. [18].

3.1 An Expected 0-1 Error Bound
In this section we give a bound on the expected 0-1 error L0´1D pWq :“ Epx yq„Dr1ty¨ fWpxq ă 0us
obtained by Algorithm 1. Our result is based on the following assumption.
Assumption 3.1. The data inputs are normalized: }x}2 “ 1 for all px  yq P supppDq.
Assumption 3.1 is a standard assumption made in almost all previous work on optimization and
generalization of over-parameterized neural networks [12  2  11  39  30  14]. As is mentioned in
Cao and Gu [8]  this assumption can be relaxed to c1 ď }x}2 ď c2 for all px  yq P supppDq  where
c2 ą c1 ą 0 are absolute constants.
For any W P W  we deﬁne its ω-neighborhood as

BpW  ωq :“ tW1 P W : }W1

l ´ Wl}F ď ω  l P rLsu.

 

FpWp0q  Rq “

Below we introduce the neural tangent random feature function class  which serves as a reference
function class to measure the “classiﬁability” of the data  i.e.  how easy it can be classiﬁed.
Deﬁnition 3.2 (Neural Tangent Random Feature). Let Wp0q be generated via the initialization
scheme in Algorithm 1. The neural tangent random feature (NTRF) function class is deﬁned as

(cid:32)
(
fp¨q “ fWp0qp¨q ` x∇WfWp0qp¨q  Wy : W P Bp0  R ¨ m´1{2q
where R ą 0 measures the size of the function class  and m is the width of the neural network.
The name “neural tangent random feature” is inspired by the neural tangent kernel proposed by
Jacot et al. [18]  because the random features are the gradients of the neural network with random
weights. Connections between the neural tangent random features and the neural tangent kernel will
be discussed in Section 3.2.
˘
We are ready to present our main result on the expected 0-1 error bound of Algorithm 1.
Theorem 3.3. For any δ P p0  e´1s and R ą 0  there exists
polypR  Lq
such that if m ě m˚pδ  R  L  nq  then with probability at least 1 ´ δ over the randomness of Wp0q 
?
+
nq for some small enough absolute constant
the output of Algorithm 1 with step size η “ κ ¨ R{pm
κ satisﬁes
(cid:96)ryi ¨ fpxiqs

m˚pδ  R  L  nq “ rO
nÿ

¨ n7 ¨ logp1{δq
c

` O

#

(3.1)

`

“

`

inf

 

n

E

4
n

ď

i“1

«

LR?
n

fPFpWp0q Rq

logp1{δq

‰
L0´1D pxWq

The expected 0-1 error bound given by Theorem 3.3 consists of two terms: The ﬁrst term in (3.1)
relates the expected 0-1 error achieved by Algorithm 1 with a reference function class–the NTRF
function class in Deﬁnition 3.2. The second term in (3.1) is a standard large-deviation error term. As

ﬀ
where the expectation is taken over the uniform draw ofxW from tWp0q  . . .   Wpn´1qu.
long as R “ rOp1q  this term matches the standard rOpn´1{2q rate in PAC learning bounds [33].
large. In particular  if we set R “ rOp1q  the second term in (3.1) will be rOpn´1{2q. In this case  the
can be classiﬁed by FpWp0q  rOp1qq. In other words  Theorem 3.3 suggests that if the data can be
classiﬁed by a function in the NTRF function class FpWp0q  rOp1qq with a small training error  the

Remark 3.4. The parameter R in Theorem 3.3 is from the NTRF class and introduces a trade-off
in the bound: when R is small  the corresponding NTRF class FpWp0q  Rq is small  making the
ﬁrst term in (3.1) large  and the second term in (3.1) is small. When R is large  the corresponding
function class FpWp0q  Rq is large  so the ﬁrst term in (3.1) is small  and the second term will be
“classiﬁability” of the underlying data distribution D is determined by how well its i.i.d. samples

over-parameterized ReLU network learnt by Algorithm 1 will have a small generalization error.

4

Remark 3.5. The expected 0-1 error bound given by Theorem 3.3 is in a very general form. It
directly covers the result given by Cao and Gu [8]. In Appendix A.1  we show that under the same
assumptions made in Cao and Gu [8]  to achieve  expected 0-1 error  our result requires a sample

complexity of order rOp´2q  which outperforms the result in Cao and Gu [8] by a factor of ´2.
two-layer neural networks. When L “ 2  the NTRF function class FpWp0q  rOp1qq can be written as
(cid:32)
(
fWp0qp¨q ` x∇W1fWp0qp¨q  W1y ` x∇W2 fWp0qp¨q  W2y : }W1}F  }W2}F ď rOpm´1{2q

Remark 3.6. Our generalization bound can also be compared with two recent results [37  14] for

.

In contrast  the reference function classes studied by Yehudai and Shamir [37] and E et al. [14] are
contained in the following random feature class:

(cid:32)
(
fWp0qp¨q ` x∇W2 fWp0qp¨q  W2y : }W2}F ď rOpm´1{2q

FpWp0q  rOp1qq is richer than F–it also contains the features corresponding to the ﬁrst layer gradient

F “
p0q
p0q
where Wp0q “ pW
2 q P Rmˆd ˆ R1ˆm are the random weights generated by the initial-
1   W
ization schemes in Yehudai and Shamir [37]  E et al. [14]2. Evidently  our NTRF function class
of the network at random initialization  i.e.  ∇W1fWp0qp¨q. As a result  our generalization bound is
sharper than those in Yehudai and Shamir [37]  E et al. [14] in the sense that we can show that neural
networks trained with SGD can compete with the best function in a larger reference function class.

 

As previously mentioned  the result of Theorem 3.3 can be easily extended to the setting where the
widths of different layers are different. We should expect that the result remains almost the same 
except that we assume the widths of hidden layers are all larger than or equal to m˚pδ  R  L  nq.
We would also like to point out that although this paper considers the cross-entropy loss  the proof
of Theorem 3.3 offers a general framework based on the fact that near initialization  the neural
network function is almost linear in terms of its weights. We believe that this proof framework can
potentially be applied to most practically useful loss functions: whenever (cid:96)p¨q is convex/Lipschitz
continuous/smooth  near initialization  LipWq is also almost convex/Lipschitz continuous/smooth
in W for all i P rns  and therefore standard online optimization analysis can be invoked with
online-to-batch conversion to provide a generalization bound. We refer to Section 4 for more details.

3.2 Connection to Neural Tangent Kernel

Besides quantifying the classiﬁability of the data with the NTRF function class FpWp0q  rOp1qq  an

alternative way to apply Theorem 3.3 is to check how large the parameter R needs to be in order to
make the ﬁrst term in (3.1) small enough (e.g.  smaller than n´1{2). In this subsection  we show that
this type of analysis connects Theorem 3.3 to the neural tangent kernel proposed in Jacot et al. [18]
and later studied by Yang [36]  Lee et al. [23]  Arora et al. [3]. Speciﬁcally  we provide an expected
0-1 error bound in terms of the neural tangent kernel matrix deﬁned over the training data. We ﬁrst
¸
deﬁne the neural tangent kernel matrix for the neural network function in (2.1).
Deﬁnition 3.7 (Neural Tangent Kernel Matrix). For any i  j P rns  deﬁne

˜
p1q
p1q
plq
i j “ xxi  xjy  A
i j “ Σ
ij “
˘rσpuqσpvqs 
`
pl`1q
i j “ 2 ¨ E
pu vq„N
plq
pl`1q
i j ¨ 2 ¨ E
pLq
i j q{2snˆn the neural tangent kernel matrix of an L-layer ReLU
network on training inputs x1  . . .   xn.

rΘ
rΘ
Then we call ΘpLq “ rprΘ

˘rσ1puqσ1pvqs ` Σ

i j “ rΘ

plq
i i Σ
plq
i j Σ

pLq
i j ` Σ

pl`1q
i j

.

pu vq„N

0 A

plq
ij

0 A

plq
ij

Σ

Σ
Σ

plq
i j
plq
j j

 

`

Deﬁnition 3.7 is the same as the original deﬁnition in Jacot et al. [18] when restricting the kernel
function on tx1  . . .   xnu  except that there is an extra coefﬁcient 2 in the second and third lines. This
extra factor is due to the difference in initialization schemes–in our paper the entries of hidden layer

2Normalizing weights to the same scale is necessary for a proper comparison. See Appendix A.2 for details.

5

matrices are randomly generated with variance 2{m  while in Jacot et al. [18] the variance of the
random initialization is 1{m. We remark that this extra factor 2 in Deﬁnition 3.7 will remove the
exponential dependence on the network depth L in the kernel matrix  which is appealing. In fact  it is
easy to check that under our scaling  the diagonal entries of ΣpLq are all 1’s  and the diagonal entries

of rΘpLq are all L’s.

The following lemma is a summary of Theorem 1 and Proposition 2 in Jacot et al. [18]  which ensures
that ΘpLq is the inﬁnite-width limit of the Gram matrix pm´1x∇WfWp0qpxiq ∇WfWp0qpxjqyqnˆn 
and is positive-deﬁnite as long as no two training inputs are parallel.
Lemma 3.8 (Jacot et al. [18]). For an L layer ReLU network with parameter set Wp0q initialized in
Algorithm 1  as the network width m Ñ 83  it holds that

m´1x∇WfWp0qpxiq ∇WfWp0qpxjqy PÝÑ Θ

pLq
i j  

where the expectation is taken over the randomness of Wp0q. Moreover  as long as each pair of inputs
among x1  . . .   xn P Sd´1 are not parallel  ΘpLq is positive-deﬁnite.
Remark 3.9. Lemmas 3.8 clearly shows the difference between our neural tangent kernel matrix
ΘpLq in Deﬁnition 3.7 and the Gram matrix KpLq deﬁned in Deﬁnition 5.1 in Du et al. [11]. For any
i  j P rns  by Lemma 3.8 we have
pLq
i j “ lim

l“1x∇Wl fWp0qpxiq ∇Wl fWp0qpxjqy.

ř

Θ

L

mÑ8 m´1

In contrast  the corresponding entry in KpLq is

K

pLq
i j “ lim

mÑ8 m´1x∇WL´1fWp0qpxiq ∇WL´1fWp0qpxjqy.

E

inf

ﬀ

` O

It can be seen that our deﬁnition of kernel matrix takes all layers into consideration  while Du
et al. [11] only considered the last hidden layer (i.e.  second last layer). Moreover  it is clear that
ΘpLq ľ KpLq. Since the smallest eigenvalue of the kernel matrix plays a key role in the analysis of
optimization and generalization of over-parameterized neural networks [12  11  4]  our neural tangent
rm˚pδ  L  n  λ0q that only depends on δ  L  n and λ0 such that if m ě rm˚pδ  L  n  λ0q  then with
kernel matrix can potentially lead to better bounds than the Gram matrix studied in Du et al. [11].
Corollary 3.10. Let y “ py1  . . .   ynqJ and λ0 “ λminpΘpLqq. For any δ P p0  e´1s  there exists
probability at least 1 ´ δ over the randomness of Wp0q  the output of Algorithm 1 with step size
ﬀ
η “ κ ¨ infryiyiě1
“
where the expectation is taken over the uniform draw ofxW from tWp0q  . . .   Wpn´1qu.
[4] gives a generalization bound rO
yields a bound rO

aryJpΘpLqq´1ry{pm
«
?
‰
nq for some small enough absolute constant κ satisﬁes
L0´1D pxWq
ď rO
ryiyiě1
L ¨
`a

Remark 3.11. Corollary 3.10 gives an algorithm-dependent generalization error bound of over-
parameterized L-layer neural networks trained with SGD. It is worth noting that recently Arora et al.
for two-layer networks with ﬁxed second
layer weights  where H8 is deﬁned as

Our result in Corollary 3.10 can be specialized to two-layer neural networks by choosing L “ 2  and

cryJpΘpLqq´1ry

yJpH8q´1y{n

i j “ xxi  xjy ¨ Ew„Np0 Iqrσ1pwJxiqσ1pwJxjqs.
H8

˘
i j ` 2 ¨ Ew„Np0 IqrσpwJxiqσpwJxjqs.

yJpΘp2qq´1y{n
p2q
i j “ H8

  where

logp1{δq

«c

Here the extra term 2 ¨ Ew„Np0 IqrσpwJxiqσpwJxjqs corresponds to the training of the second
mx∇W2fWp0qpxiq ∇W2fWp0qpxjqy. Since we have Θp2q ľ H8  our bound
layer–it is the limit of 1
is sharper than theirs. This comparison also shows that  our result generalizes the result in Arora et al.
[4] from two-layer  ﬁxed second layer networks to deep networks with all parameters being trained.
3The original result by Jacot et al. [18] requires that the widths of different layers go to inﬁnity sequen-
tially. Their result was later improved by Yang [36] such that the widths of different layers can go to inﬁnity
simultaneously.

`a

n

˘

 

n

Θ

6

Remark 3.12. Corollary 3.10 is based on the asymptotic convergence result in Lemma 3.8  which
does not show how wide the network need to be in order to make the Gram matrix close enough
to the NTK matrix. Very recently  Arora et al. [3] provided a non-asymptotic convergence result
for the Gram matrix  and showed the equivalence between an inﬁnitely wide network trained by
gradient ﬂow and a kernel regression predictor using neural tangent kernel  which suggests that the
generalization of deep neural networks trained by gradient ﬂow can potentially be measured by the
corresponding NTK. Utilizing this non-asymptotic convergence result  one can potentially specify

the detailed dependency of rm˚pδ  L  n  λ0q on δ  L  n and λ0 in Corollary 3.10.

aryJpΘpLqq´1ry factor in the generalization

classiﬁer on data tpxi ryiqun

Remark 3.13. Corollary 3.10 demonstrates that the generalization bound given by Theorem 3.3
does not increase with network width m  as long as m is large enough. Moreover  it provides a clear
characterization of the classiﬁability of data. In fact  the
bound given in Corollary 3.10 is exactly the NTK-induced RKHS norm of the kernel regression
i“1. Therefore  if y “ f˚pxq for some f˚p¨q with bounded norm in the
NTK-induced reproducing kernel Hilbert space (RKHS)  then over-parameterized neural networks
trained with SGD generalize well. In Appendix E  we provide some numerical evaluation of the
leading terms in the generalization bounds in Theorem 3.3 and Corollary 3.10 to demonstrate that
they are very informative on real-world datasets.

4 Proof of Main Theory

In this section we provide the proof of Theorem 3.3 and Corollary 3.10  and explain the intuition
behind the proof. For notational simplicity  for i P rns we denote LipWq “ Lpxi yiqpWq.

4.1 Proof of Theorem 3.3

Before giving the proof of Theorem 3.3  we ﬁrst introduce several lemmas. The following lemma
states that near initialization  the neural network function is almost linear in terms of its weights.
Lemma 4.1. There exists an absolute constant κ such that  with probability at least 1 ´ OpnL2q ¨
expr´Ωpmω2{3Lqs over the randomness of Wp0q  for all i P rns and W  W1 P BpWp0q  ωq with
ω ď κL´6rlogpmqs´3{2  it holds uniformly that
|fW1pxiq ´ fWpxiq ´ x∇fWpxiq  W1 ´ Wy| ď O
l ´ Wl}2.
Since the cross-entropy loss (cid:96)p¨q is convex  given Lemma 4.1  we can show in the following lemma
that near initialization  LipWq is also almost a convex function of W for any i P rns.
Lemma 4.2. There exists an absolute constant κ such that  with probability at least 1 ´ OpnL2q ¨
expr´Ωpmω2{3Lqs over the randomness of Wp0q  for any  ą 0  i P rns and W  W1 P BpWp0q  ωq
with ω ď κL´6m´3{8rlogpmqs´3{23{4  it holds uniformly that

¯
a
m logpmq

L´1
l“1 }W1

ω1{3L2

ř

´

¨

LipW1q ě LipWq ` x∇WLipWq  W1 ´ Wy ´ .

The locally almost convex property of the loss function given by Lemma 4.2 implies that the dynamics
of Algorithm 1 is similar to the dynamics of convex optimization. We can therefore derive a bound of
`
the cumulative loss. The result is given in the following lemma.
Lemma 4.3. For any   δ  R ą 0  there exists

˘

¨ ´14 ¨ logp1{δq

such that if m ě m˚p  δ  R  Lq  then with probability at least 1 ´ δ over the randomness of Wp0q 
for any W˚ P BpWp0q  Rm´1{2q  Algorithm 1 with η “ ν{pLmq  n “ L2R2{p2ν2q for some
small enough absolute constant ν has the following cumulative loss bound:

m˚p  δ  R  Lq “ rO
ř

polypR  Lq
ř

n

i“1LipWpi´1qq ď

n

i“1LipW˚q ` 3n.

We now ﬁnalize the proof by applying an online-to-batch conversion argument [9]  and use Lemma 4.1
to relate the neural network function with a function in the NTRF function class.

7

pWpi´1qq “ 1
Proof of Theorem 3.3. For i P rns  let L0´1
entropy loss satisﬁes 1tz ď 0u ď 4(cid:96)pzq  we have L0´1
setting  “ LR{?
nÿ
2νn in Lemma 4.3 gives that  if η is set as
at least 1 ´ δ 

nÿ

i

i

1
n

i“1

L0´1

i

pWpi´1qq ď 4
n

i“1

(cid:32)
(
a
yi ¨ fWpi´1qpxiq ă 0
. Since cross-
pWpi´1qq ď 4LipWpi´1qq. Therefore 
ν{2R{pm
nq  then with probability

?

¨ LR?
n

.

(4.1)

LipW˚q ` 12?
2ν
c

nÿ

nÿ

i“1

Note that for any i P rns  Wpi´1q only depends on px1  y1q  . . .  pxi´1  yi´1q and is independent of
pxi  yiq. Therefore by Proposition 1 in Cesa-Bianchi et al. [9]  with probability at least 1´ δ we have

n

1
n

“

E

n

i“1

L0´1

i

2 logp1{δq

n

.

(4.2)

. Therefore combining (4.1) and

L0´1D pWpi´1qq ď 1
pWpi´1qq `
“
‰
ř
i“1 L0´1D pWpi´1qq “ E
nÿ
‰
L0´1D pxWq

L0´1D pxWq

By deﬁnition  we have 1
(4.2) and applying union bound  we obtain that with probability at least 1 ´ 2δ 
n
2 logp1{δq

LipW˚q ` 12?
¨ LR?
n
2ν
for all W˚ P BpWp0q  Rm´1{2q. We now compare the neural network function fW˚pxiq with the
´
¯
a
function FWp0q W˚pxiq :“ fWp0qpxiq ` x∇fWp0qpxiq  W˚ ´ Wp0qy P FpWp0q  Rq. We have
´
¯
a
p0q
pRm´1{2q1{3L2
LipW˚q ď (cid:96)ryi ¨ FWp0q W˚pxiqs ` O
¨
m logpmq
l
¨ R4{3 ¨ m´2{3
m logpmq
ď (cid:96)ryi ¨ FWp0q W˚pxiqs ` O
ď (cid:96)ryi ¨ FWp0q W˚pxiqs ` LRn´1{2 

››W˚

l ´ W

ď 4
n

c

L´1
l“1

ř

(4.3)

››

i“1

`

L3

where the ﬁrst inequality is by the 1-Lipschitz continuity of (cid:96)p¨q and Lemma 4.1  the second inequality
is by W˚ P BpWp0q  Rm´1{2q  and last inequality holds as long as m ě C1R2L12rlogpmqs3n3 for
some large enough absolute constant C1. Plugging the inequality above into (4.3) gives

n

2

ˆ

˙

c

(cid:96)ryi ¨ FWp0q W˚pxiqs `

1 ` 12?
2ν

¨ LR?
n

`

2 logp1{δq

n

.

“
‰
L0´1D pxWq

E

nÿ

i“1

ď 4
n

Taking inﬁmum over W˚ P BpWp0q  Rm´1{2q and rescaling δ ﬁnishes the proof.

4.2 Proof of Corollary 3.10

In this subsection we prove Corollary 3.10. The following lemma shows that at initialization  with

a
Lemma 4.4. For any δ ą 0  if m ě KL logpnL{δq for a large enough absolute constant K  then
logpn{δqq for all i P rns.
with probability at least 1 ´ δ  |fWp0qpxiq| ď Op

high probability  the neural network function value at all the training inputs are of order rOp1q.
We now present the proof of Corollary 3.10. The idea is to construct suitable target valuespy1  . . .  pyn 
and then bound the norm of the solution of the linear equationspyi “ x∇fWp0qpxiq  Wy  i P rns. In
speciﬁc  for anyry withryiyi ě 1  we examine the minimum distance solution to Wp0q that ﬁt the
`aryJpΘpLqq´1ry
˘˘
data tpxi ryiqun
.
a
Proof of Corollary 3.10. Set B “ logt1{rexppn´1{2q ´ 1su “ Oplogpnqq  then for cross-entropy
logpn{δqq for all i P rns. For anyry with
loss we have (cid:96)pzq ď n´1{2 for z ě B. Moreover  let B1 “ maxiPrns |fWp0qpxiq|. Then by
Lemma 4.4  with probability at least 1 ´ δ  B1 ď Op

ryiyi ě 1  let B “ B ` B1 andpy “ B ¨ry  then it holds that for any i P rns 

yi ¨ rpyi ` fWp0qpxiqs “ yi ¨pyi ` yi ¨ fWp0qpxiq ě B ` B1 ´ B1 ě B 

i“1 well and use it to construct a speciﬁc function in F

Wp0q  rO

`

8

and therefore

(cid:96)tyi ¨ rpyi ` fWp0qpxiqsu ď n´1{2  i P rns.

we haveryJpΘpLqq´1ry ě n´1L´1}ry}2

(4.4)
Denote F “ m´1{2 ¨pvecr∇fWp0qpx1qs  . . .   vecr∇fWp0qpxnqsq P Rrmd`m`m2pL´2qsˆn. Note that
entries of ΘpLq are all bounded by L. Therefore  the largest eigenvalue of ΘpLq is at most nL  and
2 “ L´1. By Lemma 3.8 and standard matrix perturbation
bound  there exists m˚pδ  L  n  λ0q such that  if m ě m˚pδ  L  n  λ0q  then with probability at least
1 ´ δ  FJF is strictly positive-deﬁnite and

ryiyiě1
}pFJFq´1 ´ pΘpLqq´1}2 ď inf

(4.5)
Let F “ PΛQJ be the singular value decomposition of F  where P P Rmˆn  Q P Rnˆn have

orthogonal columns  and Λ P Rnˆn is a diagonal matrix. Let wvec “ PΛ´1QJpy  then we have

ryJpΘpLqq´1ry{n.

FJwvec “ pQΛPJqpPΛ´1QJpyq “py.

(4.6)

Moreover  by direct calculation we have

2 “ }PΛ´1QJpy}2
Therefore by (4.5) and the fact that }py}2

}wvec}2

2 “ }Λ´1QJpy}2

2 “pyJQΛ´2QJpy “pyJpFJFq´1py.

2

n  we have

}wvec}2

2 ¨ n ¨ }pFJFq´1 ´ pΘpLqq´1}2 ` B

2 “pyJrpFJFq´1 ´ pΘpLqq´1spy `pyJpΘpLqq´1py
2 “ B
2 ¨ryJpΘpLqq´1ry
2 ¨ryJpΘpLqq´1ry.
ď B
ď 2B
´bryJpΘpLqq´1ry ¨ m´1{2
¯
}Wl}F ď m´1{2}wvec}2 ď rO
Let W P W be the parameter collection reshaped from m´1{2wvec. Then clearly
`aryJpΘpLqq´1ry ¨ m´1{2
`
˘˘
. Moreover  by (4.6)  we have pyi “
and therefore W P B
‰(
(cid:32)
“
x∇WfWp0qpxiq  Wy. Plugging this into (4.4) then gives
`aryJpΘpLqq´1ry
`
Since pfp¨q “ fWp0qp¨q ` x∇WfWp0qp¨q  Wy P F
Wp0q  rO
fWp0qpxiq ` x∇WfWp0qpxiq  Wy
yi ¨
rem 3.3 and taking inﬁmum overry completes the proof.

ď n´1{2.

  applying Theo-

˘˘

0 O

(cid:96)

 

5 Conclusions and Future Work

In this paper we provide an expected 0-1 error bound for wide and deep ReLU networks trained with
SGD. This generalization error bound is measured by the NTRF function class. The connection to
the neural tangent kernel function studied in Jacot et al. [18] is also discussed. Our result covers a
series of recent generalization bounds for wide enough neural networks  and provides better bounds.
An important future work is to improve the over-parameterization condition in Theorem 3.3 and
Corollary 3.10. Other future directions include proving sample complexity lower bounds in the
over-parameterized regime  implementing the results in Jain et al. [19] to obtain last iterate bound
of SGD  and establishing uniform convergence based generalization bounds for over-parameterized
neural networks with methods developped in Bartlett et al. [6]  Neyshabur et al. [27]  Long and
Sedghi [26].

Acknowledgement

We would like to thank Peter Bartlett for a valuable discussion  and Simon S. Du for pointing out a
related work [3]. We also thank the anonymous reviewers and area chair for their helpful comments.
This research was sponsored in part by the National Science Foundation CAREER Award IIS-
1906169  IIS-1903202  and Salesforce Deep Learning Research Award. The views and conclusions
contained in this paper are those of the authors and should not be interpreted as representing any
funding agencies.

9

References
[1] ALLEN-ZHU  Z.  LI  Y. and LIANG  Y. (2018). Learning and generalization in overparameter-

ized neural networks  going beyond two layers. arXiv preprint arXiv:1811.04918 .

[2] ALLEN-ZHU  Z.  LI  Y. and SONG  Z. (2018). A convergence theory for deep learning via

over-parameterization. arXiv preprint arXiv:1811.03962 .

[3] ARORA  S.  DU  S. S.  HU  W.  LI  Z.  SALAKHUTDINOV  R. and WANG  R. (2019). On

exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 .

[4] ARORA  S.  DU  S. S.  HU  W.  LI  Z. and WANG  R. (2019). Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584 .

[5] ARORA  S.  GE  R.  NEYSHABUR  B. and ZHANG  Y. (2018). Stronger generalization bounds

for deep nets via a compression approach. arXiv preprint arXiv:1802.05296 .

[6] BARTLETT  P. L.  FOSTER  D. J. and TELGARSKY  M. J. (2017). Spectrally-normalized

margin bounds for neural networks. In Advances in Neural Information Processing Systems.

[7] BRUTZKUS  A.  GLOBERSON  A.  MALACH  E. and SHALEV-SHWARTZ  S. (2017). Sgd
learns over-parameterized networks that provably generalize on linearly separable data. arXiv
preprint arXiv:1710.10174 .

[8] CAO  Y. and GU  Q. (2019). A generalization theory of gradient descent for learning over-

parameterized deep relu networks. arXiv preprint arXiv:1902.01384 .

[9] CESA-BIANCHI  N.  CONCONI  A. and GENTILE  C. (2004). On the generalization ability of

on-line learning algorithms. IEEE Transactions on Information Theory 50 2050–2057.

[10] DANIELY  A. (2017). Sgd learns the conjugate kernel class of the network. In Advances in

Neural Information Processing Systems.

[11] DU  S. S.  LEE  J. D.  LI  H.  WANG  L. and ZHAI  X. (2018). Gradient descent ﬁnds global

minima of deep neural networks. arXiv preprint arXiv:1811.03804 .

[12] DU  S. S.  ZHAI  X.  POCZOS  B. and SINGH  A. (2018). Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054 .

[13] DZIUGAITE  G. K. and ROY  D. M. (2017). Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008 .

[14] E  W.  MA  C.  WU  L. ET AL. (2019). A comparative analysis of the optimization and
generalization property of two-layer neural network and random feature models under gradient
descent dynamics. arXiv preprint arXiv:1904.04326 .

[15] GOLOWICH  N.  RAKHLIN  A. and SHAMIR  O. (2017). Size-independent sample complexity

of neural networks. arXiv preprint arXiv:1712.06541 .

[16] HE  K.  ZHANG  X.  REN  S. and SUN  J. (2015). Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision.

[17] HINTON  G.  DENG  L.  YU  D.  DAHL  G. E.  MOHAMED  A.-R.  JAITLY  N.  SENIOR 
A.  VANHOUCKE  V.  NGUYEN  P.  SAINATH  T. N. ET AL. (2012). Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine 29 82–97.

[18] JACOT  A.  GABRIEL  F. and HONGLER  C. (2018). Neural tangent kernel: Convergence and

generalization in neural networks. arXiv preprint arXiv:1806.07572 .

[19] JAIN  P.  NAGARAJ  D. and NETRAPALLI  P. (2019). Making the last iterate of sgd information

theoretically optimal. arXiv preprint arXiv:1904.12443 .

10

[20] KRIZHEVSKY  A.  SUTSKEVER  I. and HINTON  G. E. (2012). Imagenet classiﬁcation with

deep convolutional neural networks. In Advances in neural information processing systems.

[21] LANGFORD  J. and CARUANA  R. (2002). (not) bounding the true error. In Advances in Neural

Information Processing Systems.

[22] LECUN  Y.  BOTTOU  L.  BENGIO  Y.  HAFFNER  P. ET AL. (1998). Gradient-based learning

applied to document recognition. Proceedings of the IEEE 86 2278–2324.

[23] LEE  J.  XIAO  L.  SCHOENHOLZ  S. S.  BAHRI  Y.  SOHL-DICKSTEIN  J. and PENNINGTON 
J. (2019). Wide neural networks of any depth evolve as linear models under gradient descent.
arXiv preprint arXiv:1902.06720 .

[24] LI  X.  LU  J.  WANG  Z.  HAUPT  J. and ZHAO  T. (2018). On tighter generalization bound

for deep neural networks: Cnns  resnets  and beyond. arXiv preprint arXiv:1806.05159 .

[25] LI  Y. and LIANG  Y. (2018). Learning overparameterized neural networks via stochastic

gradient descent on structured data. arXiv preprint arXiv:1808.01204 .

[26] LONG  P. M. and SEDGHI  H. (2019). Size-free generalization bounds for convolutional neural

networks. arXiv preprint arXiv:1905.12600 .

[27] NEYSHABUR  B.  BHOJANAPALLI  S.  MCALLESTER  D. and SREBRO  N. (2017). A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564 .

[28] NEYSHABUR  B.  LI  Z.  BHOJANAPALLI  S.  LECUN  Y. and SREBRO  N. (2018). The role

of over-parametrization in generalization of neural networks .

[29] NEYSHABUR  B.  TOMIOKA  R. and SREBRO  N. (2015). Norm-based capacity control in

neural networks. In Conference on Learning Theory.

[30] OYMAK  S. and SOLTANOLKOTABI  M. (2019). Towards moderate overparameteriza-
tion: global convergence guarantees for training shallow neural networks. arXiv preprint
arXiv:1902.04674 .

[31] RAHIMI  A. and RECHT  B. (2008). Random features for large-scale kernel machines. In

Advances in neural information processing systems.

[32] RAHIMI  A. and RECHT  B. (2009). Weighted sums of random kitchen sinks: Replacing
minimization with randomization in learning. In Advances in neural information processing
systems.

[33] SHALEV-SHWARTZ  S. and BEN-DAVID  S. (2014). Understanding machine learning: From

theory to algorithms. Cambridge university press.

[34] SILVER  D.  HUANG  A.  MADDISON  C. J.  GUEZ  A.  SIFRE  L.  VAN DEN DRIESSCHE 
G.  SCHRITTWIESER  J.  ANTONOGLOU  I.  PANNEERSHELVAM  V.  LANCTOT  M. ET AL.
(2016). Mastering the game of go with deep neural networks and tree search. Nature 529
484–489.

[35] WEI  C.  LEE  J. D.  LIU  Q. and MA  T. (2018). On the margin theory of feedforward neural

networks. arXiv preprint arXiv:1810.05369 .

[36] YANG  G. (2019). Scaling limits of wide neural networks with weight sharing: Gaussian
process behavior  gradient independence  and neural tangent kernel derivation. arXiv preprint
arXiv:1902.04760 .

[37] YEHUDAI  G. and SHAMIR  O. (2019). On the power and limitations of random features for

understanding neural networks. arXiv preprint arXiv:1904.00687 .

[38] ZHANG  C.  BENGIO  S.  HARDT  M.  RECHT  B. and VINYALS  O. (2016). Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 .

[39] ZOU  D.  CAO  Y.  ZHOU  D. and GU  Q. (2018). Stochastic gradient descent optimizes

over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888 .

11

,Yuan Cao
Quanquan Gu