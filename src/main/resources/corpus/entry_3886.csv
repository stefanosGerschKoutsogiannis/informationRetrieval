2010,On Herding and the Perceptron Cycling Theorem,The paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms. It is shown that both algorithms can be viewed as an application of the perceptron cycling theorem. This connection strengthens some herding results and suggests new (supervised) herding algorithms that  like CRFs or discriminative RBMs  make predictions by conditioning on the input attributes. We develop and investigate variants of conditional herding  and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative RBM.,On Herding and the Perceptron Cycling Theorem

Andrew E. Gelfand  Yutian Chen  Max Welling

Department of Computer Science
University of California  Irvine

{agelfand yutianc welling}@ics.uci.edu

Laurens van der Maaten

Department of CSE  UC San Diego
PRB Lab  Delft University of Tech.

lvdmaaten@gmail.com

Abstract

The paper develops a connection between traditional perceptron algorithms and
recently introduced herding algorithms. It is shown that both algorithms can be
viewed as an application of the perceptron cycling theorem. This connection
strengthens some herding results and suggests new (supervised) herding algo-
rithms that  like CRFs or discriminative RBMs  make predictions by conditioning
on the input attributes. We develop and investigate variants of conditional herd-
ing  and show that conditional herding leads to practical algorithms that perform
better than or on par with related classiﬁers such as the voted perceptron and the
discriminative RBM.

1

Introduction

w ← w + xi(yi − y∗
i )

The invention of the perceptron [12] goes back to the very beginning of AI more than half a century
ago. Rosenblatt’s very simple  neurally plausible learning rule made it an attractive algorithm for
learning relations in data: for every input xi  make a linear prediction about its label: y∗
i = wT xi
and update the weights as 

(1)
A critical evaluation by Minsky and Papert [11] revealed the perceptron’s limited representational
power. This fact is reﬂected in the behavior of Rosenblatt’s learning rule: if the data is linearly
separable  then the learning rule converges to the correct solution in a number of iterations that can
be bounded by (R/γ)2  where R represents the norm of the largest input vector and γ represents the
margin between the decision boundary and the closest data-case. However  ‘for data sets that are
not linearly separable  the perceptron learning algorithm will never converge’ (quoted from [1]).
While the above result is true  the theorem in question has something much more powerful to say.
The ‘perceptron cycling theorem’ (PCT) [2  11] states that for the inseparable case the weights re-
main bounded and do not diverge to inﬁnity. In this paper  we show that the implication of this
theorem is that certain moments are conserved on average. Denoting the data-case selected at itera-
tion t by it (note that the same data-case can be picked multiple times)  the corresponding attribute
vector and label by (xit  yit ) with xi ∈ X   and the label predicted by the perceptron at iteration t
for data-case it by y∗

it  we obtain the following result:

T(cid:88)

t=1

T(cid:88)

t=1

|| 1
T

xityit − 1
T

xity∗

it

|| ∼ O(1/T )

(2)

This result implies that  even though the perceptron learning algorithm does not converge in the
inseparable case  it generates predictions that correlate with the attributes in the same way as the true
√
labels do. More importantly  the correlations converge to the sample mean with a rate 1/T   which is
much faster than sampling based algorithms that converge at a rate 1/
T . By using general features
φ(x)  the above result can be extended to the matching of arbitrarily complicated statistics between
data and predictions.

1

In the inseparable case  we can interpret the perceptron as a bagging procedure and average predic-
tions instead of picking the single best (or last) weights found during training. Although not directly
motivated by the PCT and Eqn. 2  this is exactly what the voted perceptron (VP) [5] does. Interest-
ing generalization bounds for the voted perceptron have been derived in [5]. Extensions of VP to
chain models have been explored in  e.g. [4].
Herding is a seemingly unrelated family of algorithms for unsupervised learning [15  14  16  3]. In
traditional methods for learning Markov Random Field (MRF) models  the goal is to converge to
a single parameter estimate and then perform (approximate) inference in the resulting model. In
contrast  herding combines the learning and inference phases by treating the weights as dynamic
quantities and deﬁning a deterministic set of updates such that averaging predictions preserves cer-
tain moments of the training data. The herding algorithm generates a weakly chaotic sequence of
weights and a sequence of states of both hidden and visible variables of the MRF model. The in-
termediate states produced by herding are really ‘representative points’ of an implicit model that
interpolates between data cases. We can view these states as pseudo-samples  which analogously to
Eqn. 2  satisfy certain constraints on their average sufﬁcient statistics. However  unlike in perceptron
learning  the non-convergence of the weights is needed to generate long  non-periodic trajectories of
states that can be averaged over.
In this paper  we show that supervised perceptron algorithms and unsupervised herding algorithms
can all be derived from the PCT. This connection allows us to strengthen existing herding results.
For instance  we prove fast convergence rates of sample averages when we use small mini-batches
for making updates  or when we use incomplete optimization algorithms to run herding. Moreover 
the connection suggests new algorithms that lie between supervised perceptron and unsupervised
herding algorithms. We refer to these algorithms as “conditional herding” (CH) because  like con-
ditional random ﬁelds  they condition on the input features. From the perceptron perspective  condi-
tional herding can be understood as “voted perceptrons with hidden units”. Conditional herding can
also be interpreted as the zero temperature limit of discriminative RBMs (dRBMs) [10].

2 Perceptrons  Herding and the Perceptron Cycling Theorem

We ﬁrst review the perceptron cycling theorem that was initially introduced in [11] with a gap in the
proof that was ﬁxed in [2]. A sequence of vectors {wt}  wt ∈ RD  t = 0  1  . . . is generated by the
following iterative procedure: wt+1 = wt + vt  where vt is an element of a ﬁnite set  V  and the
norm of vt is bounded: maxi ||vi|| = R < ∞.
t vt ≤ 0  then there exists a constant M > 0
Perceptron Cycling Theorem (PCT). ∀t ≥ 0: If wT
such that (cid:107)wt − w0(cid:107) < M.
The theorem still holds when V is a ﬁnite set in a Hilbert space. The PCT immediately leads to the
following result:
Convergence Theorem. If PCT holds  then: || 1

(cid:80)T
t=1 vt|| ∼ O(1/T ).

This result is easily shown by observing that ||wT +1 − w0|| = ||(cid:80)T

T

t=1 ∆wt|| = ||(cid:80)T

t=1 vt|| < M 

and dividing all terms by T .

2.1 Voted Perceptron and Moment Matching

i )|   yi = ±1  y∗

The voted perceptron (VP) algorithm [5] repeatedly applies the update rule in Eqn. 1. Predictions
of test labels are made after each update and ﬁnal label predictions are taken as an average of all
intermediate predictions. The PCT convergence theorem leads to the result of Eqn. 2  where we
identify V = {xi(yi − y∗
i = ±1  i = 1  . . .   N}. For the VP algorithm  the PCT
thus guarantees that the moments (cid:104)xy(cid:105) ˜p(x y) (with ˜p the empirical distribution) are matched with
(cid:104)xy∗(cid:105)p(y∗|x) ˜p(x) where p(y∗|x) is the model distribution implied by how VP generates y∗.
In maximum entropy models  one seeks a model that satisﬁes a set of expectation constraints (mo-
ments) from the training data  while maximizing the entropy of the remaining degrees of free-
In contrast  a single perceptron strives to learn a deterministic mapping p(y∗|x) =
dom [9].
δ[y∗ − arg maxy(ywT x)] that has zero entropy and gets every prediction on every training case

2

correct (where δ is the delta function). Entropy is created in p(y∗|x) only when the weights wt do
not converge (i.e. for inseparable data sets). Thus  VP and maximum entropy methods are related 
but differ in how they handle the degrees of freedom that are unconstrained by moment matching.

2.2 Herding

A new class of unsupervised learning algorithms  known as “herding”  was introduced in [15].
Rather than learning a single ‘best’ MRF model that can be sampled from to estimate quantities
of interest  herding combines learning and inference into a single process. In particular  herding
produces a trajectory of weights and states that reproduce the moments of the training data.
Consider a fully observed MRF with features φ(x)  x ∈ X = [1  . . .   K]m with K the number of
states for each variable xj (j = 1  . . .   m) and with an energy function E(x) given by:

In herding [15]  the parameters w are updated as:

E(x) = −wT φ(x).

wt+1 = wt + φ − φ(x∗
t ) 

(cid:80)

(3)

t

T

i φ(xi) and x∗

t = arg maxx wT

(4)
t φ(x). Eqn. 4 looks like a maximum likeli-
where φ = 1
N
hood (ML) gradient update  with constant learning rate and maximization in place of expectation in
the right-hand side. This follows from taking the zero temperature limit of the ML objective (see
Section 2.5). The maximization prevents the herding sequence from converging to a single point
estimate on this alternative objective.
t} denote the sequence of states (pseudo-samples)
Let {wt} denote the sequence of weights and {x∗
produced by herding. We can apply the PCT to herding by identifying V = {φ − φ(x∗)| x∗ ∈ X}.
(cid:80)T
It is now easy to see that  in general  herding does not converge because under very mild conditions
we can always ﬁnd an x∗
t vt < 0. From the PCT convergence theorem  we also see
t such that wT
t )|| ∼ O(1/T )  i.e. the pseudo-sample averages of the features converge
that ||φ − 1
t=1 φ(x∗
√
to the data averages φ at a rate 1/T 1. This is considerably faster than i.i.d. sampling from the
corresponding MRF model  which would converge at a rate of 1/
Since the cardinality of the set V is exponentially large (i.e. |V| = K m)  ﬁnding the maximizing
t at each update may be hard. However  the PCT only requires us to ﬁnd some state x∗
state x∗
t vt ≤ 0 and in most cases this can easily be veriﬁed. Hence  the PCT provides a
such that wT
theoretical justiﬁcation for using a local search algorithm that performs partial energy maximization.
For example  we may start the local search from the state we ended up in during the previous
iteration (a so-called persistent chain [13  17]). Or  one may consider contrastive divergence-like
algorithms [8]  in which the sampling or mean ﬁeld approximation is replaced by a maximization.
In this case  maximizations are initialized on all data-cases and the weights are updated by the
i } found after
difference between the average over the data-cases minus the average over the {x∗
(partial) maximization. In this case  the set V is given by: V = {φ − 1
i )| x∗
i ∈ X ∀i}.
i φ(x∗
For obvious reasons  it is now guaranteed that wT
In practice  we often use mini-batches of size n < N instead of the full data set. In this case  the
cardinality of the set V is enlarged to |V| = C(n  N )K m  with C(n  N ) representing the ‘n choose
N’ ways to compute the sample mean φ(n) based on a subset of n data-cases. The negative term
t )|| ∼ O(1/T ).
remains unaltered. Since the PCT still applies: || 1
√
Depending on how the mini-batches are picked  convergence onto the overall mean φ can be either
O(1/
T ) (random sampling with replacement) or O(1/T ) (sampling without replacement which
has picked all data-cases after (cid:100)N/n(cid:101) rounds).

t vt ≤ 0.
(cid:80)T
t=1 φ(n) t − 1

(cid:80)T
t=1 φ(x∗

(cid:80)

N

T

T

T .

2.3 Hidden Variables

The discussion so far has considered only constant features: φ(x  y) = xy for VP and φ(x) for
herding. However  the PCT allows us to consider more general features that depend on the weights
1Similar convergence could also be achieved (without concern for generalization performance) by sampling
directly from the training data. However  herding converges with rate 1/T and is regularized by the weights to
prevent overﬁtting.

3

w  as long as the image of this feature mapping (and therefore  the update vector v) is a set of ﬁnite
cardinality. In [14]  such features took the form of ‘hidden units’:

φ(x  z) 

z(x  w) = arg max

z(cid:48)

wT φ(x  z(cid:48))

(5)

In this case  we identify the vector v as v = φ(x  z) − φ(x∗  z∗). In the left-hand term of this
expression  x is clamped to the data-cases and z is found as in Eqn. 5 by maximizing every data-case
separately; in the right-hand (or negative) term  x∗  z∗ are found by jointly maximizing wT φ(x  z).
The quantity φ(x  z) denotes a sample average over the training cases. We note that φ(x  z) indeed
maps to a ﬁnite domain because it depends on the real parameter w only through the discrete state
z. We also notice again that wT v ≤ 0 because of the deﬁnition of (x∗  z∗). From the convergence
theorem we ﬁnd that  || 1
t )|| ∼ O(1/T ). This result can be
extended to mini-batches as well.

(cid:80)T
t=1 φ(x  zt) − 1

(cid:80)T
t=1 φ(x∗

t   z∗

T

T

2.4 Conditional Herding

We are now ready to propose our new algorithm: conditional herding (CH). Like the VP algorithm 
CH is concerned with discriminative learning and  therefore  it conditions on the input attributes
{xi}. CH differs from VP in that it uses hidden variables  similar to the herder described in the
previous subsection. In the most general setting  CH uses features:

φ(x  y  z) 

z(x  y  w) = arg max

z(cid:48)
In the experiments in Section 3  we use the explicit form:

wT φ(x  y  z(cid:48)).

(6)

wT φ(x  y  z) = xT Wz + yT Bz + θT z + αT y.

(7)
where W  B  θ and α are the weights  z is a binary vector and y is a binary vector in a 1-of-K
scheme (see Figure 1). At each iteration t  CH randomly samples a subset of the data-cases and their
labels Dt = {xit  yit} ⊆ D. For every member of this mini-batch it computes a hidden variable zit
using Eqn. 6. The parameters are then updated as:

wt+1 = wt +

η
|Dt|

(φ(xit  yit  zit) − φ(xit  y∗

it

  z∗

it

))

(8)

(cid:88)

it∈Dt

In the positive term  zit  is found as in Eqn. 5. The negative term is obtained (similar to the percep-
tron) by making a prediction for the labels  keeping the input attributes ﬁxed:
∀it ∈ Dt.

wT φ(xit  y(cid:48)  z(cid:48)) 

) = arg max

(y∗

  z∗

(9)

it

it

y(cid:48) z(cid:48)

For the PCT to apply to CH  the set V of update vectors must be ﬁnite. The inputs x can be real-
valued because we condition on the inputs and there will be at most N distinct values (one for each
data-case). However  since we maximize over y and z these states must be discrete for the PCT to
apply.
Eqn. 8 includes a potentially vector-valued stepsize η. Notice however that scaling w ← λw will
have no affect on the values of z  z∗ or y∗ and hence on v. Therefore  if we also scale η ← λη  then
the sequence of discrete states zt  z∗
t(cid:48)=0 vt(cid:48) + w0  
the only scale that matters is the relative scale between w0 and η. In case there would just be a single
attractor set for the dynamics of w  the initialization w0 would only represent a transient affect.
However  in practice the scale of w0 relative to that of η does play an important role indicating that
many different attractor sets exist for this system.
Irrespective of the attractor we end up in  the PCT guarantees that:

t will not be affected either. Since wt = η(cid:80)t−1

t   y∗

T(cid:88)

t=1

|| 1
T

1
|Dt|

(cid:88)

it

T(cid:88)

t=1

(cid:88)

it

φ(xit  yit  zit) − 1
T

1
|Dt|

φ(xit  y∗

it

  z∗

it

)|| ∼ O(1/T ).

(10)

In general  herding systems perform better when we use normalized features: (cid:107)φ(x  z  y)(cid:107) =
R  ∀(x  z  y). The reason is that herding selects states by maximizing the inner product wT φ

4

and features with large norms will therefore become more likely to be selected. In fact  one can
show that states inside the convex hull of the φ(x  y  z) are never selected. For binary (±1) vari-
ables all states live on the convex hull  but this need not be true in general  especially when we use
continuous attributes x. To remedy this  one can either normalize features or add one additional fea-
max − ||φ(x  y  z)||2  where Rmax = maxx y z φ(x  y  z) where x is only

ture2 φ0(x  y  z) =(cid:112)R2

allowed to vary over the data-cases.
Finally  predictions on unseen test data are made by:

(y∗

tst t  z∗

tst t) = arg max

y(cid:48) z(cid:48)

t φ(xtst  y(cid:48)  z(cid:48)) 
wT

(11)

The algorithm is summarized in the algorithm-box below.

Conditional Herding (CH)

1. Initialize w0 (with ﬁnite norm) and yavg j = 0 for all test cases j.
2. For t ≥ 0:

(a) Choose a subset {xit  yit} = Dt ⊆ D. For each (xit  yit)  choose a hidden state zit.
(b) Choose a set of “negative states” {(x∗

  z∗

(cid:88)

it

1
|Dt|

= xit  y∗
t−1φ(xit  yit  zit) ≤ 1
wT
|Dt|

it

it

it

(cid:88)

)}  such that:
t−1φ(xit  y∗
wT

it

it

  z∗

it

).

(12)

3. Update wt according to Eqn. 8.
4. Predict on test data as follows:

(a) For every test case xtst j at every iteration  choose negative states (y∗

tst jt  z∗

tst jt) in the

same way as for training data.

(b) Update online average over predictions  yavg j  for all test cases j.

2.5 Zero Temperature Limit of Discriminative MRF Learning

Regular herding can be understood as gradient descent on the zero temperature limit of an MRF
model. In this limit  gradient updates with constant step size never lead to convergence  irrespective
of how small the step size is. Analogously  CH can be viewed as constant step size gradient updates
on the zero temperature limit of discriminative MRFs (see [10] for the corresponding RBM model).
The ﬁnite temperature model is given by:

z exp(cid:2)wT φ(y  z  x)(cid:3)
(cid:80)
(cid:80)
takes the limit T → 0 of (cid:96)T (cid:44) T (cid:96)  where (cid:96) =(cid:80)

z(cid:48) y(cid:48) exp [wT φ(y(cid:48)  z(cid:48)  x)]

i log p(yi|xi).

p(y|x) =

Similar to herding [14]  conditional herding introduces a temperature by replacing w by w/T and

.

(13)

3 Experiments

We studied the behavior of conditional herding on two artiﬁcial and four real-world data sets  com-
paring its performance to that of the voted perceptron [5] and that of discriminative RBMs [10]. The
experiments on artiﬁcial and real-world data are discussed separately in Section 3.1 and 3.2.
We studied conditional herding in the discriminative RBM architecture illustrated in Figure 1 (i.e. 
we use the energy function in Eqn. 7). Per the discussion in Section 2.4  we added an additional

max − ||x||2 with Rmax = maxi (cid:107)xi(cid:107) in all experiments.

feature φ0(x) =(cid:112)R2

2If in test data this extra feature becomes imaginary we simply set it to zero.

5

Figure 1: Discriminative Restricted Boltzmann Machine model of distribution p(y  z|x).

(a) Banana data set.

(b) Lithuanian data set.

Figure 2: Decision boundaries of VP  CH  and dRBMs on two artiﬁcial data sets.

3.1 Artiﬁcial Data

To investigate the characteristics of VP  dRBMs and CH  we used the techniques to construct de-
cision boundaries on two artiﬁcial data sets: (1) the banana data set; and (2) the Lithuanian data
set. We ran VP and CH for 1  000 epochs using mini-batches of size 100. The decision bound-
ary for VP and CH is located at the location where the sign of the prediction y∗
tst changes. We
used conditional herders with 20 hidden units. The dRBMs also had 20 hidden units and were
trained by running conjugate gradients until convergence. The weights of the dRBMs were ini-
tialized by sampling from a Gaussian distribution with a variance of 10−4. The decision bound-
ary for the dRBMs is located at the point where both class posteriors are equal  i.e.  where
p(y∗
Plots of the decision boundary for the artiﬁcial data sets are shown in Figure 2. The results on the
banana data set illustrate the representational advantages of hidden units. Since VP selects data
points at random to update the weights  on the banana data set  the weight vector of VP tends to
oscillate back and forth yielding a nearly linear decision boundary3. This happens because VP can
regress on only 2 + 1 = 3 ﬁxed features. In contrast  for CH the simple predictor in the top layer can
regress onto M = 20 hidden features. This prevents the same oscillatory behavior from occurring.

tst = −1|˜xtst) = p(y∗

tst = +1|˜xtst) = 0.5.

3.2 Real-World Data

In addition to the experiments on synthetic data  we also performed experiments on four real-world
data sets - namely  (1) the USPS data set  (2) the MNIST data set  (3) the UCI Pendigits data set  and
(4) the 20-Newsgroups data set. The USPS data set consists of 11 000  16 × 16 grayscale images of
handwritten digits (1  100 images of each digit 0 through 9) with no ﬁxed division. The MNIST data
set contains 70  000  28 × 28 grayscale images of digits  with a ﬁxed division into 60  000 training
and 10  000 test instances. The UCI Pendigits consists of 16 (integer-valued) features extracted from
the movement of a stylus. It contains 10  992 instances  with a ﬁxed division into 7  494 training
and 3  498 test instances. The 20-Newsgroups data set contains bag-of-words representations of
18  774 documents gathered from 20 different newsgroups. Since the bag-of-words representation

3On the Lithuanian data set  VP constructs a good boundary by exploiting the added ‘normalizing’ feature.

6

xi1xi2xiD...yi1yi2yiC...zi1zi2ziK...WBθα  Voted perceptronDiscr. RBMCond. herding  comprises over 60  000 words  we identiﬁed the 5  000 most frequently occurring words. From this
set  we created a data set of 4  900 binary word-presence features by binarizing the word counts and
removing the 100 most frequently occurring words. The 20-Newsgroups data has a ﬁxed division
into 11  269 training and 7  505 test instances. On all data sets with real-valued input attributes we
used the ‘normalizing’ feature described above.
The data sets used in the experiments are multi-class. We adopted a 1-of-K encoding  where if yi
is the label for data point xi  then yi = {yi 1  ...  yi K} is a binary vector such that yi k = 1 if the
label of the ith data point is k and yi k = −1 otherwise. Performing the maximization in Eqn. 9 is
difﬁcult when K > 2. We investigated two different procedures for doing so. In the ﬁrst procedure 
we reduce the multi-class problem to a series of binary decision problems using a one-versus-all
scheme. The prediction on a test point is taken as the label with the largest online average. In the
second procedure  we make predictions on all K labels jointly. To perform the maximization in
Eqn. 9  we explore all states of y in a one-of-K encoding - i.e. one unit is activated and all others
are inactive. This partial maximization is not a problem as long as the ensuing conﬁguration satisﬁes
t vt ≤ 0 4. The main difference between the two procedures is that in the second procedure the
wT
weights W are shared amongst the K classiﬁers. The primary advantage of the latter procedure is
it less computationally demanding than the one-versus-all scheme.
We trained the dRBMs by performing iterations of conjugate gradients (using 3 linesearches) on
mini-batches of size 100 until the error on a small held-out validation set started increasing (i.e. 
we employed early stopping) or until the negative conditional log-likelihood on the training data
stopped coming down. Following [10]  we use L2-regularization on the weights of the dRBMs;
the regularization parameter was determined based on the generalization error on the same held-
out validation set. The weights of the dRBMs were initialized from a Gaussian distribution with
variance of 10−4.
CH used mini-batches of size 100. For the USPS and Pendigits data sets CH used a burn-in period
of 1  000 updates; on MNIST it was 5  000 updates; and on 20 Newsgroups it was 20  000 updates.
Herding was stopped when the error on the training set became zero 5.
The parameters of the conditional herders were initialized by sampling from a Gaussian distribution.
Ideally  we would like each of the terms in the energy function in Eqn. 7 to contribute equally during
updating. However  since the dimension of the data is typically much greater than the number of
classes  the dynamics of the conditional herding system will be largely driven by W. To negate this
effect  we rescaled the standard deviation of the Gaussian by a factor 1/M with M the total number
of elements of the parameter involved (e.g. σW = σ/(dim(x) dim(z)) etc.). We also scale the step
sizes η by the same factor so the updates will retain this scale during herding. The relative scale
between η and σ was chosen by cross-validation. Recall that the absolute scale is unimportant (see
Section 2.4 for details).
In addition  during the early stages of herding  we adapted the parameter update for the bias on the
hidden units θ in such a way that the marginal distribution over the hidden units was nearly uniform.
This has the advantage that it encourages high entropy in the hidden units  leading to more useful
(1 − λ)(cid:104)zit(cid:105) − z∗
dynamics of the system. In practice  we update θ as θt+1 = θt + η|Dt|
it 
where (cid:104)zit(cid:105) is the batch mean. λ is initialized to 1 and we gradually half its value every 500 updates 
slowly moving from an entropy-encouraging update to the standard update for the biases of the
hidden units.
VP was also run on mini-batches of size 100 (with step size of 1). VP was run until the predictor
started overﬁtting on a validation set. No burn-in was considered for VP.
The results of our experiments are shown in Table 1. In the table  the best performance on each
data set using each procedure is typeset in boldface. The results reveal that the addition of hidden
units to the voted perceptron leads to signiﬁcant improvements in terms of generalization error.
Furthermore  the results of our experiments indicate that conditional herding performs on par with
discriminative RBMs on the MNIST and USPS data sets and better on the 20 Newsgroups data set.
The 20 Newsgroups data is high dimensional and sparse and both VP and CH appear to perform

(cid:80)

it

∗ k
∗ k
4Local maxima can also be found by iterating over y
tst j  but the proposed procedure is more efﬁcient.
tst
5We use a ﬁxed order of the mini-batches  so that if there are N data cases and the batch size is K  if the

training error is 0 for (cid:100)N/K(cid:101) iterations  the error for the whole training set is 0.

  z

7

VP

7.69%

10.92%
27.75%

VP

50

XXXXXXXXX

Technique

Data Set
MNIST
USPS
UCI Pendigits
20 Newsgroups

XXXXXXXXX

Technique

Data Set
MNIST
USPS
UCI Pendigits
20 Newsgroups

Joint Procedure
Discriminative RBM
500
1.98%
4.06%
(1.09%)
8.89%
30.07%

100
2.93%
2.84%
(0.59%)
3.23%
30.57%

8.84% 3.88%
4.86% 3.13%
(0.52%)
6.78% 3.80%
24.89% –

(0.73%)

One-Versus-All Procedure
Discriminative RBM

100
3.57%

5.32%
34.78%

200
3.58%

5.00%
34.36%

5.03% (0.4%) 3.97% (0.38%) 4.02% (0.68%)

Conditional herding

200
3.99%

100
3.97%
3.49% (0.45%) 3.35%(0.48%)
3.37%
29.78%

3.00%
25.96%

Conditional herding
500
2.09%
2.81%
(0.50%)
2.86%
24.93%

100
2.09%
3.07%
(0.52%)
2.57%
25.76%

50
2.89%
3.36%
(0.48%)
3.14%
–

Table 1: Generalization errors of VP  dRBMs  and CH on 4 real-world data sets. dRBMs and CH
results are shown for various numbers of hidden units. The best performance on each data set is
typeset in boldface; missing values are shown as ‘-’. The std. dev. of the error on the 10-fold cross
validation of the USPS data set is reported in parentheses.

quite well in this regime. Techniques to promote sparsity in the hidden layer when training dRBMs
exist (see [10])  but we did not investigate them here. It is also worth noting that CH is rather resilient
to overﬁtting. This is particularly evident in the low-dimensional UCI Pendigits data set  where the
dRBMs start to badly overﬁt with 500 hidden units  while the test error for CH remains level. This
phenomena is the beneﬁt of averaging over many different predictors.

4 Concluding Remarks

The main contribution of this paper is to expose a relationship between the PCT and herding algo-
rithms. This has allowed us to strengthen certain results for herding - namely  theoretically vali-
dating herding with mini-batches and partial optimization. It also directly leads to the insight that
√
non-convergent VPs and herding match moments between data and generated predictions at a rate
much faster than random sampling (O(1/T ) vs. O(1/
T )). From these insights  we have proposed
a new conditional herding algorithm that is the zero-temperature limit of dRBMs [10].
The herding perspective provides a new way of looking at learning as a dynamical system. In fact 
the PCT precisely speciﬁes the conditions that need to hold for a herding system (in batch mode)
to be a piecewise isometry [7]. A piecewise isometry is a weakly chaotic dynamical system that
divides parameter space into cells and applies a different isometry in each cell. For herding  the
isometry is given by a translation and the cells are labeled by the states {x∗  y∗  z  z∗}  whichever
combination applies. Therefore  the requirement of the PCT that the space V must be of ﬁnite
cardinality translates into the division of parameter space in a ﬁnite number of cells  each with
its own isometry. Many interesting results about piecewise isometries have been proven in the
mathematics literature such as the fact that the sequence of sampled states grows algebraically with
T and not exponentially as in systems with random or chaotic components [6]. We envision a fruitful
cross-fertilization between the relevant research areas in mathematics and learning theory.

Acknowledgments

This work is supported by NSF grants 0447903  0914783  0928427 and 1018433 as well as
ONR/MURI grant 00014-06-1-073. LvdM acknowledges support by the Netherlands Organisation
for Scientiﬁc Research (grant no. 680.50.0908) and by EU-FP7 NoE on Social Signal Processing
(SSPNet).

References
[1] C.M. Bishop. Pattern Recognition and Machine Learning. Springer  2006.

8

[2] H.D. Block and S.A. Levin. On the boundedness of an iterative procedure for solving a system
of linear inequalities. Proceedings of the American Mathematical Society  26(2):229–235 
1970.

[3] Y. Chen and M. Welling. Parametric herding. In Proceedings of the Thirteenth International

Conference on Artiﬁcial Intelligence and Statistics  2010.

[4] M. Collins. Discriminative training methods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical
methods in natural language processing-Volume 10  page 8. Association for Computational
Linguistics  2002.

[5] Y. Freund and R.E. Schapire. Large margin classiﬁcation using the perceptron algorithm.

Machine learning  37(3):277–296  1999.

[6] A. Goetz. Perturbations of 8-attractors and births of satellite systems. Internat. J. Bifur. Chaos 

Appl. Sci. Engrg.  8(10):1937–1956  1998.

[7] A. Goetz. Global properties of a family of piecewise isometries. Ergodic Theory Dynam.

Systems  29(2):545–568  2009.

[8] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

Computation  14:1771–1800  2002.

[9] E.T. Jaynes.

Information theory and statistical mechanics. Physical Review Series II 

106(4):620–663  1957.

[10] H. Larochelle and Y. Bengio. Classiﬁcation using discriminative Restricted Boltzmann Ma-
chines. In Proceedings of the 25th International Conference on Machine learning  pages 536–
543. ACM  2008.

[11] M.L. Minsky and S. Papert. Perceptrons; An introduction to computational geometry. Cam-

bridge  Mass. : MIT Press  1969.

[12] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization

in the brain. Psychological review  65(6):386–408  1958.

[13] T. Tieleman. Training Restricted Boltzmann Machines using approximations to the likeli-
In Proceedings of the 25th International Conference on Machine learning 

hood gradient.
volume 25  pages 1064–1071  2008.

[14] M. Welling. Herding dynamic weights for partially observed random ﬁeld models. In Proc. of

the Conf. on Uncertainty in Artiﬁcial Intelligence  Montreal  Quebec  CAN  2009.

[15] M. Welling. Herding dynamical weights to learn. In Proceedings of the 21st International

Conference on Machine Learning  Montreal  Quebec  CAN  2009.

[16] M. Welling and Y. Chen. Statistical inference using weak chaos and inﬁnite memory.

In
Proceedings of the Int’l Workshop on Statistical-Mechanical Informatics (IW-SMI 2010)  pages
185–199  2010.

[17] L. Younes. Parametric inference for imperfectly observed Gibbsian ﬁelds. Probability Theory

and Related Fields  82:625–645  1989.

9

,Nanyang Ye
Zhanxing Zhu
Rafal Mantiuk
Xiaojie Wang
Rui Zhang
Yu Sun
Jianzhong Qi