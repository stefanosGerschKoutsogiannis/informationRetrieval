2008,Bayesian Kernel Shaping for Learning Control,In kernel-based regression learning  optimizing each kernel individually is useful when the data density  curvature of regression surfaces (or decision boundaries) or magnitude of output noise (i.e.  heteroscedasticity) varies spatially. Unfortunately  it presents a complex computational problem as the danger of overfitting is high and the individual optimization of every kernel in a learning system may be overly expensive due to the introduction of too many open learning parameters. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping  typically requiring some amount of manual tuning of meta parameters. In this paper  we focus on nonparametric regression and introduce a Bayesian formulation that  with the help of variational approximations  results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efficient (suitable for large data sets)  requires no sampling  automatically rejects outliers and has only one prior to be specified. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian Processes. Our methods are particularly useful for learning control  where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law.,Bayesian Kernel Shaping for Learning Control

Jo-Anne Ting1  Mrinal Kalakrishnan1  Sethu Vijayakumar2 and Stefan Schaal1 3

1Computer Science  U. of Southern California  Los Angeles  CA 90089  USA

2School of Informatics  University of Edinburgh  Edinburgh  EH9 3JZ  UK

3ATR Computational Neuroscience Labs  Kyoto 619-02  Japan

Abstract

In kernel-based regression learning  optimizing each kernel individually is useful
when the data density  curvature of regression surfaces (or decision boundaries)
or magnitude of output noise varies spatially. Previous work has suggested gradi-
ent descent techniques or complex statistical hypothesis methods for local kernel
shaping  typically requiring some amount of manual tuning of meta parameters.
We introduce a Bayesian formulation of nonparametric regression that  with the
help of variational approximations  results in an EM-like algorithm for simulta-
neous estimation of regression and kernel parameters. The algorithm is computa-
tionally efﬁcient  requires no sampling  automatically rejects outliers and has only
one prior to be speciﬁed. It can be used for nonparametric regression with local
polynomials or as a novel method to achieve nonstationary regression with Gaus-
sian processes. Our methods are particularly useful for learning control  where
reliable estimation of local tangent planes is essential for adaptive controllers and
reinforcement learning. We evaluate our methods on several synthetic data sets
and on an actual robot which learns a task-level control law.

1 Introduction

Kernel-based methods have been highly popular in statistical learning  starting with Parzen windows 
kernel regression  locally weighted regression and radial basis function networks  and leading to
newer formulations such as Reproducing Kernel Hilbert Spaces  Support Vector Machines  and
Gaussian process regression [1]. Most algorithms start with parameterizations that are the same for
all kernels  independent of where in data space the kernel is used  but later recognize the advantage
of locally adaptive kernels [2  3  4]. Such locally adaptive kernels are useful in scenarios where the
data characteristics vary greatly in different parts of the workspace (e.g.  in terms of data density 
curvature and output noise). For instance  in Gaussian process (GP) regression  using a nonstationary
covariance function  e.g.  [5]  allows for such a treatment. Performing optimizations individually for
every kernel  however  becomes rather complex and is prone to overﬁtting due to a ﬂood of open
parameters. Previous work has suggested gradient descent techniques with cross-validation methods
or involved statistical hypothesis testing for optimizing the shape and size of a kernel in a learning
system [6  7].

In this paper  we consider local kernel shaping by averaging over data samples with the help of
locally polynomial models and formulate this approach  in a Bayesian framework  for both function
approximation with piecewise linear models and nonstationary GP regression. Our local kernel
shaping algorithm is computationally efﬁcient (capable of handling large data sets)  can deal with
functions of strongly varying curvature  data density and output noise  and even rejects outliers
automatically. Our approach to nonstationary GP regression differs from previous work by avoiding
Markov Chain Monte Carlo (MCMC) sampling [8  9] and by exploiting the full nonparametric
characteristics of GPs in order to accommodate nonstationary data.

One of the core application domains for our work is learning control  where computationally efﬁcient
function approximation and highly accurate local linearizations from data are crucial for deriving
controllers and for optimizing control along trajectories [10]. The high variations from ﬁtting noise 
seen in Fig. 3  are harmful to the learning system  potentially causing the controller to be unstable.
Our ﬁnal evaluations illustrate such a scenario by learning an inverse kinematics model for a real
robot arm.

2 Bayesian Local Kernel Shaping

We develop our approach in the context of nonparametric locally weighted regression with lo-
cally linear polynomials [11]  assuming  for notational simplicity  only a one-dimensional output—
extensions to multi-output settings are straightforward. We assume a training set of N samples 
D = {xi  yi}N
i=1  drawn from a nonlinear function y = f (x) +  that is contaminated with mean-
zero (but potentially heteroscedastic) noise . Each data sample consists of a d-dimensional input
vector xi and an output yi. We wish to approximate a locally linear model of this function at a
query point xq ∈ <d×1 in order to make a prediction yq = bT xq  where b ∈ <d×1. We assume
the existence of a spatially localized weighting kernel wi = K (xi  xq  h) that assigns a weight to
every {xi  yi} according to its Euclidean distance in input space from the query point xq. A popular
choice for K is the Gaussian kernel  but other kernels may be used as well [11]. The bandwidth
h ∈ <d×1 of the kernel is the crucial parameter that determines the local model’s quality of ﬁt. Our
goal is to ﬁnd a Bayesian formulation of determining b and h simultaneously.

2.1 Model

  where zim = bT

For the locally linear model at the query
point xq  we can introduce hidden ran-
dom variables z [12] and modify the linear
m=1zim +
mxim + zm and zm ∼

model yi = bT xi so that yi =Pd
Normal (0  ψzm)   ∼ Normal(cid:0)0  σ2(cid:1) are

both additive noise terms. Note that xim =
[xim 1]T and bm = [bm bm0]T   where xim
is the mth coefﬁcient of xi  bm is the mth
coefﬁcient of b and bm0 is the offset value.
The z variables allow us to derive compu-
tationally efﬁcient O(d) EM-like updates 
as we will see later. The prediction at the
mxqm. We as-

query point xq is thenPd

m bT

2

!

yi

!z2

!z1

i = 1 ..  N

!zd

zi1

b
1

zi 2

b

2

(cid:201)

zid

b

d

x

i1

w

i1

x

i 2

w

i 2

(cid:201)

x

id

w

id

h
1

h

2

(cid:201)

h

d

Figure 1: Graphical model. Random variables are in
circles  and observed random variables are in shaded
double circles.

sume the following prior distributions for our model  shown graphically in Fig. 1:

p(yi|zi) ∼ Normal(cid:0)1T zi  σ2(cid:1)
p(zim|xim) ∼ Normal(cid:0)bT

mxim  ψzm(cid:1)

p(bm|ψzm) ∼ Normal (0  ψzmΣbm 0)

p(ψzm) ∼ Scaled-Inv-χ2 (nm0  ψzm 0)

where 1 is a vector of 1s  zi ∈ <d×1  zim is the mth coefﬁcient of zi  and Σbm 0 is the prior
mN 0 are the prior parameters of
covariance matrix of bm and a 2 × 2 diagonal matrix. nm0 and σ2
the Scaled-inverse-χ2 distribution (nm0 is the number of degrees of freedom parameter and σ2
mN 0 is
the scale parameter). The Scaled-Inverse-χ2 distribution was used for ψzm since it is the conjugate
prior for the variance parameter of a Gaussian distribution.

In contrast to classical treatments of Bayesian weighted regression [13] where the weights enter
as a heteroscedastic correction on the noise variance of each data sample  we associate a scalar
indicator-like weight  wi ∈ {0  1}  with each sample {xi  yi} in D. The sample is fully included in
m=1 wim 
where wim is the weight component in the mth input dimension. While previous methods model the
weighting kernel K as some explicit function  we model the weights wim as Bernoulli-distributed
random variables  i.e.  p(wim) ∼ Bernoulli(qim)  choosing a symmetric bell-shaped function for the
parameter qim: qim = 1/(1 + (xim − xqm)2rhm). xqm is the mth coefﬁcient of xq  hm is the mth

the local model if wi = 1 and excluded if wi = 0. We deﬁne the weight wi to be wi =Qd

coefﬁcient of h  and r > 0 is a positive integer1. As pointed out in [11]  the particular mathematical
formulation of a weighting kernel is largely computationally irrelevant for locally weighted learning.
Our choice of function for qim was dominated by the desire to obtain analytically tractable learning
updates. We place a Gamma prior over the bandwidth hm  i.e.  p(hm) ∼ Gamma (ahm0  bhm0)
where ahm0 and bhm0 are parameters of the Gamma distribution  to ensure that a positive weighting
kernel width.

2.2

Inference

QN
L = log" N

We can treat the entire regression problem as an EM learning problem [14  15] and maximize the log
likelihood log p(y|X) for generating the observed data. We can maximize this incomplete log likeli-
hood by maximizing the expected value of the complete log likelihood p(y  Z  b  w  h  σ2  ψz|X) =
i=1 p(yi  zi  b  wi  h  σ2  ψz|xi). In our model  each data sample i has an indicator-like scalar
weight wi associated with it  allowing us to express the complete log likelihood L  in a similar
fashion to mixture models  as:

Yi=1"(cid:2)p(yi|zi  σ2)p(zi|xi  b  ψz)(cid:3)wi

d

Ym=1

p(wim)# d
Ym=1

p(bm|ψzm)p(ψzm)p(hm)p(σ2)#

Expanding the log p(wim) term from the expression above results in a problematic − log(1 +
(xim − xqm)2r) term that prevents us from deriving an analytically tractable expression for the
posterior of hm. To address this  we use a variational approach on concave/convex functions sug-
gested by [16] to produce analytically tractable expressions. We can ﬁnd a lower bound on the

term so that − log(1 +(cid:0)xim − xqm)2r(cid:1) ≥ −λim (xim − xqm)2r hm  where λim is a variational

parameter to be optimized in the M-step of our ﬁnal EM-like algorithm. Our choice of weighting
kernel allows us to ﬁnd a lower bound to L in this manner. We explored the use of other weighting
kernels (e.g.  a quadratic negative exponential)  but had issues with ﬁnding a lower bound to the
problematic terms in log p(wim) such that analytically tractable inference for hm could be done.
The resulting lower bound to L is ˆL; due to lack of space  we give the expression for ˆL in the ap-
pendix. The expectation of ˆL should be taken with respect to the true posterior distribution of all
hidden variables Q(b  ψz  z  h). Since this is an analytically tractable expression  a lower bound
can be formulated using a technique from variational calculus where we make a factorial approxi-
mation of the true posterior  e.g.  Q(b  ψz  z  h) = Q(b  ψz)Q(h)Q(z) [15]  that allows resulting
posterior distributions over hidden variables to become analytically tractable. The posterior of wim 
p(wim = 1|yi  zi  xi  θ  wi k6=m)  is inferred using Bayes’ rule:

p(yi  zi|xi  θ  wi k6=m  wim = 1)Qd

t=1 t6=mhwitip(wim = 1)

p(yi  zi|xi  θ  wi k6=m  wim = 1)Qd

t=1 t6=mhwitip(wim = 1) + p(wim = 0)

(1)

where θ = {b  ψz  h} and wi k6=m denotes the set of weights {wik}d
k=1 k6=m. For the dimension
m  we account for the effect of weights in the other d − 1 dimensions. This is a result of wi
being deﬁned as the product of weights in all dimensions. The posterior mean of wim is then
m=1 hwimi  where h.i denotes the expectation
operator. We omit the full set of posterior EM update equations (please refer to the appendix for
this) and list only the posterior updates for hm  wim  bm and zi:

hp(wim = 1|yi  zi  xi  θ  wi k6=m)i  and hwii = Qd

Σzi|yi xi =

ΨzN
hwii

−

bm 0 +

Σbm = Σ−1
hbmi = Σbm  N
Xi=1

−1

N

hwii ximxT

im!
Xi=1
hwii hzimi xim!

hwimi =

k=1 k6=mhwiki

Qd
qimA
i
Qd
qimA
i

k=1 k6=mhwiki

+ 1 − qim

1

si (cid:18) ΨzN

hwii

hwii(cid:19)
11T ΨzN
si hwii (cid:19) bxi

ΨzN 11T

hzii =

hhmi =

ΨzN 1
si hwii

+(cid:18)Id d −
ahm0 + N −PN
bhm0 +PN

i=1 hwimi

i=1 λim (xim − xqm)2r

1(xim − xqm) is taken to the power 2r in order to ensure that the resulting expression is positive. Adjusting

r affects how long the tails of the kernel are. We use r = 2 for all our experiments.

m=1 hwimi  ΨzN is a diagonal matrix with ψzN on its diagonal  si = σ2 + 1T ΨzN

Qd
xqm)2r hhmi)  and Ai = N (yi; 1T hzii   σ2)Qd

where Id d is a d × d identity matrix  bxi is a d by 1 vector with coefﬁcients hbmiT xim  hwii =
hwii 1 (to avoid di-
vision by zero  hwii needs to be capped to some small non-zero value)  qim = λim = 1/(1+(xim −
m=1 N (zim; hbmiT xim  ψzm). Closer examination
of the expression for hbmi shows that it is a standard Bayesian weighted regression update [13]  i.e. 
a data sample i with lower weight wi will be downweighted in the regression. Since the weights are
inﬂuenced by the residual error at each data point (see posterior update for hwimi)  an outlier will
be downweighted appropriately and eliminated from the local model. Fig. 2 shows how local kernel
shaping is able to ignore outliers that a classical GP ﬁts.

A few remarks should be made regarding the initialization of priors
used in the posterior EM updates. Σbm 0 can be set to 106I to
reﬂect a large uncertainty associated with the prior distribution of
b. The initial noise variance  ψzm 0  should be set to the best guess
on the noise variance. To adjust the strength of this prior  nm0 can
be set to the number of samples one believes to have seen with
noise variance ψzm 0 Finally  the initial h of the weighting kernel
should be set so that the kernel is broad and wide. We use values of
ahm0 = bhm0 = 10−6 so that hm0 = 1 with high uncertainty. Note
that some sort of initial belief about the noise level is needed to
distinguish between noise and structure in the training data. Aside
from the initial prior on ψzm  we used the same priors for all data
sets in our evaluations.

2.3 Computational Complexity

Training data
Stationary GP
Kernel Shaping

3

2.5

2

1.5

1

0.5

0

y

−0.5

−1

0
x

1

Figure 2: Effect of outliers (in
black circles)

For one local model  the EM update equations have a computational complexity of O(N d) per EM
iteration  where d is the number input dimensions and N is the size of the training set. This efﬁciency
arises from the introduction of the hidden random variables zi  which allows hzii and Σzi|yi xi to
be computed in O(d) and avoids a d × d matrix inversion which would typically require O(d3).
Some nonstationary GP methods  e.g.  [5]  require O(N 3) + O(N 2) for training and prediction 
while other more efﬁcient stationary GP methods  e.g.  [17]  require O(M 2N ) + O(M 2) training
and prediction costs (where M << N is the number of pseudoinputs used in [17]). Our algorithm
requires O(N dIEM )  where IEM is the number of EM iterations—with a maximal cap of 1000
iterations used. Our algorithm also does not require any MCMC sampling as in [8  9]  making it
more appealing to real-time applications.

3 Extension to Gaussian Processes

We can apply the algorithm in section 2 not only to locally weighted learning with linear models  but
also to derive a nonstationary GP method. A GP is deﬁned by a mean and and a covariance function 
where the covariance function K captures dependencies between any two points as a function of

the corresponding inputs  i.e.  k (xi  xj) = cov(cid:0)f (xi)  f (x0

models use a stationary covariance function  where the covariance between any two points in the
training data is a function of the distances |xi − xj|  not of their locations. Stationary GPs perform
suboptimally for functions that have different properties in various parts of the input space (e.g. 
discontinuous functions) where the stationary assumption fails to hold. Various methods have been
proposed to specify nonstationary GPs. These include deﬁning a nonstationary Mat´ern covariance
function [5]  adopting a mixture of local experts approach [18  8  9] to use independent GPs to
cover data in different regions of the input space  and using multidimensional scaling to map a
nonstationary spatial GP into a latent space [19].

j)(cid:1)  where i  j = 1  ..  N. Standard GP

Given the data set D drawn from the function y = f (x)+  as previously introduced in section 2  we
propose an approach to specify a nonstationary covariance function. Assuming the use of a quadratic
negative exponential covariance function  the covariance function of a stationary GP is k(xi  xj) =
jm)2) + v0  where the hyperparameters {h1  h2  ...  hd  v0  v1} are
v2

m=1 hm(xim − x0

1 exp(−0.5Pd

1 exp(cid:16)−0.5Pd

m=1(xim − xjm)2 himhjm

(him+hjm)(cid:17)+v0  where him is the bandwidth of the local model

optimized. In a nonstationary GP  the covariance function could then take the form2 k(xi  xj) =
v2
centered at xim and hjm is the bandwidth of the local model centered at xjm. We learn ﬁrst the
values of {him}d
m=1 for all training data samples i = 1  ...  N using our proposed local kernel
shaping algorithm and then optimize the hyperparameters v0 and v1. To make a prediction for a test
sample xq  we learn also the values of {hqm}d
m=1  i.e.  the bandwidth of the local model centered at
xq. Importantly  since the covariance function of the GP is derived from locally constant models  we
learn with locally constant  instead of locally linear  polynomials. We use r = 1 for the weighting
kernel in order keep the degree of nonlinearity consistent with that in the covariance function (i.e. 
quadratic). Even though the weighting kernel used in the local kernel shaping algorithm is not a
quadratic negative exponential  it has a similar bell shape  but with a ﬂatter top and shorter tails.
Because of this  our augmented GP is an approximated form of a nonstationary GP. Nonetheless 
it is able to capture nonstationary properties of the function f without needing MCMC sampling 
unlike previously proposed nonstationary GP methods [8  9].

4 Experimental Results

4.1 Synthetic Data

First  we show our local kernel shaping algorithm’s bandwidth adaptation abilities on several syn-
thetic data sets  comparing it to a stationary GP and our proposed augmented nonstationary GP.
For the ease of visualization  we consider the following one-dimensional functions  similar to those
in [5]: i) a function with a discontinuity  ii) a spatially inhomogeneous function  and iii) a straight
line function. The data set for function i) consists of 250 training samples  201 test inputs (evenly
spaced across the input space) and output noise with σ2 = 0.3025; the data set for function ii) con-
sists of 250 training samples  101 test inputs and an output signal-to-noise ratio (SNR) of 10; and
the data set for function iii) has 50 training samples  21 test inputs and an output SNR of 100.

Fig. 3 shows the predicted outputs of a stationary GP  augmented nonstationary GP and the local
kernel shaping algorithm for data sets i)-iii). The local kernel shaping algorithm smoothes over
regions where a stationary GP overﬁts and yet  it still manages to capture regions of highly varying
curvature  as seen in Figs. 3(a) and 3(b). It correctly adjusts the bandwidths h with the curvature
of the function. When the data looks linear  the algorithm opens up the weighting kernel so that
all data samples are considered  as Fig. 3(c) shows. Our proposed augmented nonstationary GP
also can handle the nonstationary nature of the data sets as well  and its performance is quantiﬁed
in Table 1. Returning to our motivation to use these algorithms to obtain linearizations for learning
control  it is important to realize that the high variations from ﬁtting noise  as shown by the stationary
GP in Fig. 3  are detrimental for learning algorithms  as the slope (or tangent hyperplane  for high-
dimensional data) would be wrong.

Table 1 reports the normalized mean squared prediction error (nMSE) values for function i) and
function ii) data sets  averaged over 20 random data sets. Fig. 4 shows results of the local kernel
shaping algorithm and the proposed augmented nonstationary GP on the “real-world” motorcycle
data set [20] consisting of 133 samples (with 80 equally spaced input query points used for predic-
tion). We also show results from a previously proposed MCMC-based nonstationary GP method: an
alternate inﬁnite mixture of GP experts [9]. We can see that the augmented nonstationary GP and
the local kernel shaping algorithm both capture the leftmost ﬂatter region of the function  as well as
some of the more nonlinear and noisier regions after 30msec.

4.2 Robot Data

Next  we move on to an example application: learning an inverse kinematics model for a 3 degree-of-
freedom (DOF) haptic robot arm (manufactured by SensAble  shown in Fig. 5(a)) in order to control
the end-effector along a desired trajectory. This will allow us to verify that the kernel shaping algo-

2This is derived from the deﬁnition of K as a positive semi-deﬁnite matrix  i.e. where the integral is the

product of two quadratic negative exponentials—one with parameter him and the other with parameter hjm.

2

y

0

−2

Training data
Stationary GP
Aug GP
Kernel Shaping

2

1

0

y

−1

−4
−2

−1

0
x

1

2

−2

−1

0
x

1

2

2

1

0

y

−1

−2
−2

−1

0
x

1

2

w
xq

107

h

103

100
2

w

1

0
−2

w

1

0
−2

−1

0

x

1

(a) Function i)

−1

0

x

1

(b) Function ii)

106

h

100
2

w

1  

0  
−2

−1

0

x

1

(c) Function iii)

106

h
100

10−6
2

Figure 3: Predicted outputs using a stationary GP  our augmented nonstationary GP and local kernel
shaping. Figures on the bottom show the bandwidths learnt by local kernel shaping and the corre-
sponding weighting kernels (in dotted black lines) for input query points (shown in red circles).

rithm can successfully deal with a large  noisy real-world data set with outliers and non-stationary
properties—typical characteristics of most control learning problems.

We collected 60  000 data samples from the arm while it performed random sinusoidal movements
within a constrained box volume of Cartesian space. Each sample consists of the arm’s joint angles
q  joint velocities ˙q  end-effector position in Cartesian space x  and end-effector velocities ˙x. From
this data  we ﬁrst learn a forward kinematics model: ˙x = J(q) ˙q  where J(q) is the Jacobian matrix.
The transformation from ˙q to ˙x can be assumed to be locally linear at a particular conﬁguration q
of the robot arm. We learn the forward model using kernel shaping  building a local model around
each training point only if that point is not already sufﬁciently covered by an existing local model
(e.g.  having an activation weight of less than 0.2). Using insights into robot geometry  we localize
the models only with respect to q while the regression of each model is trained only on a mapping
from ˙q to ˙x—these geometric insights are easily incorporated as priors in the Bayesian model. This
procedure resulted in 56 models being built to cover the entire space of training data.
We artiﬁcially introduce a redundancy in our inverse kinematics problem on the 3-DOF arm by
specifying the desired trajectory (x  ˙x) only in terms of x  z positions and velocities  i.e.  the move-
ment is supposed to be in a vertical plane in front of the robot. Analytically  the inverse kinematics
equation is ˙q = J#(q) ˙x − α(I − J#J) ∂g
∂q   where J #(q) is the pseudo-inverse of the Jacobian. The
second term is an optimal solution to the redundancy problem  speciﬁed here by a cost function g
in terms of joint angles q. To learn a model for J#  we can reuse the local regions of q from the
forward model  where J# is also locally linear. The redundancy issue can be solved by applying
an additional weight to each data point according to a reward function [21]. In our case  the task is
speciﬁed in terms of { ˙x  ˙z}  so we deﬁne a reward based on a desired y coordinate  ydes  that we
2 h(k(ydes−y)− ˙y)2  where
would like to enforce as a soft constraint. Our reward function is g = e− 1
k is a gain and h speciﬁes the steepness of the reward. This ensures that the learnt inverse model
chooses a solution which produces a ˙y that pushes the y coordinate toward ydes. We invert each
forward local model using a weighted linear regression  where each data point is weighted by the
weight from the forward model and additionally weighted by the reward.

We test the performance of this inverse model (Learnt IK) in a ﬁgure-eight tracking task as shown
in Fig. 5(b). As seen  the learnt model performs as well as the analytical inverse kinematics solution
(IK)  with root mean squared tracking errors in positions and velocities very close to that of the

Table 1: Average normalized mean squared prediction error values  for a stationary GP model  our
augmented nonstationary GP  local kernel shaping—averaged over 20 random data sets.

Method

Stationary GP

Augmented nonstationary GP

Local Kernel Shaping

Function i)

0.1251 ± 0.013
0.0110 ± 0.0078
0.0092 ± 0.0068

Function ii)

0.0230 ± 0.0047
0.0212 ± 0.0067
0.0217 ± 0.0058

)
g
(
 
n
o

i
t

l

a
r
e
e
c
c
A

100

50

0

−50

−100

−150
0

Training Data
AiMoGPE
SingleGP

10

20

30

40

50

60

(a) Alternate inﬁnite mix. of GPs

Time (ms)
(A)

)
g
(
 
n
o

i
t

l

a
r
e
e
c
c
A

100

50

0

−50

−100

−150
0 

Training data
Aug GP
Stationary GP

10

20

30

Time (ms)

40

50

60

)
g
(
 
n
o

i
t

l

a
r
e
e
c
c
A

100

50

0

−50

−100

−150
0 

Training data
Kernel Shaping
Stationary GP

10

20

30

Time (ms)

40

50

60

(b) Augmented nonstationary GP

(c) Local Kernel Shaping

Figure 4: Motorcycle impact data set from [20]  with predicted results shown for our augmented
GP and local kernel shaping algorithms. Results from the alternate inﬁnite mixture of GP experts
(AiMoGPE) are taken from [9].

analytical solution. This demonstrates that kernel shaping is an effective learning algorithm for use
in robot control learning applications.

Applying any arbitrary nonlinear regression method (such as a GP) to the inverse kinematics problem
would  in fact  lead to unpredictably bad performance. The inverse kinematics problem is a one-to-
many mapping and requires careful design of a learning problem to avoid problems with non-convex
solution spaces [22]. Our suggested method of learning linearizations with a forward mapping
(which is a proper function)  followed by learning an inverse mapping within the local region of
the forward mapping  is one of the few clean approaches to the problem. Instead of using locally
linear methods  one could also use density-based estimation techniques like mixture models [23].
However  these methods must select the correct mode in order to arrive at a valid solution  and
this ﬁnal step may be computationally intensive or involve heuristics. For these reasons  applying
a MCMC-type approach or GP-based method to the inverse kinematics problem was omitted as a
comparison.

5 Discussion

We presented a full Bayesian treatment of nonparametric local multi-dimensional kernel adaptation
that simultaneously estimates the regression and kernel parameters. The algorithm can also be inte-
grated into nonlinear algorithms  offering a valuable and ﬂexible tool for learning. We show that our
local kernel shaping method is particularly useful for learning control  demonstrating results on an
inverse kinematics problem  and envision extensions to more complex problems with redundancy 

 

Desired
Analytical IK

−0.1 −0.05

0

0.05

0.1

)

m

(
 
z

0.2

0.1

0

−0.1

 

)

m

(
 
z

0.2

0.1

0

−0.1

 

 

Desired
Learnt IK

−0.1 −0.05

0

0.05

0.1

(a) Robot arm

x (m)
(b) Desired versus actual trajectories

x (m)

Figure 5: Desired versus actual trajectories for SensAble Phantom robot arm

e.g.  learning inverse dynamics models of complete humanoid robots. Note that our algorithm re-
quires only one prior be set by the user  i.e.  the prior on the output noise. All other biases are
initialized the same for all data sets and kept uninformative. In its current form  our Bayesian kernel
shaping algorithm is built for high-dimensional inputs due to its low computational complexity—
it scales linearly with the number of input dimensions. However  numerical problems may arise
in case of redundant and irrelevant input dimensions. Future work will address this issue through
the use of an automatic relevant determination feature. Other future extensions include an online
implementation of the local kernel shaping algorithm.

References

[1] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression.

In David S. Touretzky 
Michael C. Mozer  and Michael E. Hasselmo  editors  In Advances in Neural Information Processing
Systems 8  volume 8. MIT Press  1995.

[2] J. H. Friedman. A variable span smoother. Technical report  Stanford University  1984.
[3] T. Poggio and F. Girosi. Regularization algorithms for learning that are equivalent to multilayer networks.

Science  247:213–225  1990.

[4] J. Fan and I. Gijbels. Local polynomial modeling and its applications. Chapman and Hall  1996.
[5] C. J. Paciorek and M. J. Schervish. Nonstationary covariance functions for Gaussian process regression.

In Advances in Neural Information Processing Systems 16. MIT Press  2004.

[6] J. Fan and I. Gijbels. Data-driven bandwidth selection in local polynomial ﬁtting: Variable bandwidth

and spatial adaptation. Journal of the Royal Statistical Society B  57:371–395  1995.

[7] S. Schaal and C.G. Atkeson. Assessing the quality of learned local models.

In G. Tesauro J. Cowan
and J. Alspector  editors  Advances in Neural Information Processing Systems  pages 160–167. Morgan
Kaufmann  1994.

[8] C. E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian processes. In Advances in Neural

Information Processing Systems 14. MIT Press  2002.

[9] E. Meeds and S. Osindero. An alternative inﬁnite mixture of Gaussian process experts. In Advances in

Neural Information Processing Systems 17. MIT Press  2005.

[10] C. Atkeson and S. Schaal. Robot learning from demonstration. In Proceedings of the 14th international

conference on Machine learning  pages 12–20. Morgan Kaufmann  1997.

[11] C. Atkeson  A. Moore  and S. Schaal. Locally weighted learning. AI Review  11:11–73  April 1997.
[12] A. D’Souza  S. Vijayakumar  and S. Schaal. The Bayesian backﬁtting relevance vector machine.

Proceedings of the 21st International Conference on Machine Learning. ACM Press  2004.

In

[13] A. Gelman  J. Carlin  H.S. Stern  and D.B. Rubin. Bayesian Data Analysis. Chapman and Hall  2000.
[14] A. Dempster  N. Laird  and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm.

Journal of Royal Statistical Society. Series B  39(1):1–38  1977.

[15] Z. Ghahramani and M.J. Beal. Graphical models and variational methods. In D. Saad and M. Opper 

editors  Advanced Mean Field Methods - Theory and Practice. MIT Press  2000.

[16] T. S. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods. Statistics and

Computing  10:25–37  2000.

[17] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural

Information Processing Systems 18. MIT Press  2006.

[18] V. Tresp. Mixtures of Gaussian processes. In Advances in Neural Information Processing Systems 13.

MIT Press  2000.

[19] A. M. Schmidt and A. O’Hagan. Bayesian inference for nonstationary spatial covariance structure via

spatial deformations. Journal of Royal Statistical Society. Series B  65:745–758  2003.

[20] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve

ﬁtting. Journal of Royal Statistical Society. Series B  47:1–52  1985.
[21] J. Peters and S. Schaal. Learning to control in operational space.

Research  27:197–212  2008.

International Journal of Robotics

[22] M. I. Jordan and D. E. Rumelhart. Internal world models and supervised learning. In Machine Learning:

Proceedings of Eighth Internatinoal Workshop  pages 70–85. Morgan Kaufmann  1991.

[23] Z. Ghahramani. Solving inverse problems using an EM approach to density estimation. In Proceedings

of the 1993 Connectionist Models summer school  pages 316–323. Erlbaum Associates  1994.

,Josip Djolonga
Stefanie Jegelka
Sebastian Tschiatschek
Andreas Krause
Daniel Cullina
Arjun Nitin Bhagoji
Prateek Mittal