2011,SpaRCS: Recovering low-rank and sparse matrices from compressive measurements,We consider the problem of recovering a matrix $\mathbf{M}$ that is the sum of a low-rank matrix $\mathbf{L}$ and a sparse matrix $\mathbf{S}$ from a small set of linear measurements of the form $\mathbf{y} = \mathcal{A}(\mathbf{M}) = \mathcal{A}({\bf L}+{\bf S})$.  This model subsumes three important classes of signal recovery problems:  compressive sensing  affine rank minimization  and robust principal component analysis.  We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it.  SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms  including exponential convergence and efficient implementation.  Simulation results with video compressive sensing  hyperspectral imaging  and robust matrix completion data sets demonstrate both the accuracy and efficacy of the algorithm.,SpaRCS: Recovering Low-Rank and Sparse Matrices

from Compressive Measurements

Andrew E. Waters  Aswin C. Sankaranarayanan  Richard G. Baraniuk

Rice University

{andrew.e.waters  saswin  richb}@rice.edu

Abstract

We consider the problem of recovering a matrix M that is the sum of a low-rank
matrix L and a sparse matrix S from a small set of linear measurements of the
form y = A(M) = A(L + S). This model subsumes three important classes
of signal recovery problems: compressive sensing  afﬁne rank minimization  and
robust principal component analysis. We propose a natural optimization problem
for signal recovery under this model and develop a new greedy algorithm called
SpaRCS to solve it. Empirically  SpaRCS inherits a number of desirable proper-
ties from the state-of-the-art CoSaMP and ADMiRA algorithms  including expo-
nential convergence and efﬁcient implementation. Simulation results with video
compressive sensing  hyperspectral imaging  and robust matrix completion data
sets demonstrate both the accuracy and efﬁcacy of the algorithm.

1

Introduction

The explosion of digital sensing technology has unleashed a veritable data deluge that has pushed
current signal processing algorithms to their limits. Not only are traditional sensing and processing
algorithms increasingly overwhelmed by the sheer volume of sensor data  but storage and transmis-
sion of the data itself is also increasingly prohibitive without ﬁrst employing costly compression
techniques. This reality has driven much of the recent research on compressive data acquisition  in
which data is acquired directly in a compressed format [1]. Recovery of the data typically requires
ﬁnding a solution to an undetermined linear system  which becomes feasible when the underly-
ing data possesses special structure. Within this general paradigm  three important problem classes
have received signiﬁcant recent attention: compressive sensing  afﬁne rank minimization  and robust
principal component analysis (PCA).
Compressive sensing (CS): CS is concerned with the recovery a vector x that is sparse in some
transform domain [1]. Data measurements take the form y = A(x)  where A is an underdetermined
linear operator. To recover x  one would ideally solve
(1)
where kxk0 is the number of non-zero components in x. This problem formulation is non-convex 
CS recovery is typically accomplished either via convex relaxation or greedy approaches.
Afﬁne rank minimization: The CS concept extends naturally to low-rank matrices. In the afﬁne
rank minimization problem [14  23]  we observe the linear measurements y = A(L)  where L is a
low-rank matrix. One important sub-problem is that of matrix completion [3  5  22]  where A takes
the form of a sampling operator. To recover L  one would ideally solve
(2)
subject to y = A(L).
As with CS  this problem is non-convex and so several algorithms based on convex relaxation and
greedy methods have been developed for ﬁnding solutions.

subject to y = A(x) 

min kxk0

min rank(L)

1

Robust PCA: In the robust PCA problem [2 8]  we wish to decompose a matrix M into a low-rank
matrix L and a sparse matrix S such that M = L + S. This problem is known to have a stable
solution provided L and S are sufﬁciently incoherent [2]. To date  this problem has been studied
only in the non-compressive setting  i.e  when M is fully available. A variety of convex relaxation
methods have been proposed for solving this case.
The work of this paper stands at the intersection of these three problems. Speciﬁcally  we aim to
recover the entries of a matrix M in terms of a low-rank matrix L and sparse matrix S from a small
set of compressive measurements y = A(L + S). This problem is relevant in several application set-
tings. A ﬁrst application is the recovery of a video sequence obtained from a static camera observing
a dynamic scene under changing illumination. Here  each column of M corresponds to a vectorized
image frame of the video. The changing illumination has low-rank properties  while the foreground
innovations exhibit sparse structures [2]. In such a scenario  neither sparse nor low-rank models
are individually sufﬁcient for capturing the underlying information of the signal. Models that com-
bine low-rank and sparse components  however  are well suited for capturing such phenomenon. A
second application is hyperspectral imaging  where each column of M is the vectorized image of a
particular spectral band; a low-rank plus sparse model arises naturally due to material properties [7].
A third application is robust matrix completion [11]  which can be cast as a compressive low-rank
and sparse recovery problem.
The natural optimization problem that unites the above three problem classes above is
subject to rank(L)  r  kvec(S)k0  K.

(3)
The main contribution of this paper is a novel greedy algorithm for solving (P1)  which we dub
SpaRCS for SPArse and low Rank decomposition via Compressive Sensing. To the best of our
knowledge  we are the ﬁrst to propose a computationally efﬁcient algorithm for solving a prob-
lem like (P1). SpaRCS combines the best aspects of CoSaMP [20] for sparse vector recovery and
ADMiRA [17] for low-rank matrix recovery.

(P1) min ky  A(L + S)k2

2 Background

Here we introduce the relevant background information regarding signal recovery from CS measure-
ments  where our deﬁnition of signal is broadened to include both vectors and matrices. We further
provide background on incoherency between low-rank and sparse matrices.

Restricted isometry and rank-restricted isometry properties: Signal recovery for a K-sparse
vector from CS measurements is possible when the measurement operator A obeys the so-called
restricted isometry property (RIP) [4] with constant K
(4)
This property implies that the information in x is nearly preserved after being measured by A. Anal-
ogous to CS  it has been shown that a low-rank matrix can be recovered from a set of CS measure-
ments when the measurement operator A obeys the rank-restricted isometry (RRIP) property [23]
with constant ⇤r
(5)

2  (1 + K)kxk2

2  8kxk0  K.

(1  K)kxk2

2  kA(x)k2

F   8 rank(L)  r.

(1  ⇤r )kLk2

F  kA(L)k2

F  (1 + ⇤r )kLk2

Recovery algorithms: Recovery of sparse vectors and low-rank matrices can be accomplished
when the measurement operator A satisﬁes the appropriate RIP or RRIP condition. Recovery algo-
rithms typically fall into one of two broad classes: convex optimization and greedy iteration. Convex
optimization techniques recast (1) or (2) in a form that can be solved efﬁciently using convex pro-
gramming [2  27]. In the case of CS  the `0 norm is relaxed to the `1 norm; for low-rank matrices 
the rank operator is relaxed to the nuclear norm.
In contrast  greedy algorithms [17  20] operate iteratively on the signal measurements  constructing
a basis for the signal and attempting signal recovery restricted to that basis. Compared to convex
approaches  these algorithms often have superior speed and scale better to large problems. We
highlight the CoSaMP algorithm [20] for sparse vector recovery and the ADMiRA algorithm [17]
for low-rank matrix recovery in this paper. Both algorithms have strong convergence guarantees
when the measurement operator A satisﬁes the appropriate RIP or RRIP condition  most notably
exponential convergence to the true signal.

2

Matrix Incoherency: For matrix decomposition problems such as the Robust PCA problem or the
problem deﬁned in (3) to have unique solutions  there must exist a degree of incoherence between
the low-rank matrix L and the sparse matrix S. It is known that the decomposition of a matrix into
its low-rank and sparse components makes sense only when the low-rank matrix is not sparse and 
similarly  when the sparse matrix is not low-rank. A simple deterministic condition can be found in
the work by Chandrasekaran  et al [9].
For our purposes  we assume the following model for non-sparse low rank matrices.
Deﬁnition 2.1 (Uniformly bounded matrix [5]) An N ⇥ N matrix L of rank r is uniformly
bounded if its singular vectors {uj  vj  1  j  r} obey

k=1 u2

µB

kujk1 kvjk1 pµB/N  

with µB = O(1)  where kxk1 denotes the largest entry in magnitude of x.
When µB is small (note that µB  1)  this model for the low-rank matrix L ensures that its singular
vectors are not sparse. This can be seen in the case of the a singular vector u by noting that 1 =
2 = PN
.
1  N
kuk2
Thus  µB controls the sparsity of the matrix L by bounding the sparsity of its singular vectors.
A sufﬁcient model for a sparse matrix that is not low-rank is to assume that the support set ⌦ is
uniform. As shown in the work of Candes  et al [2] this model is equivalent to deﬁning the sparse
support set ⌦ = {(i  j) : i j = 1} with each i j being an i.i.d. Bernoulli with sufﬁciently small
parameter ⇢S.

. Rearranging terms enables us to write kuk0  1
kuk2

k  kuk0kuk2
1

3 SpaRCS: CS recovery of low-rank and sparse matrices

We now present the SpaRCS algorithm to solve (P1) and disucss its empirical properties. Assume
that we are interested in a matrix M 2 RN1⇥N2 such that M = L + S  with rank(L)  r  L uni-
formly bounded with constant µB  and kSk0  K with support distributed uniformly. Further as-
sume that a known linear operator A : RN1⇥N2 ! Rp provides us with p compressive measurements
y of M. Let A⇤ denote the adjoint of the operator A and  given the index set T ⇢ {1  . . .   N1N2}  let
A|T denote the restriction of the operator to T . Given y = A(M)+e  where e denotes measurement
noise  our goal is to estimate a low rank matrixbL and a sparse matrixbS such that y ⇡ A(bL +bS).

3.1 Algorithm

SpaRCS iteratively estimates L and S; the estimation of L is closely related to ADMiRA [17]  while
the estimation of S is closely related to CoSaMP [20]. At each iteration  SpaRCS computes a signal
proxy and then proceeds through four steps to update its estimates of L and S. These steps are laid
out in Algorithm 1. We use the notation supp(X; K) to denote the largest K-term support set of the
matrix X. This forms a natural basis for sparse signal approximation. We further use the notation
svd(X; r) to denote computation of the rank-r singular value decomposition (SVD) of X and the
arrangement of its singular vectors into a set of up to r rank-1 matrices. This set of rank-1 matrices
serve as a natural basis for approximating uniformly bounded low-rank matrices.

3.2 Performance characterization

Empirically  SpaRCS produces a series of estimates bLk and bSk that converge convergence expo-

nentially towards the true values L and S. This performance is inhereted largely from the behavior
of the CoSaMP and ADMiRA algorithms with one noteworthy modiﬁcation. The key difference is
that  for SpaRCS  the sparse and low-rank estimation problems are coupled. While CoSaMP and
ADMiRA operate solely in the presence of the measurement noise  SpaRCS must estimate L in
the presence of the residual error of S  and vice-versa. Proving convergence for the algorithm in
the presence of the additional residual terms is non-trivial; simply lumping these additional residual
errors together with the measurement noise e is insufﬁcient for analysis.

As a concrete example  consider the support identiﬁcation step b S supp(P; 2K)  with

P = A⇤(wk1) = A⇤(A(S bSk1) + A(L bLk1) + e) 

3

Support merger:

Algorithm 1: (bL bS) = SpaRCS (y A A⇤  K  r  ✏)
Initialization: k 1 bL0 0 bS0 0  L ;  S ;  w0 y
while kwk1k2  ✏ do
Compute signal proxy:
P A⇤(wk1)
Support identiﬁcation:
b L svd(P; 2r); b S supp(P; 2K)
e L b LS L; e S b SS S
BL e †L(y  A(bSk1)); BS e †S(y  A(bLk1))
(bLk   L) svd(BL; r); (bSk   S) supp(BS; K)
wk y  A(bLk +bSk)

Least squares estimation:

Support pruning:

Update residue:

k k + 1

end

bL =bLk1;bS =bSk1

that estimates the support set of S. CoSaMP relies on high correlation between supp(P; 2K) and

supp(S bSk1; 2K); to achieve the same in SpaRCS  (L bLk1) must be well behaved.

We are currently preparing a full theoretical characterization of the SpaRCS algorithm along with
the necessary conditions that guarantee this exponential convergence property. We reserve the pre-
sentation of the convergence proof for an extended version of this work.

Phase transition: The empirical performance of SpaRCS can be charted using phase transition
plots  which predicts sufﬁcient and necessary conditions on its success/failure. Figure 1 shows
phase transition results on a problem of size N1 = N2 = 512 for various values of p  r  and K. As
expected  SpaRCS degrades gracefully as we decrease p or increase r and K.

Figure 1: Phase transitions for a recovery problem of size N1 = N2 = N = 512. Shown are
aggregate results over 20 Monte-Carlo runs at each speciﬁcation of r  K  and p. Black indicates
recovery failure  while white indicates recovery success.

Computational cost: SpaRCS is highly computationally efﬁcient and scales well as N1  N2 grow
large. The largest computational cost is that of computing the two truncated SVDs per iteration. The
SVDs can be performed efﬁciently via the Lanczos algorithm or similar method. The least squares
estimation can be solved efﬁciently using conjugate gradient or Richardson iterations. Support es-
timation for the sparse vector merely entails sorting the signal proxy magnitudes and choosing the
largest 2K elements.

4

r=5 r=10 r=15 r=20 r=25 Figure 2 compares the performance of SpaRCS with two alternate recovery algorithms. We imple-
ment CS versions of the IT [18] and APG [19] algorithms  which solve the problems

min ⌧ (kLk⇤ + kvec(S)k1) +

1
2kLk2

F +

1
2kSk2

F s.t. y = A(L + S)

and

min kLk⇤ + kvec(S)k1 s.t. y = A(L + S) 

respectively. We endeavor to tune the parameters of these algorithms (which we refer to as CS
IT and CS APG  respectively) to optimize their performance. Details of our implementation can
be found in [26].
In all experiments  we consider matrices of size N ⇥ N with rank(L) = 2
and kSk0 = 0.02N 2 and use permuted noiselets [12] for the measurement operator A. As a ﬁrst
experiment  we generate convergence plots for matrices with N = 128 and vary the measurement
ratio p/N 2 from 0.05 to 0.5. We then recoverbL andbS and measure the recovered signal to noise
ratio (RSNR) forcM = bL +bS via 20 log10⇣
kMbLbSkF⌘. These results are displayed in Figure
2(a)  where we see that SpaRCS provides the best recovery. As a second experiment  we vary the
problem size N 2 {128  256  512  1024} while holding the number of measurements constant at
p = 0.2N 2. We measure the recovery time required by each algorithm to reach a residual error
kyA(bL+bS)k2
 5 ⇥ 104. These results are displayed in Figure 2(b)  which demonstrate that

SpaRCS converges signiﬁcantly faster than the two other recovery methods.

kMkF

kyk2

40

30

20

10

)

B
d
(
 

R
N
S
R

 
0
0.1

0.2

0.3
p/N2

 

)
c
e
s
(
 

i

 

e
m
T
e
c
n
e
g
r
e
v
n
o
C

SpaRCS
CS APG
CS IT
0.4

0.5

(a) Performance

105
104
103
102
101

 
7

SpaRCS
CS APG
CS IT

8

9

log2(N)

(b) Timing plot

 

10

Figure 2: Performance and run-time comparisons between SpaRCS  CS IT  and CS APG. Shown are
average results over 10 Monte-Carlo runs for problems of size N1 = N2 = N with rank(L) = 2
(a) Performance for a problem with N = 128 for various values of the
and kSk0 = 0.02N 2.
measurement ratio p/N 2. SpaRCS exhibits superior recovery over the alternate approaches. (b)
Timing plot for problems of various sizes N. SpaRCS converges in time several orders of magnitude
faster than the alternate approaches.

4 Applications

We now present several experiments that validate SpaRCS and showcase its performance in several
applications. In all experiments  we use permuted noiselets for the measurement operator A; these
provide both a fast transform as well as save memory  since we do not have to store A explicitly.
Video compressive sensing: The video CS problem is concerned with recovering multiple image
frames of a video sequence from CS measurements [6 21 24]. We consider a 128⇥ 128⇥ 201 video
sequence consisting of a static background with a number of people moving in the foreground. We
aim to not only recover the original video but also separate the background and foreground. We
resize the data cube into a 1282 ⇥ 201 matrix M  where each column corresponds to a (vectorized)
image frame. The measurement operator A operates on each column of M independently  simulat-
ing acquisition using a single pixel camera [13]. We acquire p = 0.15 ⇥ 1282 measurements per
image frame. We recover with SpaRCS using r = 1 and K = 20 000. The results are displayed in
Figure 3  where it can be seen that SpaRCS accurately estimates and separates the low-rank back-
ground and the sparse foreground. Figure 4 shows recovery results on a more challenging sequence

5

(a)

(b)

(c)

Figure 3: SpaRCS recovery results on a 128 ⇥ 128 ⇥ 201 video sequence. The video sequence is
reshaped into an N1 ⇥ N2 matrix with N1 = 1282 and N2 = 201. (a) Ground truth for several
frames. (b) Estimated low-rank component L. (c) Estimated sparse component S. The recovery
SNR is 31.2 dB at the measurement ratio p/(N1N2) = 0.15. The recovery is accurate in spite of the
measurement operator A working independently on each frame.

Parameters
Col-only measurement matrix
M per col = 2458
Overall K = 49398
Rank = 1 
Rho = 0.15

Details
128x128x201 Video
Compression 6.6656
SNR = 31.1637

(a)

(b)

Figure 4: SpaRCS recovery results on a 64 ⇥ 64 ⇥ 234 video sequence. The video sequence is
reshaped into an N1⇥N2 matrix with N1 = 642 and N2 = 234. (a) Ground truth for several frames.
(b) Recovered frames. The recovery SNR is 23.9 dB at the measurement ratio of p/(N1N2) = 0.33.
The recovery is accurate in spite of the changing illumination conditions.

with changing illumination. In contrast to SpaRCS  existing video CS algorithms do not work well
with dramatically changing illumination.

Hyperspectral compressive sensing: Low-rank/sparse decomposition has an important physical
relevance in hyperspectral imaging [7]. Here we consider a hyperspectral cube  which contains a
vector of spectral information at each image pixel. A measurement device such as [25] can provide
compressive measurements of such a hyperspectral cube. We employ SpaRCS on a hyperspectral
cube of size 128 ⇥ 128 ⇥ 128 rearranged as a matrix of size 1282 ⇥ 128 such that each column
corresponds to a different spectral band. Figure 5 demonstrates recovery using p = 0.15 ⇥ 1282 ⇥
128 total measurements of the entire data cube with r = 8  K = 3000. SpaRCS performs well
in terms of residual error (Figure 5(c)) despite the number of rows being much larger than the
number of columns. Figure 5(d) emphasizes the utility the sparse component. Using only a low-
rank approximation (corresponding to traditional PCA) causes a signiﬁcant increase in residual error
over what is achieved by SpaRCS.

Parameter mismatch:
In Figure 6  we analyze the inﬂuence of incorrect selection of the parame-
ters r using the hyperspectral data as an example. We plot the recovered SNR that can be obtained
at various levels of the measurement ratio p/(N1N2) for both the case of r = 8 and r = 4. There
are interesting tradeoffs associated with the choice of parameters. Larger values of r and K enable
better approximation to the unknown signals. However  by increasing r and K  we also increase the
number of independent parameters in the problem  which is given by (2 max(N1  N2)r r2 + 2K).
An empirical rule-of-thumb for greedy recovery algorithms is that the number of measurements p
should be 2–5 times the number of independent parameters. Consequently  there exists a tradeoff
between the values of r  K  and p to ensure stable recovery.

6

(a)

(b)

(c)

(d)

Figure 5: SpaRCS recovery results on a 128⇥ 128⇥ 128 hyperspectral data cube. The hyperspectral
data is reshaped into an N1 ⇥ N2 matrix with N1 = 1282 and N2 = 128. Each image pane
corresponds to a different spectral band. (a) Ground truth. (b) Recovered images. (c) Residual
error using both the low-rank and sparse component. (d) Residual error using only the low-rank
component. The measurement ratio is p/(N1N2) = 0.15.

Datasize: 128x128x128 == 128^2 x 128 matrix.
Measurement matrix takes inner product with the WHOLE data matrix.
Rank = 4. Measurement = 15%. K = 3000. Wavelet transformed data– db4.

Recons SNR with Xhat = 27.3 dB
Recons SNR with Lhat = 21.9 dB

Figure 6: Hyperspectral data recovery for various values of the rank r of the low-rank matrix L. The
data used is the same as in Figure 5. (a) r = 1  SNR = 12.81 dB. (b) r = 2  SNR = 19.42 dB. (c)
r = 4  SNR = 27.46 dB. (d) Comparison of compression ratio (N1N2)/p and recovery SNR using
r = 4 and r = 8. All results were obtained with K = 3000.

Robust matrix completion: We apply SpaRCS to the robust matrix completion problem [11]

subject to L⌦ + s = y

minkLk⇤ + ksk1

(6)
where s models outlier noise and ⌦ denotes the set of observed entries. This problem can be cast as
a compressive low-rank and sparse matrix recovery problem by using a sparse matrix S in place of
the outlier noise s and realizing that the support of S is a subset of ⌦. This enables recovery of both
L and S from samples of their sum L + S.
Matrix completion under outlier noise [10 11] has received some attention and  in many ways  is the
work that is closest to this paper. There are  however  several important distinctions. Chen et al. [11]
analyze the convex problem of (6) to provide performance guarantees. Yet  convex optimization
methods often do not scale well with the size of the problem. SpaRCS  by contrast  is computation-
ally efﬁcient and does scale well as the problem size increases. Furthermore  [10] is tied to the case
when A is a sampling operator; it is not immediately clear whether this analysis can extend to the
more general case of (P1)  where the sparse component cannot be modeled as outlier noise in the
measurements.

7

(d) 510152015202530N2/pRSNR (dB) r = 4r = 8(c) (b) (a) )

B
d
(
 

R
N
S
R

80
60
40
20
0
−20

 
10−3

 

CVX
SpaRCS
CS IT
OptSpace

10−2

K/p

10−1

(a) Performance

100

)
e
m

i
t
 
n
o
i
t
u
c
e
x
e
(
g
o

l

2.5

2

1.5

1

0.5

0

−0.5

−1

 

1/1000 1/100 1/50

 

SpaRCS
CS IT
CVX
OptSpace

1/5

1/4

1/3

1/25

1/10

K/p

(b) Timing plot

Figure 7: Comparison of several algorithms for the robust matrix completion problem. (a) RSNR
averaged over 10 Monte-Carlo runs for an N ⇥ N matrix completion problem with N = 128 
r = 1  and p/N 2 = 0.2. Non-robust formulations  such OptSpace  fail. SpaRCS acheives perfor-
mance close to that of the convex solver (CVX). (b) Comparison of convergence times for the various
algorithms. SpaRCS converges in only a fraction of the time required by the other algorithms.

In our robust matrix completion experiments we compare SpaRCS with CS SVT  OptSpace [16] (a
non-robust matrix completion algorithm)  and a convex solution using CVX [15]. Figure 7 shows
the performance of these algorithms. OptSpace  being non-robust  fails as expected. The accuracy
of SpaRCS is closest to that of CVX  although the convergence time of SpaRCS is several orders of
magnitude faster.

5 Conclusion

We have considered the problem of recovering low-rank and sparse matrices given only a few linear
measurements. Our proposed greedy algorithm  SpaRCS  is both fast and accurate even for large
matrix sizes and enjoys strong empirical performance in its convergence to the true solution. We
have demonstrated the applicability of SpaRCS to video compressive sensing  hyperspectral imag-
ing  and robust matrix completion.
There are many avenues for future work. Model-based extensions of SpaRCS are important di-
rections. Both low-rank and sparse matrices exhibit rich structure in practice  including low-rank
Hankel matrices in system identiﬁcation and group sparsity in background subtraction. The use
of models could signiﬁcantly enhance the performance of the algorithm. This would be especially
useful in applications such as video CS  where the measurement operator is typically constrained to
operate on each image frame individually.

Acknowledgements

This work was partially supported by the grants NSF CCF-0431150  CCF-0728867  CCF-0926127 
CCF-1117939  ARO MURI W911NF-09-1-0383  W911NF-07-1-0185  DARPA N66001-11-1-
4090  N66001-11-C-4092  N66001-08-1-2065  AFOSR FA9550-09-1-0432  and LLNL B593154.
Additionally  the authors wish to thank Prof. John Wright for his helpful comments and corrections
to a previous version of this manuscript.

8

References
[1] E. J. Cand`es. Compressive sampling. In Intl. Cong. of Math.  Madrid  Spain  Aug. 2006.
[2] E. J. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? J. ACM  58(1):1–37 

2009.

[3] E. J. Cand`es and Y. Plan. Matrix completion with noise. Proc. IEEE  98(6):925–936  2010.
[4] E. J. Cand`es and J. Romberg. Quantitative robust uncertainty principles and optimally sparse decomposi-

tions. Found. Comput. Math.  6(2):227–254  2006.

[5] E.J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans.

on Info. Theory  56(5):2053–2080  2010.

[6] V. Cevher  A. C. Sankaranarayanan  M. Duarte  D. Reddy  R. G. Baraniuk  and R. Chellappa. Compressive

sensing for background subtraction. In European Conf. Comp. Vision  Marseilles  France  Oct. 2008.

[7] A. Chakrabarti and T. Zickler. Statistics of Real-World Hyperspectral Images. In IEEE Int. Conf. Comp.

Vis.  Colorado Springs  CO  June 2011.

[8] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky. Sparse and low-rank matrix decompo-

sitions. In Allerton Conf. on Comm.  Contr.  and Comp.  Monticello  IL  Sep. 2009.

[9] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A.S. Willsky. Rank-sparsity incoherence for matrix

decomposition. Arxiv preprint arXiv:0906.2220  2009.

[10] Y. Chen  A. Jalali  S. Sanghavi  and C. Caramanis. Low-rank matrix recovery from errors and erasures.

Arxiv preprint arXiv:1104.0354  2011.

[11] Y. Chen  H. Xu  C. Caramanis  and S. Sanghavi. Robust matrix completion with corrupted columns. Arxiv

preprint arXiv:1102.2254  2011.

[12] R. Coifman  F. Geshwind  and Y. Meyer. Noiselets. Appl. Comput. Harmon. Anal.  10:27–44  2001.
[13] M. F. Duarte  M. A. Davenport  D. Takhar  J. N. Laska  T. Sun  K. F. Kelly  and R. G. Baraniuk. Single

pixel imaging via compressive sampling. IEEE Signal Processing Mag.  25(2):83–91  2008.

[14] M. Fazel  E. Cand`es  B. Recht  and P. Parrilo. Compressed sensing and robust recovery of low rank

matrices. In Asilomar Conf. Signals  Systems  and Computers  Paciﬁc Grove  CA  Nov. 2008.

[15] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming  version 1.21.

http://cvxr.com/cvx  Apr. 2011.

[16] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. J. Mach. Learn. Res. 

11:2057–2078  2010.

[17] K. Lee and Y. Bresler. Admira: Atomic decomposition for minimum rank approximation. IEEE Trans.

on Info. Theory  56(9):4402–4416  2010.

[18] Z. Lin  M. Chen  L. Wu  and Y. Ma. The augmented lagrange multiplier method for exact recovery
of corrupted low-rank matrices. Technical report  University of Illinois at Urbana-Champaign  Urbana-
Champaign  IL.

[19] Z. Lin  A. Ganesh  J. Wright  L. Wu  M. Chen  and Y. Ma. Fast convex optimization algorithms for
exact recovery of a corrupted low-rank matrix. In Intl. Workshop on Comp. Adv. in Multi-Sensor Adapt.
Processing  Aruba  Dutch Antilles  Dec. 2009.

[20] D. Needell and J.A. Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples.

Appl. Comput. Harmon. Anal.  26(3):301–321  2009.

[21] J. Y. Park and M. B. Wakin. A multiscale framework for compressive sensing of video. In Picture Coding

Symp.  Chicago  IL  May 2009.

[22] B. Recht. A simpler approach to matrix completion. J. Mach. Learn. Res.  posted Oct. 2009  to appear.
[23] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed minimum rank solutions of linear matrix equations via

nuclear norm minimization. SIAM Rev.  52(3):471–501  2010.

[24] A. C. Sankaranarayanan  P. Turaga  R. G. Baraniuk  and R. Chellappa. Compressive acquisition of dy-

namic scenes. In European Conf. Comp. Vision  Crete  Greece  Sep. 2010.

[25] T. Sun and K. Kelly. Compressive sensing hyperspectral imager. In Comput. Opt. Sensing and Imaging 

San Jose  CA  Oct. 2009.

[26] A. E. Waters  A. C. Sankaranarayanan  and R. G. Baraniuk. SpaRCS: Recovering low-rank and sparse

matrices from compressive measurements. Technical report  Rice University  Houston  TX  2011.

[27] W. Yin  S. Osher  D. Goldfarb  and J. Darbon. Bregman iterative algorithms for `1-minimization with

applications to compressed sensing. SIAM J. Imag. Sci.  1(1):143–168  2008.

9

,Karthika Mohan
Judea Pearl
Jin Tian
Francesco Orabona