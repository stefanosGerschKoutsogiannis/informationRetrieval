2014,Mind the Nuisance: Gaussian Process Classification using Privileged Noise,The learning with privileged information setting has recently attracted a lot of attention within the machine learning community  as it allows the integration of additional knowledge into the training process of a classifier  even when this comes in the form of a data modality that is not available at test time. Here  we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is  in contrast to the standard GPC setting  the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise  called GPC+  improves over a standard GPC without privileged knowledge  and also over the current state-of-the-art SVM-based method  SVM+. Moreover  we show that advanced neural networks and deep learning methods can be compressed as privileged information.,Mind the Nuisance: Gaussian Process
Classiﬁcation using Privileged Noise

Daniel Hern´andez-Lobato

Universidad Aut´onoma de Madrid

Madrid  Spain

daniel.hernandez@uam.es

Kristian Kersting

TU Dortmund

Dortmund  Germany

Viktoriia Sharmanska

IST Austria

Klosterneuburg  Austria
vsharman@ist.ac.at

Christoph H. Lampert

IST Austria

Klosterneuburg  Austria

first.last@cs.tu-dortmund.de

chl@ist.ac.at

Novi Quadrianto

SMiLe CLiNiC  University of Sussex

Brighton  United Kingdom

n.quadrianto@sussex.ac.uk

Abstract

The learning with privileged information setting has recently attracted a lot of at-
tention within the machine learning community  as it allows the integration of ad-
ditional knowledge into the training process of a classiﬁer  even when this comes
in the form of a data modality that is not available at test time. Here  we show
that privileged information can naturally be treated as noise in the latent function
of a Gaussian process classiﬁer (GPC). That is  in contrast to the standard GPC
setting  the latent function is not just a nuisance but a feature: it becomes a natural
measure of conﬁdence about the training data by modulating the slope of the GPC
probit likelihood function. Extensive experiments on public datasets show that the
proposed GPC method using privileged noise  called GPC+  improves over a stan-
dard GPC without privileged knowledge  and also over the current state-of-the-art
SVM-based method  SVM+. Moreover  we show that advanced neural networks
and deep learning methods can be compressed as privileged information.

1

Introduction

Prior knowledge is a crucial component of any learning system as without a form of prior knowl-
edge learning is provably impossible [1]. Many forms of integrating prior knowledge into machine
learning algorithms have been developed: as a preference of certain prediction functions over others 
as a Bayesian prior over parameters  or as additional information about the samples in the training
set used for learning a prediction function. In this work  we rely on the last of these setups  adopting
Vapnik and Vashist’s learning using privileged information (LUPI)  see e.g. [2  3]: we want to learn
a prediction function  e.g. a classiﬁer  and in addition to the main data modality that is to be used for
prediction  the learning system has access to additional information about each training example.
This scenario has recently attracted considerable interest within the machine learning community
because it reﬂects well the increasingly relevant situation of learning as a service: an expert trains a
machine learning system for a speciﬁc task on request from a customer. Clearly  in order to achieve
the best result  the expert will use all the information available to him or her  not necessarily just the

1

information that the system itself will have access to during its operation after deployment. Typical
scenarios for learning as a service include visual inspection tasks  in which a classiﬁer makes real-
time decisions based on the input from its sensor  but at training time  additional sensors could be
made use of  and the processing time per training example plays less of a role. Similarly  a classiﬁer
built into a robot or mobile device operates under strong energy constraints  while at training time 
energy is less of a problem  so additional data can be generated and made use of. A third scenario is
when the additional data is conﬁdential  as e.g. in health care applications. Speciﬁcally  a diagnosis
system may be improved when more information is available at training time  e.g.  speciﬁc blood
tests  genetic sequences  or drug trials  for the subjects that form the training set. However  the same
data may not be available at test time  as obtaining it could be impractical  unethical  or illegal.
We propose a novel method for using privileged information based on the framework of Gaussian
process classiﬁers (GPCs). The privileged data enters the model in form of a latent variable  which
modulates the noise term of the GPC. Because the noise is integrated out before obtaining the ﬁnal
model  the privileged information is only required at training time  not at prediction time. The most
interesting aspect of the proposed model is that by this procedure  the inﬂuence of the privileged
information becomes very interpretable: its role is to model the conﬁdence that the GPC has about
any training example  which can be directly read off from the slope of the probit likelihood. Instances
that are easy to classify by means of their privileged data cause a faster increasing probit  which
means the GP trusts the training example and tried to ﬁt it well. Instances that are hard to classify
result in a slowly increasing slope  so that the GPC considers them less reliable and does not put a
lot of effort in ﬁtting their label well. Our experiments on multiple datasets show that this procedure
leads not just to more interpretable models  but also to better prediction accuracy.
Related work: The LUPI framework was originally proposed by Vapnik and Vashist [2]  inspired
by a thought-experiment: when training a soft-margin SVM  what if an oracle would provide us
with the optimal values of the slack variables? As it turns out  this would actually provably reduce
the amount of training data needed  and consequently  Vapnik and Vashist proposed the SVM+
classiﬁer that uses privileged data to predict values for the slack variables  which led to improved
performance on several categorisation tasks and found applications  e.g.  in ﬁnance [4]. This setup
was subsequently improved  by a faster training algorithm [5]  better theoretical characterisation [3] 
and it was generalised  e.g.  to the learning to rank setting [6]  clustering [7]  metric learning [8] and
multi-class data classiﬁcation [9]. Recently  however  it was shown that the main effect of the SVM+
procedure is to assign a data-dependent weight to each training example in the SVM objective [10].
The proposed method  GPC+  constitutes the ﬁrst Bayesian treatment of classiﬁcation using priv-
ileged information. The resulting privileged noise approach is related to input-modulated noise
commonly done in the regression task  where several Bayesian treatments of heteroscedastic regres-
sion using GPs have been proposed. Since the predictive density and marginal likelihood are no
longer analytically tractable  most works deal with approximate inference  i.e.  techniques such as
Markov Chain Monte Carlo [11]  maximum a posteriori [12]  and variational Bayes [13]. To our
knowledge  however  there is no prior work on heteroscedastic classiﬁcation using GPs — we will
elaborate the reasons in Section 2.1 — and this work is the ﬁrst to develop approximate inference
based on expectation propagation for the heteroscedastic noise case in the context of classiﬁcation.

2 GPC+: Gaussian process classiﬁcation with privileged noise

For self-consistency we ﬁrst review the GPC model [14] with an emphasis on the noise-corrupted
latent Gaussian process view. Then  we show how to treat privileged information as heteroscedastic
noise in this process. An elegant aspect of this view is how the privileged noise is able to distinguish
between easy and hard samples and to re-calibrate the uncertainty on the class label of each instance.

2.1 Gaussian process classiﬁer with noisy latent process
Consider a set of N input-output data points or samples D = {(x1  y1)  . . .   (xN   yN )} ⊂ Rd ×
{0  1}. Assume that the class label yi of the sample xi has been generated as yi = I[ ˜f (xi) ≥ 0 ] 
where ˜f (·) is a noisy latent function and I[·] is the Iverson’s bracket notation  i.e.  I[ P ] = 1 when
the condition P is true  and 0 otherwise. Induced by the label generation process  we adopt the

2

following form of likelihood function for ˜f = ( ˜f (x1)  . . .   ˜f (xN ))(cid:62):
Pr(yn = 1|xn  ˜f ) =

Pr(y|˜f   X = (x1  . . .   xN )(cid:62)) =

n=1

(cid:89)N

(cid:89)N

I[ ˜f (xn) ≥ 0 ] 

(1)

n=1

where ˜f (xn) = f (xn) + n with f (xn) being the noise-free latent function. The noise term n
is assumed to be independent and normally distributed with zero mean and variance σ2  that is
n ∼ N (n|0  σ2). To make inference about ˜f (xn)  we need to specify a prior over this function.
We proceed by imposing a zero mean Gaussian process prior [14] on the noise-free latent function 
that is f (xn) ∼ GP(0  k(xn ·)) where k(· ·) is a positive-deﬁnite kernel function [15] that speciﬁes
prior properties of f (·). A typical kernel function that allows for non-linear smooth functions is the
squared exponential kernel kf (xn  xm) = θ exp(− 1
)  where θ controls the prior
amplitude of f (·) and l controls its prior smoothness. The prior and the likelihood are combined
using Bayes’ rule to get the posterior of ˜f (·). Namely  Pr(˜f|X  y) = Pr(y|˜f   X)Pr(˜f )/Pr(y|X).
We can simplify the above noisy latent process view by integrating out the noise term n and writing
down the individual likelihood at sample xn in terms of the noise-free latent function f (·). Namely 
(2)

(cid:90)
Pr(yn = 1|xn  f ) =

I[ ˜f (xn) ≥ 0]N (n|0  σ2)dn = Φ(0 σ2)(f (xn)) 

2l (cid:107)xn − xm(cid:107)2

(cid:96)2

where we have used that ˜f (xn) = f (xn) + n and Φ(µ σ2)(·) is a Gaussian cumulative distribution
function (CDF) with mean µ and variance σ2. Typically the standard Gaussian CDF is used  that is
Φ(0 1)(·)  in the likelihood of (2). Coupled with a Gaussian process prior on the latent function f (·) 
this results in the widely adopted noise-free latent Gaussian process view with probit likelihood.
The equivalence between a noise-free latent process with probit likelihood and a noisy latent process
with step-function likelihood is widely known [14]. It is also widely accepted that the function ˜f (·)
(or the functionf (·)) is a nuisance function as we do not observe its value and its sole purpose is
for a convenient formulation of the model [14]. However  in this paper  we show that by using
privileged information as the noise term  the latent function ˜f now plays a crucial role. The latent
function with privileged noise adjusts the slope transition in the Gaussian CDF to be faster or slower
corresponding to more certainty or more uncertainty about the samples in the original input space.

2.2

Introducing privileged information into the nuisance function

In the learning under privileged information (LUPI) paradigm [2]  besides input data points
{x1  . . .   xN} and associated labels {y1  . . .   yN}  we are given additional information x∗
n ∈ Rd∗
about each training instance xn. However  this privileged information will not be available for un-
seen test instances. Our goal is to exploit the additional data x∗ to inﬂuence our choice of the latent
function ˜f (·). This needs to be done while making sure that the function does not directly use the
privileged data as input  as it is simply not available at test time. We achieve this naturally by treating
the privileged information as a heteroscedastic (input-dependent) noise in the latent process.
Our classiﬁcation model with privileged noise is then as follows:

i.i.d.

(3)
(4)

n) = exp(g(x∗

Privileged noise model : n

Likelihood model : Pr(yn = 1|xn  ˜f ) = I[ ˜f (xn) ≥ 0 ]   where xn ∈ Rd
n)))   where x∗
g(x∗

Assume : ˜f (xn) = f (xn) + n
∼ N (n|0  z(x∗

GP prior model : f (xn) ∼ GP(0  kf (xn ·))

(5)
(6)
In the above  the function exp(·) is needed to ensure positivity of the noise variance. The term kg(· ·)
is a positive-deﬁnite kernel function that speciﬁes the prior properties of another latent function g(·) 
which is evaluated in the privileged space x∗. Crucially  the noise term n is now heteroscedastic 
that is  it has a different variance z(x∗
n) at each input point xn. This is in contrast to the standard GPC
approach discussed in Section 2.1 where the noise term is homoscedastic  n ∼ N (n|0  z(x∗
n) =
σ2). An input-dependent noise term is very common in regression tasks with continuous output
values yn ∈ R  resulting in heteroscedastic regression models  which have been proven more ﬂexible
in numerous applications as already touched upon in the section on related work. However  to our
knowledge  there is no prior work on heteroscedastic classiﬁcation models. This is not surprising as
the nuisance view of the latent function renders a ﬂexible input-dependent noise point-less.

n) ∼ GP(0  kg(x∗

n ∈ Rd∗
n ·)).

and

3

Figure 1: Effects of privileged noise on the nuisance function. (Left) On synthetic data. Suppose for an input
xn  the latent function value is f (xn) = 1. Now also assume that the associated privileged information x∗
n for
the n-th data point deems the sample as difﬁcult  say exp(g(x∗
n)) = 5.0. Then the likelihood will reﬂect this
uncertainty Pr(yn = 1|f  g  xn  x∗
n) = 0.58. In contrast  if the associated privileged information considers the
n)) = 0.5  the likelihood is very certain Pr(yn = 1|f  g  xn  x∗
sample as easy  say e.g. exp(g(x∗
n) = 0.98.
(Right) On real data taken from our experiments in Sec. 4. The posterior means of the Φ(·) function (solid)
and its 1-standard deviation conﬁdence interval (dash-dot) for easy (blue) and difﬁcult (black) instances of the
Chimpanzee v. Giant Panda binary task on the Animals with Attributes (AwA) dataset. (Best viewed in color).

In the context of privileged information heteroscedastic classiﬁcation is a very sensible idea  which is
best illustrated when investigating the effect of privileged information in the equivalent formulation
of a noise free latent process  i.e.  when one integrates out the privileged input-dependent noise term:

(cid:90)

(cid:112)exp(g(x∗

Pr(yn = 1|xn  x∗

n  f  g) =

I[ ˜f (xn) ≥ 0 ]N (n|0  exp(g(x∗

n))dn

= Φ(0 exp(g(x∗

n)))(f (xn)) = Φ(0 1)(f (xn)/

(7)
This equation shows that the privileged information adjusts the slope transition of the Gaussian
CDF through the latent function g(·). For difﬁcult samples the latent function g(·) will be high 
the slope transition will be slower  and thus more uncertainty will be in the likelihood Pr(yn =
1|xn  x∗
n  f  g). For easy samples  however  g(·) will be low  the slope transition will be faster 
and thus less uncertainty will be in the likelihood term. This behaviour is illustrated in Figure 1.
For non-informative samples in the privileged space  the value of g for those samples should be
equal to a global noise value  as in a standard GPC. Thus  privileged information should in principle
never hurt. Proving this theoretically is  however  an interesting and challenging research direction.
Experimentally  however  we observe in the section on experiments the scenario described.

n)).

2.3 Posterior and prediction on test data
Deﬁne g = (g(x∗

Pr(y|X  X(cid:63)  f   g) = (cid:81)N

n))T and X∗ = (x∗

1)  . . .   g(x∗
n=1 Pr(yn = 1|f  g  xn  x∗

1  . . .   x∗

Given the likelihood
n) with the individual term Pr(yn|f  g  xn  x∗
n)

n)T.

given in (7) and the Gaussian process priors on functions  the posterior for f and g is:

Pr(f   g|y  X  X(cid:63)) =

Pr(y|X  X(cid:63)  f   g)Pr(f )Pr(g)

Pr(y|X  X(cid:63))

 

(8)

where Pr(y|X  X(cid:63)) can be maximised with respect to a set of hyper-parameter values such as the
amplitude θ and the smoothness l of the kernel functions [14]. For a previously unseen test point
xnew ∈ Rd  the predictive distribution for its label ynew is given as:

Pr(ynew = 1|y  X  X(cid:63)) =

I[ ˜f (xnew) ≥ 0 ]Pr(fnew|f )Pr(f   g|y  X  X(cid:63))df dgdfnew  

(9)

(cid:90)

where Pr(fnew|f ) is a Gaussian conditional distribution. We note that in (9) we do not consider the
privileged information x(cid:63)
new associated to xnew. The interpretation is that we consider homoscedastic

4

−10−50510f(xn)0.00.20.40.60.81.0Φ(0 exp(g(x∗n))(f(xn))0.840.580.981exp(g(x∗n))=1.0exp(g(x∗n))=5.0exp(g(x∗n))=0.5−2−10120.00.20.40.60.81.0PosteriormeanofforadifficultinstancePosteriormeanofforaneasyinstanceAwA (DeCAF) / Chimpanzee v. Giant Pandanoise at test time. This is a reasonable approach as there is no additional information for increasing
or decreasing our conﬁdence in the newly observed data xnew. Finally  we predict the label for a test
point via Bayesian decision theory: the label being predicted is the one with the largest probability.

3 Expectation propagation with numerical quadrature

Unfortunately  as for most interesting Bayesian models  inference in the GPC+ model is very chal-
lenging. Already in the homoscedastic case  the predictive density and marginal likelihood are
not tractable. Here  we therefore adapt Minka’s expectation propagation (EP) [16] with numerical
quadrature for approximate inference. Our choice is supported on the fact that EP is the preferred
method for approximate inference in GPCs  in terms of accuracy and computational cost [17  18].
Consider the joint distribution of f  g and y  Pr(y|X  X∗  f   g)Pr(f )Pr(g)  where Pr(f ) and Pr(g)
n  f  g) 
with Pr(yn|xn  x∗
n  f  g) given by (7). EP approximates each non-normal factor in this distribution
by an un-normalised bi-variate normal distribution of f and g (we assume independence between f
and g). The only non-normal factors are those of the likelihood  which are approximated as:

are Gaussian process priors and the likelihood Pr(y|X  X∗  f   g) equals(cid:81)N
n  f  g) ≈ γn(f  g) = znN (f (xn)|mf   vf )N (g(x∗

(10)
where the parameters with the super-script
are to be found by EP. The posterior approximation Q
computed by EP results from normalising with respect to f and g the EP approximate joint. That is 
Q is obtained by replacing each likelihood factor by the corresponding approximate factor γn:

n=1 Pr(yn|xn  x∗

Pr(yn|xn  x∗

n)|mg  vg)  

(cid:89)N

−1[

n=1

Pr(f   g|X  X∗

n(cid:48)(cid:54)=n γn(cid:48)

γ(f  g)]Pr(f )Pr(g)  

  y) ≈ Q(f   g) := Z

n  f  g)Qold/Zn||Q

Pr(f )Pr(g) with all variables different from f (xn) and g(x∗

(11)
where Z is a normalisation constant that approximates the model evidence  Pr(y|X  X∗). The
normal distribution belongs to the exponential family of probability distributions and is closed under
the product and division. It is hence possible to show that Q is the product of two multi-variate
normals [19]. The ﬁrst normal approximates the posterior for f and the second the posterior for g.
to minimise KL(cid:0)Pr(yn|xn  x(cid:63)
EP tries to ﬁx the parameters of γn so that it is similar to the exact factor Pr(yn|xn  x∗
n  f  g) in
(cid:104)(cid:81)
regions of high posterior probability [16]. For this  EP iteratively updates each γn until convergence

(cid:1)  where Qold is a normal distribution proportional

to
n) marginalised out  Zn
is simply a normalisation constant and KL(·||·) denotes the Kullback-Leibler divergence between
probability distributions. Assume Qnew is the distribution minimising the previous divergence. Then 
γn ∝ Qnew/Qold and the parameter zn of γn is ﬁxed to guarantee that γn integrates the same as
the exact factor with respect to Qold. The minimisation of the KL divergence involves matching
n  f  g)Qold/Zn and Qnew.
expected sufﬁcient statistics (mean and variance) between Pr(yn|xn  x(cid:63)
These expectations can be obtained from the derivatives of log Zn with respect to the (natural)
parameters of Qold [19]. Unfortunately  the computation of log Zn in closed form is intractable. We
show here that it can be approximated by a one dimensional quadrature. Denote by mf   vf   mg and
vg the means and variances of Qold for f (xn) and g(x∗
vf + exp(g(x∗

n)  respectively. Then 

(cid:113)

Zn =

Φ(0 1)

ynmf /

n))

N (g(x∗

n)|mg  vg)dg(x∗

n) .

(cid:18)

(cid:19)

(cid:105)

(cid:90)

(12)

Thus  EP only requires ﬁve quadratures to update each γn. One to compute log Zn and four extras
to compute its derivatives with respect to mf   vf   mg and vg. After convergence  Q can be used
to approximate predictive distributions and the normalisation constant Z can be maximised to ﬁnd
good values for the model’s hyper-parameters. In particular  it is possible to compute the gradient
of Z with respect to the parameters of the Gaussian process priors for f and g [19]. An R language
implementation of GPC+ using EP for approximate inference is found in the supplementary material.

4 Experiments

We investigate the performance of GPC+. To this aim we considered three types of binary classiﬁca-
tion tasks corresponding to different privileged information using two real-world datasets: Attribute
Discovery and Animals with Attributes. We detail these experiments in turn in the following sections.

5

Methods: We compared our proposed GPC+ method with the well-established LUPI method based
on SVM  SVM+ [5]. As a reference  we also ﬁt standard GP and SVM classiﬁers when learning on
the original space Rd (GPC and SVM baselines). For all four methods  we used a squared exponential
kernel with amplitude parameter θ and smoothness parameter l. For simplicity  we set θ = 1.0 in
all cases. There are two hyper-parameters in GPC (smoothness parameter l and noise variance σ2)
and also two in GPC+ (smoothness parameters l of kernel kf (· ·) and of kernel kg(· ·)). In GPC
and GPC+  we used type II-maximum likelihood for ﬁnding all hyper-parameters. SVM has two
knobs  i.e.  smoothness and regularisation  and SVM+ has four knobs  two smoothness and two
regularisation parameters. In SVM we used a grid search guided by cross-validation to set all hyper-
parameters. However  this procedure was too expensive for ﬁnding the best parameters in SVM+.
Thus  we used the performance on a separate validation set to guide the search. This means that we
give a competitive advantage to SVM+ over the other methods  which do not use the validation set.
Evaluation metric: To evaluate the performance of each method we used the classiﬁcation error
measured on an independent test set. We performed 100 repeats of all the experiments to get the
better statistics of the performance and we report the mean and the standard deviation of the error.

4.1 Attribute discovery dataset

The data set was collected from a website that aggregates product data from a variety of e-commerce
sources and includes both images and associated textual descriptions [20]. The images and texts are
grouped into 4 broad shopping categories: bags  earrings  ties  and shoes. We used 1800 samples
from this dataset. We generated 6 binary classiﬁcation tasks for each pair of the 4 classes with 200
samples for training  200 samples for validation  and the rest of the samples for testing performance.
Neural networks on texts as privileged information: We used images as the original domain and
texts as the privileged domain. This setting was also explored in [6]. However  we used a different
dataset because textual descriptions of the images used in [6] are sparse and contain duplicates. More
precisely  we extracted more advanced text features instead of simple term frequency (TF) features.
For the images representation  we extracted SURF descriptors [21] and constructed a codebook of
100 visual words using the k-means clustering. For the text representation  we extracted 200 dimen-
sional continuous word-vectors using a neural network skip-gram architecture [22]1. To convert this
word representation into a ﬁxed-length sentence representation  we constructed a codebook of 100
word-vectors using again k-means clustering. We note that a more elaborate approach to transform
word to sentence or document features has recently been developed [23]  and we are planning to
explore this in the future. We performed PCA for dimensionality reduction in the original and priv-
ileged domains and only kept the top 50 principal components. Finally  we standardised the data so
that each feature had zero mean and unit standard deviation.
The experimental results are summarised in Table 1. On average over 6 tasks  SVM with hinge loss
outperforms GPC with probit likelihood. However  GPC+ signiﬁcantly improves over GPC provid-
ing the best results on average. This clearly shows that GPC+ is able to employ the neural network
textual representation as privileged information. In contrast  SVM+ produced the same result as
SVM. We suspect this is due to the fact that that SVM has already shown strong performance on
the original image space coupled with the difﬁculties of ﬁnding the best values of the four hyper-
parameters of SVM+. Keep in mind that in SVM+ we discretised the hyper-parameter search space
over 625 (5 × 5 × 5 × 5) possible combination values and used a separate validation set to estimate
the resulting prediction performance.

4.2 Animals with attributes (AwA) dataset

The dataset was collected by querying image search engines for each of the 50 animals categories
which have complimentary high level descriptions of their semantic properties such as shape  colour 
or habitat information among others [24]. The semantic attributes per animal class were retrieved
from a prior psychological study. We focused on the 10 categories corresponding to the test set of this
dataset for which the predicted attributes are provided based on the probabilistic DAP model [24].
The 10 classes are: chimpanzee  giant panda  leopard  persian cat  pig  hippopotamus  humpback
whale  raccoon  rat  seal  which have 6180 images associated in total. As in Section 4.1 and also in

1https://code.google.com/p/word2vec/

6

Table 1: Average error rate in % (the lower the better) on the Attribute Discovery dataset over 100 repetitions.
We used images as the original domain and neural networks word-vector representation on texts as the privi-
leged domain. The best method for each binary task is highlighted in boldface. An average rank equal to one
means that the corresponding method has the smallest error on the 6 tasks.
GPC+ (Ours)
9.50±0.11
10.03±0.15
9.22±0.11
10.56±0.13
7.33±0.10
15.54±0.16
10.36±0.12
1.8

bags v. earrings
bags v. ties
bags v. shoes
earrings v. ties
earrings v. shoes
ties v. shoes

SVM+
9.89±0.13
9.47±0.13
9.29±0.14
11.11±0.16
7.63±0.13
15.10±0.18
10.42±0.11

SVM
9.89±0.14
9.44±0.16
9.31±0.12
11.15±0.16
7.75±0.13
14.90±0.21
10.41±0.11

2.7

average error on each task
average ranking

GPC

9.79±0.12
10.36±0.16
9.66±0.13
10.84±0.14
7.74±0.11
15.51±0.16
10.65±0.11

3.0

2.5

[6]  we generated 45 binary classiﬁcation tasks for each pair of the 10 classes with 200 samples for
training  200 samples for validation  and the rest of samples for testing the predictive performance.
Neural networks on images as privileged information: Deep learning methods have gained an in-
creased attention within the machine learning and computer vision community over the recent years.
This is due to their capability in extracting informative features and delivering strong predictive per-
formance in many classiﬁcation tasks. As such  we are interested to explore the use of deep learning
based features as privileged information so that their predictive power can be used even if we do not
have access to them at prediction time. We used the standard SURF features [21] with 2000 visual
words as the original domain and the recently proposed DeCAF features [25] extracted from the
activation of a deep convolutional network trained in a fully supervised fashion as the privileged do-
main. The DeCAF features have 4096 dimensions. All features are provided with the AwA dataset2.
We again performed PCA for dimensionality reduction in the original and privileged domains and
only kept the top 50 principal components  as well as standardised the data.
Attributes as privileged information: Following the experimental setting of [6]  we also used
images as the original domain and attributes as the privileged domain. Images were represented by
2000 visual words based on SURF descriptors and attributes were in the form of 85 dimensional
predicted attributes based on probabilistic binary classiﬁers [24]. As previously  we also performed
PCA and kept the top 50 principal components in the original domain and standardised the data.
The results of these experiments are shown in Figure 2 in terms of pairwise comparisons over 45
binary tasks between GPC+ and the main baselines  GPC and SVM+. The complete results with
the error of each method GPC  GPC+  SVM  and SVM+ on each problem are relegated to the
supplementary material. In contrast to the results on the attribute discovery dataset  on the AwA
dataset it is clear that GPC outperforms SVM in almost all of the 45 binary classiﬁcation tasks
(see the supplementary material). The average error of GPC over 4500 (45 tasks and 100 repeats
per task) experiments is much lower than SVM. On the AwA dataset  SVM+ can take advantage
of privileged information – be it deep belief DeCAF features or semantic attributes – and shows
signiﬁcant performance improvement over SVM. However  GPC+ still shows the best overall results
and further improves the already strong performance of GPC. As illustrated in Figure 1 (right)  the
privileged information modulates the slope of the probit likelihood function differently for easy
and difﬁcult examples: easy examples gain slope and hence importance whereas difﬁcult ones lose
importance in the classiﬁcation.
In this dataset we analysed our experimental results using the
multiple dataset statistical comparison method described in [26]3. The results of the statistical tests
are summarised in Figure 3. When DeCAF attributes are used as privileged information  there is
statistical evidence supporting that GPC+ performs best among the four methods  while when the
semantic attributes are used as privileged information  GPC+ still performs best but there is not
enough evidence to reject that GPC+ performs comparable to GPC.

2http://attributes.kyb.tuebingen.mpg.de
3Note that we are not able to use this method on the results of the attribute discovery dataset in Table 1

because the number of methods compared (i.e.  4) is almost equal to the number of tasks or datasets (i.e.  6).

7

(DeCAF as privileged)

(Attributes as privileged)

Figure 2: Pairwise comparison of the proposed GPC+ method and main baselines is shown via the relative
difference of the error rate (top: GPC+ versus GPC  bottom: GPC+ versus SVM+). The length of the 45 bars
corresponds to relative difference of the error rate over 45 cases. Average error rates of each method on the
AwA dataset across each of the 45 tasks are found in the supplementary material. (Best viewed in color).

(DeCAF as privileged)

(Attributes as privileged)

Figure 3: Average rank (the lower the better) of the four methods and critical distance for statistically signif-
icant differences (see [26]) on the AwA dataset. An average rank equal to one means that particular method
has the smallest error on the 45 tasks. Whenever the average ranks differ by more than the critical distance 
there is statistical evidence (p-value < 10%) supporting a difference in the average ranks and hence in the
performance. We also link two methods with a solid line if they are not statistically different from each other
(p-value > 10%). When the DeCAF features are used as privileged information  there is statistical evidence
supporting that GPC+ performs best among the four methods considered. When the attributes are used  GPC+
still performs best  but there is not enough evidence to reject that GPC+ performs comparable to GPC.

5 Conclusions and future work

We presented the ﬁrst treatment of the learning with privileged information paradigm under the
Gaussian process classiﬁcation (GPC) framework  and called it GPC+. In GPC+ privileged infor-
mation is used in the latent noise layer  resulting in a data-dependent modulation of the slope of the
likelihood. The training time of GPC+ is about twice times the training time of a standard Gaussian
process classiﬁer. The reason is that GPC+ must train two latent functions  f and g  instead of only
one. Nevertheless  our results show that GPC+ is an effective way to use privileged information 
which manifest itself in signiﬁcantly better prediction accuracy. Furthermore  to our knowledge 
this is the ﬁrst time that a heteroscedastic noise term is used to improve GPC. We have also shown
that recent advances in continuous word-vector neural networks representations [23] and deep con-
volutional networks for image representations [25] can be used as privileged information. For future
work  we plan to extend the GPC+ framework to the multi-class case and to speed up computation
by devising a quadrature-free expectation propagation method  similar to the ones in [27  28].
Acknowledgement: D. Hern´andez-Lobato is supported by Direcci´on General de Investigaci´on MCyT and by
Consejer´ıa de Educaci´on CAM (projects TIN2010-21575-C02-02  TIN2013-42351-P and S2013/ICE-2845).
V. Sharmanska is funded by the European Research Council under the ERC grant agreement no 308036.

References
[1] D.H. Wolpert. The lack of a priori distinctions between learning algorithms. Neural computation  8:1341–

1390  1996.

8

1234SVMSVM+GPC+GPCCritical Distance1234SVMSVM+GPC+GPCCritical Distance[2] V. Vapnik and A. Vashist. A new learning paradigm: Learning using privileged information. Neural

Networks  22:544 – 557  2009.

[3] D. Pechyony and V. Vapnik. On the theory of learnining with privileged information. In Advances in

Neural Information Processing Systems (NIPS)  pages 1894–1902  2010.

[4] B. Ribeiro  C. Silva  A. Vieira  A. Gaspar-Cunha  and J.C. das Neves. Financial distress model prediction

using SVM+. In International Joint Conference on Neural Networks (IJCNN)  2010.

[5] D. Pechyony and V. Vapnik. Fast optimization algorithms for solving SVM+. In Statistical Learning and

Data Science  2011.

[6] V. Sharmanska  N. Quadrianto  and C. H. Lampert. Learning to rank using privileged information. In

International Conference on Computer Vision (ICCV)  2013.

[7] J. Feyereisl and U. Aickelin. Privileged information for data clustering. Information Sciences  194:4–23 

2012.

[8] S. Fouad  P. Tino  S. Raychaudhury  and P. Schneider.

Incorporating privileged information through

metric learning. IEEE Transactions on Neural Networks and Learning Systems  24:1086 – 1098  2013.

[9] V. Sharmanska  N. Quadrianto  and C. H. Lampert. Learning to transfer privileged information  2014.

arXiv:1410.0389 [cs.CV].

[10] M. Lapin  M. Hein  and B. Schiele. Learning using privileged information: SVM+ and weighted SVM.

Neural Networks  53:95–108  2014.

[11] P. W. Goldberg  C. K. I. Williams  and C. M. Bishop. Regression with input-dependent noise: A Gaussian

process treatment. In Advances in Neural Information Processing Systems (NIPS)  1998.

[12] N. Quadrianto  K. Kersting  M. D. Reid  T. S. Caetano  and W. L. Buntine. Kernel conditional quantile

estimation via reduction revisited. In International Conference on Data Mining (ICDM)  2009.

[13] M. L´azaro-Gredilla and M. K. Titsias. Variational heteroscedastic Gaussian process regression. In Inter-

national Conference on Machine Learning (ICML)  2011.

[14] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation

and Machine Learning). The MIT Press  2006.

[15] B. Scholkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regularization  Opti-

mization  and Beyond. MIT Press  Cambridge  MA  USA  2001.

[16] T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis  Massachusetts

Institute of Technology  2001.

[17] H. Nickisch and C. E. Rasmussen. Approximations for Binary Gaussian Process Classiﬁcation. Journal

of Machine Learning Research  9:2035–2078  2008.

[18] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁca-

tion. Journal of Machine Learning Research  6:1679–1704  2005.

[19] M. Seeger. Expectation propagation for exponential families. Technical report  Department of EECS 

University of California  Berkeley  2006.

[20] T. L. Berg  A. C. Berg  and J. Shih. Automatic attribute discovery and characterization from noisy web

data. In European Conference on Computer Vision (ECCV)  2010.

[21] H. Bay  A. Ess  T. Tuytelaars  and L. Van Gool. Speeded-up robust features (SURF). Computer Vision

and Image Understanding  110:346–359  2008.

[22] T. Mikolov  I. Sutskever  K. Chen  G. Corrado  and J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS)  2013.
In International

[23] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents.

Conference on Machine Learning (ICML)  2014.

[24] C. H. Lampert  H. Nickisch  and S. Harmeling. Attribute-based classiﬁcation for zero-shot visual object

categorization. IEEE Transactions on Pattern Analysis and Machine Intelligence  36:453–465  2014.

[25] J. Donahue  Y. Jia  O. Vinyals  J. Hoffman  N. Zhang  E. Tzeng  and T. Darrell. Decaf: A deep convolu-
tional activation feature for generic visual recognition. In International Conference on Machine Learning
(ICML)  2014.

[26] J. Demˇsar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine Learning

Research  7:1–30  2006.

[27] J. Riihim¨aki  P. Jyl¨anki  and A. Vehtari. Nested Expectation Propagation for Gaussian Process Classiﬁca-

tion with a Multinomial Probit Likelihood. Journal of Machine Learning Research  14:75–109  2013.

[28] D. Hern´andez-Lobato  J. M. Hern´andez-Lobato  and P. Dupont. Robust multi-class Gaussian process

classiﬁcation. In Advances in Neural Information Processing Systems (NIPS)  2011.

9

,Fabian Wauthier
Nebojsa Jojic
Michael Jordan
Daniel Hernández-lobato
Viktoriia Sharmanska
Kristian Kersting
Christoph Lampert
Novi Quadrianto
Wei Shen
KAI ZHAO
Yilu Guo
Alan Yuille