2011,Improved Algorithms for Linear Stochastic Bandits,We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular  we show that a simple modification of Auer’s UCB algorithm (Auer  2002) achieves with high probability constant regret. More importantly  we modify and  consequently  improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002)  Dani et al. (2008)  Rusmevichientong and Tsitsiklis (2010)  Li et al. (2010). Our modification improves the regret bound by a logarithmic factor  though experiments show a vast improvement. In both cases  the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.,Improved Algorithms for Linear Stochastic Bandits

Yasin Abbasi-Yadkori
abbasiya@ualberta.ca
Dept. of Computing Science

University of Alberta

D´avid P´al

dpal@google.com

Dept. of Computing Science

University of Alberta

Csaba Szepesv´ari

szepesva@ualberta.ca
Dept. of Computing Science

University of Alberta

Abstract

We improve the theoretical analysis and empirical performance of algorithms for
the stochastic multi-armed bandit problem and the linear stochastic multi-armed
bandit problem.
In particular  we show that a simple modiﬁcation of Auer’s
UCB algorithm (Auer  2002) achieves with high probability constant regret.
More importantly  we modify and  consequently  improve the analysis of the
algorithm for the for linear stochastic bandit problem studied by Auer (2002) 
Dani et al. (2008)  Rusmevichientong and Tsitsiklis (2010)  Li et al. (2010).
Our modiﬁcation improves the regret bound by a logarithmic factor  though
experiments show a vast improvement.
In both cases  the improvement stems
from the construction of smaller conﬁdence sets. For their construction we use a
novel tail inequality for vector-valued martingales.

1

Introduction

Linear stochastic bandit problem is a sequential decision-making problem where in each time step
we have to choose an action  and as a response we receive a stochastic reward  expected value of
which is an unknown linear function of the action. The goal is to collect as much reward as possible
over the course of n time steps. The precise model is described in Section 1.2.
Several variants and special cases of the problem exist differing on what the set of available
actions is in each round. For example  the standard stochastic d-armed bandit problem  introduced
by Robbins (1952) and then studied by Lai and Robbins (1985)  is a special case of linear stochastic
bandit problem where the set of available actions in each round is the standard orthonormal basis of
Rd. Another variant  studied by Auer (2002) under the name “linear reinforcement learning”  and
later in the context of web advertisement by Li et al. (2010)  Chu et al. (2011)  is a variant when the
set of available actions changes from time step to time step  but has the same ﬁnite cardinality in
each step. Another variant dubbed “sleeping bandits”  studied by Kleinberg et al. (2008)  is the case
when the set of available actions changes from time step to time step  but it is always a subset of the
standard orthonormal basis of Rd. Another variant  studied by Dani et al. (2008)  Abbasi-Yadkori
et al. (2009)  Rusmevichientong and Tsitsiklis (2010)  is the case when the set of available actions
does not change between time steps but the set can be an almost arbitrary  even inﬁnite  bounded
subset of a ﬁnite-dimensional vector space. Related problems were also studied by Abe et al.
(2003)  Walsh et al. (2009)  Dekel et al. (2010).
In all these works  the algorithms are based on the same underlying idea—the optimism-in-the-
face-of-uncertainty (OFU) principle. This is not surprising since they are solving almost the same
problem. The OFU principle elegantly solves the exploration-exploitation dilemma inherent in the
problem. The basic idea of the principle is to maintain a conﬁdence set for the vector of coefﬁcients
of the linear function.
In every round  the algorithm chooses an estimate from the conﬁdence
set and an action so that the predicted reward is maximized  i.e.  estimate-action pair is chosen
optimistically. We give details of the algorithm in Section 2.

1

Thus the problem reduces to the construction of conﬁdence sets for the vector of coefﬁcients of the
linear function based on the action-reward pairs observed in the past time steps. This is not an easy
problem  because the future actions are not independent of the actions taken in the past (since the
algorithm’s choices of future actions depend on the random conﬁdence set constructed from past
data). In fact  several authors (Auer  2000  Li et al.  2010  Walsh et al.  2009) fell victim of making
a mistake because they did not recognize this issue. Correct solutions require new martingale
techniques which we provide here.
The smaller conﬁdence sets one is able to construct  the better regret bounds one obtains for
the resulting algorithm  and  more importantly  the better the algorithm performs empirically.
With our new technique  we vastly reduce the size of the conﬁdence sets of Dani et al. (2008)
and Rusmevichientong and Tsitsiklis (2010). First  our conﬁdence sets are valid uniformly over all
time steps  which immediately saves log(n) factor by avoiding the otherwise needed union bound.
Second  our conﬁdence sets are “more empirical” in the sense that some worst-case quantities from
the old bounds are replaced by empirical quantities that are always smaller  sometimes substantially.
As a result  our experiments show an order-of-magnitude improvement over the CONFIDENCEBALL
algorithm of Dani et al. (2008). To construct our conﬁdence sets  we prove a new martingale tail
inequality. The new inequality is derived using techniques from the theory of self-normalized
processes (de la Pe˜na et al.  2004  2009).
Using our conﬁdence sets  we modify the UCB algorithm (Auer  2002) for the d-armed bandit prob-
lem and show that with probability 1    the regret of the modiﬁed algorithm is O(d log(1/)/)
where  is the difference between the expected rewards of the best and the second best action.
In particular  note that the regret does not depend on n. This seemingly contradicts the result
of Lai and Robbins (1985) who showed that the expected regret of any algorithm is at least
(Pi6=i⇤
) o(1)) log n where pi⇤ and pi are the reward distributions of the optimal arm
and arm i respectively and D is the Kullback-Leibler divergence. However  our algorithm receives
 as an input  and thus its expected regret depends on . With  = 1/n our algorithm has the same
expected regret bound  O((d log n)/)  as Auer (2002) has shown for UCB.
For the general linear stochastic bandit problem  we improve regret of the CONFIDENCEBALL

1/D(pj | pi⇤

algorithm of Dani et al. (2008). They showed that its regret is at most O(d log(n)pn log(n/))
with probability at least 1  . We modify their algorithm so that it uses our new conﬁdence
sets and we show that its regret is at most O(d log(n)pn +pdn log(n/)) which is roughly
improvement a multiplicative factorplog(n). Dani et al. (2008) prove also a problem dependent

regret bound. Namely  they show that the regret of their algorithm is O( d2
 log(n/) log2(n)) where
 is the “gap” as deﬁned in (Dani et al.  2008). For our modiﬁed algorithm we prove an improved
O( log(1/)

 (log(n) + d log log n)2) bound.

1.1 Notation
We use kxkp to denote the p-norm of a vector x 2 Rd. For a positive deﬁnite matrix A 2 Rd⇥d  the
weighted 2-norm of vector x 2 Rd is deﬁned by kxkA = px>Ax. The inner product is denoted
by h· ·i and the weighted inner-product x>Ay = hx  yiA. We use min(A) to denote the minimum
eigenvalue of the positive deﬁnite matrix A. For any sequence {at}1t=0 we denote by ai:j the
sub-sequence ai  ai+1  . . .   aj.

1.2 The Learning Model
In each round t  the learner is given a decision set Dt ✓ Rd from which he has to choose an
action Xt. Subsequently he observes reward Yt = hXt ✓ ⇤i + ⌘t where ✓⇤ 2 Rd is an unknown
parameter and ⌘t is a random noise satisfying E[⌘t | X1:t ⌘ 1:t1] = 0 and some tail-constraints  to
be speciﬁed soon.
The goal of the learner is to maximize his total rewardPn
t=1 hXt ✓ ⇤i accumulated over the course
of n rounds. Clearly  with the knowledge of ✓⇤  the optimal strategy is to choose in round t the
point x⇤t = argmaxx2Dt hx  ✓⇤i that maximizes the reward. This strategy would accumulate total
rewardPn
t=1 hx⇤t  ✓ ⇤i. It is thus natural to evaluate the learner relative to this optimal strategy. The
difference of the learner’s total reward and the total reward of the optimal strategy is called the

2

for t := 1  2  . . . do

(Xt e✓t) = argmax(x ✓)2Dt⇥Ct1 hx  ✓i

Play Xt and observe reward Yt
Update Ct

end for

Figure 1: OFUL ALGORITHM

pseudo-regret (Audibert et al.  2009) of the algorithm and it can be formally written as

Rn = nXt=1

hx⇤t  ✓ ⇤i!  nXt=1

hXt ✓ ⇤i! =

nXt=1

hx⇤t  Xt ✓ ⇤i .

As compared to the regret  the pseudo-regret has the same expected value  but lower variance
because the additive noise ⌘t is removed. However  the omitted quantity is uncontrollable  hence
we have no interest in including it in our results (the omitted quantity would also cancel  if ⌘t was a
sequence which is independently selected of X1:t.) In what follows  for simplicity we use the word
regret instead of the more precise pseudo-regret in connection to Rn.
The goal of the algorithm is to keep the regret Rn as low as possible. As a bare minimum  we
require that the algorithm is Hannan consistent  i.e.  Rn/n ! 0 with probability one.
In order to obtain meaningful upper bounds on the regret  we will place assumptions on {Dt}1t=1 
✓⇤ and the distribution of {⌘t}1t=1. Roughly speaking  we will need to assume that {Dt}1t=1 lies in
a bounded set. We elaborate on the details of the assumptions later in the paper.
However  we state the precise assumption on the noise sequence {⌘t}1t=1 now. We will assume that
⌘t is conditionally R-sub-Gaussian where R  0 is a ﬁxed constant. Formally  this means that

8 2 R

E⇥e⌘t | X1:t ⌘ 1:t1⇤  exp✓ 2R2
2 ◆ .

The sub-Gaussian condition automatically implies that E[⌘t | X1:t ⌘ 1:t1] = 0. Furthermore  it
also implies that Var[⌘t | Ft]  R2 and thus we can think of R2 as the (conditional) variance of
the noise. An example of R-sub-Gaussian ⌘t is a zero-mean Gaussian noise with variance at most
R2  or a bounded zero-mean noise lying in an interval of length at most 2R.

2 Optimism in the Face of Uncertainty

A natural and successful way to design an algorithm is the optimism in the face of uncertainty
principle (OFU). The basic idea is that the algorithm maintains a conﬁdence set Ct1 ✓ Rd
for the parameter ✓⇤.
It is required that Ct1 can be calculated from X1  X2  . . .   Xt1 and
Y1  Y2  . . .   Yt1 and “with high probability” ✓⇤ lies in Ct1. The algorithm chooses an optimistic

estimatee✓t = argmax✓2Ct1(maxx2Dt hx  ✓i) and then chooses action Xt = argmaxx2DtDx e✓tE
which maximizes the reward according to the estimatee✓t. Equivalently  and more compactly  the

algorithm chooses the pair

which jointly maximizes the reward. We call the resulting algorithm the OFUL ALGORITHM for
“optimism in the face of uncertainty linear bandit algorithm”. Pseudo-code of the algorithm is given
in Figure 1.
The crux of the problem is the construction of the conﬁdence sets Ct. This construction is the
subject of the next section.

(Xt e✓t) = argmax

(x ✓)2Dt⇥Ct1 hx  ✓i  

3 Self-Normalized Tail Inequality for Vector-Valued Martingales
Since the decision sets {Dt}1t=1 can be arbitrary  the sequence of actions Xt 2 Dt is arbitrary as
well. Even if {Dt}1t=1 is “well-behaved”  the selection rule that OFUL uses to choose Xt 2 Dt

3

generates a sequence {Xt}1t=1 with complicated stochastic dependencies that are hard to handle.
Therefore  for the purpose of deriving conﬁdence sets it is easier to drop any assumptions on
{Xt}1t=1 and pursue a more general result.
If we consider the -algebra Ft = (X1  X2  . . .   Xt+1 ⌘ 1 ⌘ 2  . . .  ⌘ t) then Xt becomes Ft1-
measurable and ⌘t becomes Ft-measurable. Relaxing this a little bit  we can assume that {Ft}1t=0 is
any ﬁltration of -algebras such that for any t  1  Xt is Ft1-measurable and ⌘t is Ft-measurable
and therefore Yt = hXt ✓ ⇤i + ⌘t is Ft-measurable. This is the setup we consider for derivation of
the conﬁdence sets.
The sequence {St}1t=0  St =Pt
s=1 ⌘tXt  is a martingale with respect {Ft}1t=0 which happens to
be crucial for the construction of the conﬁdence sets for ✓⇤. The following theorem shows that with
high probability the martingale stays close to zero. Its proof is given in Appendix A
Theorem 1 (Self-Normalized Bound for Vector-Valued Martingales). Let {Ft}1t=0 be a ﬁltration.
Let {⌘t}1t=1 be a real-valued stochastic process such that ⌘t is Ft-measurable and ⌘t is conditionally
R-sub-Gaussian for some R  0 i.e.

8 2 R

E⇥e⌘t | Ft1⇤  exp✓ 2R2
2 ◆ .

Let {Xt}1t=1 be an Rd-valued stochastic process such that Xt is Ft1-measurable. Assume that V
is a d ⇥ d positive deﬁnite matrix. For any t  0  deﬁne

V t = V +

Then  for any > 0  with probability at least 1    for all t  0 

⌘sXs .

St =

XsX>s

tXs=1
tXs=1
◆ .
t  2R2 log✓ det(V t)1/2 det(V )1/2



kStk2

V 1

Note that the deviation of the martingale kStk2
V 1
t which is itself derived from the martingale  hence the name “self-normalized bound”.

is measured by the norm weighted by the matrix

V 1

t

4 Construction of Conﬁdence Sets

(1)

b✓t = (X>1:tX1:t + I)1X>1:tY1:t

where X1:t is the matrix whose rows are X>1   X>2   . . .   X>t and Y1:t = (Y1  . . .   Yt)>. The

Letb✓t be the `2-regularized least-squares estimate of ✓⇤ with regularization parameter > 0:
following theorem shows that ✓⇤ lies with high probability in an ellipsoid with center atb✓t. Its proof
can be found in Appendix B.
Theorem 2 (Conﬁdence Ellipsoid). Assume the same as in Theorem 1  let V = I  > 0  deﬁne
Yt = hXt ✓ ⇤i + ⌘t and assume that k✓⇤k2  S. Then  for any > 0  with probability at least
1    for all t  0  ✓⇤ lies in the set
◆ + 1/2 S9=;
Ct =8<:
✓ 2 Rd : b✓t  ✓V t  Rs2 log✓ det(V t)1/2 det(I)1/2
C0t =(✓ 2 Rd : b✓t  ✓V t  Rsd log✓ 1 + tL2/
◆ + 1/2 S) .

Furthermore  if for all t  1  kXtk2  L then with probability at least 1    for all t  0  ✓⇤ lies
in the set





.

4

8
3

(2)

The above bound could be compared with a similar bound of Dani et al. (2008) whose bound  under
identical conditions  states that (with appropriate initialization) with probability 1   
log✓ t2
for all t large enough

b✓t  ✓⇤V t  R max(s128 d log(t) log✓ t2
◆ 

◆)  
where large enough means that t satisﬁes 0 << t 2e1/16. Denote bypt() the right-hand side
in the last bound. The restriction on t comes from the fact that t()  2d(1 + 2 log(t)) is needed
in the proof of the last inequality of their Theorem 5.
On the other hand  Rusmevichientong and Tsitsiklis (2010) proved that for any ﬁxed t  2  for any
0 << 1  with probability at least 1   
where  = p3 + 2 log((L2 + trace(V ))/. To get a uniform bound one can use a union bound
with t = /t2. ThenP1t=2 t = ( ⇡2
6  1)  . This thus gives that for any 0 << 1  with
probability at least 1   

b✓t  ✓⇤V t  2 2Rplog tpd log(t) + log(1/) + 1/2S  
b✓t  ✓⇤V t  22Rplog tpd log(t) + log(t2/) + 1/2S  

This is tighter than (2)  but is still lagging behind the result of Theorem 2. Note that the new conﬁ-
dence set seems to require the computation of a determinant of a matrix  a potentially expensive step.
However  one can speed up the computation by using the matrix determinant lemma  exploiting that
the matrix whose determinant is needed is obtained via a rank-one update (cf. the proof of Lemma 11
in the Appendix). This way  the determinant can be kept up-to-date with linear time computation.

8t  2 

5 Regret Analysis of the OFUL ALGORITHM

We now give a bound on the regret of the OFUL algorithm when run with conﬁdence sets Cn
constructed in Theorem 2 in the previous section. We will need to assume that expected rewards
are bounded. We can view this as a bound on ✓⇤ and the bound on the decision sets Dt. The next
theorem states a bound on the regret of the algorithm. Its proof can be found in Appendix C.
Theorem 3 (The regret of the OFUL algorithm). Assume that for all t and all x 2 Dt 
hx  ✓⇤i 2 [1  1]. Then  with probability at least 1    the regret of the OFUL algorithm satisﬁes

8n  0  Rn  4pnd log( + nL/d)⇣1/2S + Rp2 log(1/) + d log(1 + nL/(d))⌘ .

Figure 2 shows the experiments with the new conﬁdence set. The regret of OFUL is signiﬁcantly
better compared to the regret of CONFIDENCEBALL of Dani et al. (2008). The ﬁgure also shows
a version of the algorithm that has a similar regret to the algorithm with the new bound  but spends
about 350 times less computation in this experiment. Next  we explain how we can achieve this
computation saving.

5.1 Saving Computation

In this section  we show that we essentially need to recomputee✓t only O(log n) times up to time
n and hence saving computations.1 The idea is to recomputee✓t whenever det(Vt) increases by a
constant factor (1 + C). We call the resulting algorithm the RARELY SWITCHING OFUL algorithm
and its pseudo-code is given in Figure 3. As the next theorem shows its regret bound is essentially
the same as the regret for OFUL.
Theorem 4. Under the same assumptions as in Theorem 3  with probability at least 1    for all
n  0  the regret of the RARELY SWITCHING OFUL ALGORITHM satisﬁes
d◆ + 2 log

Rn  4s(1 + C)nd log✓ +

d ◆(pS + Rsd log✓1 +

) + 4rd log

n
d

nL

nL

1

.

1Note this is very different than the common “doubling trick” in online learning literature. The doubling is
used to cope with a different problem. Namely  the problem when the time horizon n is unknown ahead of time.

5

t

e
r
g
e
R

3000

2500

2000

1500

1000

500

 

0
0

New bound
Old bound
New bound with rare switching

 

2000

4000

Time

6000

8000

10000

Figure 2: The application of the new bound to a linear bandit problem. A 2-dimensional linear
bandit  where the parameters vector and the actions are from the unit ball. The regret of OFUL is
signiﬁcantly better compared to the regret of CONFIDENCEBALL of Dani et al. (2008). The noise
is a zero mean Gaussian with standard deviation  = 0.1. The probability that conﬁdence sets fail
is  = 0.0001. The experiments are repeated 10 times.

Input: Constant C > 0

for t := 1  2  . . . do

end for

if det(Vt) > (1 + C) det(V⌧ ) then

⌧ = 1 {This is the last time step that we changede✓t}
(Xt e✓t) = argmax(x ✓)2Dt⇥Ct1 h✓  xi.
Xt = argmaxx2DtDe✓⌧   xE.

Play Xt and observe reward Yt.

⌧ = t.

end if

Figure 3: The RARELY SWITCHING OFUL ALGORITHM

0.2

t

e
r
g
e
r
 
e
g
a
r
e
v
A

0.15

0.1

0.05

0

0

0.2

0.4

0.6

0.8

1

C

Figure 4: Regret against computation. We ﬁxed the number of times the algorithm is allowed to
update its action in OFUL. For larger values of C  the algorithm changes action less frequently 
hence  will play for a longer time period. The ﬁgure shows the average regret obtained during the
given time periods for the different values of C. Thus  we see that by increasing C  one can actually
lower the average regret per time step for a given ﬁxed computation budget.

The proof of the theorem is given in Appendix D. Figure 4 shows a simple experiment with the
RARELY SWITCHING OFUL ALGORITHM.

5.2 Problem Dependent Bound

Let t be the “gap” at time step t as deﬁned in (Dani et al.  2008). (Intuitively  t is the difference
between the rewards of the best and the “second best” action in the decision set Dt.) We consider

6

the smallest gap ¯n = min1tn t. This includes the case when the set Dt is the same polytope
in every round or the case when Dt is ﬁnite.
The regret of OFUL can be upper bounded in terms of ( ¯n)n as follows.
Theorem 5. Assume that   1 and k✓⇤k2  S where S  1. With probability at least 1    for
all n  1  the regret of the OFUL satisﬁes
⇣log(Ln) + (d  1) log

64R2S2L

16R2S2

Rn 

¯n

+ 2(d  1) log✓d log

¯2
n
d + nL2

d

+ 2 log(1/)⌘ + 2 log(1/)◆2

.

The proof of the theorem can be found in the Appendix E.
The problem dependent regret of (Dani et al.  2008) scales like O( d2
 (log2 n + d log n + d2 log log n))  where = inf n ¯n.
scales like O( 1

 log3 n)  while our bound

6 Multi-Armed Bandit Problem

In this section we show that a modiﬁed version of UCB has with high probability constant regret.
Let µi be the expected reward of action i = 1  2  . . .   d. Let µ⇤ = max1id µi be the expected
reward of the best arm  and let i = µ⇤  µi  i = 1  2  . . .   d  be the “gaps” with respect to the
best arm. We assume that if we choose action It in round t we obtain reward µIt + ⌘t. Let Ni t
denote the number of times that we have played action i up to time t  and X i t denote the average
of the rewards received by action i up to time t. We construct conﬁdence intervals for the expected
rewards µi based on X i t in the following lemma. (The proof can be found in the Appendix F.)
Lemma 6 (Conﬁdence Intervals). Assuming that the noise ⌘t is conditionally 1-sub-Gaussian. With
probability at least 1   

8i 2{ 1  2  . . .   d}  8t  0

|X i t  µi| ci t  

where

ci t =s (1 + Ni t)

N 2
i t

✓1 + 2 log✓ d(1 + Ni t)1/2



◆◆ .

(3)

Using these conﬁdence intervals  we modify the UCB algorithm of Auer et al. (2002) and change
the action selection rule accordingly. Hence  at time t  we choose the action

It = argmax

i

X i t + ci t.

(4)

We call this algorithm UCB().
The main difference between UCB() and UCB is that the length of conﬁdence interval ci t depends
neither on n  nor on t. This allows us to prove the following result that the regret of UCB() is
constant. (The proof can be found in the Appendix G.)
Theorem 7 (Regret of UCB()). Assume that the noise ⌘t is conditionally 1-sub-Gaussian  with
probability at least 1    the total regret of the UCB() is bounded as
i◆ .

Rn  Xi:i>0✓3i +

16
i

log

2d

Lai and Robbins (1985) prove that for any suboptimal arm j 

where  p⇤ and pj are the reward density of the optimal arm and arm j respectively  and D is the
KL-divergence. This lower bound does not contradict Theorem 7  as Theorem 7 only states a high

E Ni t 

log t

 

D(pj  p⇤)

7

800

600

400

200

t

e
r
g
e
R

 

0
0

New bound
Old bound

 

2000

4000

Time

6000

8000

10000

Figure 5: The regret of UCB() against-time when it uses either the conﬁdence bound based on
Hoeffding’s inequality  or the bound in (3). The results are shown for a 10-armed bandit problem 
where the mean value of each arm is ﬁxed to some values in [0  1]. The regret of UCB() is
improved with the new bound. The noise is a zero-mean Gaussian with standard deviation  = 0.1.
The value of  is set to 0.0001. The experiments are repeated 10 times and the average is shown 
together with the error bars.

probability upper bound for the regret. Note that UCB() takes delta as its input. Because with
probability   the regret in time t can be t  on expectation  the algorithm might have a regret of t.
Now if we select  = 1/t  then we get O(log t) upper bound on the expected regret.
If one is interested in an average regret result  then  with slight modiﬁcation of the proof technique
one can obtain an identical result to what Auer et al. (2002) proves.
Figure 5 shows the regret of UCB() when it uses either the conﬁdence bound based on Hoeffding’s
inequality  or the bound in (3). As can be seen  the regret of UCB() is improved with the new bound.
Coquelin and Munos (2007)  Audibert et al. (2009) prove similar high-probability constant regret
bounds for variations of the UCB algorithm. Compared to their bounds  our bound is tighter thanks
to that with the new self-normalized tail inequality we can avoid one union bound. The improve-
ment can also be seen in experiment as the curve that we get for the performance of the algorithm
of Coquelin and Munos (2007) is almost exactly the same as the curve that is labeled “Old Bound”
in Figure 5.

7 Conclusions

In this paper  we showed how a novel tail inequality for vector-valued martingales allows one to
improve both the theoretical analysis and empirical performance of algorithms for various stochastic
bandit problems. In particular  we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer 
2002) achieves with high probability constant regret. Further  we modify and improve the analysis
of the algorithm for the for linear stochastic bandit problem studied by Auer (2002)  Dani et al.
(2008)  Rusmevichientong and Tsitsiklis (2010)  Li et al. (2010). Our modiﬁcation improves the
regret bound by a logarithmic factor  though experiments show a vast improvement  stemming
from the construction of smaller conﬁdence sets. To our knowledge  ours is the ﬁrst  theoretically
well-founded algorithm  whose performance is practical for this latter problem. We also proposed
a novel variant of the algorithm with which we can save a large amount of computation without
sacriﬁcing performance.
We expect that the novel tail inequality will also be useful in a number of other situations thanks
to its self-normalized form and that it holds for stopped martingales and thus can be used to derive
bounds that hold uniformly in time. In general  the new inequality can be used to improve deviation
bounds which use a union bound (over time). Since many modern machine learning techniques
rely on having tight high-probability bounds  we expect that the new inequality will ﬁnd many
applications. Just to mention a few examples  the new inequality could be used to improve the
computational complexity of the HOO algorithm Bubeck et al. (2008) (when it is used with a ﬁxed
  by avoiding union bounds  or the need to know the horizon  or the doubling trick) or to improve
the bounds derived by Garivier and Moulines (2008) for UCB for changing environments  or the
stopping rules and racing algorithms of Mnih et al. (2008).

8

References
Y. Abbasi-Yadkori  A. Antos  and Cs. Szepesv´ari. Forced-exploration based algorithms for playing
in stochastic linear bandits. In COLT Workshop on On-line Learning with Limited Feedback  2009.
N. Abe  A. W. Biermann  and P. M. Long. Reinforcement learning with immediate rewards and

linear hypotheses. Algorithmica  37:263293  2003.

A. Antos  V. Grover  and Cs. Szepesv´ari. Active learning in heteroscedastic noise. Theoretical

Computer Science  411(29-30):2712–2728  2010.

J.-Y. Audibert  R. Munos  and Csaba Szepesv´ari. Exploration-exploitation tradeoff using variance

estimates in multi-armed bandits. Theoretical Computer Science  410(19):1876–1902  2009.

P. Auer. Using upper conﬁdence bounds for online learning. In FOCS  pages 270–279  2000.
P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. JMLR  2002.
P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002.

S. Bubeck  R. Munos  G. Stoltz  and Cs. Szepesv´ari. Online optimization in X-armed bandits. In

NIPS  pages 201–208  2008.

N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. 2006.
W. Chu  L. Li  L. Reyzin  and R. E. Schapire. Contextual bandits with linear payoff functions. In

AISTATS  2011.

P.-A. Coquelin and R. Munos. Bandit algorithms for tree search. In UAI  2007.
V. Dani  T. P. Hayes  and S. M. Kakade. Stochastic linear optimization under bandit feedback. In

Rocco Servedio and Tong Zhang  editors  COLT  pages 355–366  2008.

V. H. de la Pe˜na  M. J. Klass  and T. L. Lai. Self-normalized processes: exponential inequalities 

moment bounds and iterated logarithm laws. Annals of Probability  32(3):1902–1933  2004.

V. H. de la Pe˜na  T. L. Lai  and Q.-M. Shao. Self-normalized processes: Limit theory and Statistical

Applications. Springer  2009.

O. Dekel  C. Gentile  and K. Sridharan. Robust selective sampling from single and multiple

teachers. In COLT  2010.

D. A. Freedman. On tail probabilities for martingales. The Annals of Probability  3(1):100–118 

1975.

A. Garivier and E. Moulines. On upper-conﬁdence bound policies for non-stationary bandit

problems. Technical report  LTCI  2008.

R. Kleinberg  A. Niculescu-Mizil  and Y. Sharma. Regret bounds for sleeping experts and bandits.

Machine learning  pages 1–28  2008.

T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied

Mathematics  6:4–22  1985.

T. L. Lai and C. Z. Wei. Least squares estimates in stochastic regression models with applications
to identiﬁcation and control of dynamic systems. The Annals of Statistics  10(1):154–166  1982.
T. L. Lai  H. Robbins  and C. Z. Wei. Strong consistency of least squares estimates in multiple

regression. Proceedings of the National Academy of Sciences  75(7):3034–3036  1979.

L. Li  W. Chu  J. Langford  and R. E. Schapire. A contextual-bandit approach to personalized news
In Proceedings of the 19th International Conference on World Wide

article recommendation.
Web (WWW 2010)  pages 661–670. ACM  2010.

V. Mnih  Cs. Szepesv´ari  and J.-Y. Audibert. Empirical Bernstein stopping. pages 672–679  2008.
H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American

Mathematical Society  58:527–535  1952.

P. Rusmevichientong and J. N. Tsitsiklis. Linearly parameterized bandits. Mathematics of

Operations Research  35(2):395–411  2010.

G. W. Stewart and J.-G. Sun. Matrix Perturbation Theory. Academic Press  1990.
T. J. Walsh  I. Szita  C. Diuk  and M. L. Littman. Exploring compact reinforcement-learning

representations with linear regression. In UAI  pages 591–598. AUAI Press  2009.

9

,Peter Sadowski
Daniel Whiteson
Pierre Baldi
Akshay Krishnamurthy
Alekh Agarwal
Miro Dudik
Shannon McCurdy