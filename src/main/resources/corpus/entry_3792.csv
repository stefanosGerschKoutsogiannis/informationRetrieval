2019,Non-asymptotic Analysis of Stochastic Methods for Non-Smooth Non-Convex Regularized Problems,Stochastic Proximal Gradient (SPG) methods have been widely used for solving optimization problems with a simple (possibly non-smooth) regularizer in machine learning and statistics. However  to the best of our knowledge no non-asymptotic  convergence analysis of SPG exists for non-convex optimization with a non-smooth and non-convex regularizer. All existing non-asymptotic  analysis of SPG for solving non-smooth non-convex problems require the non-smooth regularizer to be a convex function  and hence are not applicable to a non-smooth non-convex regularized problem. This work initiates the analysis to bridge this gap and opens the door to non-asymptotic convergence analysis of non-smooth non-convex regularized problems. We analyze several variants of mini-batch SPG methods for minimizing a non-convex objective that consists of a smooth non-convex loss and a non-smooth non-convex regularizer. Our contributions are two-fold: (i) we show that they enjoy the same complexities as their counterparts for solving convex regularized non-convex problems in terms of finding an approximate stationary point; (ii) we develop more practical variants using dynamic mini-batch size instead of a fixed mini-batch size without requiring  the target accuracy level of solution.  The significance of our results  is that they improve  upon the-state-of-art results for solving non-smooth non-convex regularized problems. We also empirically demonstrate the effectiveness of the considered SPG methods in comparison with other peer stochastic methods.,Non-asymptotic Analysis of Stochastic Methods for
Non-Smooth Non-Convex Regularized Problems

Yi Xu1  Rong Jin2  Tianbao Yang1

1. Department of Computer Science  The University of Iowa  Iowa City  IA 52246  USA

2. Machine Intelligence Technology  Alibaba Group  Bellevue  WA 98004  USA

{yi-xu  tianbao-yang}@uiowa.edu  jinrong.jr@alibaba-inc.com

Abstract

Stochastic Proximal Gradient (SPG) methods have been widely used for solv-
ing optimization problems with a simple (possibly non-smooth) regularizer in
machine learning and statistics. However  to the best of our knowledge no non-
asymptotic convergence analysis of SPG exists for non-convex optimization with a
non-smooth and non-convex regularizer. All existing non-asymptotic analysis
of SPG for solving non-smooth non-convex problems require the non-smooth
regularizer to be a convex function  and hence are not applicable to a non-smooth
non-convex regularized problem. This work initiates the analysis to bridge this
gap and opens the door to non-asymptotic convergence analysis of non-smooth
non-convex regularized problems. We analyze several variants of mini-batch
SPG methods for minimizing a non-convex objective that consists of a smooth
non-convex loss and a non-smooth non-convex regularizer. Our contributions are
two-fold: (i) we show that they enjoy the same complexities as their counterparts
for solving convex regularized non-convex problems in terms of ﬁnding an ap-
proximate stationary point; (ii) we develop more practical variants using dynamic
mini-batch size instead of a ﬁxed mini-batch size without requiring the target
accuracy level of solution. The signiﬁcance of our results is that they improve upon
the-state-of-art results for solving non-smooth non-convex regularized problems.
We also empirically demonstrate the effectiveness of the considered SPG methods
in comparison with other peer stochastic methods.

1

Introduction

In this work  we consider the following stochastic non-smooth non-convex optimization problem:

F (x) := Eξ[f (x; ξ)]

+r(x) 

(1)

min
x∈Rd

where ξ is a random variable  f (x) is a smooth non-convex function  and r(x) : Rd → R is a proper
non-smooth non-convex lower-semicontinuous function. A special case of problem (1) in machine
learning is of the following ﬁnite-sum form:

(cid:124)

(cid:123)(cid:122)

f (x)

(cid:125)

min
x∈Rd

F (x) :=

1
n

n(cid:88)

i=1

fi(x) + r(x) 

(2)

where n is the number of data samples. In the sequel  we refer to the problem (1) with a ﬁnite-sum
structure as in the ﬁnite-sum setting and otherwise as in the online setting [29  43]. The family of
optimization problems with a non-convex smooth loss and a non-convex non-smooth regularizer
is important and broad in machine learning and statistics. Examples of smooth non-convex losses
include non-linear square loss for classiﬁcation [20]  truncated square loss for regression [44]  and
cross-entropy loss for learning a neural network with a smooth activation function. Examples of
33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Summary of Complexities for ﬁnding an -stationary point of (1). LC denotes Lipchitz
continuous function; FV means ﬁnite-valued over Rd; PM denotes the proximal mapping exists and

can be obtained efﬁciently. (cid:101)O(·) suppresses a logarithmic factor in terms of −1.

complexity

Algorithm
MBSGA [29]  SSDC-SPG [43] O(−5)
O(−6)
SSDC-SPG [43]
O(−4)
MB-SPG (this work)
O(−3)
SPGR (this work)
O(n2/3−3)

Problem
Online
Online
Online
Online
Finite-sum VRSGA [29]
Finite-sum SSDC-SVRG [43]
Finite-sum SSDC-SVRG [43]
Finite-sum SPGR (this work)

(cid:101)O(n−3)
(cid:101)O(n−4)

O(n1/2−2 + n)

r(x)
PM  LC
PM  FV
PM
PM
PM  LC
PM  LC
PM  FV
PM

non-smooth non-convex regualerizers include (cid:96)p (0 ≤ p < 1) norm  smoothly clipped absolute
deviation (SCAD) [17]  log-sum penalty (LSP) [9]  minimax concave penalty (MCP) [48]  and an
indicator function of a non-convex constraint as well (e.g.  (cid:107)x(cid:107)0 ≤ k).
Although non-convex minimization with a non-smooth convex regularizer has been extensively
studied in both online setting [19  14  41  35] and ﬁnite-sum setting [16  38  1  34  26  12  41  35] 
stochastic optimization for the considered problem with a non-smooth non-convex regularizer is
still under-explored. The presence of non-smooth non-convex functions r makes the analysis more
challenging  which renders previous analysis that hinges on the convexity of r not applicable. A
special case of non-convex r that can be written as a DC (Difference of Convex) function  i.e. 
r(x) = r1(x) − r2(x) with r1 and r2 being convex  has been recently tackled by several studies
with stochastic algorithms [43  33  40]. In this paper  we focus on ﬁrst-order stochastic algorithms
for solving the problem (1) with a general non-smooth non-convex regularizer and study their
non-asymptotic convergence rates.
Although there are plenty of studies devoted to non-smooth non-convex regularized problems [3  5 
49  23  25  47  6  2  46  27]  they are restricted to deterministic algorithms and asymptotic or local
convergence analysis. There are few studies concerned with the non-asymptotic convergence analysis
of stochastic algorithms for the problem (1). To the best of our knowledge  [43] is the ﬁrst work that
presents stochastic algorithms with non-asymptotic convergence results for ﬁnding an approximate
critical point of a non-convex problem with a non-convex non-smooth regularizer. Indeed  they
considered a more general problem in which f is a DC function and assumed that the second
component of the DC decomposition of f has a Hölder-continuous gradient. Their convergence
results are the state-of-the-art for stochastic optimization of the problem (1) in the online setting.
Later  [29] presented two algorithms  namely mini-batch stochastic gradient algorithm (MBSGA) and
variance reduced stochastic gradient algorithm (VRSGA)  for solving (1) and (2) with an improved
complexity for the ﬁnite-sum setting. To tackle the non-smooth non-convex regularizer  both of these
works use a Moreau envelope of r to approximate r  which inevitably introduces approximation error
and hence worsen the convergence rates.
A simple idea for tackling a non-smooth regularizer is to use proximal gradient methods  which
has been studied extensively in the literature for a convex regularizer [19  14  16  38  1  34  26 
12  41  35]. A natural question is whether stochastic proximal gradient (SPG) methods still enjoy
similar convergence guarantee for solving a non-smooth non-convex regularized problem as their
counterparts for convex regularized non-convex minimization problems. In this paper  we provide an
afﬁrmative answer to this question. Our contributions are summarized below:

• We establish the ﬁrst convergence rate of standard mini-batch SPG (MB-SPG) for solving (1) in
terms of ﬁnding an approximate stationary point  which is the same as its counterpart for solving a
non-convex minimization problem with a convex regularizer [19].

• Furthermore  we analyze improved variants of mini-batch SPG that use a recursive stochastic
gradient estimator (SARAH [32  31] or SPIDER [18  41]) referred to as SPGR  and achieve the
new state of the art convergence results for both online setting and the ﬁnite-sum setting.

2

• Moreover  we propose more practical variants of MB-SPG and SPGR by using dynamic mini-
batch size instead of a ﬁxed mini-batch size to remove the requirement on the target accuracy level
of solution for running the algorithms.

The complexity results of our algorithms and other works for ﬁnding an -stationary solution of the
considered problem are summarized in Table 1. It is notable that the complexity result of SPGR
for the ﬁnite-sum setting is optimal matching an existing lower bound [18]. Before ending this
section  it is worth mentioning that the differences between this work and [15] that provides the ﬁrst
convergence analysis of SPG to critical points of a non-smooth non-convex minimization problem: (i)
their convergence analysis is asymptotic and hence provides no convergence rate; (ii) their analysis
applies to non-smooth f but requires stronger assumptions on r (e.g.  local Lipchitz continuity) that
precludes (cid:96)0 norm regularizer or an indicator function of a non-convex constraint; (ii) their analyzed
SPG imposes no requirement on the mini-batch size.

(cid:110)

(cid:80)

2 Preliminaries
In this section  we present some preliminaries and notations. Let (cid:107)x(cid:107) denote the Euclidean
norm of a vector x ∈ Rd. Denote by S = {ξ1  . . .   ξm} a set of random variables  let
|S| be the number of elements in set S and fS (x) = 1|S|
ξi∈S f (x; ξi). We denote by
dist(x S) the distance between the vector x and a set S. Denote by ˆ∂h(x) the Fréchet sub-
gradient and ∂h(x) the limiting subgradient of a non-convex function h(x) : Rd → R  i.e. 
h−→ ¯x  vk ∈
ˆ∂h(¯x) =
ˆ∂h(xk)  vk → v}  where x h−→ ¯x means x → ¯x and h(x) → h(¯x).
We aim to ﬁnd an -stationary point of problem (1)  i.e.  to ﬁnd a solution x such that dist(0  ˆ∂F (x)) ≤
. Since f is differentiable  then we have ˆ∂F (x) = ˆ∂(f + r)(x) = ∇f (x) + ˆ∂r(x) (see Exercise
8.8  [39]). Thus  it is equivalent to ﬁnd a solution x satisfying

v ∈ Rd : limx→¯x inf h(x)−h(¯x)−v(cid:62)(x−¯x)

  ∂h(¯x) = {v ∈ Rd : ∃xk

(cid:107)x−¯x(cid:107)

≥ 0

(cid:111)

dist(0 ∇f (x) + ˆ∂r(x)) ≤ .

(3)
For problem (1)  we make the following basic assumptions  which are standard in the literature on
stochastic gradient methods for non-convex optimization [19  29].
Assumption 1. Assume the following conditions hold:
(i) Eξ[∇f (x; ξ)] = ∇f (x)  and there exists a constant σ > 0  such that Eξ[(cid:107)∇f (x; ξ) −
(ii) Given an initial point x0  there exists ∆ < ∞ such that F (x0) − F (x∗) ≤ ∆  where x∗ denotes

∇f (x)(cid:107)2] ≤ σ2.

(iii) f (x) is smooth with a L-Lipchitz continuous gradient  i.e.  it is differentiable and there exists a

the global minimum of (1).
constant L > 0 such that (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107) ∀x  y.

In addition  we assume r(x) is simple enough such that its proximal mapping exists and can be
obtained efﬁciently:

(cid:27)

proxηr[x] = arg min
y∈Rd

(cid:107)y − x(cid:107)2 + r(y)

.

This assumption is standard to proximal algorithms for non-convex functions [3  8  24]. The notation
arg min denotes the set of minimizers. The closed form of proximal mapping for non-convex regular-
izers include hard thresholding for (cid:96)0 regularizer [3]  and (cid:96)p thresholding for (cid:96)1/2 regularizer [45]
and (cid:96)2/3 regularizer [10].
An immediate difﬁculty in solving problem (1) is the presence of non-smoothness non-convexity in
the regularizer r(x). To deal with this issue  [43  29] use the the Moreau envelope of r to approximate
r  which is deﬁned as rµ(x) = miny∈Rd
  where µ > 0 is an approximation
parameter. It is easy to see that the Moreau envelope of r(x) is a DC function:

(cid:110) 1
(cid:111)
2µ(cid:107)y − x(cid:107)2 + r(y)
(cid:124)
(cid:123)(cid:122)

y(cid:62)x − 1
2µ

1
µ

Rµ(x)

(cid:107)y(cid:107)2 − r(y)
 

(cid:125)

rµ(x) =

1
2µ

(cid:107)x(cid:107)2 − max
y∈Rd

(cid:26) 1

2η

3

where Rµ(x) is convex since it is the max of convex functions in terms of x [7]. Instead of solving
the problem (1) directly  their idea is to solve the following approximated problem:

min
x∈Rd

Fµ(x) := f (x) +

(cid:107)x(cid:107)2 − Rµ(x).

1
2µ

(4)

However  this is a bad idea because it introduces the approximation error on one hand and slows
down the convergence on the other hand. For example  [29] considers algorithms that update the
solution based on a smooth function that is constructed by linearizing the term Rµ(x). As a result 
the smoothness constant of the resulting function is proportional to 1/µ. In order to maintain a small
approximation error  µ has to be a small value which ampliﬁes the smoothness constant dramatically.
In this paper  we consider a direct approach that updates the solution simply by a stochastic proximal
gradient update  i.e.  xt+1 ∈ proxηr[xt − ηgt]  where gt is a stochastic gradient of ∇f (xt) with
well-controlled variance  and η is a step size.

2.1 Warm-up: Proximal Gradient Descent Method
As a warm-up  we ﬁrst present the analysis of the deterministic proximal gradient descent (PGD)
method (also known as forward-backward splitting  FBS)  which updates the solutions for t =
0  . . .   T − 1 iteratively given an initial solution x0:

xt+1 ∈proxηr[xt − η∇f (xt)] = arg min
x∈Rd

r(x) + (cid:104)∇f (xt)  x − xt(cid:105) +

(cid:107)x − xt(cid:107)2

1
2η

 

(5)

(cid:26)

(cid:27)

where η is a step size. To our knowledge  non-asymptotic analysis of PGD for non-convex r(x) is not
available  though asymptotic analysis of PGD was provided in [3]. We summarize the non-asymptotic
convergence result of PGD in the following theorem  and provide a proof sketch to highlight the key
steps. The detailed proofs are provided in the supplement.
Theorem 1. Suppose Assumption 1 (ii) and (iii) hold  run (5) with η = c
T = 4(η2L2+1)
E[dist(0  ˆ∂F (xR))] ≤ .
Remark: It is notable that this complexity result is optimal according to [11] for smooth non-convex
optimization  which is the same as that for solving problem (1) when r(x) is convex [30].

L (0 < c < 1) and
η(1−ηL)2 ∆ = O(1/2) iterations  with R being uniformly sampled from {1  . . .   T} we have

Proof Sketch. For the update (5)  we can only leverage its optimality condition (e.g.  by Exercise 8.8
and Theorem 10.1 of [39]):

(xt+1 − xt) ∈ ˆ∂r(xt+1) 

− ∇f (xt) − 1
η
r(xt+1) + (cid:104)∇f (xt)  xt+1 − xt(cid:105) +
1
2η
η (xt+1 − xt) ∈ ˆ∂F (xt+1). Combining the
where the ﬁrst implies that ∇f (xt+1) − ∇f (xt) − 1
second inequality with the smoothness of f (x)  i.e.  f (xt+1) ≤ f (xt) + (cid:104)∇f (xt)  xt+1 − xt(cid:105) +
2 (cid:107)xt+1 − xt(cid:107)2  we get 1
2 (1/η − L)(cid:107)xt+1 − xt(cid:107)2 ≤ F (xt) − F (xt+1). By telescoping the above
inequality and connecting ˆ∂F (xt+1) with (cid:107)xt+1 − xt(cid:107) we can ﬁnish the proof.

(cid:107)xt+1 − xt(cid:107)2 ≤ r(xt) 

L

3 Mini-Batch Stochastic Proximal Gradient Methods

In this and next section  we analyze mini-batch stochastic proximal gradient methods that use a
stochastic gradient gt for updating the solution. The key idea of the two methods is to control the
variance of the stochastic gradient properly.
We present the detailed updates of the ﬁrst algorithm (named MB-SPG) in Algorithm 1  which is to
update the solution based on a mini-batched stochastic gradient of f (x) at the t-th iteration and the
proximal mapping of r(x). We ﬁrst present a general convergence result of Algorithm 1.
Theorem 2. Suppose Assumption 1 holds  run Algorithm 1 with η = c
xR of Algorithm 1 satisﬁes E[dist(0  ˆ∂F (xR))2] ≤ c1
c1 = 2c(1−2c)+2
c(1−2c)

(cid:80)T−1
t=0 E[(cid:107)gt − ∇f (xt)(cid:107)2] + c2∆

2 )  then the output
ηT   where

1−2c are two positive constants.

and c2 = 6−4c

L (0 < c < 1

T

4

Algorithm 1 Mini-Batch Stochastic Proximal Gradient: MB-SPG
1: Initialize: x0 ∈ Rd  η = c
2: for t = 0  1  . . .   T − 1 do
Draw samples St = {ξi  . . .   ξmt}  let gt = 1
3:
xt+1 ∈ proxηr[xt − ηgt]
4:
5: end for
6: Output: xR  where R is uniformly sampled from {1  . . .   T}.

L with 0 < c < 1
2.

(cid:80)mt
it=1 ∇f (xt; ξit)

mt

L with 0 < c < 1
3.

Algorithm 2 Stochastic Proximal Gradient using SPIDER/SARAH: SPGR
1: Initialize: x0 ∈ Rd  η = c
2: for t = 0  1  . . .   T − 1 do
if mod(t  q) == 0 then
3:
4:
5:
6:
7:
8:
9: end for
10: Output: xR  where R is uniformly sampled from {1  . . .   T}.

Draw samples S1  let gt = ∇fS1(xt) // For ﬁnite-sum setting  |S1| = n
Draw samples S2  let gt = ∇fS2(xt) − ∇fS2(xt−1) + gt−1

end if
xt+1 ∈ proxηr[xt − ηgt]

else

L (0 < c < 1

Next  we present two corollaries by using a ﬁxed mini-batch size and increasing mini-batch sizes.
Corollary 3 (Fixed mini-batch size). Suppose Assumption 1 holds  run MB-SPG (Algorithm 1)
with η = c
2 )  T = 2c2∆/(η2) and a ﬁxed mini-batch size mt = 2c1σ2/2 for
t = 0  . . .   T − 1  then the output xR of Algorithm 1 satisﬁes E[dist(0  ˆ∂F (xR))2] ≤ 2  where c1  c2
are two positive constants as in Theorem 2.
Corollary 4 (Increasing mini-batch sizes). Suppose Assumption 1 holds  run MB-SPG (Algorithm 1)
2 ) and a sequence of mini-batch sizes mt = b(t + 1) for t = 0  . . .   T − 1 
with η = c
where b > 0 is a constant  then the output xR of Algorithm 1 satisﬁes E[dist(0  ˆ∂F (xR))2] ≤
ηT   where c1  c2 are constants as in Theorem 2. In particular in order to have

E[dist(0  ˆ∂F (xR))] ≤   it sufﬁces to set T = (cid:101)O(1/2). The total complexity is (cid:101)O(1/4).

L (0 < c < 1

+ c2∆

c1σ2(log(T )+1)

bT

Remark: Although using increasing mini-batch sizes has an additional logarithmic factor in the
complexity than that using a ﬁxed mini-batch size  it would be more practical and user-friendly
because it does not require knowing the target accuracy  to run the algorithm .
4 Stochastic Proximal Gradient Methods with Recursive Stochastic

Gradient Estimator

In this section  we levearage the novel recursive stochastic gradient estimator (SARAH/SPIDER) for
achieving a better complexity. We present the detailed updates of the proposed algorithm referred
to as SPGR in Algorithm 2  where the stochastic gradient estimate gt is periodically updated by
adding current stochastic gradient ∇fS2(xt) and subtracting the past stochastic gradient ∇fS2 (xt−1)
from gt−1. To our knowledge  this framework was ﬁrstly introduced in SARAH [32  31] for
solving convex/nonconvex smooth ﬁnite-sum problems with r(x) = 0. Another algorithm so-called
SPIDER with same recursive framework was proposed in [18] for solving non-convex smooth
problems with r(x) = 0 both in ﬁnite-sum and online settings. One difference is that SPIDER
uses normalized gradient update with step size η = O(/L). Recently  [41] and [35] respectively
extended SPIDER and SARAH to their proximal versions for solving non-convex smooth problems
with convex non-smooth regularizer r(x). By contrast  we consider more challenging problems in
this paper  i.e.  non-convex non-smooth regularized non-convex minimization problems. In order to
use the SARAH/SPIDER technique to construct a variance-reduced stochastic gradient of f  we need
additional assumption  which is also used in previous studies [31  18  41  35].
Assumption 2. Assume that every random function f (x; ξ) is smooth with a L-Lipchitz continuous
gradient  i.e.  it is differentiable and there exists a constant L > 0 such that (cid:107)∇f (x; ξ)−∇f (y; ξ)(cid:107) ≤
L(cid:107)x − y(cid:107) ∀x  y.

5

6  b ≥ 1

L with 0 < c < 1

Draw samples S1 s  let gt = ∇fS1 s (xt)
xt+1 ∈ proxηr[xt − ηgt]  t = t + 1
for q = 1  . . .   bs do

Algorithm 3 SPGR with Increasing Mini-Batch sizes: SPGR-imb
1: Initialize: x0 ∈ Rd  η = c
2: Set: t = 0  x−1 = x0
3: for s = 1  . . .   S do
4:
5:
6:
7:
8:
9:
10: end for
11: Output: xR  where R is uniformly sampled from {1  . . .   T}.

Draw samples S2 s  let gt = ∇fS2 s (xt) − ∇fS2 s (xt−1) + gt−1
xt+1 ∈ proxηr[xt − ηgt]  t = t + 1

end for

(cid:5) |S1 s| = b2s2

(cid:5) |S2 s| = bs

1

2η

ηθT

are two positive constants.

η and θ = 1−3ηL

First  we present a general non-asymptotic convergence result of SPGR  which is summarized below.
Theorem 5. Suppose Assumptions 1 and 2 hold  run Algorithm 2 with η = c
L (0 < c < 1
3 ) and
q = |S2|  then the output xR of Algorithm 2 satisﬁes E[dist(0  ˆ∂F (xR))2] ≤ 2θ∆+γη∆
ηθT + (γ+4θL)σ2
2θL|S1|
for online setting and E[dist(0  ˆ∂F (xR))2] ≤ 2θ∆+γη∆
for ﬁnite-sum setting  where γ = 4L2 +
η2 + 2L
Although the SARAH/SPIDER update used in Algorithm 2 is similar to that used in [41  35] for
handling convex regularizers  our analysis has some key differences from that in [41  35]. In particular 
the analysis in [41  35] heavily relies on the convexity of the regularizer. In addition  they proved the
convergence of the proximal gradient deﬁned as Gη(x) = 1
η (x − proxηr(x − η∇f (x)))  while we
directly prove the convergence of the subgradient ˆ∂F (x). The convergence of the proximal gradient
only implies a weak convergence of subgradient (i.e.  a solution x which satisﬁes (cid:107)Gη(x)(cid:107) ≤ 
indicates that it is close to a solution x+ = proxηr(x − η∇f (x)) such that (cid:107) ˆ∂F (x+)(cid:107) ≤ O() when
η = Θ(1/L)). The following corollary summarizes results in the two settings.
Corollary 6. Under the same conditions and notations as in Theorem 5  in order to have
E[dist(0  ˆ∂F (xR))] ≤  we can set:

• (Online setting) q = |S2| = (cid:112)|S1|  |S1| = (γ+4θL)σ2

  and T = 2(2θ+γη)∆

ηθ2

  giving a total

θL2

complexity of O(−3).

• (Finite-sum setting) q = |S2| =

√

n  |S1| = n  and T = (2θ+γη)∆

ηθ2

  leading to a total complexity

√

of O(

n−2 + n).

Remark: It is notable that the above complexity result is near-optimal according to [18  50] for the
ﬁnite-sum setting. For same special cases of r(x)  similar complexities have been established when
r(x) = 0 [18  51] or when r(x) is convex [41  35].

4.1 SPGR with Increasing Mini-Batch Sizes
One limitation of SPGR for the online setting is that it requires knowing the target accuracy level
 in order to set q and the sizes of S1 and S2  which makes it not practical. An user will need to
worry about what is the right value of  for running the algorithm  as a small  may waste at lot of
computations and a relatively large  may not lead to an accurate solution. To address this issue  we
propose a practical variant of SPGR  namely SPGR-imb  which uses increasing mini-batch sizes. The
detailed updates are presented in Algorithm 3. The key idea is that we divide the whole progress into
S stages  and for each stage s ∈ [S]  the mini-batch sizes |S1| and |S2| are set to be proportional s2
and s  respectively. The insight of this design is similar to Algorithm 1 with increasing mini-batch
sizes  i.e.  at earlier stages when the solution is far from a stationary solution we can tolerate a large
variance in the stochastic gradient estimator and hence allow for a smaller mini-batch size. We
summarize the non-asymptotic convergence result of SPGR-imb in the following theorem.
Theorem 7. Suppose Assumptions 1 and 2 hold  run Algorithm 3 with η = c
3 ) and S
L (0 < c < 1
satisfying bS(S + 1)/2 = T   then the output xR of Algorithm 3 satisﬁes E[dist(0  ˆ∂F (xR))2] ≤
for
are two positive constants. In

for online setting and E[dist(0  ˆ∂F (xR))2] ≤ (2θ+γη)∆

ﬁnite-sum setting  where γ = 4L2 + 1

+ (4θL+γ)σ2(log(2T /b)+2)

η and θ = 1−3ηL

η2 + 2L

(2θ+γη)∆

4bθLT

θηT

θηT

2η

6

particular in order to have E[dist(0  ˆ∂F (xR))] ≤   it sufﬁces to set T = (cid:101)O(1/2). The total
complexity is (cid:101)O(1/3).

Remark: Compared to the result in Corollary 6  the complexity result of Theorem 7 is only worse
by a logarithmic factor.

5 Experiments

√

(cid:80)n
i=1(bi−σ(x(cid:62)ai))2+r(x) with a sigmod function σ(s) = 1

Regularized loss minimization. First  we compare MB-SPG  SPGR with MBSGA  VRSGA  SSDC-
(cid:80)n
SPG and SSDC-SVRG for solving the regularized non-linear least square (NLLS) classiﬁcation
problems 1
1+e−s for classiﬁcation  and
n
i=1 α log(1+(yi−w(cid:62)xi)2/α)+r(x)
the regularized truncated least square (TLS) loss function 1
2n
for regression [44]. Two data sets (covtype and a9a) are used for classiﬁcation  and two data sets
E2006 and triazines are used for regression. These data sets are downloaded from the libsvm website.
We use three different non-smooth non-convex regularizers  i.e.  (cid:96)0 regularizer r(x) = λ(cid:107)x(cid:107)0  (cid:96)0.5
regularizer r(x) = λ(cid:107)x(cid:107)0.5  and indicator function of (cid:96)0 constraint I{(cid:107)x(cid:107)0≤κ}(x). The truncation
10n following [44]. The value of regularization parameter λ is ﬁxed as 10−4
value α is set to
and the value of κ is ﬁxed as 0.2d where d is the dimension of data. For all algorithms  we use the
theoretical values of the parameters for the sake of fairness in comparison. All algorithms start with
the same initial solution with all zero entries. We implement the increasing mini-batch versions of
MB-SPG and SPGR (online setting) with b = 1. The unknown parameter σ in MBSGA is estimated
following [29]. The objective value (in log scale) versus the number of gradient computations for
different tasks are plotted in Figure 1. The solid lines correspond to algorithms running in the online
setting and the dashed lines correspond to algorithms running in the ﬁnite-sum setting. By comparing
algorithms running in the online setting including MB-SPG  SPGR  MBSGA and SSDC-SPG  we can
see that the proposed algorithms (MB-SPG and SPGR) are faster across different tasks. In addition 
SPGR is faster than MB-SPG. These results are consistent with our theory. By comparing algorithms
running in the ﬁnite-sum setting including VRSGA  SSDC-SVRG and SPGR  we can see that the
proposed SPGR is much faster  which also corroborates our theory.
Learning with Quantization. Second  we consider the problem of learning a quantized model
where the model parameter is represented by a small number of bits (e.g.  2 bits that can encode 1
or −1). It has received tremendous attention in deep learning for model compression [21  42  36].
An idea to formulate the problem is to consider a constrained optimization problem: minx∈Ω f (x)
where Ω denotes a discrete set including the values that can be represented by a small number of bits.
However  ﬁnding a stationary point for this problem is meaningless. This is because that for a discrete
set Ω  the subgradient of its indicator function IΩ(x) is the whole space [13  22]. Hence  we have
0 ∈ ˆ∂(f (x) + IΩ(x)) for any x ∈ Ω. To avoid this issue  we consider a different formulation by using
2(cid:107)x − PΩ(x)(cid:107)2  where PΩ(x) is a projection onto
a penalization of the constraint: minx∈Rd f (x) + λ
the set Ω and λ > 0 is a penalization parameter. This penalization-based approach is one standard
way to handle complicated constraints [4  28]. It is notable that in general the penalization term is
a non-smooth non-convex function of x for a non-convex set Ω  though its local smoothness has
been proved under some regularity condition of Ω [37]. The proximal mapping of the penalization
term has a closed-form solution as long as PΩ(x) can be easily computed [24]  which corresponds to
quantization for our considered problem.
In the experiment  we use the NLLS loss similar to regularized loss minimization for learning a
quantized non-linear model  and focus on comparison of algorithms running in the online setting
including MBSGA  SSDC-SPG  MB-SPG and SPGR. We also implement a popular heuristic SGD
approach in deep learning for learning a quantized model [36]  which updates the solution simply by
xt+1 = xt − ηt∇f (ˆxt; ξt) where ˆxt = PΩ(xt) is the quantized model. We conduct the experiments
on four data sets mnist  news20  rcv1  w8a  where the last three data sets are downloaded from the
libsvm website. We compare the testing accuracy of learned quantized model versus the number of
iterations  and the results are plotted in Figure 2  where q denotes the number of bits for quantization.
We ﬁx λ = 1  and decrease the step size by half every 100 iterations for heuristic SGD  MBSGA and
MB-SPG. This is helpful for generalization purpose. We can see that the proposed SPGR algorithm
has better testing accuracy in most cases  and the proposed MB-SPG has comparable performance if
not better results than other baselines.

7

Figure 1: Comparisons of different algorithms for regularized loss minimization.

Figure 2: Comparisons of different algorithms for learning with quantization.

6 Conclusions
In this paper  we have presented the ﬁrst non-asymptotic convergence analysis of stochastic proximal
gradient methods for solving a non-convex optimization problem with a smooth loss function and
a non-smooth non-convex regularizer. The proposed algorithms enjoy improved complexities than
the state-of-the-art results for the same problems  and also match the existing complexity results for
solving non-convex minimization problems with a smooth loss and a non-smooth convex regularizer.
Acknowledgements
The authors thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are
partially supported by National Science Foundation (IIS-1545995).

8

# gradient#10602468log10(objective)-0.35-0.3-0.25-0.2-0.15-0.1-0.0500.050.1NLLS+`0 covtypeMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1050246810log10(objective)00.050.10.150.20.25NLLS+`0 a9aMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1040246810log10(objective)-1.2-1-0.8-0.6-0.4-0.200.20.40.60.8TLS+`0 E2006MBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1050246810log10(objective)-2-1.8-1.6-1.4-1.2-1-0.8-0.6TLS+`0 triazinesMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#10602468log10(objective)-0.3-0.25-0.2-0.15-0.1-0.0500.050.1NLLS+`0:5 covtypeMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1050246810log10(objective)00.050.10.150.20.25NLLS+`0:5 a9aMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1040246810log10(objective)-1.2-1-0.8-0.6-0.4-0.200.20.40.60.8TLS+`0:5 E2006MBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1050246810log10(objective)-2-1.8-1.6-1.4-1.2-1-0.8-0.6TLS+`0:5 triazinesMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#10602468log10(objective)-0.35-0.3-0.25-0.2-0.15-0.1-0.0500.050.1NLLS+`0constraint covtypeMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1050246810log10(objective)00.050.10.150.20.25NLLS+`0constraint a9aMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1040246810log10(objective)-1.4-1.2-1-0.8-0.6-0.4-0.200.2TLS+`0constraint E2006MBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1050246810log10(objective)-2.2-2-1.8-1.6-1.4-1.2-1-0.8-0.6TLS+`0constraint triazinesMBSGAVRSGASSDC-SPGSSDC-SVRGMB-SPGSPGR(online)SPGR(-nite-sum)# gradient#1040246810testing accuracy0.40.450.50.550.60.650.70.750.80.850.9quantization(q=2) mnistSGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.20.250.30.350.40.450.50.550.60.65quantization(q=2) news20SGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.9550.960.9650.970.9750.98quantization(q=2) w8aSGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.650.70.750.80.850.90.951quantization(q=2) rcv1.binarySGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.40.450.50.550.60.650.70.750.80.850.9quantization(q=4) mnistSGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.20.250.30.350.40.450.50.550.60.65quantization(q=4) news20SGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.9550.960.9650.970.9750.98quantization(q=4) w8aSGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.650.70.750.80.850.90.951quantization(q=4) rcv1.binarySGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.40.450.50.550.60.650.70.750.80.850.9quantization(q=8) mnistSGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.20.250.30.350.40.450.50.550.60.65quantization(q=8) news20SGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.9550.960.9650.970.9750.98quantization(q=8) w8aSGD-heuristicMBSGASSDC-SPGMB-SPGSPGR# gradient#1040246810testing accuracy0.650.70.750.80.850.90.951quantization(q=8) rcv1.binarySGD-heuristicMBSGASSDC-SPGMB-SPGSPGRReferences
[1] Z. Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex

parameter. In International Conference on Machine Learning  pages 89–97  2017.

[2] N. T. An and N. M. Nam. Convergence analysis of a proximal point algorithm for minimizing

differences of functions. Optimization  66(1):129–147  2017.

[3] H. Attouch  J. Bolte  and B. F. Svaiter. Convergence of descent methods for semi-algebraic and
tame problems: proximal algorithms  forward–backward splitting  and regularized gauss–seidel
methods. Mathematical Programming  137(1):91–129  Feb 2013.

[4] D. P. Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press 

2014.

[5] J. Bolte  S. Sabach  and M. Teboulle. Proximal alternating linearized minimization for noncon-

vex and nonsmooth problems. Mathematical Programming  146(1-2):459–494  Aug. 2014.

[6] R. I. Bot  E. R. Csetnek  and S. C. László. An inertial forward–backward algorithm for the
minimization of the sum of two nonconvex functions. EURO Journal on Computational
Optimization  4(1):3–25  Feb 2016.

[7] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

[8] K. Bredies  D. A. Lorenz  and S. Reiterer. Minimization of non-smooth  non-convex functionals
by iterative thresholding. Journal of Optimization Theory and Applications  165(1):78–112 
2015.

[9] E. J. Candès  M. B. Wakin  and S. P. Boyd. Enhancing sparsity by reweighted l1 minimization.

Journal of Fourier Analysis and Applications  14(5):877–905  Dec 2008.

[10] W. Cao  J. Sun  and Z. Xu. Fast image deconvolution using closed-form thresholding formulas of
lq (q = 1/2  2/3) regularization. Journal of Visual Communication and Image Representation 
24(1):31–41  2013.

[11] Y. Carmon  J. C. Duchi  O. Hinder  and A. Sidford. Lower bounds for ﬁnding stationary points

i. arXiv preprint arXiv:abs/1710.11606  2017.

[12] Z. Chen and T. Yang. A variance reduction method for non-convex optimization with improved

convergence under large condition number. arXiv preprint arXiv:1809.06754  2018.

[13] F. H. Clarke. Optimization and nonsmooth analysis  volume 5. SIAM  1990.

[14] D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions.

SIAM Journal on Optimization  29(1):207–239  2019.

[15] D. Davis  D. Drusvyatskiy  S. Kakade  and J. D. Lee. Stochastic subgradient method converges

on tame functions. Foundations of Computational Mathematics  pages 1–36  2018.

[16] A. Defazio  F. R. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems  pages 1646–1654  2014.

[17] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties.

Journal of the American Statistical Association  96(456):1348–1360  2001.

[18] C. Fang  C. J. Li  Z. Lin  and T. Zhang. SPIDER: Near-optimal non-convex optimization via
stochastic path-integrated differential estimator. In Advances in Neural Information Processing
Systems  pages 687–697  2018.

[19] S. Ghadimi  G. Lan  and H. Zhang. Mini-batch stochastic approximation methods for nonconvex

stochastic composite optimization. Mathematical Programming  155(1-2):267–305  2016.

[20] I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. MIT Press.

9

[21] S. Han  H. Mao  and W. J. Dally. Deep compression: Compressing deep neural networks with

pruning  trained quantization and huffman coding. arXiv preprint arXiv:1510.00149  2015.

[22] A. Y. Kruger. On fréchet subdifferentials. Journal of Mathematical Sciences  116(3):3325–3358 

2003.

[23] G. Li and T. K. Pong. Global convergence of splitting methods for nonconvex composite

optimization. SIAM Journal on Optimization  25(4):2434–2460  2015.

[24] G. Li and T. K. Pong. Douglas-rachford splitting for nonconvex optimization with application

to nonconvex feasibility problems. Mathematical Programming  159(1-2):371–401  2016.

[25] H. Li and Z. Lin. Accelerated proximal gradient methods for nonconvex programming. In
Advances in Neural Information Processing Systems  pages 379–387  Cambridge  MA  USA 
2015. MIT Press.

[26] Z. Li and J. Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems  pages 5569–5579  2018.

[27] T. Liu  T. K. Pong  and A. Takeda. A successive difference-of-convex approximation method
for a class of nonconvex nonsmooth optimization problems. Mathematical Programming  pages
1–29  2017.

[28] D. G. Luenberger and Y. Ye. Linear and Nonlinear Programming  volume 228. Springer  2015.

[29] M. R. Metel and A. Takeda. Stochastic gradient methods for non-smooth non-convex regularized

optimization. arXiv preprint arXiv:1901.08369  2019.

[30] Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Program-

ming  140(1):125–161  2013.

[31] L. M. Nguyen  J. Liu  K. Scheinberg  and M. Takác. Stochastic recursive gradient algorithm for

nonconvex optimization. arXiv preprint arXiv:1705.07261  2017.

[32] L. M. Nguyen  J. Liu  K. Scheinberg  and M. Takác. SARAH: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning  pages 2613–2621  2017.

[33] A. Nitanda and T. Suzuki. Stochastic Difference of Convex Algorithm and its Application to
Training Deep Boltzmann Machines. In International Conference on Artiﬁcial Intelligence and
Statistics  pages 470–478  2017.

[34] C. Paquette  H. Lin  D. Drusvyatskiy  J. Mairal  and Z. Harchaoui. Catalyst for gradient-based
nonconvex optimization. In International Conference on Artiﬁcial Intelligence and Statistics 
pages 1–10  2018.

[35] N. H. Pham  L. M. Nguyen  D. T. Phan  and Q. Tran-Dinh. ProxSARAH: An efﬁcient
algorithmic framework for stochastic composite nonconvex optimization. arXiv preprint
arXiv:1902.05679  2019.

[36] A. Polino  R. Pascanu  and D. Alistarh. Model compression via distillation and quantization. In

International Conference on Learning Representations  2018.

[37] R. Poliquin  R. R. T.  and T. L. Local differentiability of distance functions. Transactions of the

American Mathematical Society  352:5231–5249  01 2000.

[38] S. J. Reddi  S. Sra  B. Póczos  and A. J. Smola. Proximal stochastic methods for nonsmooth
nonconvex ﬁnite-sum optimization. In Advances in Neural Information Processing Systems 
pages 1145–1153  2016.

[39] R. Rockafellar and R. J.-B. Wets. Variational Analysis. Springer Verlag  Heidelberg  Berlin 

New York  1998.

10

[40] H. A. L. Thi  H. M. Le  D. N. Phan  and B. Tran. Stochastic DCA for the large-sum of non-
convex functions problem and its application to group variable selection in classiﬁcation. In
International Conference on Machine Learning  pages 3394–3403  2017.

[41] Z. Wang  K. Ji  Y. Zhou  Y. Liang  and V. Tarokh. SpiderBoost: A class of faster variance-

reduced algorithms for nonconvex optimization. arXiv preprint arXiv:1810.10690  2018.

[42] J. Wu  C. Leng  Y. Wang  Q. Hu  and J. Cheng. Quantized convolutional neural networks
for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 4820–4828  2016.

[43] Y. Xu  Q. Qi  Q. Lin  R. Jin  and T. Yang. Stochastic optimization for dc functions
and non-smooth non-convex regularizers with non-asymptotic convergence. arXiv preprint
arXiv:1811.11829  2018.

[44] Y. Xu  S. Zhu  S. Yang  C. Zhang  R. Jin  and T. Yang. Learning with non-convex truncated

losses by SGD. arXiv preprint arXiv:1805.07880  2018.

[45] Z. Xu  X. Chang  F. Xu  and H. Zhang. l1/2 regularization: A thresholding representation theory
and a fast solver. IEEE Transactions on neural networks and learning systems  23(7):1013–1027 
2012.

[46] L. Yang. Proximal gradient method with extrapolation and line search for a class of nonconvex

and nonsmooth problems. arXiv preprint arXiv:1711.06831  2018.

[47] Y. Yu  X. Zheng  M. Marchetti-Bowick  and E. P. Xing. Minimizing nonconvex non-separable
functions. In International Conference on Artiﬁcial Intelligence and Statistics  pages 1107–1115 
2015.

[48] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals

of Statistics  38:894 – 942  2010.

[49] W. Zhong and J. T. Kwok. Gradient descent with proximal average for nonconvex and composite

regularization. In AAAI Conference on Artiﬁcial Intelligence  pages 2206–2212  2014.

[50] D. Zhou and Q. Gu. Lower bounds for smooth nonconvex ﬁnite-sum optimization. arXiv

preprint arXiv:1901.11224  2019.

[51] D. Zhou  P. Xu  and Q. Gu. Stochastic nested variance reduced gradient descent for nonconvex
optimization. In Advances in Neural Information Processing Systems  pages 3925–3936  2018.

11

,Yi Xu
Rong Jin
Tianbao Yang