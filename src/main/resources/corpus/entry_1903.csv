2014,Exclusive Feature Learning on Arbitrary Structures via $\ell_{1 2}$-norm,Group lasso is widely used to enforce the structural sparsity  which achieves the sparsity at inter-group level. In this paper  we propose a new formulation called ``exclusive group lasso''  which brings out sparsity at intra-group level in the context of feature selection. The proposed exclusive group lasso is applicable on any feature structures  regardless of their overlapping or non-overlapping structures. We give analysis on the properties of exclusive group lasso  and propose an effective iteratively re-weighted algorithm to solve the corresponding optimization problem with rigorous convergence analysis. We show applications of exclusive group lasso for uncorrelated feature selection. Extensive experiments on both synthetic and real-world datasets indicate the good performance of proposed methods.,Exclusive Feature Learning on Arbitrary Structures

via (cid:96)1 2-norm

Deguang Kong1  Ryohei Fujimaki2  Ji Liu3  Feiping Nie1  Chris Ding1
1 Dept. of Computer Science  University of Texas Arlington  TX  76019;

2 NEC Laboratories America  Cupertino  CA  95014;

3 Dept. of Computer Science  University of Rochester  Rochester  NY  14627
Email: doogkong@gmail.com  rfujimaki@nec-labs.com 

jliu@cs.rochester.edu  feipingnie@gmail.com  chqding@uta.edu

Abstract

Group LASSO is widely used to enforce the structural sparsity  which achieves
the sparsity at the inter-group level. In this paper  we propose a new formulation
called “exclusive group LASSO”  which brings out sparsity at intra-group level in
the context of feature selection. The proposed exclusive group LASSO is applica-
ble on any feature structures  regardless of their overlapping or non-overlapping
structures. We provide analysis on the properties of exclusive group LASSO  and
propose an effective iteratively re-weighted algorithm to solve the corresponding
optimization problem with rigorous convergence analysis. We show applications
of exclusive group LASSO for uncorrelated feature selection. Extensive experi-
ments on both synthetic and real-world datasets validate the proposed method.

1

Introduction

Structure sparsity induced regularization terms [1  8] have been widely used recently for feature
learning purpose  due to the inherent sparse structures of the real world data. Both theoretical and
empirical studies have suggested the powerfulness of structure sparsity for feature learning  e.g. 
Lasso [24]  group LASSO [29]  exclusive LASSO [31]  fused LASSO [25]  and generalized LAS-
SO [22]. To make a compromise between the regularization term and the loss function  the sparse-
induced optimization problem is expected to ﬁt the data with better statistical properties. Moreover 
the results obtained from sparse learning are easier for interpretation  which give insights for many
practical applications  such as gene-expression analysis [9]  human activity recognition [14]  elec-
tronic medical records analysis [30]  etc.
Motivation Of all the above sparse learning methods  group LASSO [29] is known to enforce the
sparsity on variables at an inter-group level  where variables from different groups are competing to
survive. Our work is motivated from a simple observation: in practice  not only features from differ-
ent groups are competing to survive (i.e.  group LASSO)  but also features in a seemingly cohesive
group are competing to each other. The winner features in a group are set to large values  while the
loser features are set to zeros. Therefore  it leads to sparsity at the intra-group level. In order to make
a distinction with standard LASSO and group LASSO  we called it “exclusive group LASSO” regu-
larizer. In “exclusive group LASSO” regularizer  intra-group sparsity is achieved via (cid:96)1 norm  while
inter-group non-sparsity is achieved via (cid:96)2 norm. Essentially  standard group LASSO achieves spar-
sity via (cid:96)2 1 norm  while the proposed exclusive group LASSO achieves sparsity via (cid:96)1 2 norm. An
example of exclusive group LASSO is shown in Fig.(1) via Eq.(2). The signiﬁcant difference from
the standard LASSO is to encourage similar features in different groups to co-exist (Lasso usually
allows only one of them surviving). Overall  the exclusive group LASSO regularization encourages
intra-group competition but discourages inter-group competition.

1

Figure 1: Explanation of differences between group LASSO and exclusive group LASSO. Group setting:
G1 = {1  2} G2 = {3  4} G3 = {5  6  7}. Group LASSO solution of Eq.(3) at λ = 2 using least square loss
is: w = [0.0337; 0.0891; 0; 0;−0.2532; 0.043; 0.015]. exclusive group LASSO solution of Eq.(2) at λ = 10
is: w = [0.0749; 0; 0;−0.0713;−0.1888; 0; 0]. Clearly  group LASSO introduces sparsity at an inter-group
level  whereas exclusive LASSO enforces sparsity at an intra-group level.

We note that “exclusive LASSO” was ﬁrst used in [31] for multi-task learning. Our “exclusive group
LASSO” work  however  has clear difference from [31]: (1) we give a clear physical intuition of
“exclusive group LASSO”  which leads to sparsity at an intra-group level (Eq.2)  whereas [31] fo-
cuses on “Exclusive LASSO” problem in a multi-task setting; (2) we target a general “group” setting
which allows arbitrary group structure  which can be easily extended to multi-task/multi-label learn-
ing. The main contributions of this paper include: (1) we propose a new formulation of “exclusive
group LASSO” with clear physical meaning  which allow any arbitrary structure on feature space;
(2) we propose an effective iteratively re-weighted algorithm to tackle non-smooth “exclusive group
LASSO” term with rigorous convergence guarantees. Moreover  an effective algorithm is proposed
to handle both non-smooth (cid:96)1 and exclusive group LASSO term (Lemma 4.1); (3) The proposed ap-
proach is validated via experiments on both synthetic and real data sets  speciﬁcally for uncorrelated
feature selection problems.
Notation Throughout the paper  matrices are written as boldface uppercase  vectors are written
(cid:19) 1
as boldface lowercase  and scalars are denoted by lower-case letters (a  b). n is the number of
data points  p is the dimension of data  K is the number of class in a dataset. For any vector
w ∈ (cid:60)p  (cid:96)q norm of w is (cid:107)w(cid:107)q =
q for q ∈ (0 ∞). A group of variables is
a subset g ⊂ {1  2 ···   p}. Thus  the set of possible groups is the power set of {1  2 ···   p}:
P({1  2 ···   p}). Gg ∈ P({1  2 ···   p}) denotes a set of group g  which is known in advance
depending on applications. If two groups have one or more overlapped variables  we say that they are
overlapped. For any group variable wGg ∈ (cid:60)p  only the entries in the group g are preserved which
are the same as those in w  while the other entries are set to zeros. For example  if Gg = {1  2  4} 
4. Let supp(w) ⊂ {1  2 ···   p}
be a set which wi (cid:54)= 0  and zero(w) ⊂ {1  2 ···   p} be a set which wi = 0. Clearly  zero(w) =
{1  2 ···   p} \ supp(w). Let (cid:53)f (w) be gradient of f at w ∈ (cid:60)p  for any differentiable function f:
(cid:60)p → (cid:60).

wGg = [w1  w2  0  w4  0 ···   0]  then (cid:107)wGg(cid:107)2 =(cid:112)w2

(cid:18)(cid:80)p
i=1 |wj|q

1 + w2

2 + w2

2 Exclusive group LASSO
Let G be a group set  the exclusive group LASSO penalty is deﬁned as:

(cid:88)

g∈G

∀w ∈ (cid:60)p  ΩG

Eg(w) =

(cid:107)wGg(cid:107)2
1.

(1)

When the groups of g form different partitions of the set of variables  ΩG
Eg is a (cid:96)1/(cid:96)2 norm penalty.
A (cid:96)2 norm is enforced on different groups  while in each group  (cid:96)1 norm is used to make a sum
over each intra-group variable. Minimizing such a convex risk function often leads to a solution that
some entries in a group are zeros. For example  for a group Gg = {1  2  4}  there exists a solution
w  such that w1 = 0  w2 (cid:54)= 0  w4 (cid:54)= 0. A concrete example is shown in Fig.1  in which we solve:

J1(w)  J1(w) = f (w) + λΩG

Eg(w).

min
w∈(cid:60)p

(2)

using least square loss function f (w) = (cid:107)y−XT w(cid:107)2
solution of Eq.(3) 

2. As compared to standard group LASSO [29]

2

w1w2w3w4w5w6w7G1G2G3w1w2w3w4w5w6w7G1G2G3(a)grouplasso(b)exclusivelassozerovalue(a) non-overlap
(c) feature correlation ma-
trix
(a-b): Geometric shape of Ω(w) ≤ 1 in R3. (a) non-overlap exclusive group LASSO: Ω(w) =
Figure 2:
(|w1| + |w2|)2 + (|w3|)2; (b) overlap exclusive group LASSO: Ω(w) = (|w1| + |w2|)2 + (|w2| + |w3|)2;
(c) feature correlation matrix R on dataset House (506 data points  14 variables). Rij indicates the feature
correlation between feature i and j. Red colors indicate large values  while blue colors indicate small values.

(b) overlap

(cid:88)

g

f (w) + λ

(cid:107)wGg(cid:107)2.

(3)

of w =(cid:80)

We observe that group LASSO introduces sparsity at an inter-group level  whereas exclusive LASSO
enforces sparsity at an intra-group level.
Analysis of exclusive group LASSO For each group g  feature index u ∈ supp(g) will be non-zero.
Let vg ∈ (cid:60)p be a variable which preserves the values of non-zero index for group g. Consider all
groups  for optimization goal w  we have supp(w) = ∪
supp(vg). (1) For non-overlapping case 
different groups form a partition of feature set {1  2 ···   p}  and there exists a unique decomposition
g vg. Since there is not any common elements for any two different groups Gu and Gv 
i.e.  supp(wGu) ∩ supp(wGv ) = φ. thus it is easy to see: vg = wGg  ∀g ∈ G. (2) However  for
overlapping groups  there could be element sets I ⊂ (Gu ∩ Gv)  and therefore  different groups Gu
and Gv may have opposite effects to optimize the features in set I. For feature i ∈ I  it is prone to
give different values if optimized separately  i.e.  (wGu )i (cid:54)= (wGv )i. For example  Gu = [1  2] Gv =
[2  3]  whereas group u may require w2 = 0 and group v may require w2 (cid:54)= 0. Thus  there will be
many possible combinations of feature values  and it leads to: ΩG
1. Further 
if some groups are overlapped  the ﬁnal zeros sets will be a subset of unions of all different groups.
zero(w) ⊂ ∩

(cid:80)
g (cid:107)vg(cid:107)2

Eg = inf(cid:80)

g vg=w

zero(vg).

g

g

Illustration of Geometric shape of exclusive LASSO Figure 2 shows the geometric shape for
both norms in R3 with different group settings  where in (a): G1 = [1  2] G2 = [3]; and in (b):
G1 = [1  2] G2 = [2  3]. For the non-overlapping case  variables w1  w2 usually can not be zero
simultaneously. In contrast  for the overlapping case  variable w2 cannot be zero unless both groups
G1 and G2 require w2 = 0.
Properties of exclusive LASSO The regularization term of Eq.(1) is a convex formulation.
∪g∈G = {1  2 ···   p}  then ΩG

ΩG
Eg is a norm. See Appendix for proofs.

(cid:113)

E :=

If

Eg regularizer

3 An effective algorithm for solving ΩG
The challenge of solving Eq. (1) is to tackle the exclusive group LASSO term  where f (w) can be
any convex loss function w.r.t w. It is generally felt that exclusive group LASSO term is much more
difﬁcult to solve than the standard LASSO term (shrinkage thresholding). Existing algorithm can
formulate it as a quadratic programming problem [19]  which can be solved by interior point method
or active set method. However  the computational cost is expensive  which limits its use in practice.
Recently  a primal-dual algorithm [27] is proposed to solve the similar problem  which casts the
non-smooth problem into a min-max problem. However  the algorithm is a gradient descent type
method and converges slowly. Moreover  the algorithm is designed for multi-task learning problem 
and cannot be applied directly for exclusive group LASSO problem with arbitrary structures.
In the following  we ﬁrst derive a very efﬁcient yet simple algorithm. Moreover  the proposed
algorithm is a generic algorithm  which allows arbitrary structure on feature space  irrespective of
speciﬁc feature structures [10]  e.g.  linear structure [28]  tree structure [15]  graph structure [7]  etc.

3

−101−101−1−0.500.51−101−101−1−0.500.51 246810121424681012140.10.20.30.40.50.60.70.80.91Theoretical analysis guarantees the convergence of algorithm. Moreover  the algorithm is easy to
implement and ready to use in practice.
Key idea The idea of the proposed algorithm is to ﬁnd an auxiliary function for Eq.(1) which can be
easily solved. Then the updating rules for w is derived. Finally  we prove the solution is exactly the
optimal solution we are seeking for the original problem. Since it is a convex problem  the optimal
solution is the global optimal solution.
Procedure Instead of directly optimizing Eq. (1)  we propose to optimize the following objective
(the reasons will be seen immediately below)  i.e. 

(4)
where F ∈ (cid:60)p×p is a diagonal matrices which encodes the exclusive group information  and its
diagonal element is given by1

J2(w) = f (w) + λwT Fw 

(cid:16)(cid:88)

g

Fii =

(cid:17)

(IGg )i(cid:107)wGg(cid:107)1

|wi|

.

(5)

Let IGg ∈ {0  1}p×1 be group index indicator for group g ∈ G. For example  group G1 is {1  2} 
then IG1 = [1  1  0 ···   0]. Thus the group variable wGg can be explicitly expressed as wGg =
diag(IGg ) × w.
Note computation of F depends on w  thus minimization of w depends on both F. In the following 
we propose an efﬁcient iteratively re-weighted algorithm to ﬁnd out the optimal global solution for
w  where in each iteration  w is updated along the gradient descent direction. This process is iterated
until the algorithm converges. Taking the derivative of Eq.(4) w.r.t w and set ∂J2

∂w = 0. We have

∇wf (w) + 2λFw = 0.

(6)

Then the complete algorithm is:
(1) Updating wt via Eq.(6);

(2) Updating Ft via Eq.(5).
The above two steps are iterated until the algorithm converges. We can prove the obtained optimal
solution is exactly the global optimal solution for Eq.(1).
3.1 Convergence Analysis
In the following  we prove the convergence of algorithm.
Theorem 3.1. Under the updating rule of Eq. (6)  J1(wt+1) − J1(wt) ≤ 0.
The proof is provided in Appendix.
Discussion We note reweighted strategy [26] was also used in solving problems like zero-norm
of the parameters of linear models. However  it cannot be directly used to solve “exclusive group
LASSO” problem proposed in this paper  and cannot handle arbitrary structures on feature space.
4 Uncorrelated feature learning via exclusive group LASSO
Motivation It is known that in Lasso-type (including elastic net) [24  32] variable selection  variable
correlations are not taken into account. Therefore  some strongly correlated variables tend to be in
or out of the model together. However  in practice  feature variables are often correlated. See an
example shown on housing dataset [4] with 506 samples and 14 attributes. Although there are only
14 attributes  feature 5 is highly correlated with feature 6  7  11  12  etc. Moveover  the strongly cor-
related variables may share similar properties  with overlapped or redundant information. Especially

1when wi = 0 

then Fii is related to subgradient of w w.r.t to wi. However  we can not set
Fii = 0  otherwise the derived algorithm cannot be guaranteed to converge. We can regularize Fii =
  then the derived algorithm can be proved to minimize the regularized

(IGg )i(cid:107)wGg(cid:107)1/(cid:112)w2

i + 

(cid:19)

1. It is easy to see the regularized exclusive (cid:96)1 norm of w approximates exclusive (cid:96)1 norm of

(cid:18)(cid:80)
(cid:80)
g (cid:107)(w + )Gg(cid:107)2
w when  → 0+.

g

4

Table 1: Characteristics of datasets

Dataset
isolet
ionosphere
mnist(0 1)
Leuml

# data
1560
351
3125
72

#dimension
617
34
784
3571

#domain
UCI
UCI
image
biology

when the number of selected variables are limited  more discriminant information with minimum
correlations are desirable for prediction or classiﬁcation purpose. Therefore  it is natural to eliminate
the correlations in the feature learning process.
Formulation The above observations motivate our work of uncorrelated feature learning via exclu-
sive group LASSO. We consider the variable selection problem based on the LASSO-type optimiza-
tion  where we can make the selected variables uncorrelated as much as possible. To be exact  we
propose to optimize the following objective:

f (w) + α(cid:107)w(cid:107)1 + β

min
w∈(cid:60)p

(cid:107)wGg(cid:107)2
1 

(7)

(cid:88)

g

where f (w) is loss function involving class predictor y ∈ (cid:60)n and data matrix X =
[x1  x2 ···   xn] ∈ (cid:60)p×n  (cid:107)wGg(cid:107)2
1 is the exclusive group LASSO term involving feature correla-
tion information α and β are tuning parameters  which can make balances between plain LASSO
term and the exclusive group LASSO term.
The core part of Eq.(7) is to use exclusive group LASSO regularizer to eliminate the correlated
features  which cannot be done by plain LASSO. Let the feature correlation matrix be R = (Rst) ∈
(cid:60)p×p  clearly  R = RT   Rst represents the correlations between features s and t  i.e. 

Rst =

  Rst > θ

(8)

|(cid:80)
(cid:112)(cid:80)

(cid:112)(cid:80)
i XsiXti|
i X 2
si

i X 2
ti

To let the selected features uncorrelated as much as possible  for any two features s  t  if their
correlations Rst > θ  then we put them in an exclusive group. Therefore  only one feature can
survive. For example  on the example shown in Fig.2(c)  if we use θ = 0.93 as a threshold  we will
generate the following exclusive group LASSO term:

(cid:107)wGg(cid:107)2

1

= (|w3| + |w10|)2 + (|w5| + |w6|)2 + (|w5| + |w7|)2 + (|w5| + |w11|)2 + (|w6| + |w11|)2

(cid:88)

g

+(|w6| + |w12|)2 + (|w6| + |w14|)2 + (|w7| + |w11|)2.

(9)
Algorithm Solving Eq.(7) is to solve a convex optimization problem  because all the three involved
terms are convex. This also indicates that there exists a unique global solution. Eq.(7) can be
efﬁciently solved via accelerated proximal gradient (FISTA) method [17  2]  irrespective of what
kind of loss function used in minimization of empirical risk. Thus solving Eq.(7) is transformed into
solving:

min
w∈(cid:60)p

1
2

(cid:107)w − a(cid:107)2

2 + α(cid:107)w(cid:107)1 + β

(cid:107)wGg(cid:107)2
1 

(10)

Lt

∇f (wt) which involves the current wt value and step size Lt. The challenge
where a = wt − 1
of solving Eq.(10) is that  it involves two non-smooth terms. Fortunately  we have the following
lemma to establish the relations between the optimal solution of Eq.(10) to Eq.(11)  the solution of
which has been well discussed in §3.
1
2

(cid:107)w − u(cid:107)2

(cid:107)wGg(cid:107)2
1.

min
w∈(cid:60)p

2 + β

(11)

(cid:88)

g

Lemma 4.1. The optimal solution to Eq.(10) is the optimal solution to Eq.(11)  where

u = arg min

x

1
2

(cid:107)x − a(cid:107)2

2 + α(cid:107)x(cid:107)1 = sgn(a)(a − α)+ 

(12)

and sgn(.)  SGN(.) are the operators deﬁned in the component fashion: if v > 0  sgn(v) = 1  SGN(v) =
{1}; else if v = 0  sgn(v) = 0  SGN(v) = [−1  1]; else if v < 0  sgn(v) = −1  SGN(v) = {−1}.

The proof is provided in Appendix.

5

(cid:88)

g

(a) RMSE on linear struc-
ture

(b) MAE on linear struc-
ture

(c) RMSE on hub structure (d) MAE on hub structure

(e) isolet

(f) ionosphere

(g) mnist (0 1)

(h) leuml

Figure 3: (a-d): Feature selection results on synthetic dataset using (a  b) linear structure; (c  d) hub structure.
Evaluation metrics: RMSE  MAE. x-axis: number of selected features. y-axis: RMSE or MAE error in log
scale. (g-j): Classiﬁcation accuracy using SVM (linear kernel) with different number of selected features on
four datasets. Compared methods: Exclusive LASSO of Eq.(7)  LASSO  ReliefF [21]  F-statistics [3]. x-axis:
number of selected features; y-axis: classiﬁcation accuracy.

5 Experiment Results

To validate the effectiveness of our method  we ﬁrst conduct experiment using Eq.(7) on two syn-
thetic datasets  and then show experiments on real-world datasets.

5.1 Synthetic datasets

i

i

i

i

∼ N (0  0.1).

2 ···   x2

1  x2

∼ N [0p×1  Ip×p]  x2

2 ···   x1

n] ∈ (cid:60)p×n  X2 = [x2

∼ Uniform(−0.5  0.5)  and w2 ∈ (cid:60)p  where each w2

n] ∈
(1) Linear-correlated features. Let data X1 = [x1
1  x1
(cid:60)p×n  where each data x1
∼ N [0p×1  Ip×p]  I is identity matrix. We generate
a group of p-features  which is a linear combination of features in X1 and X2  i.e.  X3 = 0.5(X1 +
X2) +    ∼ N (−0.1e  0.1Ip×p). Construct data matrix X = [X1; X2; X3]  clearly  X ∈ (cid:60)3p×n.
Features in dimension [2p + 1  3p] are highly correlated with features in dimension [1  p] and [p +
1  2p]. Let w1 ∈ (cid:60)p  where each w1
∼
Uniform(−0.5  0.5). Let ˜w = [w1; w2; 0p×1]. We generate predicator y ∈ (cid:60)n  and y = ˜wT X +
y  where (y)i
We solve Eq.(7) using current y and X with least square loss. The group settings are: (i  p+i  2p+i) 
for 1 ≤ i ≤ p. We compare the computed w∗ against ground truth solution ˜w and plain LASSO
solution (i.e.  β = 0 in Eq.7). We use the root mean square error (RMSE) and mean absolute
error (MAE) error to evaluate the differences of values predicted by a model and the values actually
observed. We generate n = 1000 data  with p = [120  140 ···   220  240] and do 5-fold cross
validation. Generalization error of RMSE and MAE are shown in Figures 3(a) and 3(b). Clearly 
our approach outperforms standard LASSO solution and exactly recovers the true features.
(2) Correlated features on Hub structure. Let data X = [X1; X2;···   XB] ∈ (cid:60)q×n  where
In each block  for
each block Xb = [X b
∼ N (0  1)  zi ∼
each data point 1 ≤ i ≤ n  X b
N (0  1) and b
1 0]T   where
∼ Uniform(−0.5  0.5). Let ˜w = [w1; w2;··· ; wB]  we generate predicator y ∈ (cid:60)n  and
wb
1
y = ˜wT X + y  where (y)i
The group settings are: ((b − 1) × p + 1 ···   b × p)  for 1 ≤ b ≤ B. We generate n = 1000 data 
B = 10  with varied p = [20  21 ···   24  25] and do 5-fold cross validation. Generalization error of
RMSE and MAE are shown in Figs.3(c) 3(d). Clearly  our approach outperforms standard LASSO
solution  and recovers the exact features.

p:] ∈ (cid:60)p×n  1 ≤ b ≤ B  q = p × B.
i  where X b
ji

∼ Uniform(−0.1  0.1). Let w1  w2 ···   wB ∈ (cid:60)p  where wb = [wb

2≤j≤p X b

ji + 1

B zi + b

1:; X b

2:;··· ; X b

(cid:80)

1i = 1
B

∼ N (0  0.1).

i

6

120140160180200220240−2.8−2.6−2.4−2.2−2−1.8−1.6−1.4# of featureslog Generalization: RMSE error L1Exclusive lasso+L1optimal solution120140160180200220240−3−2.8−2.6−2.4−2.2−2−1.8−1.6# of featureslog Generalization: MAE error L1Exclusive lasso+L1optimal solution200210220230240250−0.6−0.59−0.58−0.57−0.56−0.55−0.54# of featureslog Generalization: RMSE error L1Exclusive lasso+L1optimal solution200210220230240250−0.79−0.78−0.77−0.76−0.75−0.74−0.73−0.72−0.71# of featureslog Generalization: MAE error L1Exclusive lasso+L1optimal solution010020030040050060074767880828486# of featuresFeature Selection Accuracy F−statisticReliefFLASSOExclusive05101520253035727476788082848688# of featuresFeature Selection Accuracy F−statisticReliefFLASSOExclusive0204060801008082848688909294# of featuresFeature Selection Accuracy F−statisticReliefFLASSOExclusive020406080100919293949596979899# of featuresFeature Selection Accuracy F−statisticReliefFLASSOExclusive5.2 Real-world datasets

To validate the effectiveness of proposed method  we perform feature selection via proposed un-
correlated feature learning framework of Eq.(7) on 4 datasets (shown in Table.1)  including 2 UCI
datasets: isolet [6]  ionosphere [5]  1 image dataset: mnist with only ﬁgure “0” and “1” [16]  and 1
biology dataset: Leuml [13].
We perform classiﬁcation tasks on these different datasets. The compared methods include: pro-
posed method of Eq.(7) (shown as Exclusive)  plain LASSO  ReliefF [21]  F-statistics [3]. We use
logistic regression as the loss function in our method and plain LASSO method. In our method 
parameter α  β are tuned to select different numbers of features. Exclusive LASSO groups are set
according to feature correlations (i.e.  threshold θ is set to 0.90 in Eq.8). After the speciﬁc number
of features are selected  we feed them into support vector machine (SVM) with linear kernel  and
classiﬁcation results with different number of selected features are shown in Fig.(3).
A ﬁrst glance at the experimental results indicates the better performance of our method as com-
pared to plain LASSO. Moreover  our method is also generally better than the other two popularly
used feature selection methods  such as ReliefF and F-statistics. The experiment result also further
conﬁrms our intuition: elimination of correlated features is really helpful for feature selection and
thus improves the classiﬁcation performance. Because (cid:96)1 ∞ [20]  (cid:96)2 1 [12  18]  or non-convex fea-
ture learning via (cid:96)p ∞ [11](0 < p < 1) are designed for multi-task or multi-label feature learning 
thus we do not compare against these methods.
Further  we list the mean and variance of classiﬁcation accuracy of different algorithms in the fol-
lowing table  using 50% of all the features. Compared methods include (1) Lasso (L1); (2) Plain
exclusive group LASSO (α = 0 in Eq. (7)); (3) Exclusive group LASSO (α > 0 in Eq. (7)).
dataset
isolet
ionosphere
mnist(0 1)
leuml
The above experiment results indicate that the advantage of our method (exclusive group LASSO)
over plain LASSO comes from the exclusive LASSO term. The experiment results also suggest that
the plain exclusive LASSO performs very similar to LASSO. However  the exclusive group LASSO
(α > 0 in Eq.7) performs deﬁnitely better than both standard LASSO and plain exclusive LASSO
(1%-4% performance improvement). The exclusive LASSO regularizer eliminates the correlated
and redundant features.
We show the running time of plain exclusive LASSO and exclusive group LASSO (α > 0 in Eq.7)
in the following table. We run different algorithms on a Intel i5-3317 CPU  1.70GHz  8GRAM
desktop.
dataset
isolet
ionosphere
mnist(0 1)
leuml

exclusive group LASSO (running time: sec)
51.93
24.18
126.51
144.08

plain exclusive (running time: sec)
47.24
22.75
123.45
142.19

exclusive group LASSO
83.24 ± 0.23
87.28 ± 0.42
94.51 ± 0.19
97.70 ± 0.27

# of features
308
17
392
1785

LASSO
81.75 ± 0.49
85.10 ± 0.27
92.35 ± 0.13
95.10 ± 0.31

plain exclusive
82.05 ± 0.50
85.21 ± 0.31
93.07 ± 0.20
95.67± 0.24

The above experiment results indicate that the computational cost of exclusive group LASSO is
slightly higher than that of plain exclusive LASSO. The reason is that  the solution to exclusive
group LASSO is given by simple thresholding on the plain exclusive LASSO result. This further
conﬁrms our theoretical analysis results shown in Lemma 4.1.
6 Conclusion
In this paper  we propose a new formulation called “exclusive group LASSO” to enforce the sparsity
for features at an intra-group level. We investigate its properties and propose an effective algorithm
with rigorous convergence analysis. We show applications for uncorrelated feature selection  which
indicate the good performance of proposed method. Our work can be easily extended for multi-task
or multi-label learning.
Acknowledgement The majority of the work was done during the internship of the ﬁrst author at NEC Labo-
ratories America  Cupertino  CA.

7

g (cid:107)vg(cid:107)2

1  and ΩG

(cid:113)(cid:80)

(cid:113)(cid:80)

E: Note if ΩG

E(w) = 0  then w = 0. For any scalar a  ΩG

E(w + ˜w) ≤(cid:113)(cid:80)

Appendix
Proof of a valid norm of ΩG
E(aw) =
|a|ΩG
E(w). This proves absolute homogeneity and zero property hold. Next we consider triangle
inequality. Consider w  ˜w ∈ (cid:60)p. Let vg and ˜vg be optimal decomposition of w  ˜w such that
ΩG
1. Since vg + ˜vg is a decomposition of w + ˜w 
E(w) =
thus we have: 1 ΩG
ΩG
E( ˜w).
To prove Theorem 3.1  we need two lemmas.
Lemma 6.1. Under the updating rule of Eq.(6)  J2(wt+1) < J2(wt).
Lemma 6.2. Under the updating rule of Eq.(6) 
≤

g (cid:107)˜vg(cid:107)2
g (cid:107)vg + ˜vg(cid:107)2

1 ≤(cid:113)(cid:80)

J1(wt+1) − J1(wt)

J2(wt+1) − J2(wt)

E(w) +
(cid:117)–

(cid:113)(cid:80)

g (cid:107)˜vg(cid:107)2

g (cid:107)vg(cid:107)2

1 = ΩG

E( ˜w) =

(cid:19)

(cid:18)

(cid:18)

(cid:19)

1 +

(13)

.

≤
Proof of Theorem 3.1 From Lemma 6.1 and Lemma 6.2  it is easy to see
(cid:117)–
0. This completes the proof.
Proof of Lemma 6.1 Eq.(4) is a convex function  and optimal solution of Eq.(6) is obtained by
(cid:117)–
taking derivative ∂J2
Before proof of Lemma 6.2  we need the following Proposition.

∂w = 0  thus obtained w∗ is global optimal solution  J2(wt+1) < J2(wt).

g=1((cid:107)wGg(cid:107)1)2.

Proposition 6.3. wT Fw =(cid:80)G
1 −(cid:88)
1 −(cid:88)

Proof of Lemma 6.2 Let ∆ = LHS -RHS of Eq.(13). We have ∆  where
(IGg )i(cid:107)wtGg(cid:107)1
(cid:88)

(IGg )i(cid:107)wtGg(cid:107)1

(cid:88)
(cid:20)
(cid:88)

(cid:107)wt+1Gg (cid:107)2

|wt+1

|wt
i|

|wt
i|

(wt+1

(wt+1

)2 =

)2 +

∆ =

=

i g

i g

(

i

|)2 − (

i

(wt

i)2 −(cid:88)
(cid:88)

|wt

i|)(

g

1

(cid:107)wtGg(cid:107)2
(cid:88)

(cid:21)

(14)
(wt+1
)2
i|
|wt

)

i

i∈Gg

i∈Gg

i∈Gg

(cid:18)
(cid:19)
J1(wt+1)− J1(wt)

g

(cid:88)
(cid:88)
(cid:88)

g

=

g

i g

i|
|wt

(IGg )i(cid:107)wtGg(cid:107)1
(cid:107)wt+1Gg (cid:107)2
(cid:20)
(cid:88)
(cid:88)
(cid:88)
i|   bi =(cid:112)|wt

i∈Gg
|√|wt
|wt+1

aibi)2 − (

a2
i )(

i∈Gg

i∈Gg

(

i

i

(cid:21)

g

≤ 0 

b2
i )

(15)

(16)

(17)

where ai =

holds due to cauchy inequality [23]: for any scalar ai  bi  ((cid:80)
Eg(w) = (cid:80)

i|. Due to proposition 6.3  Eq.(14) is equivalent to Eq.(15). Eq.(16)
(cid:117)–
i a2
1. Let w∗ be the optimal

i aibi)2 ≤ ((cid:80)

Proof of Lemma 4.1 For notation simplicity  let ΩG
solution to Eq.(11)  then we have

i )((cid:80)

g (cid:107)wGg(cid:107)2

i ).
i b2

0 ∈ w∗ − u + β∂ΩG

Eg(w∗).

In order to prove that w∗ is also the global optimal solution to Eq.(10)  i.e. 

0 ∈ w

∗ − a + αSGN(w

∗

G
) + β∂Ω
Eg(w

∗

(18)
First  from Eq.(12)  we have 0 ∈ u− a + αSGN(u)  and this leads to u ∈ a− αSGN(u). According
to the deﬁnition of ΩG
Eg(w)  from Eq.(11)  it is easy to verify that (1) if ui = 0  then wi = 0; (2) if
ui (cid:54)= 0  then sign(wi) = sign(ui) and 0 ≤ |wi| ≤ |ui|. This indicates that SGN(u) ⊂ SGN(w) 
and thus

).

Put Eqs.(17  19) together  and this exactly recovers Eq.(18)  which completes the proof.

1Note the derivation needs Cauchy inequality [23]  where for any scalar ai  bi  ((cid:80)
((cid:80)
g)((cid:80)

g). Let ag = (cid:107)vg(cid:107)1  bg = (cid:107)˜vg(cid:107)1  then we can get the inequality.

g a2

g b2

(19)

g agbg)2 ≤

u ∈ a − αSGN(w).

8

References
[1] F. Bach. Structured sparsity and convex optimization. In ICPRAM  2012.
[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM J. Imaging Sci.  2(1):183–202  2009.

[3] J. D. F. Habbema and J. Hermans. Selection of variables in discriminant analysis by f-statistic and error

rate. Technometrics  1977.

[4] Housing. http://archive.ics.uci.edu/ml/datasets/Housing.
[5] Ionosphere. http://archive.ics.uci.edu/ml/datasets/Ionosphere.
[6] isolet. http://archive.ics.uci.edu/ml/datasets/ISOLET.
[7] L. Jacob  G. Obozinski  and J.-P. Vert. Group lasso with overlap and graph lasso. In ICML  page 55  2009.
[8] R. Jenatton  J.-Y. Audibert  and F. Bach. Structured variable selection with sparsity-inducing norms.

Journal of Machine Learning Research  12:2777–2824  2011.

[9] S. Ji  L. Yuan  Y. Li  Z. Zhou  S. Kumar  and J. Ye. Drosophila gene expression pattern annotation using

sparse features and term-term interactions. In KDD  pages 407–416  2009.

[10] D. Kong and C. H. Q. Ding. Efﬁcient algorithms for selecting features with arbitrary group constraints

via group lasso. In ICDM  pages 379–388  2013.

[11] D. Kong and C. H. Q. Ding. Non-convex feature learning via (cid:96)p ∞ operator. In AAAI  pages 1918–1924 

2014.

[12] D. Kong  C. H. Q. Ding  and H. Huang. Robust nonnegative matrix factorization using (cid:96)2 1-norm. In

CIKM  pages 673–682  2011.

[13] Leuml. http://www.stat.duke.edu/courses/Spring01/sta293b/datasets.html.
[14] J. Liu  R. Fujimaki  and J. Ye. Forward-backward greedy algorithms for general convex smooth functions

over a cardinality constraint. In ICML  2014.

[15] J. Liu and J. Ye. Moreau-yosida regularization for grouped tree structure learning. In NIPS  pages 1459–

1467  2010.

[16] mnist. http://yann.lecun.com/exdb/mnist/.
[17] Y. Nesterov. Gradient methods for minimizing composite objective function. ECORE Discussion Paper 

2007.

[18] F. Nie  H. Huang  X. Cai  and C. H. Q. Ding. Efﬁcient and robust feature selection via joint (cid:96)2 1-norms

minimization. In NIPS  pages 1813–1821. 2010.

[19] S. Nocedal  J.; Wright. Numerical Optimization. Springer-Verlag  Berlin  New York  2006.
[20] A. Quattoni  X. Carreras  M. Collins  and T. Darrell. An efﬁcient projection for (cid:96)1 ∞ regularization. In

ICML  page 108  2009.

[21] M. Robnik-Sikonja and I. Kononenko. Theoretical and empirical analysis of relieff and rrelieff. Machine

Learning  53(1-2):23–69  2003.

[22] V. Roth. The generalized lasso. IEEE Transactions on Neural Networks  15(1):16–28  2004.
[23] J. M. Steele. The Cauchy-Schwarz master class : an introduction to the art of mathematical inequalities.

MAA problem book series. Cambridge University Press  Cambridge  New York  NY  2004.

[24] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58:267–288  1994.

[25] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via the fused lasso.

Journal of the Royal Statistical Society Series B  pages 91–108  2005.

[26] J. Weston  A. Elisseeff  B. Schlkopf  and P. Kaelbling. Use of the zero-norm with linear models and kernel

methods. Journal of Machine Learning Research  3:1439–1461  2003.

[27] T. Yang  R. Jin  M. Mahdavi  and S. Zhu. An efﬁcient primal-dual prox method for non-smooth optimiza-

tion. CoRR  abs/1201.5283  2012.

[28] L. Yuan  J. Liu  and J. Ye. Efﬁcient methods for overlapping group lasso. In NIPS  pages 352–360  2011.
[29] M. Yuan and M. Yuan. Model selection and estimation in regression with grouped variables. Journal of

the Royal Statistical Society  Series B  68:49–67  2006.

[30] J. Zhou  F. Wang  J. Hu  and J. Ye. From micro to macro: data driven phenotyping by densiﬁcation of

longitudinal electronic medical records. In KDD  pages 135–144  2014.

[31] Y. Zhou  R. Jin  and S. C. H. Hoi. Exclusive lasso for multi-task feature selection. Journal of Machine

Learning Research - Proceedings Track  9:988–995  2010.

[32] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal

Statistical Society  Series B  67:301–320  2005.

9

,Eunho Yang
Pradeep Ravikumar
Genevera Allen
Zhandong Liu
Deguang Kong
Ryohei Fujimaki
Ji Liu
Feiping Nie
Chris Ding
Qiang Liu
John Fisher III
Alexander Ihler
Yilun Du
Zhijian Liu
Hector Basevi
Ales Leonardis
Bill Freeman
Josh Tenenbaum
Jiajun Wu