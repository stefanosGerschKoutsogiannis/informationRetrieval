2014,Estimation with Norm Regularization,Analysis of estimation error and associated structured statistical recovery based on norm regularized regression  e.g.  Lasso  needs to consider four aspects: the norm  the loss function  the design matrix  and the noise vector. This paper presents generalizations of such estimation error analysis on all four aspects  compared to the existing literature. We characterize the restricted error set  establish relations between error sets for the constrained and regularized problems  and present an estimation error bound applicable to {\em any} norm. Precise characterizations of the bound are presented for a variety of noise vectors  design matrices  including sub-Gaussian  anisotropic  and dependent samples  and loss functions  including least squares and generalized linear models. Gaussian widths  as a measure of size of suitable sets  and associated tools play a key role in our generalized analysis.,Estimation with Norm Regularization

Arindam Banerjee

Sheng Chen

Farideh Fazayeli

Vidyashankar Sivakumar

Department of Computer Science & Engineering

{banerjee shengc farideh sivakuma}@cs.umn.edu

University of Minnesota  Twin Cities

Abstract

Analysis of non-asymptotic estimation error and structured statistical recovery
based on norm regularized regression  such as Lasso  needs to consider four as-
pects: the norm  the loss function  the design matrix  and the noise model. This
paper presents generalizations of such estimation error analysis on all four aspects.
We characterize the restricted error set  where the estimation error vector lies  es-
tablish relations between error sets for the constrained and regularized problems 
and present an estimation error bound applicable to any norm. Precise charac-
terizations of the bound is presented for a variety of design matrices  including
sub-Gaussian  anisotropic  and dependent samples  noise models  including both
Gaussian and sub-Gaussian noise  and loss functions  including least squares and
generalized linear models. Gaussian width  a geometric measure of size of sets 
and associated tools play a key role in our generalized analysis.

1

Introduction

Over the past decade  progress has been made in developing non-asymptotic bounds on the esti-
mation error of structured parameters based on norm regularized regression. Such estimators are
usually of the form [16  9  3]:

L(θ; Z n) + λnR(θ)  

ˆθλn = argmin
θ∈Rp

(1)
i=1 where yi ∈
where R(θ) is a suitable norm  L(·) is a suitable loss function  Z n = {(yi  Xi)}n
R  Xi ∈ Rp is the training set  and λn > 0 is a regularization parameter. The optimal parameter θ∗
is often assumed to be ‘structured’  usually characterized as a small value according to some norm
R(·). Since ˆθλn is an estimate of the optimal structure θ∗  the focus has been on bounding a suitable
measure of the error vector ˆ∆n = (ˆθλn − θ∗)  e.g.  the L2 norm (cid:107) ˆ∆n(cid:107)2.
To understand the state-of-the-art on non-asymptotic bounds on the estimation error for norm-
regularized regression  four aspects of (1) need to be considered: (i) the norm R(θ)  (ii) properties
(cid:80)n
of the design matrix X ∈ Rn×p  (iii) the loss function L(·)  and (iv) the noise model  typically in
terms of ω = y − E[y|x]. Most of the literature has focused on a linear model: y = Xθ + ω 
i=1(yi − (cid:104)θ  Xi(cid:105))2. Early work
and a squared-loss function: L(θ; Z n) = 1
on such estimators focussed on the L1 norm [21  20  8]  and led to sufﬁcient conditions on the
design matrix X  including the restricted-isometry properties (RIP) and restricted eigenvalue (RE)
conditions [2  9  13  3]. While much of the development has focussed on isotropic Gaussian design
matrices  recent work has extended the analysis for L1 norm to correlated Gaussian designs [13] as
well as anisotropic sub-Gaussian design matrices [14].
Building on such development  [9] presents a uniﬁed framework for the case of decomposable norms
and also considers generalized linear models (GLMs) for certain norms such as L1. Two key insights
are offered in [9]: ﬁrst  for suitably large λn  the error vector ˆ∆n lies in a restricted set  a cone or

n(cid:107)y − Xθ(cid:107)2

2 = 1
n

1

a star  and second  on the restricted error set  the loss function needs to satisfy restricted strong
convexity (RSC)  a generalization of the RE condition  for the analysis to work out.
For isotropic Gaussian design matrices  additional progress has been made. [4] considers a con-
strained estimation formulation for all atomic norms  where the gain condition  equivalent to the
RE condition  uses Gordons inequality [5  7] and is succinctly represented in terms of the Gaussian
width of the intersection of the cone of the error set and a unit ball/sphere. [11] considers three
related formulations for generalized Lasso problems  establish recovery guarantees based on Gor-
dons inequality  and quantities related to the Gaussian width. Sharper analysis for recovery has been
considered in [1]  yielding a precise characterization of phase transition behavior using quantities
related to the Gaussian width. [12] consider a linear programming estimator in a 1-bit compressed
sensing setting and  interestingly  the concept of Gaussian width shows up in the analysis. In spite
of the advances  most of these results are restricted to isotropic Gaussian design matrices.
In this paper  we consider structured estimation problems with norm regularization  which substan-
tially generalize existing results on all four pertinent aspects: the norm  the design matrix  the loss 
and the noise model. The analysis we present applies to all norms. We characterize the structure of
the restricted set for all norms  develop precise relationships between the error sets of the regular-
ized and constrained versions [2]  and establish an estimation error bound in Section 2. The bound
depends on the regularization parameter λn and a certain RSC condition constant κ. In Section 3 
for both Gaussian and sub-Gaussian noise ω  we develop suitable characterizations for λn in terms
of the Gaussian width of the unit norm ball ΩR = {u|R(u) ≤ 1}. In Section 4  we characterize
the RSC condition for any norm  considering two families of design matrices X ∈ Rn×p: Gaussian
and sub-Gaussian  and three settings for each family: independent isotropic designs  independent
anisotropic designs where the rows are correlated as Σp×p  and dependent isotropic designs where
the rows are isotropic but columns are correlated as Γn×n  implying dependent samples. In Sec-
tion 5  we show how to extend the analysis to generalized linear models (GLMs) with sub-Gaussian
design matrices and any norm.
Our analysis techniques are simple and largely uniform across different types of noise and design
matrices. Parts of our analysis are geometric  where Gaussian widths  as a geometric measure of size
of suitable sets  and associated tools play a key role [4  7]. We also use standard covering arguments 
use Sudakov-Dudley inequality to switch from covering numbers to Gaussian widths [7]  and use
generic chaining to upper bound ‘sub-Gaussian widths’ with Gaussian widths [15].

2 Restricted Error Set and Recovery Guarantees

In this section  we give a characterization of the restricted error set Er in which the error vector
ˆ∆n lives  establish clear relationships between the error sets for the regularized and constrained
problems  and ﬁnally establish upper bounds on the estimation error. The error bound is determin-
istic  but has quantities which involve θ∗  X  ω  for which we develop high probability bounds in
Sections 3  4  and 5.

2.1 The Restricted Error Set and the Error Cone

We start with a characterization of the restricted error set Er where ˆ∆n will belong.

Lemma 1 For any β > 1  assuming

where R∗(·) is the dual norm of R(·). Then the error vector ˆ∆n = ˆθλn − θ∗ belongs to the set

Er = Er(θ∗  β) =

∆ ∈ Rp

λn ≥ βR∗(∇L(θ∗; Z n))  

(cid:26)

(cid:12)(cid:12)(cid:12)(cid:12) R(θ∗ + ∆) ≤ R(θ∗) +

1
β

(cid:27)

R(∆)

.

(2)

(3)

The restricted error set Er need not be convex for general norms. Interestingly  for β = 1  the
inequality in (3) is just the triangle inequality  and is satisﬁed by all ∆. Note that β > 1 restricts the
set of ∆ which satisfy the inequality  yielding the restricted error set. In particular  ∆ cannot go in
the direction of θ∗  i.e.  ∆ (cid:54)= αθ∗ for any α > 0. Further  note that the condition in (2) is similar

2

to that in [9] for β = 2  but the above characterization holds for any norm  not just decomposable
norms [9].
While Er need not be a convex set  we establish a relationship between Er and Cc  the cone for the
constrained problem [4]  where

Cc = Cc(θ∗) = cone{∆ ∈ Rp | R(θ∗ + ∆) ≤ R(θ∗)} .

(4)
2 = {u|(cid:107)u(cid:107)2 ≤ 1} is the unit ball

Theorem 1 Let Ar = Er ∩ ρBp
of L2 norm and ρ > 0 is any suitable radius. Then  for any β > 1 we have

2 and Ac = Cc ∩ ρBp

(cid:18)

(cid:19)

2  where Bp
(cid:107)θ∗(cid:107)2
ρ

w(Ar) ≤

2

1 +

w(Ac)  

β − 1

(5)
where w(A) denotes the Gaussian width of any set A given by: w(A) = Eg[supa∈A(cid:104)a  g(cid:105)]  where
g is an isotropic Gaussian random vector  i.e.  g ∼ N (0  Ip×p).
Thus  the Gaussian width of the error sets of regularized and constrained problems are closely re-
In particular  for (cid:107)θ∗(cid:107)2 = 1  with ρ = 1  β = 2  we have w(Ar) ≤ 3w(Ac). Related
lated.
observations have been made for the special case of the L1 norm [2]  although past work did not
provide an explicit characterization in terms of Gaussian widths. The result also suggests that it is
possible to move between the error analysis of the regularized and the constrained versions of the
estimation problem.

2.2 Recovery Guarantees

In order to establish recovery guarantees  we start by assuming that restricted strong convexity (RSC)
is satisﬁed by the loss function in Cr = cone(Er)  i.e.  for any ∆ ∈ Cr  there exists a suitable
constant κ so that

δL(∆  θ∗) (cid:44) L(θ∗ + ∆) − L(θ∗) − (cid:104)∇L(θ∗)  ∆(cid:105) ≥ κ(cid:107)∆(cid:107)2
2 .

(6)
In Sections 4 and 5  we establish precise forms of the RSC condition for a wide variety of design
matrices and loss functions. In order to establish recovery guarantees  we focus on the quantity

F(∆) = L(θ∗ + ∆) − L(θ∗) + λn(R(θ∗ + ∆) − R(θ∗)) .

(7)
Since ˆθλn = θ∗ + ˆ∆n is the estimated parameter  i.e.  ˆθλn is the minimum of the objective  we
clearly have F( ˆ∆n) ≤ 0  which implies a bound on (cid:107) ˆ∆n(cid:107)2. Unlike previous analysis  the bound
can be established without making any additional assumptions on the norm R(θ). We start with the
following result  which expresses the upper bound on (cid:107) ˆ∆n(cid:107)2 in terms of the gradient of the objective
at θ∗.
Lemma 2 Assume that the RSC condition is satisﬁed in Cr by the loss L(·) with parameter κ. With
ˆ∆n = ˆθλn − θ∗  for any norm R(·)  we have

(cid:107) ˆ∆n(cid:107)2 ≤ 1
κ

(cid:107)∇L(θ∗) + λn∇R(θ∗)(cid:107)2  

(8)

where ∇R(·) is any sub-gradient of the norm R(·).
Note that the right hand side is simply the L2 norm of the gradient of the objective evaluated at
θ∗. For the special case when ˆθλn = θ∗  the gradient of the objective is zero  implying correctly
that (cid:107) ˆ∆n(cid:107)2 = 0. While the above result provides useful insights about the bound on (cid:107) ˆ∆n(cid:107)2 
the quantities on the right hand side depend on θ∗  which is unknown. We present another form
of the result in terms of quantities such as λn  κ  and the norm compatibility constant Ψ(Cr) =
supu∈Cr
Theorem 2 Assume that the RSC condition is satisﬁed in Cr by the loss L(·) with parameter κ.
With ˆ∆n = ˆθλn − θ∗  for any norm R(·)  we have
(cid:107) ˆ∆n(cid:107)2 ≤ 1 + β
β

  which are often easier to compute or bound.

Ψ(Cr) .

R(u)
(cid:107)u(cid:107)2

λn
κ

(9)

The above result is deterministic  but contains λn and κ. In Section 3  we give precise characteri-
zations of λn  which needs to satisfy (2). In Sections 4 and 5  we characterize the RSC condition
constant κ for different losses and a variety of design matrices.

3

3 Bounds on the Regularization Parameter

Recall that the parameter λn needs to satisfy the inequality

λn ≥ βR∗(∇L(θ∗; Z n)) .

(10)
The right hand side of the inequality has two issues: the expression depends on θ∗  the optimal
parameter which is unknown  and the quantity is a random variable  since it depends on Z n. In
this section  we characterize E[R∗(∇L(θ∗; Z n))] in terms of the Gaussian width of the unit norm
ball ΩR = {u : R(u) ≤ 1}  and also discuss the upper bounds of R∗(∇L(θ∗; Z n)). For ease of
exposition  we present results for the case of squared loss  i.e.  L(θ∗; Z n) = 1
2n(cid:107)y − Xθ∗(cid:107)2 with
the linear model y = Xθ + ω  where ω can be Gaussian or sub-Gaussian noise. For this setting 
∇L(θ∗; Z n) = 1
n X T ω. The analysis can be extended to GLMs  using analysis
techniques discussed in Section 5.
Gaussian Designs: First  we consider Gaussian designs X  where xij ∼ N (0  1) are independent 
and ω is elementwise independent Gaussian or sub-Gaussian noise.
Theorem 3 Let ΩR = {u : R(u) ≤ 1}. Then  for Gaussian design X and Gaussian or sub-
Gaussian noise ω  for a suitable constant η0 > 0  we have
E[R∗(∇L(θ∗; Z n))] ≤ η0√
n

n X T (y − Xθ∗) = 1

w(ΩR) .
Further  for any τ > 0  with probability at least 1 − 3 exp(− min( τ 2
(w(ΩR) + τ )  

2Φ2   cn))

(12)

(11)

R∗(∇L(θ∗; Z n)) ≤ η1√
n

where c is an absolute constant  η1 is a constant depending on sub-Gaussian norm of ω  and Φ is a
constant depending on the norm R(·).
For anisotropic Gaussian design or correlated isotropic design  the result continues to hold with
different η0 and η1  which depend on the covariance structure of X.
Sub-Gaussian Designs: Recall that for a sub-Gaussian variable x  the sub-Gaussian norm |||x|||ψ2
supp≥1
xij are i.i.d.  and ω is elementwise independent Gaussian or sub-Gaussian noise.
Theorem 4 Let ΩR = {u : R(u) ≤ 1}. Then  for sub-Gaussian design X and Gaussian or sub-
Gaussian noise ω  for a suitable constant η2 > 0  we have
E[R∗(∇L(θ∗; Z n))] ≤ η2√
n

p (E[|x|p])1/p [18]. Now  we consider sub-Gaussian design X  where |||xij|||ψ2
1√

=
≤ k and

w(ΩR) .

(13)

Interestingly  the analysis for the result above involves ‘sub-Gaussian width’ which can be upper
bounded by a constant times the Gaussian width  using generic chaining [15]. Further  one can
get Gaussian-like exponential concentration around the expectation for important classes of sub-
Gaussian random variables  including bounded random variables [6]  and when Xu = (cid:104)h  u(cid:105)  where
u is any unit vector  are such that their Malliavin derivatives have almost surely bounded norm in

L2[0  1]  i.e. (cid:82) 1

0 |DrXu|2dr ≤ η [19].

Next  we provide a mechanism for bounding the Gaussian width w(ΩR) of the unit norm ball in
terms of the Gaussian width of a suitable cone  obtained by shifting or translating the norm ball. In
particular  the result involves taking any point on the boundary of the unit norm ball  considering
that as the origin  and constructing a cone using the norm ball. Since such a construction can be done
with any point on the boundary  the tightest bound is obtained by taking the inﬁmum over all points
on the boundary. The motivation behind getting an upper bound of the Gaussian width w(ΩR) of
the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances
have been made in recent years in upper bounding Gaussian widths of such cones.
Lemma 3 Let ΩR = {u : R(u) ≤ 1} be the unit norm ball and ΘR = {u : R(u) = 1} be the
boundary. For any ˜θ ∈ ΘR  ρ(˜θ) = supθ:R(θ)≤1 (cid:107)θ − ˜θ(cid:107)2 is the diameter of ΩR measured with
respect to ˜θ. Let G(˜θ) = cone(ΩR − ˜θ) ∩ ρ(˜θ)Bp
2  i.e.  the cone of (ΩR − ˜θ) intersecting the ball of
radius ρ(˜θ). Then
w(ΩR) ≤ inf
˜θ∈ΘR

w(G(˜θ)) .

(14)

4

≥ √

n(cid:107)X∆(cid:107)2

4 Least Squares Models: Restricted Eigenvalue Conditions
When the loss function is squared loss  i.e.  L(θ; Z n) = 1
2n(cid:107)y − Xθ(cid:107)2  the RSC condition (6)
2 ≥ κ(cid:107)∆(cid:107)2
2 
becomes equivalent to the Restricted Eigenvalue (RE) condition [2  9]  i.e.  1
or equivalently  (cid:107)X∆(cid:107)2
κn for any ∆ in the error cone Cr. Since the absolute magnitude of
(cid:107)∆(cid:107)2
(cid:107)∆(cid:107)2 does not play a role in the RE condition  without loss of generality we work with unit vectors
u ∈ A = Cr ∩ Sp−1  where Sp−1 is the unit sphere.
In this section  we establish RE conditions for a variety of Gaussian and sub-Gaussian design ma-
trices  with isotropic  anisotropic  or dependent rows  i.e.  when samples (rows of X) are correlated.
Results for certain types of design matrices for certain types of norms  especially the L1 norm  have
appeared in the literature [2  13  14]. Our analysis considers a wider variety of design matrices and
establishes RSC conditions for any A ⊆ Sp−1  thus corresponding to any norm. Interestingly  the
Gaussian width w(A) of A shows up in all bounds  as a geometric measure of the size of the set A 
even for sub-Gaussian design matrices. In fact  all existing RE results do implicitly have the width
term  but in a form speciﬁc to the chosen norm [13  14]. The analysis on atomic norm in [4] has the
w(A) term explicitly  but the analysis relies on Gordon’s inequality [5  7]  which is applicable only
for isotropic Gaussian design matrices.
The proof technique we use is simple  a standard covering argument  and is largely the same across
all the cases considered. A unique aspect of our analysis  used in all the proofs  is a way to go
from covering numbers of A to the Gaussian width of A using the Sudakov-Dudley inequality [7].
Our general techniques are in contrast to much of the existing literature on RE conditions  which
commonly use specialized tools such as Gaussian comparison principles [13  9]  and/or specialized
analysis geared to a particular norm such as L1 [14].

ij] = σ2.

j ] = Γ ∈ Rn×n. For convenience  we assume E[x2

4.1 Restricted Eigenvalue Conditions: Gaussian Designs
In this section  we focus on the case of Gaussian design matrices X ∈ Rn×p  and consider three
settings: (i) independent-isotropic  where the entries are elementwise independent  (ii) independent-
i ] = Σ ∈ Rp×p 
anisotropic  where rows Xi are independent but each row has a covariance E[XiX T
and (iii) dependent-isotropic  where the rows are isotropic but the columns Xj are correlated with
ij] = 1  noting that the analysis easily
E[XjX T
extends to the general case of E[x2
Independent Isotropic Gaussian (IIG) Designs: The IIG setting has been extensively studied
in the literature [3  9]. As discussed in the recent work on atomic norms [4]  one can use Gordon’s
inequality [5  7] to get RE conditions for the IIG setting. Our goal in this section is two-fold: ﬁrst  we
present the RE conditions obtained using our simple proof technique  and show that it is equivalent 
up to constants  the RE condition obtained using Gordon’s inequality  an technique only applicable
to the IIG setting; and second  we go over some facets of how we present the results  which will
apply to all subsequent RE-style results as well as give a way to plug-in κ in the estimation error
bound in (9).
Theorem 5 Let the design matrix X ∈ Rn×p be elementwise independent and normal  i.e.  xij ∼
N (0  1). Then  for any A ⊆ Sp−1  any n ≥ 2  and any τ > 0  with probability at least (1 −
η1 exp(−η2τ 2))  we have

(cid:107)Xu(cid:107)2 ≥ 1
2

inf
u∈A

√

n − η0w(A) − τ  

η0  η1  η2 > 0 are absolute constants.

(15)

(16)

We consider the equivalent result one could obtain by directly using Gordon’s inequality [5  7]:
Theorem 6 Let the design matrix X be elementwise independent and normal  i.e.  xij ∼ N (0  1).
Then  for any A ⊆ Sp−1 and any τ > 0  with probability at least (1 − 2 exp(−τ 2/2))  we have

where γn = E[(cid:107)h(cid:107)2] > n√

n+1

(cid:107)Xu(cid:107)2 ≥ γn − w(A) − τ  

inf
u∈A

is the expected length of a Gaussian random vector in Rn.

5

Interestingly  the results are equivalent  up to constants. However  unlike Gordon’s inequality  our
proof technique generalizes to all the other design matrices considered in the sequel.
We emphasize three additional aspects in the context of the above analysis  which will continue to
hold for all the subsequent results but will not be discussed explicitly. First  to get a form of the
result which can be used as κ and plugged in to the estimation error bound (9)  one can simply
choose τ = 1

n − η0w(A)) so as to get

√

2 ( 1

2

w(A)  

(17)

(cid:107)Xu(cid:107)2 ≥ 1
4

inf
u∈A

√

n − η0
2

2

with high probability. Table 1 shows a summary of recovery bounds on Independent Isotropic
Gaussian design matrices with Gaussian noise. Second  the result does not depend on the fact that
u ∈ A ⊆ Cr ∩ Sp−1 so that (cid:107)u(cid:107)2 = 1. For example  one can consider the cone Cr to be intersecting
with a sphere ρSp−1 of a different radius ρ  to give Aρ = Cr ∩ ρSp−1 so that u ∈ Aρ has (cid:107)u(cid:107)2 = ρ.
√
For simplicity  let A = A1  i.e.  corresponding to ρ = 1. Then  a straightforward extension yields
inf u∈Aρ (cid:107)Xu(cid:107)2 ≥ ( 1
n− η0w(A)− τ )(cid:107)u(cid:107)2  with probability at least (1− η1 exp(−η2τ 2))  since
(cid:107)2(cid:107)u(cid:107)2 and w(A(cid:107)u(cid:107)2) = (cid:107)u(cid:107)2w(A) [4]. Such a scale independence is in fact
(cid:107)Xu(cid:107)2 = (cid:107)X u(cid:107)u(cid:107)2
necessary for the error bound analysis in Section 2. Finally  note that the leading constant 1
2 was
a consequence of our choice of  = 1
4 for the -net covering of A in the proof. One can get other
constants  less than 1  with different choices of   and the constants η0  η1  η2 will change based on
this choice.
Independent Anisotropic Gaussian (IAG) Designs: We consider a setting where the rows Xi of
the design matrix are independent  but each row is sampled from an anisotropic Gaussian distribu-
tion  i.e.  Xi ∼ N (0  Σp×p) where Xi ∈ Rp. The setting has been considered in the literature [13]
for the special case of L1 norms  and sharp results have been established using Gaussian comparison
techniques [7]. We show that equivalent results can be obtained by our simple technique  which does
not rely on Gaussian comparisons [7  9].
Theorem 7 Let the design matrix X be row wise independent and each row Xi ∼ N (0  Σp×p).
Then  for any A ⊆ Sp−1 and any τ > 0  with probability at least 1 − η1 exp(−η2τ 2)  we have

(18)
ν = inf u∈A (cid:107)Σ1/2u(cid:107)2  Λmax(Σ) denotes the largest eigenvalue of Σ and η0  η1  η2 > 0 are

inf
u∈A

ν

ν

n − η0Λmax(Σ)

√

w(A) − τ  

(cid:107)Xu(cid:107)2 ≥ 1
2

√

√

√
where
constants.

(cid:107) ˜Xu(cid:107)2 ≥ 3
4
where η0  η1  η2 > 0 are constants.

inf
u∈A

Note that with the assumption that E[x2
ij] = 1  Γ will be a correlation matrix implying Tr(Γ) = n 
and making the sample size dependence explicit. Intuitively  due to sample correlations  n samples
are effectively equivalent to Tr(Γ)

Λmax(Γ) =

n

Λmax(Γ) samples.

6

√

ν appears in [13] as
A comparison with the results of [13] is instructive. The leading term
well—we have simply considered inf u∈A on both sides  and the result in [13] is for any u with the
(cid:107)Σ1/2u(cid:107)2 term. The second term in [13] depends on the largest entry in the diagonal of Σ 
log p 
and (cid:107)u(cid:107)1. These terms are a consequence of the special case analysis for L1 norm. In contrast  we
consider the general case and simply get the scaled Gaussian width term Λmax(Σ)√
Dependent Isotropic Gaussian (DIG) Designs: We now consider a setting where the rows of the
j ] = Γ ∈
design matrix ˜X are isotropic Gaussians  but the columns ˜Xj are correlated with E[ ˜Xj ˜X T
Rn×n. Interestingly  correlation structure over the columns make the samples dependent  a scenario
which has not yet been widely studied in the literature [22  10]. We show that our simple technique
continues to work in this scenario and gives a rather intuitive result.
Theorem 8 Let ˜X ∈ Rn×p be a matrix whose rows ˜Xi are isotropic Gaussian random vectors in
j ] = Γ. Then  for any set A ⊆ Sp−1 and any
Rp and the columns ˜Xj are correlated with E[ ˜Xj ˜X T
τ > 0  with probability at least (1 − η1 exp(−η2τ 2)  we have

ν w(A).

(cid:112)Tr(Γ) −(cid:112)Λmax(Γ)

(cid:18)

η0w(A) +

5
2

− τ

(19)

√

(cid:19)

j ] = Γn×n. For convenience  we assume E[x2

4.2 Restricted Eigenvalue Conditions: Sub-Gaussian Designs
In this section  we focus on the case of sub-Gaussian design matrices X ∈ Rn×p  and consider three
settings: (i) independent-isotropic  where the rows are independent and isotropic  (ii) independent-
anisotropic  where the rows Xi are independent but each row has a covariance E[XiX T
i ] = Σp×p 
and (iii) dependent-isotropic  where the rows are isotropic and the columns Xj are correlated
with E[XjX T
ij] = 1 and the sub-Gaussian norm
≤ k [18]. In recent work  [17] also considers generalizations of RE conditions to sub-
|||xij|||ψ2
Gaussian designs  although our proof techniques are different.
Independent Isotropic Sub-Gaussian Designs: We start with the setting where the sub-Gaussian
design matrix X ∈ Rn×p has independent rows Xi and each row is isotropic.
Theorem 9 Let X ∈ Rn×p be a design matrix whose rows Xi are independent isotropic sub-
Gaussian random vectors in Rp. Then  for any set A ⊆ Sp−1 and any τ > 0  with probability at
least (1 − 2 exp(−η1τ 2))  we have
inf
u∈A

n − η0w(A) − τ  

(cid:107)Xu(cid:107)2 ≥ √

(20)

= k and E[XiX T

(cid:107)Xu(cid:107)2 ≥ √

where η0  η1 > 0 are constants which depend only on the sub-Gaussian norm |||xij|||ψ2
Independent Anisotropic Sub-Gaussian Designs: We consider a setting where the rows Xi of the
design matrix are independent  but each row is sampled from an anisotropic sub-Gaussian distribu-
tion  i.e.  |||xij|||ψ2
Theorem 10 Let the sub-Gaussian design matrix X be row wise independent  and each row has
i ] = Σ ∈ Rp×p. Then  for any A ⊆ Sp−1 and any τ > 0  with probability at least
E[XiX T
(1 − 2 exp(−η1τ 2))  we have
inf
u∈A

(21)
ν = inf u∈A (cid:107)Σ1/2u(cid:107)2  Λmax(Σ) denotes the largest eigenvalue of Σ  and η0  η1 > 0 are

where
constants which depend on the sub-Gaussian norm |||xij|||ψ2
Note that [14] establish RE conditions for anisotropic sub-Gaussian designs for the special case of
L1 norm. In contrast  our results are general and in terms of the Gaussian width w(A).
Dependent Isotropic Sub-Gaussian Designs: We consider the setting where the sub-Gaussian de-
sign matrix ˜X has isotropic sub-Gaussian rows  but the columns ˜Xj are correlated with E[ ˜Xj ˜X T
j ] =
Γ  implying dependent samples.
Theorem 11 Let ˜X ∈ Rn×p be a sub-Gaussian design matrix with isotropic rows and correlated
j ] = Γ ∈ Rn×n. Then  for any A ⊆ Sp−1 and any τ > 0  with probability at
columns with E[ ˜Xj ˜X T
least (1 − 2 exp(−η1τ 2))  we have

n − η0 Λmax(Σ) w(A) − τ  

(cid:107) ˜Xu(cid:107)2 ≥(cid:112)Tr(Γ) − η0 Λmax(Γ)w(A) − τ  

i ] = Σp×p.

√

ν

= k.

= k.

√

n(cid:88)

i=1

7

where η0  η1 are constants which depend on the sub-Gaussian norm |||xij|||ψ2

inf
u∈A

(22)

= k.

5 Generalized Linear Models: Restricted Strong Convexity
In this section  we consider the setting where the conditional probabilistic distribution of y|x follows
(cid:80)n
an exponential family distribution: p(y|x; θ) = exp{y(cid:104)θ  x(cid:105) − ψ((cid:104)θ  x(cid:105))}  where ψ(·) is the log-
partition function. Generalized linear models consider the negative likelihood of such conditional
i=1(ψ((cid:104)θ  Xi(cid:105)) − (cid:104)θ  yiXi(cid:105)). Least squares
distributions as the loss function: L(θ; Z n) = 1
regression and logistic regression are popular special cases of GLMs. Since ∇ψ((cid:104)θ  x(cid:105)) = E[y|x] 
we have ∇L(θ∗; Z n) = 1
n X T ω  where ωi = ∇ψ((cid:104)θ  Xi(cid:105)) − yi = E[y|Xi] − yi plays the role of
noise. Hence  the analysis in Section 3 can be applied assuming ω is Gaussian or sub-Gaussian. To
obtain RSC conditions for GLMs  ﬁrst note that

n

δL(θ∗  ∆; Z n) =

1
n

∇2ψ((cid:104)θ∗  Xi(cid:105) + γi(cid:104)∆  Xi(cid:105))(cid:104)∆  Xi(cid:105)2  

(23)

Table 1: A summary of various values for L1 and L2 norms with all values correct up to constants.

(cid:104)

(cid:110)(cid:16)

(cid:17)

(cid:111)(cid:105)2

1 − c2

w(A)√
n

  0

R(u)

λn := c1

κ :=

max

w(ΩR)√
n

(cid:19)

(cid:18)(cid:113) log p
O(cid:0)(cid:112) p
(cid:1)

n

n

L1 norm

O

L2 norm

O (1)

O(1)

Ψ(Cr)

√

s

1

(cid:107) ˆ∆n(cid:107)2 := c3

Ψ(Cr)λn

κ

(cid:19)

(cid:18)(cid:113) s log p
O(cid:0)(cid:112) p
(cid:1)

n

n

O

n(cid:88)

i=1

where γi ∈ [0  1]  by mean value theorem. Since ψ is of Legendre type  the second derivative
∇2ψ(·) is always positive. Since the RSC condition relies on a non-trivial lower bound for the above
quantity  the analysis considers a suitable compact set where (cid:96) = (cid:96)ψ(T ) = min|a|≤2T ∇2ψ(a) is
bounded away from zero. Outside this compact set  we will only use ∇2ψ(·) > 0. Then 

δL(θ∗  ∆; Z n) ≥ (cid:96)
n

(cid:104)Xi  ∆(cid:105)2 I[|(cid:104)Xi  θ∗(cid:105)| < T ] I[|(cid:104)Xi  ∆(cid:105)| < T ] .

(24)

We give a characterization of the RSC condition for independent isotropic sub-Gaussian design ma-
trices X ∈ Rn×p. The analysis can be suitably generalized to the other design matrices considered in
Section 4 by using the same techniques. As before  we denote ∆ as u  and consider u ∈ A ⊆ Sp−1
so that (cid:107)u(cid:107)2 = 1. Further  we assume (cid:107)θ∗(cid:107)2 ≤ c1 for some constant c1. Assuming X has sub-
Gaussian entries with |||xij|||ψ2
≤ k  (cid:104)Xi  θ∗(cid:105) and (cid:104)Xi  u(cid:105) are sub-Gaussian random variables with
sub-Gaussian norm at most Ck. Let φ1 = φ1(T ; u) = P{|(cid:104)Xi  u(cid:105)| > T} ≤ e · exp(−c2T 2/C 2k2) 
and φ2 = φ2(T ; θ∗) = P{|(cid:104)Xi  θ∗(cid:105)| > T} ≤ e · exp(−c2T 2/C 2k2). The result we present is in
terms of the constants (cid:96) = (cid:96)ψ(T )  φ1 = φ(T ; u) and φ2 = φ(T  θ∗) for any suitably chosen T .
Theorem 12 Let X ∈ Rn×p be a design matrix with independent isotropic sub-Gaussian rows.
Then  for any set A ⊆ Sp−1  any α ∈ (0  1)  any τ > 0  and any n ≥
α2(1−φ1−φ2) (cw2(A) +
c2(1−φ1−φ2)5
K4
we have

(1−α)τ 2) for suitable constants c and c2  with probability at least 1−3 exp(cid:0)−η1τ 2(cid:1) 

(25)
where π = (1 − α)(1 − φ1 − φ2)  (cid:96) = (cid:96)ψ(T ) = min|a|≤2T +β ∇2ψ(a)  and constants (η0  η1)
depend on the sub-Gaussian norm |||xij|||ψ2

inf
u∈A

(cid:112)n∂L(θ∗; u  X) ≥ (cid:96)

n − η0w(A) − τ )(cid:1)  

π(cid:0)√

= k.

√

2

The form of the result is closely related to the corresponding result for the RE condition on
inf u∈A (cid:107)Xu(cid:107)2 in Section 4.2. Note that RSC analysis for GLMs was considered in [9] for spe-
ciﬁc norms  especially L1  whereas our analysis applies to any set A ⊆ Sp−1  and hence to any
norm. Further  following similar argument structure as in Section 4.2  the analysis for GLMs can be
extended to anisotropic and dependent design matrices.

6 Conclusions

The paper presents a general set of results and tools for characterizing non-asymptotic estimation
error in norm regularized regression problems. The analysis holds for any norm  and includes much
of existing literature focused on structured sparsity and related themes as special cases. The work
can be viewed as a direct generalization of results in [9]  which presented related results for decom-
posable norms. Our analysis illustrates the important role Gaussian widths  as a geometric measure
of size of suitable sets  play in such results. Further  the error sets of regularized and constrained
versions of such problems are shown to be closely related [2]. Going forward  it will be interesting
to explore similar generalizations for the semi-parametric and non-parametric settings.
Acknowledgements: We thank the anonymous reviewers for helpful comments and suggestions on
related work. We thank Sergey Bobkov  Snigdhansu Chatterjee  and Pradeep Ravikumar for discus-
sions related to the paper. The research was supported by NSF grants IIS-1447566  IIS-1422557 
CCF-1451986  CNS-1314560  IIS-0953274  IIS-1029711  and by NASA grant NNX12AQ39A.

8

References

[1] D. Amelunxen  M. Lotz  M. B. McCoy  and J. A. Tropp. Living on the edge: A geometric

theory of phase transitions in convex optimization. Inform. Inference  3(3):224–294  2013.

[2] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.

Annals of Statistics  37(4):1705–1732  2009.

[3] P. Buhlmann and S. van de Geer. Statistics for High Dimensional Data: Methods  Theory and

Applications. Springer Series in Statistics. Springer  2011.

[4] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[5] Y. Gordon. On Milmans inequality and random subspaces which escape through a mesh in Rn.
In Geometric Aspects of Functional Analysis  volume 1317 of Lecture Notes in Mathematics 
pages 84–106. Springer  1988.

[6] M. Ledoux. The concentration of measure phenomenon. Mathematical Surveys and Mon-

graphs. American Mathematical Society.

[7] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.

Springer  2013.

[8] N. Meinshausen and B Yu. Lasso-type recovery of sparse representations for high-dimensional

data. The Annals of Statistics  37(1):246—270  2009.

[9] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for the analysis

of regularized M-estimators. Statistical Science  27(4):538–557  December 2012.

[10] S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and

high-dimensional scaling. Annals of Statistics  39(2):1069–1097  2011.

[11] S. Oymak  C. Thrampoulidis  and B. Hassibi. The Squared-Error of Generalized Lasso: A

Precise Analysis. Arxiv  arXiv:1311.0830v2  2013.

[12] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A
convex programming approach. IEEE Transactions on Information Theory  59(1):482–494 
2013.

[13] G. Raskutti  M. J. Wainwright  and B. Yu. Restricted Eigenvalue Properties for Correlated

Gaussian Designs. Journal of Machine Learning Research  11:2241–2259  2010.

[14] Z. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements.

Transactions on Information Theory  59(6):3434–3447  2013.

IEEE

[15] M. Talagrand. The Generic Chaining. Springer  2005.
[16] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society  Series B  58(1):267–288  1996.

[17] J. A. Tropp. Convex recovery of a structured signal from independent random linear measure-

ments. In Sampling Theory  a Renaissance. (To Appear)  2014.

[18] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and
G. Kutyniok  editors  Compressed Sensing  chapter 5  pages 210–268. Cambridge University
Press  2012.

[19] A. B. Vizcarra and F. G. Viens. Some applications of the Malliavin calculus to sub-Gaussian
and non-sub-Gaussian random ﬁelds. In Seminar on Stochastic Analysis  Random Fields and
Applications  Progress in Probability  volume 59  pages 363–396. Birkhauser  2008.

[20] M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using
IEEE Transactions on Information Theory 

(cid:96)1-constrained quadratic programming(Lasso).
55:2183–2202  2009.

[21] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning

Research  7:2541–2567  November 2006.

[22] S. Zhou. Gemini: Graph estimation with matrix variate normal instances. The Annals of

Statistics  42(2):532–562  2014.

9

,Arindam Banerjee
Sheng Chen
Farideh Fazayeli
Vidyashankar Sivakumar
Jason Lee
Yuekai Sun
Jonathan Taylor
Joel Tropp
Alp Yurtsever
Madeleine Udell
Volkan Cevher