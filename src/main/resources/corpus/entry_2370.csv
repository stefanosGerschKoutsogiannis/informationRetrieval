2012,Graphical Gaussian Vector for Image Categorization,This paper proposes a novel image representation called a Graphical Gaussian Vector  which is a counterpart of the codebook and local feature matching approaches. In our method  we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry  proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters  which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features  it is able to achieve both high classification accuracy and high efficiency.,Graphical Gaussian Vector for Image Categorization

Tatsuya Harada

The University of Tokyo/JST PRESTO
7-3-1 Hongo Bunkyo-ku  Tokyo Japan

harada@isi.imi.i.u-tokyo.ac.jp

kuniyosh@isi.imi.i.u-tokyo.ac.jp

Yasuo Kuniyoshi

The University of Tokyo

7-3-1 Hongo Bunkyo-ku  Tokyo Japan

Abstract

This paper proposes a novel image representation called a Graphical Gaussian
Vector (GGV)  which is a counterpart of the codebook and local feature matching
approaches. We model the distribution of local features as a Gaussian Markov
Random Field (GMRF) which can efﬁciently represent the spatial relationship
among local features. Using concepts of information geometry  proper parameters
and a metric from the GMRF can be obtained. Then we deﬁne a new image feature
by embedding the proper metric into the parameters  which can be directly applied
to scalable linear classiﬁers. We show that the GGV obtains better performance
over the state-of-the-art methods in the standard object recognition datasets and
comparable performance in the scene dataset.

1

Introduction

The Bag of Words (BoW) [7] is the de facto standard image feature for the image categorization.
In a BoW  each local feature is assigned to the nearest codeword and an image is represented by
a histogram of the quantized features. Several approaches inspired by a BoW have been proposed
in recent years [9]  [23]  [28]  [27]  [29]. While it is well established that using a large number
of codewords improves classiﬁcation performance  the drawback is that assigning local features to
the nearest codeword is computationally expensive. To overcome this problem  some studies have
proposed building an efﬁcient image representation with a smaller number of codewords [22]  [24].
Finding an explicit correspondence between local features is another way of categorizing images
using a BoW [4]  [12]  [26]  and this approach has been improved by representing a spatial layout
of local features as a graph [11]  [2]  [16]  [8]. Explicit correspondences between features have an
advantage over a BoW as information loss in the vector quantization can be avoided. However  the
drawback with this approach is that the identiﬁcation of corresponding points with minimum dis-
tortion is also computationally expensive. Therefore  the aim of our research is to build an efﬁcient
image representation without using codewords or explicit correspondences between local features 
while still achieving high classiﬁcation accuracy.

Since having a spatial layout of local features is important for an image to have semantic meaning 
it is natural that embedding spatial information into an image feature improves classiﬁcation perfor-
mance [18]  [5]  [14]  [17]. Several approaches take advantage of this fact  ranging from local (e.g. 
SIFT) to global (e.g.  Spatial Pyramid). Meanwhile  we will focus on the spatial layout of local
features  which is the midlevel of the spatial information.

In this paper  we model an image as a graph representing the spatial layout of local features and
deﬁne a new image feature based on this graph  where a proper metric is embedded into the feature.
We show that the new feature provides high classiﬁcation accuracy  even with a linear classiﬁer.
Speciﬁcally  we model an image as a Gaussian Markov Random Field (GMRF) whose nodes cor-
respond to local features and consider the GMRF parameters as the image feature. Although the
GMRF is commonly used for image segmentation  it is rarely used in modern image categorization
pipelines despite being an effective way of modeling the spatial layout. In order to extract the repre-

1

sentative feature vector from the GMRF  the choice of coordinates for the parameters and the metric
between them needs to be carefully made. We deﬁne the proper coordinates and the metric from
an information geometry standpoint [1] and derive an optimal feature vector. The resultant feature
vector is called a Graphical Gaussian Vector.

The contributions of this study are summarized as follows: 1) A novel and efﬁcient image feature
is developed by utilizing the GMRF as a tool for object categorization. 2) This approach is imple-
mented by developing the Graphical Gaussian Vector feature  which is based on the GMRF and the
information geometry. 3) Using standard image categorization benchmarks  we demonstrate that
the proposed feature has better performance over the state-of-the-art methods  even though it is not
based on mainstream modules (such as codebooks and correspondence between local features). To
the best of our knowledge  this is the ﬁrst image feature for the object categorization that utilizes
the expectation parameters of the GMRF with its Fisher information metric  and achieves a level of
accuracy comparable to that of the codebook and local feature matching approaches.

2 Graphical Gaussian Vector

2.1 Overview of Proposed Method

(a) Densely sampled 
local features

Local features

i R∈x

d

(b) Multivariate Gaussian
Markov Random Field
(MGMRF)
3x
2x
9x
x

4x
1x
8x
(
x
L=
T
1

5x
6x
7x
)TT
x

9

(e) Parameter space

MGMRF)
1ξxp
;(

MGMRF
4ξxp
)
;(

3ξ

MGMRF
2ξxp
)
;(

MGMRF
5ξxp
)
;(

(c) PDF (d) Feature Vector

);( ξxp

ξ

Parameter of MGMRF

MGMRF
3ξxp
)
;(

3ξ

4ξ

2ξ
6ξ

5ξ

1ξ

Manifold

MGMRF
6ξxp
)
;(

2ξ

1ξ

Geodesic distance ?

Figure 1: Overview of image feature extraction based on a multivariate GMRF.

In this section  we present an overview of our method. Initially  local features {xi ∈ Rd}M
i=1 are
extracted using a dense sampling strategy (Fig. 1(a)). We then use a multivariate GMRF to model
the spatial relationships among local features (Fig. 1(b)). The GMRF is represented as a graph
G(V E)  whose vertices V and edges E correspond to local features and the dependent relationships
between those features  respectively. Let the vector x be a concatenation of local features in V and
let ξ
j be a parameter of the GMRF of an image Ij  the image Ij can be represented by a probability
distribution p(x; ξ
j of the GMRF to
be a feature vector of the image Ij (Fig. 1(d)). Assuming that ξ is a coordinate system  the whole
probability distribution model can be considered as a manifold  where each probability distribution is
represented as a point in that space (Fig. 1(e)). However  because the space spanned by parameters
of a probability distribution is not a Euclidean space  we have to be very careful when choosing
parameters for the probability distribution and the metric among them. We make use of concepts
from the information geometry [1] and extract proper parameters and a metric from the GMRF.
Finally  we deﬁne the new image feature by embedding the metric into the extracted parameters to
build an image categorization system with a scalable linear classiﬁer. In the following sections  we
describe this process in more detail.

j) of the GMRF (Fig. 1(c)). We consider the parameter ξ

2.2
Image Model and Parameters
Given M local features {xi ∈ Rd}M
i=1  the aim is to model a probability distribution of the local
features representing the spatial layout of the image using the multivariate GMRF G = (V E).
First  a vector x is built by concatenating the local features corresponding to the vertices V of the
GMRF. Let {xi}n
i=1 are local features that we are focusing on  we obtain the concatenated vector as
1 ··· x(cid:2)
n )(cid:2)
x = (x(cid:2)
(e.g.  Fig. 1(b)  where n = 9). Note that the dimensionality of x is nd and does
not depend on the number of local features M  the image size  or the aspect ratio. However  since
all results valid for a scalar local feature are also valid for a multivariate local feature  in this section
we consider the dimensionality of local features is 1 (d = 1) for simplicity. That is dim(x) = n.

2

Let μ = E[x]  P = E[(x − μ)(x − μ)(cid:2)]  and J = P
−1. A random vector x is called a Gaussian
Markov Random Field (GMRF) with respect to G = (V E)  if and only if its density has the form
2(x − μ)(cid:2)
J(x − μ)) and Jij = 0 for all {i  j} /∈ E. Because the
p(x) = (2π)−n/2|J|1/2 exp(− 1
(cid:2)
Gaussian distribution can be represented as an exponential family  here  we consider an exponential
family as follows:

(cid:3)
θ(cid:2)φ(x) − Φ(θ)

p(x) = exp

 

(1)

(cid:4)

where θ are the natural parameters  φ(x)
log

exp(θ(cid:2)φ(x))dx is the log-normalizer. θ and φ(x) of the GMRF are obtained as [15]:

the sufﬁcient

statistic 

and Φ(θ) =

is

(2)

θi = hi  θii = −1
φi(x) = xi  φii(x) = x2

2 Jii  θjk = −Jjk  (i ∈ V {j  k} ∈ E) 
i   φjk(x) = xjxk  (i ∈ V {j  k} ∈ E) 

(3)
where h = Jμ. The expectation parameter η = E[φ(x)] is an implicit parameterization belonging
to the exponential family. The expectation parameters are obtained as [15]:

i   ηjk = Pjk + μjμk  (i ∈ V {j  k} ∈ E).

ηi = μi  ηii = Pii + μ2

∂ηj   where G

∗ij(η) = ∂θi

(4)
The natural and expectation parameters can be transformed into each other [1]. They are called
mutually dual as each is the dual coordinate system of the other. The two coordinate systems are
∗ij(η): Gij(θ) = ∂ηi
closely related through the Fisher information matrices (FIMs) Gij(θ) and G
∂θj  
−1(θ). If we take the natural parameters or the expectation
and G
parameters as a coordinate system for an exponential family  a ﬂat structure can be realized [1].
In particular  θ is called a 1-afﬁne coordinate system  and the space spanned by θ is called 1-ﬂat.
Similarly  η is called a (-1)-afﬁne coordinate system  and the space spanned by η is called (-1)-ﬂat.
Those spaces are similar to a Euclidean space  but we need to be careful that the spaces spanned by
the natural or expectation parameters are different from a Euclidean space  as the metrics vary for
different parameters. We will discuss how to determine the metrics in those spaces in Sections. 2.4
and 2.5.

∗(η) = G

To summarize this section  the natural and expectation parameters are similar and interchangeable
through the FIMs. By using these parameters  we can obtain ﬂatness similar to the Euclidean space.
Although it does not matter whether we choose natural or expectation parameters  we use expecta-
tion parameters (Eq. (4)) as feature vectors because they can be calculated directly from the mean
and covariance of local features. We will see a multivariate extension of the GMRF and its calcula-
tion in the next section.

2.3 Calculation of Expectation Parameters

In this section  we describe the calculations of the expectation parameters of the multivariate GMRF.
First  we deﬁne the graph structure of the GMRF. We use star graphs shown in Fig. 2  where
four neighbors (Fig. 2(a)) or eight neighbors (Fig. 2(b)) are usually used. While a graph having
more neighbors is obviously able to represent richer spatial information  the compact structure is
preferable for efﬁciency. Therefore  we employed the approximated graph structures shown in Fig.
2(c)  which represents the vertical and horizontal relationships among local features  and Fig. 2(d) 
which represents vertical  horizontal and  diagonal relationships.

r +k

2a

r +k

3a

r +k

4a

r +k

2a

r +k

kr1a

r +k

kr1a

(a)

(b)

(c)

(d)

Figure 2: Structures of the GMRF.

(cid:42)(cid:78)(cid:66)(cid:72)(cid:70)(cid:1)
(cid:83)(cid:70)(cid:72)(cid:74)(cid:80)(cid:79)(cid:1)(cid:43)

(cid:9)(cid:70)(cid:10)

Next  we show a method for estimating the expectation parameters of each image.
In practice 
Eq. (4) in a multivariate case can be determined by calculating the local auto-correlations of local

3

(cid:5)

features. Here we present the detailed calculations of Eq. (4) using Fig. 2(c) as an example. Let
x(rk) ∈ Rd be the local feature at a reference point rk and let ai and aj be the displacement
vectors  which are deﬁned by the structure of the GMRF. Then  the local auto-correlation matrices
are obtained as: Ci j = 1
  where NJ is the number of local features
NJ
in the image region J. Especially if we deﬁne a0 = 0  C0 i = 1
. Let a
vector concatenating local features in the vertices at the reference point rk be x(cid:2)
k = (x(rk)(cid:2)x(rk+
a1)(cid:2)x(rk + a2)(cid:2))  P + μμ(cid:2)

x(rk+ai)x(rk+aj)(cid:2)

x(rk)x(rk + ai)(cid:2)
(cid:8)

is calculated to be:

(cid:5)

(cid:7)

k∈J

k∈J

NJ

P + μμ(cid:2)

=

1
NJ

xkx(cid:2)

k =

C0 0 C0 1 C0 2
C1 0 C1 1 C1 2
C2 0 C2 1 C2 2

.

(5)

(cid:6)

k∈J

(cid:9)

0 μ(cid:2)

1 μ(cid:2)
2 f

(cid:2)(C0 0) f

(cid:2)(C1 1) f

2(c) can be obtained as: η =
The expectation parameters of the GMRF depicted in Fig.
  where f(·) returns a column vector
(cid:2)(C0 2))(cid:2)
(μ(cid:2)
consisting of the elements of the upper triangular portion of the input matrix  g(·) returns a column
x(rk + ai). Note that
vector containing all the elements of the input matrix and μ
C1 2 is omitted  because there is no edge between the vertices at rk + a1 and rk + a2. In general 
the expectation parameters (Eq. (4)) on the star graph can be calculated by:

(cid:2)(C2 2) g

(cid:2)(C0 1) g

i = 1

(cid:5)

k∈J

NJ

0 ··· μ(cid:2)
μ(cid:2)

(cid:2)

(C0 0)··· f

(cid:2)

(cid:2)

(C0 1)··· g

(cid:2)

η =

n−1 f

(Cn−1 n−1) g

(6)
where n = |V| is the number of vertices. The dimensionality of η is: nd + n(d + 1)d/2 + (n− 1)d2 
where d is the dimensionality of the local feature. Also note that {Ci j}i(cid:5)=j∧i j(cid:5)=0 can be omitted.
}n−1
By scanning the image region J (Fig. 2(e))  if we have enough local features  the means {μ
and covariance matrices {Ci i}n−1
i=0
i=0 of local features in the region J come to the vector μ0 and matrix
C0 0  respectively. The expectation parameters (Eq. (4)) can be approximated by:
(cid:11)
(cid:12)(cid:13)
(C0 0)··· f
(cid:2)
)

(cid:11)
η = (μ(cid:2)

(cid:12)(cid:13)
(cid:14)
0 ··· μ(cid:2)

(C0 n−1)

(C0 0)

(cid:14)

(cid:14)

(cid:11)

(7)

(C0 n−1)

f

0

(cid:2)

(cid:2)

g

(cid:2)

(cid:2)

.

i

(cid:12)(cid:13)
(C0 1)··· g
n−1

n

n

(cid:10)(cid:2)

 

Equation (7) is calcuated more efﬁciently than Eq. (6) and comes to the same vector as Eq. (6).
However  in the preliminary experiment  Eq. (6) is better than Eq. (7) in terms of the classiﬁcation
accuracies. In the following sections  we use Eq. (6) for the expectation parameters.

2.4 Metric

(cid:5)
In Section 2.2  we mentioned that the metric varies depending on the parameters. We now derive
a metric between the expectation parameters [1]. Let ds represent the length of the small line-
element connecting η and η + dη. dη is represented by using basis vectors e∗i: dη =
i ηie∗i.
The squared distance can be calculated by: ds2 = (cid:3)dη  dη(cid:4) =
(cid:3)e∗i  e∗j(cid:4)dηidηj  where (cid:3)· ·(cid:4)
is the inner product of two vectors. By applying the Taylor expansion to KL divergence between
p(x; η) and p(x; η + dη)  ds2 can be represented as follows: ds2 = KL[p(x; η) : p(x; η + dη)] =
2 dη(cid:2)
1
is the FIM. By comparing these equations  it is clear
that the metric matrix consisting of the inner products of the basis vectors corresponds to the FIM:

∗
∗ijdηidηj  where G

dη = 1
2

(cid:5)

(cid:5)

i j G

∗
G

i j

∗ij = (cid:3)e∗i  e∗j(cid:4).
G

(8)

Thus  the FIM is a proper metric for the feature vectors (the expectation parameters) obtained from
the GMRF.
The Cram´er-Rao inequality gives us a better understanding of the FIM. Assuming that ˆη is an un-
biased estimator  the variance-covariance matrix of ˆη satisﬁes: Var[ˆη] ≥ 1
∗)−1. Consequently 
the FIM is considered to be the inverse of the variance of an estimator  making it natural to use the
matrix as a distance metric between the parameters.

N (G

2.5
Implementation of Graphical Gaussian Vector
At ﬁrst  we build the concatenated vector as x = (x(cid:2)
  where each xi corresponds to
the local feature of the vertex i. By using all training data  the mean μ = E[x] and the precision

1 ··· x(cid:2)

n )(cid:2)

4

T

)

2

1=z
zz
(
z

Tyy
)

2

(

1=y
y

x
1=x
Txx
(
)

2

(cid:9)(cid:66)(cid:10)

*F =

x
1
x
2
M
z
1
z
2
xx
11
xx
22
M
zz
11
zz
22
xx
21
yy
21
zz
21
yx
11
yx
21
yx
12
yx
2
2
M
zy
11
zy
21
zy
12
zy
22

xx L
1

 

 

2

 

zz
 
1

2

xxxx
22
11

 

 

L

zzzz
22
11

 

zzyyxx
21
21

21

 

 

yxyxyxyx
2
11

21

12

 

 

 

 

L

 

zyzy
21
11

 

 

zyzy
22
12

 

2

ijF *

ppiF  *

ppiF  *

*

ppF  

rr

*

GGF

pqiF  *

rrF  *

pq

pqiF  *

rrF  *

pq

*

pqF  

rs

*

\GGF

(cid:9)(cid:67)(cid:10)

*G =

*

\GGF

GGF
*
GGF
*

*

\ GGF

\

-1

\GGF
*
\GGF
*

\ GGF
*
\ GGF
*
\

\

\GGF
*
\GGF
*

(cid:9)(cid:68)(cid:10)

  z = (z1  z2)(cid:2)

  y = (y1  y2)(cid:2)

Figure 3: Here V = {x  y  z} and E = {{x  y} {x  z}}. The dimensionality of the local features is
). A vector concatenating the local features in V is
2 (x = (x1  x2)(cid:2)
v = (x1  x2  y1  y2  z1  z2)(cid:2)
. Using the training data  we calculate a mean μ and a precision matrix
J of v. Using μ and J  the Fisher information matrix of the full Gaussian family can be calculated
as in (b)  whose rows and columns correspond to the elements of the expectation parameters. In (b) 
∗
∗(η) can be partitioned into the submatrices F
\G \G(η). The
F
∗(η).
Fisher information matrix of the GMRF is obtained as shown in (c) using the submatrices of F

∗
\G G(η) and F

∗
G \G(η)  F

∗
G G(η)  F

(cid:5)

k μkJki

∗(η) of the GMRF
matrix J = P
∗(η) with μ and
is derived from the FIM of the full Gaussian family F
(cid:5)
J. Let e∗i  e∗ij denote the basis vectors corresponding to μi and Pij + μiμj in Eq. (4) respec-
(cid:5)
∗(η) are obtained by [20]: F
Jμ) +
tively. The elements of F
∗i pq(η) = (cid:3)e∗i  e∗pq(cid:4) = −Jpi
∗i pp(η) =
k μkJkj  F
(cid:3)e∗i  e∗pp(cid:4) = −Jpi
∗pq rr(η) =
k μkJkp  F
(cid:3)e∗pq  e∗rr(cid:4) = JprJrq  F
∗pp rr(η) = (cid:3)e∗pp  e∗rr(cid:4) = 1
Next we derive G

−1  P = E[(x − μ)(x − μ)(cid:2)] are obtained. Since the FIM G
∗(η)  we now calculate F
(cid:5)
(cid:5)
∗ij(η) = (cid:3)e∗i  e∗j(cid:4) = Jij(1 + μ(cid:2)
k μkJkq − Jqi
(cid:16)

k μkJkp  F
∗pq rs(η) = (cid:3)e∗pq  e∗rs(cid:4) = JpsJqr + JqsJpr  F
(cid:15)
∗(η) can be partitioned according to the graphs G and \G:
∗
∗
G \G(η)
G G(η)
F
F
∗
∗
\G G(η) F
\G \G(η)
F

∗(η). F
∗

∗(η) from F

2 J 2
pr.

(η) =

(9)

F

.

∗(η) is obtained as the Schur complement of F

∗(η) with respect to the

∗
\G G(η).

G G(η) − F
∗

∗
G \G(η)

∗
\G \G(η)

(η) = F

(10)
As these calculations may be complicated  we present a simple example using a GMRF with n = 3
vertices  shown in Fig. 3.

F

F

∗(η) is difﬁcult to deal with as it depends on the expectation parameters. Thus  we
However  G
approximate the model space using the tangent space at the center point of all training data [20]:
∗(η) ≈ G
∗(η
i  and N is the number of training images. In order to embed
G
the proper metric into the expectation parameters  we multiply G
∗
\G G(η

(cid:16)1/2

c)1/2 by η:

∗
\G \G(η

c) where η

(cid:3)−1

c) − F

∗
G \G(η

∗
G G(η

(cid:5)

∗(η

N

i=1 η

(cid:15)

c =

ζ =

(cid:2)

(11)

c)

c)

c)

η.

F

F

F

We call ζ Graphical Gaussian Vector (GGV). This vector is used directly to build sophisticated linear
classiﬁers.

We have a derivation of GGV  and the algorithm for it is very simple  consisting of the following
three steps: 1) calculation of local auto-correlations of local features; 2) estimation of the expectation
parameters of the GMRF; and 3) embedding the distance metric (the Fisher information metric) into
the expectation parameters. The calculation of GGV is given in Algorithm 1. Before the calculation
of GGV  we have to estimate the FIM of GMRF by decomposing the FIM of the full Gaussian. As a
consequence  we obtain one common FIM for all expectation parameters. In practice  since using all
training data is infeasible to estimate the FIM  we use a subset of local features randomly sampled
from training data. Note that since the calculation of the FIM is done in the preprocessing stage  it
is not necessary to calculate the FIM when extracting GGVs.

5

A FIM of the GMRF G
∗
\G \G(η) [15]:
submatrix F

∗
G

(cid:2)

(cid:3)−1

Algorithm 1 Calculation of GGV.
Input: An image region J  and the Fisher information matrix of the GMRF G
Output: GGV ζ

∗(η

c)

1. Calculate local auto-correlations of local features:

(cid:5)

(cid:5)
i = 1
μ
k∈J
0 ··· μ(cid:2)
η = (μ(cid:2)
∗(η

ζ = (G

NJ

c))1/2 η

x(rk + ai)  Ci j = 1
NJ

2. Estimate the expectation parameters:

(cid:2)(C0 0)··· f

n−1 f

k∈J
(cid:2)(Cn−1 n−1) g

x(rk + ai)x(rk + aj)(cid:2)
(cid:2)(C0 1)··· g

(cid:2)(C0 n−1))(cid:2)

3. Embed the Fisher information metric into the expectation parameters:

3 Experiment

(cid:2)(C0 0))(cid:2)

glc = (μ(cid:2)
0 f
(cid:2)(C0 0) g

We tested our method on the standard object and scene datasets (Caltech101  Caltech256  and
15-Scenes). For the ﬁrst experiment  we evaluated the effects of the graph structure (i.e. spatial
information) and the FIM. As baseline methods  we used Generalized Local Correlation (GLC)
[19]: η
without the FIM  Local Auto-Correlation features (LAC) [21]  [14]:
(cid:2)(C0 1)··· g
lac = (μ(cid:2)
η
without the FIM  and the Global Gaussian with
0 f
c). The comparison among these methods are
a center linear kernel (GG) [20]: η
shown in Table 1. Two types of graph structures were utilized for the GGVs. The ﬁrst is shown in
Fig. 2(c) (GGV  n = 3)  which models a horizontal and vertical spatial layout of the local features.
The second is shown in Fig. 2(d) (GGV  n = 5)  which adds diagonal spatial layouts of the features
to Fig. 2(c). We also compared L2 normalized GGVs (i.e.  ˆζ = ζ/||ζ||). To embed the global
spatial information  we used the spatial pyramid representation with a 1× 1 + 2× 2 + 3× 3 pyramid
structure.

(cid:2)(C0 n−1))(cid:2)
∗(η
glc with F

Table 1: The relationships between GLC  LAC  GG  and GGV in terms of spatial information and
Fisher information metrics.

Method
GLC
LAC
GG

GGV (proposed)

Spatial information

√
-
√
-

Fisher information metric

-
√
-
√

In the second experiment  we compared GGVs with the Improved Fisher kernel (IFK) [24]  [25] 
which is the best image representation available at the time of writing. In this experiment  we used
the spatial pyramid representation with a 1× 1 + 2× 2 + 3× 1 structure. The number of components
c in GMMs is an important parameter for IFK. We tested GMMs with c = 32  64  128  and 256
Gaussians to compute IFKs and compared them with GGVs.
For all datasets  SIFT features were densely sampled and were described for 16 × 16 patches. We
downsized images if their longest side was more than 300 pixels. As the aforementioned features
depend on the dimensionality of the local feature  we reduced its dimensionality using PCA and
compared performance as a function of its new dimensionality. As a linear classiﬁer  we used the
multi-class Passive-Aggressive Algorithm (PA) [6].

3.1 Caltech101

Caltech101 is the de facto standard object-recognition dataset [10]. To evaluate the classiﬁcation
performance  we followed the most commonly used methodology. Fifteen images were randomly
selected from all 102 categories for training purposes and the remaining images were used for test-
ing. The classiﬁcation score was averaged over 10 trials.

Before comparison between GGVs and the baselines  we evaluate the sensitivities of the sampling
step of local features. The sampling step is one of the important parameters of GGV  because GGV
calculates auto-correlations of the neighboring local features. In this preliminary experiment  we ﬁx
the number of vertices is 5 (n = 5) and the dimensionality of local feature is 32. We do not use

6

75

70

65

60

55

50

]

%

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

45
(cid:1)
0

5

Caltech101

(cid:1)

Generalized local correlation
Local auto−correlation
Global gaussian
GGV (proposed)  n=3
GGV (proposed)  n=5
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

10

15

20

25

dimensionality of local feature

30

35

70

68

66

64

62

60

58

56

54

52

]

%

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

50
(cid:1)
0

5

Caltech101

(cid:1)

IFK c=32
IFK c=64
IFK c=128
IFK c=256
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

10

15

20

25

30

35

dimensionality of local feature

70

68

66

64

62

60

58

56

54

52

]

%

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

50
(cid:1)
0

2

Caltech101

(cid:1)

IFK c=32
IFK c=64
IFK c=128
IFK c=256
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

4
10
dimensionality of image feature

6

8

12

14
x 104

Figure 4: A comparison of classiﬁcation accuracies of: (left) GGV  GLC  LAC and GG; (center)
GGV and IFK with respect to the dimensionality of “local features”; (right) GGV and IFK with
respect to the dimensionality of “image features” in the Caltech101 dataset.

the spatial pyramid. The results are as follows: 56.7 % (step = 4 pixels)  57.7 % (step = 6 pixels)  
57.7 % (step = 8 pixels)   57.2 % (step = 10 pixels)   56.5 % (step = 12 pixels). There is no clear
difference between step sizes of 6 and 8 pixels. Therefore in the following experiments  we use 6
pixels sampling step for local feature extraction.

Figure 4 (left) shows the classiﬁcation accuracies as a function of the dimensionality of the local
features. A large dimensionality yielded better performance  and the proposed method (GGV) out-
performed the other methods (GLC  LAC  and GG). By comparing GGV with LAC  and GG with
GLC  it is clear that embedding the Fisher information metric improved the classiﬁcation accuracy
signiﬁcantly. By comparing GGV with GG  as well as LAC with GLC  it can also be seen that
embedding the spatial layout of local features also improved the accuracy. In a comparison between
the graph structures  the four-neighbor structure (Fig. 2(d)) performed slightly better than the two-
neighbor structure (Fig. 2(c)). If we compare the regular GGVs with the L2 normalized GGVs  we
ﬁnd that the L2 normalization improved the accuracy by almost 2 %.

In the second experiment  we compared the L2 normalized GGVs with IFKs. The results are shown
in Fig. 4 (center). For all the dimensionalities and numbers of components  GGVs performed better
than IFKs. Fig. 4 (right) shows the classiﬁcation accuracy as a function of the dimensionality of
the image features which are converted from the results shown in Fig. 4 (center). We see that
GGVs achieved higher accuracy for a lower dimensionality of image features. The results were
also compared against those of leading methods that use a linear classiﬁer. The performance scores
are referenced from the original papers. LLC [27] scored 65.4 % and ScSPM [28] scored 67.0 % 
whereas our method achieved 71.3 % when the dimensionality of the local feature is 32 and the
number of vertices is 5. Therefore  our method is better than the best available methods in this
dataset  despite using a linear classiﬁer and not requiring a codebook or descriptor matching.

3.2 Caltech256

Caltech256 consists of images from 256 object categories [13]. This database is signiﬁcant for
its large inter-class variability  as well as an intra-class variability greater than that found in Cal-
tech101. To evaluate performance  we followed a commonly used methodology. Fifteen images
were randomly selected from all categories for training purposes and the remaining images were
used for testing. The classiﬁcation score was averaged over 10 trials.

Figure 5 (left) shows a comparison of classiﬁcation accuracies of GGV  GLC  LAC and GG. Fig. 5
(center) and (right) show comparisons of the L2 normalized GGVs and IFKs using the Caltech256
dataset with respect to the dimensionality of local features and image features  respectively. The
results show the same trends as for Caltech101. Our method is better than all baseline methods and
IFKs. [24] reported that IFK achieved 34.7% and [27] reported that LLC scored 34.4%  while GGV
obtained 33.4%. However  a fair comparison is difﬁcult because our method used only single-scale
SIFT whereas [24] and [27] used 5-scale SIFT and 3-scale HOG  respectively. It is known that
using multi-scale local features improves classiﬁcation accuracies (e.g. [3]). To be fair comparison 
we used 3-scale SIFT (patch size = 16 × 16  24 × 24  32 × 32) for GGV with n = 5  and L2
normalization. GGV with 3-scale SIFT achieved 36.2% which is better than those leading methods.

7

34

32

30

28

26

24

22

20

18

16

]

%

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

14
(cid:1)
0

5

Caltech256

(cid:1)

Generalized local correlation
Local auto−correlation
Global gaussian
GGV (proposed)  n=3
GGV (proposed)  n=5
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

10

15

20

25

30

35

dimensionality of local feature

34

32

30

28

26

24

22

20

]

%

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

18
(cid:1)
0

5

Caltech256

(cid:1)

IFK c=32
IFK c=64
IFK c=128
IFK c=256
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

10

15

20

dimensionality of local feature

25

30

35

34

32

30

28

26

24

22

20

]

%

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

18
(cid:1)
0

2

Caltech256

(cid:1)

IFK c=32
IFK c=64
IFK c=128
IFK c=256
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

4
10
dimensionality of image feature

6

8

12

14
x 104

Figure 5: A comparison of classiﬁcation accuracies of: (left) GGV  GLC  LAC and GG; (center)
GGV and IFK with respect to the dimensionality of “local features”; (right) GGV and IFK with
respect to the dimensionality of “image features” in the Caltech256 dataset.

3.3

15-Scenes

We experimented with 15-Scenes  a commonly used scene classiﬁcation dataset [18]. We randomly
selected 100 training images for each class and used the remaining samples as test data. We calcu-
lated the mean of the classiﬁcation rate for each class. This score was averaged over 10 trials  where
the training and test sets were randomly re-selected for each trial. This is the same methodology as
that used in previous studies.

84

82

80

78

76

74

72

70

68

66

]

%

[
 

t

e
a
r
 
n
o

i
t

a
c
i
f
i
s
s
a
c

l

64
(cid:1)
0

5

15scenes

(cid:1)

Generalized local correlation
Local auto−correlation
Global gaussian
GGV (proposed)  n=3
GGV (proposed)  n=5
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

10

15

20

25

dimensionality of local feature

30

35

84

82

80

78

76

74

]

%

[
 

t

e
a
r
 
n
o

i
t

a
c
i
f
i
s
s
a
c

l

72
(cid:1)
0

5

15scenes

(cid:1)

IFK c=32
IFK c=64
IFK c=128
IFK c=256
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

10

15

20

dimensionality of local feature

25

30

35

84

82

80

78

76

74

]

%

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

72
(cid:1)
0

2

15scenes

(cid:1)

IFK c=32
IFK c=64
IFK c=128
IFK c=256
GGV L2 norm (proposed)  n=3
GGV L2 norm (proposed)  n=5

4
10
dimensionality of image feature

8

6

12

14
x 104

Figure 6: A comparison of classiﬁcation accuracies of: (left) GGV  GLC  LAC and GG; (center)
GGV and IFK with respect to the dimensionality of “local features”; (right) GGV and IFK with
respect to the dimensionality of “image features” in the 15-Scenes dataset.

Figure 6 (left) shows a comparison of classiﬁcation accuracies of GGV  GLC  LAC and GG using
the 15-Scenes dataset. The results show similar trends as for Caltech101 and Caltech256  except
that there is no difference between the scores of the graph structures. In the second experiment 
the results with respect to the dimensionality of local features and image features are shown in
Figs. 6 (center) and (right)  respectively. In contrast to the results for Caltech101 and 256  IFKs
scored slightly higher than GGVs (IFK (c = 256  d = 32): 84.0%  GGV (n = 5  d = 32 and
L2 normalized): 83.5%). As the leading method  the spatial Fisher kernel [17] reported the highest
score (88.1%). However  since [17] used 8-scale SIFT descriptors  which provide richer information
than the single-scale SIFT descriptors we used  it is difﬁcult to make a direct comparison.

4 Conclusion

In this paper  we proposed an efﬁcient image feature called a Graphical Gaussian Vector  which uses
neither codebook nor local feature matching. In the proposed method  spatial information about
local features and the Fisher information metric are embedded into a feature by modeling the image
as the Gaussian Markov Random Field (GMRF). Experimental results using three standard datasets
demonstrated that the proposed method offers a performance that is superior or comparable to other
state-of-the-art methods. The proposed image feature calculates the expectation parameters of the
GMRF simply and effectively while maintaining a high classiﬁcation rate.

8

References
[1] S. Amari and H. Nagaoka. Methods of Information Geometry  volume 191 of Translations of mathemati-

cal monographs. American Mathematical Society  2001.

[2] A.C. Berg  T.L. Berg  and J. Malik. Shape matching and object recognition using low distortion corre-

spondence. In CVPR  2005.

[3] L. Bo  X. Ren  and D. Fox. Kernel descriptors for visual recognition. In NIPS  2010.
[4] O. Boiman  E. Shechtman  and M. Irani. In defense of nearest-neighbor based image classiﬁcation. In

CVPR  2008.

[5] Y. Cao  C. Wang  Z. Li  L. Zhang  and L. Zhang. Spatial-bag-of-features. In CVPR  2010.
[6] K. Crammer  O. Dekel  J. Keshet  S. Shalev-Shwartz  and Y. Singer. Online passive-aggressive algorithms.

JMLR  7:551–585  2006.

[7] G. Csurka  C. R. Dance  L. Fan  J. Willamowski  and C. Bray. Visual categorization with bags of key-

points. In ECCV International Workshop on SLCV  2004.

[8] O. Duchenne  A. Joulin  and J. Ponce. A graph-matching kernel for object categorization. In ICCV  2011.
[9] J.D.R. Farquhar  S. Szedmak  H. Meng  and J. Shawe-Taylor. Improving “bag-of-keypoints” image cate-

gorisation: Generative models and pdf-kernels. Technical report  University of Southampton  2005.

[10] L. Fei-Fei  R. Fergus  and P. Perona. Learning generative visual models from few training examples: an

incremental bayesian approach tested on 101 object categories. In CVPR  Workshop on GMBV  2004.

[11] R. Fergus  P. Perona  and A. Zisserman. Object class recognition by unsupervised scale-invariant learning.

In CVPR  2003.

[12] R. Fergus  P. Zisserman  and A. Perona. Weakly supervised scale-invariant learning of models for visual

recognition. IJCV  71(3):273–303  2007.

[13] G. Grifﬁn  A. Holub  and P. Perona. Caltech-256 object category dataset. Technical Report 7694  Cali-

fornia Institute of Technology  2007.

[14] T. Harada  H. Nakayama  and Y. Kuniyoshi. Improving local descriptors by embedding global and local

spatial information. In ECCV  2010.

[15] Jason K. Johnson. Convex Relaxation Methods for Graphical Models: Lagrangian and Maximum Entropy

Approaches. PhD thesis  MIT  2008.

[16] J. Kim and K. Grauman. Asymmetric region-to-image matching for comparing images with generic object

categories. In CVPR  2010.

[17] J. Krapac  J. Verbeek  and F. Jurie. Modeling spatial layout with ﬁsher vectors for image categorization.

In ICCV  2011.

[18] S. Lazebnik  C. Schmid  and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing

natural scene categories. In CVPR  2006.

[19] H. Nakayama  T. Harada  and Y. Kuniyoshi. Dense sampling low-level statistics of local features. In

CIVR   2009.

[20] H. Nakayama  T. Harada  and Y. Kuniyoshi. Global gaussian approach for scene categorization using

information geometry. In CVPR  2010.

[21] N. Otsu and T. Kurita. A new scheme for practical  ﬂexible and intelligent vision systems. In Proc. IAPR

Workshop on Computer Vision  1988.

[22] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR 

2007.

[23] F. Perronnin  C. Dance  G. Csurka  and M. Bressan. Adapted vocabularies for generic visual categoriza-

tion. In ECCV  2006.

[24] F. Perronnin  J. S´anchez  and T. Mensink. Improving the ﬁsher kernel for large-scale image classiﬁcation.

In ECCV  2010.

[25] J. S´anchez and F. Perronnin. High-dimensional signature compression for large-scale image classiﬁcation.

In CVPR  2011.

[26] C. Wallraven  B. Caputo  and A. Graf. Recognition with local features: the kernel recipe. In ICCV  2003.
[27] J. Wang  J. Yang  K. Yu  F. Lv  T. Huang  and Y. Gong. Locality-constrained linear coding for image

classiﬁcation. In CVPR   2010.

[28] J. Yang  K. Yu  Y. Gong  and T. Huang. Linear spatial pyramid matching using sparse coding for image

classiﬁcation. In CVPR  2009.

[29] X. Zhou  K. Yu  T. Zhang  and T. S. Huang. Image classiﬁcation using super-vector coding of local image

descriptors. In ECCV  2010.

9

,François-Xavier Briol
Chris Oates
Mark Girolami
Michael Osborne
Yan Duan
Marcin Andrychowicz
Bradly Stadie
OpenAI Jonathan Ho
Jonas Schneider
Pieter Abbeel
Wojciech Zaremba