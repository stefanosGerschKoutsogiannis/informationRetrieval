2015,Learning structured densities via infinite dimensional exponential families,Learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications. Current approaches are mainly focused on learning the structure under restrictive parametric assumptions  which limits the applicability of these methods. In this paper  we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model. We consider probabilities that are members of an infinite dimensional exponential family  which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel $k$. One difficulty in learning nonparametric densities is evaluation of the normalizing constant. In order to avoid this issue  our procedure minimizes the penalized score matching objective. We show how to efficiently minimize the proposed objective using existing group lasso solvers. Furthermore  we prove that our procedure recovers the graph structure with high-probability under mild conditions. Simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.,Learning Structured Densities via Inﬁnite

Dimensional Exponential Families

Siqi Sun

TTI Chicago

siqi.sun@ttic.edu

Mladen Kolar

University of Chicago

mkolar@chicagobooth.edu

Jinbo Xu
TTI Chicago

jinbo.xu@gmail.com

Abstract

Learning the structure of a probabilistic graphical models is a well studied prob-
lem in the machine learning community due to its importance in many applica-
tions. Current approaches are mainly focused on learning the structure under re-
strictive parametric assumptions  which limits the applicability of these methods.
In this paper  we study the problem of estimating the structure of a probabilistic
graphical model without assuming a particular parametric model. We consider
probabilities that are members of an inﬁnite dimensional exponential family [4] 
which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its
kernel k. One difﬁculty in learning nonparametric densities is the evaluation of
the normalizing constant. In order to avoid this issue  our procedure minimizes
the penalized score matching objective [10  11]. We show how to efﬁciently min-
imize the proposed objective using existing group lasso solvers. Furthermore  we
prove that our procedure recovers the graph structure with high-probability under
mild conditions. Simulation studies illustrate ability of our procedure to recover
the true graph structure without the knowledge of the data generating process.

1

Introduction

Undirected graphical models  or Markov random ﬁelds [13]  have been extensively studied and ap-
plied in ﬁelds ranging from computational biology [15  28]  to natural language processing [16  20]
and computer vision [9  17]. In an undirected graphical model  conditional independence assump-
tions underlying a probability distribution are encoded in the graph structure. Furthermore  the joint
probability density function can be factorized according to the cliques of the graph [14]. One of the
fundamental problems in the literature is learning the structure of a graphical model given an i.i.d.
sample from an unknown distribution. A lot of work has been done under speciﬁc parametric as-
sumptions on the unknown distribution. For example  in Gaussian Graphical Models the structure of
the graph is encoded by the sparsity pattern of the precision matrix [6  30]. Similarly  in the context
of exponential family graphical models  where the node conditional distribution given all the other
nodes is a member of an exponential family  the structure is described by the non-zero coefﬁcients
[29]. Most existing approaches to learn the structure of a high-dimensional undirected graphical
model are based on minimizing a penalized loss objective  where the loss is usually a log-likelihood
or a composite likelihood and the penalty induces sparsity on the resulting parameter vector [see 
for example  6  12  18  22  24  29  30]. In addition to sparsity inducing penalties  methods that
use other structural constraints have been proposed. For example  since many real-world networks
are scale-free [1]  several algorithms are designed speciﬁcally to learn structure of such networks

1

[5  19]. Graphs tend to have cluster structure and learning simultaneously the structure and cluster
assignment has been investigated [2  27].
In this paper  we focus on learning the structure of a pairwise graphical models without assuming
a parametric class of models. The main challenge in estimating nonparametric graphical models
is computation of the log normalizing constant. To get around this problem  we propose to use
score matching [10  11] as a divergence  instead of the usual KL divergence  as it does not require
evaluation of the log partition function. The probability density function is estimated by minimizing
the expected distance between the model score function and the data score function  where the score
function is deﬁned as gradient of the corresponding probability density functions. The advantage
of this measure is that the normalization constant is canceled out when computing the distance. In
order to learn the underlying graph structure  we assume that the logarithm of the density is additive
in node-wise and edge-wise potentials and use a sparsity inducing penalty to select non-zero edge
potentials. As we will prove later  our procedure will allow us to consistently estimate the underlying
graph structure.
The rest of paper is organized as follows. We ﬁrst introduce the notations  background and related
work. Then we formulate our model  establish a representer theorem and present a group lasso
algorithm to optimize the objective. Next we prove that our estimator is consistent by showing that
it can recover the true graph with high probability given sufﬁcient number of samples. Finally the
results for simulated data are presented to demonstrate the correctness of our algorithm empirically.

i∈[d] |θi|p)

1

1.1 Notations
Let [n] denote the set {1  2  . . .   n}. For a vector θ = (θ1  . . .   θd)T ∈ Rd 

((cid:80)
(cid:107)f(cid:107)Lp(χ p0) = (cid:107)f(cid:107)p = ((cid:82)

let (cid:107)θ(cid:107)p =
p denote its lp norm. Let column vector vec(D) denote the vectorization of ma-
d ) the
trix D  cat(a  b) denote the concatenation of two vectors a and b  and mat(aT
d . For χ ⊆ Rd  let Lp(χ  p0) denote the space of func-
matrix with rows given by aT
tion for which the p-th power of absolute value is p0 integrable; and for f ∈ Lp(χ  p0)  let
p denote its Lp norm. Throughout the paper  we denote H
(or Hi Hij) as Hilbert space and (cid:104)· ·(cid:105)H (cid:107) · (cid:107)H as corresponding inner product and norm.
For any operator C : H1 → H2  we use (cid:107)C(cid:107) to denote the usual operator norm  which is deﬁned as

1   . . .   aT
χ |f|pdx)

1

1   . . .   aT

(cid:107)C(cid:107) = inf{a ≥ 0 : (cid:107)Cf(cid:107)H2 ≤ a(cid:107)f(cid:107)H1 for all f ∈ H1};

and (cid:107)C(cid:107)HS to denote its Hilbert-Schmidt norm  which is deﬁned as

(cid:88)

i∈I

(cid:107)C(cid:107)2

HS =

(cid:107)Cei(cid:107)2H2

 

where ei is an orthonormal basis of H for an index set I. Also  we use R(C) to denote operator C’s
range space. For any f ∈ H1 and g ∈ H2  let f ⊗ g denote their tensor product.

2 Background & Related Work

2.1 Learning graphical models in exponential families

Let x = (x1  x2  ...  xd) be a d-dimensional random vector from a multivariate Gaussian distribution.
It is well known that the conditional independency of two variables given all the others is encoded
in the zero pattern of its precision matrix Ω  that is  xi and xj are conditionally independent given
x−ij if and only if Ωij = 0  where x−ij is the vector of x without xi and xj. A sparse estimate
of Ω can be obtained by maximum-likelihood (joint selection) or pseudo-likelihood (neighborhood
selection) optimization with an added l1 penalty [6  22  30]. Given n independent realizations of x
(rows of X ∈ Rn×d)  the penalized maximum-likelihood estimate of the precision matrix can be
obtained as

tr( ˆSΩ) − log det Ω + λ(cid:107)Ω(cid:107)1 
where ˆS = n−1X T X and λ controls the sparsity level of estimated graph.

ˆΩλ = arg min
Ω(cid:31)0

(1)

2

The pseudo-likelihood method estimates the neighborhood of a node a by the non-zeros of the
solution to a regularized linear model

ˆθs = arg min
θ

(cid:107)Xs − X−sθ(cid:107)2
The estimated neighborhood is then ˆN (s) = {a : θsa (cid:54)= 0}.
Another way to specify a parametric graphical model is by assuming that each node-conditional
distributions is a part of the exponential family [29]. Speciﬁcally  the conditional distribution of xs
given x−s is assumed to be

2 + λ(cid:107)θ(cid:107)1.

1
n

(2)

P (xs|x−s) = exp(

θstxsxt + C(xs) − D(x−s  θ)) 

(3)

(cid:88)

t∈N (s)

model assumptions for count data  the normalization constant is − exp((cid:80)

where C is the base measure  D is the log-normalization constant and N (s) is the neighborhood a the
node s. Similar to (2)  the neighborhood of each node can be estimated by minimizing the negative
log-likelihood with l1 penalty on θ. The optimization is tractable when the normalization constant
D can be easily computed based on the model assumption. For example  under Poisson graphical
t∈N (s) θstxt). When using
the neighborhood estimation  the graph can be estimated as the union of the neighborhoods of each
node  which leads to consistent graph estimation [22  29].

2.2 Generalized Exponential Family and RKHS
We say H is a RKHS associated with kernel k : χ × χ → R+ if and only if for each x ∈ χ  the
following two conditions are satisﬁed: (1) k(·  x) ∈ H and (2) it has reproducing properties such that
f (x) = (cid:104)f  k(·  x)(cid:105)H for all f (·) ∈ H  where k is a symmetric and positive semideﬁnite function.
Denote the RKHS H with kernel k as H(k).
i=1 αik(·  xi). Similarly
j=1 βjk(·  yj)  the inner product of f and g is deﬁned as (cid:104)f  g(cid:105)H =
i j αiαjk(xi  xj). The sum-
mation is guaranteed to be larger than or equal to zero because the kernel k is positive semideﬁnite.
We consider the exponential family in inﬁnite dimensions [4]  where

For any f ∈ H(k)  there exists a set of xi and αi  such that f (·) = (cid:80)∞
for any g ∈ H(k)  g(·) = (cid:80)∞
(cid:113)(cid:80)
(cid:80)∞
i j=1 αiβjk(xi  yj). Therefore the norm of f simply is (cid:107)f(cid:107)H =

P = {pf (x) = ef (x)−A(f )q0(x)  x ∈ χ; f ∈ F}

and the function space F is deﬁned as

(cid:90)

(cid:90)

(cid:13)(cid:13)(cid:13)(cid:13) ∂ log p(x)

∂x

3

F = {f ∈ H(k) : A(f ) = log

ef (x)q0(x)dx < ∞} 

where q0(x) is the base measure  A(f ) is a generalized normalization constant such that pf (x) is
a valid probability density function  and H is a RKHS [3] associated with kernel k. To see it as
a generalization of the exponential family  we show some examples that can generate useful ﬁnite
dimension exponential families:

χ

• Normal: χ = R  k(x  y) = xy + x2y2
• Poisson: χ = N ∪ {0}  k(x  y) = xy
• Exponential: χ = R+  k(x  y) = xy.

For more detailed information  please refer to [4].
When learning structure of a graphical model  we will further impose structural conditions on H(k)
in order ensure that F consists of additive functions.

2.3 Score Matching

Score matching is a convenient procedure that allows for estimating a probability density without
computing the normalizing constant [10  11]. It is based on minimizing Fisher divergence

J(p(cid:107)p0) =

1
2

p(x)

− ∂ log p0(x)

∂x

dx 

(4)

(cid:13)(cid:13)(cid:13)(cid:13)2

2

1

∂x

∂x1

= ( ∂ log p(x)

  . . .   ∂ log p(x)

where ∂ log p(x)
) is the score function. Observe that for p(x  θ) =
Z(θ) q(x  θ) the normalization constant Z(θ) cancels out in the gradient computation  which makes
the divergence independent of Z(θ). Since the score matching objective involves the unknown or-
acle probability density function p0  it is typically not computable. However  under some mild
conditions which we will discuss in METHODS section  (4) can be rewritten as

∂xd

(cid:90)

(cid:88)
(cid:88)

i∈[d]

(cid:88)

J(p(cid:107)p0) =

p0(x)

1
2

(

∂ log p(x)

∂xi

)2 +

∂2 log p(x)

∂x2
i

dx.

After substituting the expectation with an empirical average  we get

ˆJ(p(cid:107)p0) =

1
n

a∈[n]

i∈[d]

1
2

(

∂ log p(Xa)

∂xi

)2 +

∂2 log p(Xa)

∂x2
i

.

(5)

(6)

(cid:88)

i≤j

(i j)∈S

(cid:88)

Compared to maximum likelihood estimation  minimizing ˆJ(p(cid:107)p0) is computationally tractable.
While we will be able to estimate p0 only up to a scale factor  this will be sufﬁcient for the purpose
of graph structure estimation.

3 Methods

3.1 Model Formulation and Assumptions
We assume that the true probability density function p0 is in P. Furthermore  for simplicity we
assume that

log p0(x) = f (x) =

f0 ij(xi  xj) 

where f0 ii(xi  xi) is a node potential and f0 ij(xi  xj) is an edge potential. The set S denotes the
edge set of the graph. Extensions to models where potentials are deﬁned over larger cliques are
possible. We further assume that f0 ij ∈ Hij(kij)  where Hij is a RKHS with kernel kij. To
simplify the notation  we use f0 ij(x) or kij(·  x) to denote f0 ij(xi  xj) and kij(·  (xi  xj)). If the
context is clear  we drop the subscript for norm or inner product. Deﬁne

H(S) = {f =

fij|fij ∈ Hij}

(7)

(i j)∈S

(i j)∈S kij.

(i j)∈S (cid:107)fij(cid:107)2Hij

as a set of functions that decompose as sum of bivariate functions on edge set S. Note that
and kernel

H(S) is also (a subset of) a RKHS with the norm (cid:107)f(cid:107)2H(S) = (cid:80)
k =(cid:80)
Let Ω(f ) = (cid:107)f(cid:107)H 1 =(cid:80)
denote ΩS(fS) = (cid:80)
i≤j (cid:107)fij(cid:107)Hij . For any edge set S (not necessarily the true edge set)  we
s∈S (cid:107)fs(cid:107)Hs as the norm Ω reduced to S. Similarly  denote its dual norm as
(cid:16) ∂f (x)
(cid:88)
(cid:88)
(cid:88)

S[fS] = maxΩS (gS )≤1(cid:104)fS  gS(cid:105) [25].
Ω∗
Under the assumption that the unknown f0 is additive  the loss function becomes

i∈[d]
(cid:104)fij − f0 ij 

⊗ ∂kij(cid:48)(·  (xi  xj(cid:48)))

(cid:90)
(cid:88)
(cid:88)

dx(fij(cid:48) − f0 ij(cid:48))(cid:105)

∂kij(·  (xi  xj))

− ∂f0(x)
∂xi

(cid:17)2

j j(cid:48)∈[d]

J(f ) =

p0(x)

p0(x)

(cid:90)

i∈[d]

∂xi

∂xi

∂xi

dx

(cid:104)fij − f0 ij  Cijij(cid:48)(fij(cid:48) − f0 ij(cid:48))(cid:105).

1
2

1
2

1
2

=

=

i∈[d]

j j(cid:48)∈[d]

Intuitively  C can be viewed as a d2 matrix  and the operator at position (ij  ij(cid:48)) is Cij ij(cid:48). For
general (ij  i(cid:48)j(cid:48))  i (cid:54)= i(cid:48) the corresponding operator simply is 0. Deﬁne CSS(cid:48) as
⊗ ∂ki(cid:48)j(cid:48)(·  (xi(cid:48)  xj(cid:48)))

∂kij(·  (xi  xj))

(cid:88)

(cid:90)

dx 

p0(x)

(i j)∈S (i(cid:48) j(cid:48))∈S(cid:48)

∂xi

∂xi

4

which intuitively can be treated as a sub matrix of C with rows S and columns S(cid:48). We will use this
notation intensively in the main theorem and its proof.
Following [26]  we make the following assumptions.
A1. Each kij is twice differentiable on χ × χ.
A2. For any i and ˜xj ∈ χj = [aj  bj]  we assume that
∂2kij(x  y)

lim
xi→a+
i or b

−
i

∂xi∂yi
where x = (xi  ˜xj) and ai  bi could be −∞ or ∞.
A3. This condition ensures that J(p(cid:107)p0) < ∞ for any p ∈ P [for more details see 26]:

|y=x p2

0(x) = 0 

(cid:107) ∂kij(·  x)

(cid:107)Hij ∈ L2(χ  p0) (cid:107) ∂2kij(·  x)

(cid:107)Hij ∈ L2(χ  p0).

∂xi

∂x2
i

Sc[CScSC−1

SS ] ≤ 1 − η  where η > 0.

A4. The operator CSS  is compact and the smallest eigenvalue ωmin = λmin(CSS) > 0.
A5. Ω∗
A6. f0 ∈ R(C)  which means there exists γ ∈ H  such that f0 = Cγ. f0 is the oracle function.
We will discuss the deﬁnition of operator C and Ω∗ in section 4. Compared with [29]  A4 can be
interpreted as the dependency condition and the A5 is the incoherence condition  which is a standard
condition for structure learning in high dimensional statistical estimators.

3.2 Estimation Procedure

We estimate f by minimizing the following penalized score matching objective

min

ˆLµ(f ) = ˆJ(f ) +
s.t. fij ∈ Hij 

(cid:107)f(cid:107)H 1

µ
2

f

where ˆJ(f ) is given in (6). The norm (cid:107)f(cid:107)H 1 = (cid:80)

(8)
i≤j (cid:107)fij(cid:107)Hij is used as a sparsity inducing
penalty. A simpliﬁed form of ˆJ(f ) is given below that will lead to efﬁcient algorithm for solving
(8).
The following theorem states that the score matching objective can be written as a penalized
quadratic function on f.

Theorem 3.1 (i) The score matching objective can be represented as

where C =(cid:82) p0(x)(cid:80)

Lµ(f ) =
∂k(· x)

1
2

⊗ ∂k(· x)

dx is a trace operator.
(ii) Given observed data Xn×d  the empirical estimation of Lµ is

i∈[d]

∂xi

∂xi

(cid:104)f − f0  C(f − f0)(cid:105) +

(cid:107)f(cid:107)H 1

µ
2

(cid:80)

1
2

(cid:80)

ˆLµ(f ) =

(cid:104)f  ˆCf(cid:105) +

(cid:104)fij − ˆξij(cid:105) +

(cid:107)f(cid:107)H 1 + const

µ
2

∂k(· Xa)

i≤j
⊗ ∂k(· Xa)
a∈n

∂xi

(cid:80)

∂x2
i

and ˆξij = 1
n

a∈[n]

∂2kij (· (Xai Xaj ))

otherwise.

(cid:80)

where ˆC = 1
n
∂2kij (· (Xai Xaj ))

∂x2
j

i∈[d]

a∈[n]
if i (cid:54)= j  or ˆξij = 1

∂xi

n

(cid:88)

(9)

(10)

∂2kij (· (Xai Xaj ))

∂x2
i

+

Please refer to our supplementary material for detailed proof 1.
The above theorem still requires us to minimize over F. Our next results shows that the solution is
ﬁnite dimensional. That is  we establish a representer theorem for our problem.

1Please visit ttic.uchicago.edu/∼siqi for supplementary material and code.

5

Theorem 3.2 (i) The solution to (10) can be represented as

ˆfij(·) =

∂kij(·  (Xbi  Xbj))

∂xi

+ βbji

∂kij(·  (Xbi  Xbj))

∂xj

βbij

+ αij

ˆξij 

(11)

where i ≤ j.
(ii) Minimizing (10) is equivalent to minimizing the following quadratic function:

(cid:33)2

(cid:88)

j

(cid:88)

b∈[n]

1
2n

(cid:32)(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

ai

bj

b

i≤j
1
2n

ai

+

=

(βbijGab

ij11 + βbjiGab

ij12) +

αijh1a
ij

(βbijh1b

ij + βbjih2b

ij ) +

αij(cid:107) ˆξij(cid:107)2 +

(DT

ai · θ)2 + Etθ +

µ
2

θt
ijFijθij

(cid:107)f(cid:107)H 1

µ
2

(12)

(cid:88)
(cid:113)
(cid:88)

i≤j

i≤j

ijrs = ∂2kij (Xa Xb)

  ˆξij(cid:105) are constant that only depends on X  θ =
where Gab
cat(vec(α)  vec(β)) is the vector parameter and θij = cat(αij  vec(β·ij)) is a group of parameters.
Dai  E and F are corresponding constant vectors and matrices based on G  h and the order of
parameters. Then the above problem can be solved by group lasso [7  21].

ij = (cid:104) ∂kij (· Xb)

  hrb

∂xr∂ys

∂xr

The ﬁrst part of theorem states our representer theorem  and the second part is obtained by plugging
in (11) to (10). See supplementary material for a detailed proof. Theorem 3.2 provides us with an
efﬁcient way to minimize (8)  as it reduced the optimization to a group lasso problem for which
many efﬁcient solvers exist.
Let ˆf µ = arg minf∈H ˆLµ(f ) denote the solution to (12). We can estimate the graph as follows:

ˆSµ = {(i  j) : (cid:107) ˆf µ

ij(cid:107) (cid:54)= 0} 

(13)

That is  the graph is encoded in the sparsity pattern of ˆf µ.

4 Statistical Guarantees

In this section we study statistical properties of the proposed estimator (13). Let S denote the true
edge set and Sc its complement. We prove that ˆSµ recovers S with high probability when the sample
size n is sufﬁciently large.
Denote D = mat(DT
operator ˆC 

nd). We will need the following result on the estimated

11  . . .   DT

ai  . . .   DT

Proposition 4.1 (Lemma 5 in [8] or Theorem 5 in [26] ) (Properties of C)

1. (cid:107) ˆC − C(cid:107)HS = Op0 (n− 1
2 )
2. (cid:107)(C + µL)−1(cid:107) ≤
positive constants.

µ min diag(L)   (cid:107)C(C + µL)−1(cid:107) ≤ 1  where µ > 0 and L is diagonal with

1

The following result gives ﬁrst order optimality conditions for the optimization problem (8).

Proposition 4.2 (Optimality Condition)
ˆJ(f ) + µ

2 Ω(f )2 achieves optimality when the following two conditions are satisﬁed:

(1) ∇fs
(2) Ω∗

ˆJ(f ) + µΩ(f )
Sc[∇fSc

(cid:107)fs(cid:107)Hs
ˆJ(f )] ≤ µΩ(f ).

fs

= 0 ∀s ∈ S

6

With these preliminary results  we have the following main results.

4 and satisﬁes µ ≤
4(1−η)κmax
s (cid:107) > 0. Then P ( ˆSµ = S) → 1.

Theorem 4.3 Assume that conditions A1-A7 are satisﬁed. The regularization parameter µ is se-
s (cid:107) > 0
lected at the order of n− 1
and κmax = maxs∈S (cid:107)f∗
Proof Idea: The theorem above is the main theoretical guarantee for our score matching estimator.
We use the “witness” proof framework inspired by [23  29]. Let f∗ denote the true density function
and p∗ the probability density function. We ﬁrst construct a solution ˆfS on true edge set S as

  where κmin = mins∈S (cid:107)f∗

ηκminωmin

|S|+ η

√

5

ˆfS = min
fSc =0

ˆJ(f ) +

µ
2

(cid:107)fij(cid:107))2

(14)

(cid:88)

(
(i j)∈S

and set ˆfSc as zero. Using Proposition 4.1  we prove that (cid:107) ˆfS − f∗
4 ). Then we
compute the subgradient on Sc and prove that its dual norm is upper bounded by µΩ(f ) by using
assumptions A4  A5 and A6. Therefore we construct a solution that satisﬁed the optimality condition
and converges in probability to the true graph. Refer to supplementary material for detailed proof.

S(cid:107) = Op(n− 1

5 Experiments

We illustrate performance of our method on two simulations. In our experiments  we use the same
kernel deﬁned as follows:

k(x  y) = exp(−(cid:107)x − y(cid:107)2

2

2σ2

) + r(xT y + c)2 

(15)

|S=1|

|S=0|

|S=1 and ˆSµ=1|

| ˆSµ=1 and S=0|

  and false positive rate is FPRµ =

that is  the summation of a Gaussian kernel and a polynomial kernel. We set σ2 = 1.5  r = 0.1 and
c = 0.5 for all the simulations.
We report the true positive rate vs false positive rate (ROC) curve to measure the performance of
different procedures. Let S be the true edge set  and let ˆSµ be the estimated graph. The true positive
  where |·|
rate is deﬁned as TPRµ =
is the cardinality of the set. The curve is then plotted based on 100 uniformly-sampled regularization
parameters and based on 20 independent runs.
In the ﬁrst simulation  we apply our algorithm to data sampled from a simple chain graph-based
Gaussian model (see Figure 1 for detail)  and compare its performance with glasso [6]. We use the
same sampling method as in [31] to generate the data: we set Ωs = 0.4 for s ∈ S and its diagonal
to a constant such that Ω is positive deﬁnite. We set the dimension d to 25 and change the sample
size n ∈ {20  40  60  80  100} data points.
Except for the low sample size case (n = 20)  the performance of our method is comparable with
glasso  without utilizing the fact that the underlying distribution is of a particular parametric form.
Intuitively  to capture the graph structure  the proposed nonparametric method requires more data
because of much weaker assumptions.
To further show the strength of our algorithm  we test it on a nonparanormal (NPN) distribution
([18]). A random vector x = (x1  . . .   xp) has a nonparanormal distribution if there exist functions
(f1  . . .   fp) such that (f1(x1)  . . .   fd(xd)) ∼ N (µ  Σ). When f is monotone and differentiable 
the probability density function is given by
exp{− 1
2

(f (x) − µ)T Σ−1(f (x) − µ)}(cid:89)

j|.
|f(cid:48)

P (x) =

1
2 |Σ| 1
p

2

(2π)

j

Here the graph structure is still encoded in the sparsity pattern of Ω = Σ−1  that is  xi⊥xj|x−i j if
and only if Ωij = 0 [18].
In our experiments we use the “Symmetric Power Transformation” [18]  that is 

fj(zj) = σj(

(cid:113)(cid:82) g2

g0(zj − µj)
0(t − µj)φ( t−µj

σj

) + µj 

)dt

7

Figure 1: The estimation results for Gaussian graphical models. left: The adjacent matrix of true
graph. center: the ROC curve of glasso. right: the ROC curve of score matching estimator (SME).

Figure 2: The estimated ROC curves of nonparanormal graphical models for glasso (left)  NPN
(center) and SME (right).

where g0(t) = sign(t)|t|α  to transform data. For comparison with graph lasso  we ﬁrst use a
truncation method to Gaussianize the data  and then apply graphical lasso to the transformed data.
See [18  31] for details. From ﬁgure 2  without knowing the underlying data distribution  the score
matching estimator outperforms glasso  and show similar results to nonparanormal when the sample
size is large.

6 Discussion

In this paper  we have proposed a new procedure for learning the structure of a nonparametric graph-
ical model. Our procedure is based on minimizing a penalized score matching objective  which can
be performed efﬁciently using existing group lasso solvers. Particularly appealing aspect of our
approach is that it does not require computing the normalization constant. Therefore  our proce-
dure can be applied to a very broad family of inﬁnite dimensional exponential families. We have
established that the procedure provably recovers the true underlying graphical structure with high-
probability under mild conditions. In the future  we plan to investigate more efﬁcient algorithms for
solving (10)  since it is often the case that ˆC is well structured and can be efﬁciently approximated.

Acknowledgments

The authors are grateful to the ﬁnancial support from National Institutes of Health R01GM0897532 
National Science Foundation CAREER award CCF-1149811 and IBM Corporation Faculty Re-
search Fund at the University of Chicago Booth School of Business. This work was completed in
part with resources provided by the University of Chicago Research Computing Center.

8

0.00.20.40.60.81.00.00.20.40.60.81.0Adjacent Matrixllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0GlassoFalsePositiveRateTruePositiveRatel20406080100llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0SMEFalsePositiveRateTruePositiveRatel20406080100llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0GlassoFalsePositiveRateTruePositiveRatel20406080100llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0NonParaNormalFalsePositiveRateTruePositiveRatel20406080100llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0SMEFalsePositiveRateTruePositiveRatel20406080100References
[1] R. Albert. Scale-free networks in cell biology. Journal of cell science  118(21):4947–4957  2005.
[2] C. Ambroise  J. Chiquet  C. Matias  et al. Inferring sparse gaussian graphical models with latent structure.

Electronic Journal of Statistics  3:205–238  2009.

[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society  pages

[4] S. Canu and A. Smola. Kernel methods and the exponential family. Neurocomputing  69(7):714–720 

337–404  1950.

2006.

[5] A. Defazio and T. S. Caetano. A convex formulation for learning scale-free networks via submodular

relaxation. In Advances in Neural Information Processing Systems  pages 1250–1258  2012.

[6] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.

[7] J. Friedman  T. Hastie  and R. Tibshirani. A note on the group lasso and a sparse group lasso. arXiv

Biostatistics  9(3):432–441  2008.

preprint arXiv:1001.0736  2010.

[8] K. Fukumizu  F. R. Bach  and A. Gretton. Statistical consistency of kernel canonical correlation analysis.

The Journal of Machine Learning Research  8:361–383  2007.

[9] S. Geman and C. Grafﬁgne. Markov random ﬁeld image models and their applications to computer vision.

In Proceedings of the International Congress of Mathematicians  volume 1  page 2  1986.

[10] A. Hyv¨arinen. Estimation of non-normalized statistical models by score matching. In Journal of Machine

Learning Research  pages 695–709  2005.

[11] A. Hyv¨arinen. Some extensions of score matching. Computational statistics & data analysis  51(5):2499–

2512  2007.

[12] Y. Jeon and Y. Lin. An effective method for high-dimensional log-density anova estimation  with appli-

cation to nonparametric graphical model building. Statistica Sinica  16(2):353  2006.

[13] R. Kindermann  J. L. Snell  et al. Markov random ﬁelds and their applications  volume 1. American

Mathematical Society Providence  RI  1980.

[14] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press  2009.
[15] Y. A. Kourmpetis  A. D. Van Dijk  M. C. Bink  R. C. van Ham  and C. J. ter Braak. Bayesian markov
random ﬁeld analysis for protein function prediction based on network data. PloS one  5(2):e9293  2010.
[16] J. Lafferty  A. McCallum  and F. C. Pereira. Conditional random ﬁelds: Probabilistic models for segment-

ing and labeling sequence data. 2001.

[17] S. Z. Li. Markov random ﬁeld modeling in Image Analysis. 2011.
[18] H. Liu  J. Lafferty  and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimen-

sional undirected graphs. The Journal of Machine Learning Research  10:2295–2328  2009.

[19] Q. Liu and A. T. Ihler. Learning scale free networks by reweighted l1 regularization. In International

Conference on Artiﬁcial Intelligence and Statistics  pages 40–48  2011.

[20] C. D. Manning and H. Sch¨utze. Foundations of statistical natural language processing. MIT press  1999.
[21] L. Meier  S. Van De Geer  and P. B¨uhlmann. The group lasso for logistic regression. Journal of the Royal

Statistical Society: Series B (Statistical Methodology)  70(1):53–71  2008.

[22] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the lasso. The

[23] P. Ravikumar  M. J. Wainwright  and J. Lafferty. High-dimensional graphical model selection using l1-

Annals of Statistics  pages 1436–1462  2006.

regularized logistic regression. 2008.

[24] P. Ravikumar  M. J. Wainwright  J. D. Lafferty  et al. High-dimensional ising model selection using

1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319  2010.
[25] R. T. Rockafellar. Convex analysis. Number 28. Princeton university press  1970.
[26] B. Sriperumbudur  K. Fukumizu  R. Kumar  A. Gretton  and A. Hyv¨arinen. Density estimation in inﬁnite

dimensional exponential families. arXiv preprint arXiv:1312.3516  2013.

[27] S. Sun  H. Wang  and J. Xu. Inferring block structure of graphical models in exponential families. In
Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics  pages
939–947  2015.

[28] Z. Wei and H. Li. A markov random ﬁeld model for network-based analysis of genomic data. Bioinfor-

matics  23(12):1537–1544  2007.

[29] E. Yang  G. Allen  Z. Liu  and P. K. Ravikumar. Graphical models via generalized linear models. In

Advances in Neural Information Processing Systems  pages 1358–1366  2012.

[30] M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika 

94(1):19–35  2007.

[31] T. Zhao  H. Liu  K. Roeder  J. Lafferty  and L. Wasserman. The huge package for high-dimensional

undirected graph estimation in r. The Journal of Machine Learning Research  13(1):1059–1062  2012.

9

,Qiang Cheng
Qiang Liu
Feng Chen
Alexander Ihler
Siqi Sun
Mladen Kolar
Jinbo Xu
Huishuai Zhang
Yingbin Liang