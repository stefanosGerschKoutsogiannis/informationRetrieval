2019,Bayesian Learning of Sum-Product Networks,Sum-product networks (SPNs) are flexible density estimators and have received significant attention due to their attractive inference properties. While parameter learning in SPNs is well developed  structure learning leaves something to be desired: Even though there is a plethora of SPN structure learners  most of them are somewhat ad-hoc and based on intuition rather than a clear learning principle. In this paper  we introduce a well-principled Bayesian framework for SPN structure learning. First  we decompose the problem into i) laying out a computational graph  and ii) learning the so-called scope function over the graph. The first is rather unproblematic and akin to neural network architecture validation. The second represents the effective structure of the SPN and needs to respect the usual structural constraints in SPN  i.e. completeness and decomposability. While representing and learning the scope function is somewhat involved in general  in this paper  we propose a natural parametrisation for an important and widely used special case of SPNs. These structural parameters are incorporated into a Bayesian model  such that simultaneous structure and parameter learning is cast into monolithic Bayesian posterior inference. In various experiments  our Bayesian SPNs often improve test likelihoods over greedy SPN learners. Further  since the Bayesian framework protects against overfitting  we can evaluate hyper-parameters directly on the Bayesian model score  waiving the need for a separate validation set  which is especially beneficial in low data regimes. Bayesian SPNs can be applied to heterogeneous domains and can easily be extended to nonparametric formulations. Moreover  our Bayesian approach is the first  which consistently and robustly learns SPN structures under missing data.,Bayesian Learning of Sum-Product Networks

Martin Trapp1 2  Robert Peharz3  Hong Ge3 

Franz Pernkopf1  Zoubin Ghahramani4 3
1Graz University of Technology  2OFAI 

3University of Cambridge  4Uber AI

martin.trapp@tugraz.at  rp587@cam.ac.uk  hg344@cam.ac.uk

pernkopf@tugraz.at  zoubin@eng.cam.ac.uk

Abstract

Sum-product networks (SPNs) are ﬂexible density estimators and have received
signiﬁcant attention due to their attractive inference properties. While parameter
learning in SPNs is well developed  structure learning leaves something to be
desired: Even though there is a plethora of SPN structure learners  most of them are
somewhat ad-hoc and based on intuition rather than a clear learning principle. In
this paper  we introduce a well-principled Bayesian framework for SPN structure
learning. First  we decompose the problem into i) laying out a computational
graph  and ii) learning the so-called scope function over the graph. The ﬁrst is
rather unproblematic and akin to neural network architecture validation. The second
represents the effective structure of the SPN and needs to respect the usual structural
constraints in SPN  i.e. completeness and decomposability. While representing
and learning the scope function is somewhat involved in general  in this paper 
we propose a natural parametrisation for an important and widely used special
case of SPNs. These structural parameters are incorporated into a Bayesian model 
such that simultaneous structure and parameter learning is cast into monolithic
Bayesian posterior inference. In various experiments  our Bayesian SPNs often
improve test likelihoods over greedy SPN learners. Further  since the Bayesian
framework protects against overﬁtting  we can evaluate hyper-parameters directly
on the Bayesian model score  waiving the need for a separate validation set  which
is especially beneﬁcial in low data regimes. Bayesian SPNs can be applied to
heterogeneous domains and can easily be extended to nonparametric formulations.
Moreover  our Bayesian approach is the ﬁrst  which consistently and robustly learns
SPN structures under missing data.

1

Introduction

Sum-product networks (SPNs) [29] are a prominent type of deep probabilistic model  as they are
a ﬂexible representation for high-dimensional distributions  yet allowing fast and exact inference.
Learning SPNs can be naturally organised into structure learning and parameter learning  following
the same dichotomy as in probabilistic graphical models (PGMs) [16]. Like in PGMs  state-of-
the-art SPN parameter learning covers a wide range of well-developed techniques. In particular 
various maximum likelihood approaches have been proposed using either gradient-based optimisation
[37  27  3  40] or expectation-maximisation (and related schemes) [25  29  46]. In addition  several
discriminative criteria  e.g. [9  14  39  32]  as well as Bayesian approaches to parameter learning 
e.g. [44  33  43]  have been developed.
Concerning structure learning  however  the situation is remarkably different. Although there is a
plethora of structure learning approaches for SPNs  most of them can be described as a heuristic. For
example  the most prominent structure learning scheme  LearnSPN [10]  derives an SPN structure

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

+ S1

× P1

× P2

+ S2

+ S3

+ S4

+ S5

× P3 × P4 × P5 × P6 × P7 × P8 × P9 × P10

L1

L2

L3

L4

L5

L6

L7

L8

L9

L10 L11 L12 L13 L14 L15 L16

p(X1  X2  X3  X4) =

+ S1

=====⇒

ψ

p(X1  . . .   X4) =

× P1

p(X1  X4)p(X2  X3) =

× P2

+ S2

+

p(X1  X4) =

+ S4

p(X2  X3) =

+ S5

× P3 × P4 ×

×

× P7 × P8 × P9 × P10

L1
{1  2}

L2
{3  4}

L3
{1  4}

L4
{2  3}

L10
{1  4}

L11
{4}

L12
{1}

L13
{2}

L14
{3}

L15
{2  3}

Figure 1: A computational graph G (left) and an SPN structure (right)  deﬁned by the scope function ψ 
discovered using posterior inference on an encoding of ψ. The SPN contains only a subset of the nodes in G as
some sub-trees are allocated with an empty scope (dotted) – evaluating to constant 1. Note that the graph G only
encodes the topological layout of nodes  while the “effective” SPN structure is encoded via ψ.

by recursively clustering the data instances (yielding sum nodes) and partitioning data dimensions
(yielding product nodes). Each of these steps can be understood as some local structure improvement 
and as an attempt to optimise a local criterion. While LearnSPN is an intuitive scheme and elegantly
maps the structural SPN semantics onto an algorithmic procedure  the fact that the global goal of
structure learning is not declared is unsatisfying. This principal shortcoming of LearnSPN is shared
by its many variants such as online LearnSPN [18]  ID-SPN [35]  LearnSPN-b [42]  mixed SPNs
[22]  and automatic Bayesian density analysis (ABDA) [43]. Also other approaches lack a sound
learning principle  such as [5  1] which derive SPN structures from k-means and SVD clustering 
respectively  [23] which grows SPNs bottom up using a heuristic based on the information bottleneck 
[6] which uses a heuristic structure exploration  or [13] which use a variant of hard EM to decide
when to enlarge or shrink an SPN structure.
All of the approaches mentioned above fall short of posing some fundamental questions: What is
a good SPN structure? or What is a good principle to derive an SPN structure? This situation is
somewhat surprising since the literature on PGMs offers a rich set of learning principles: In PGMs 
the main strategy is to optimise a structure score such as minimum-description-length (MDL) [38] 
Bayesian information criterion (BIC) [16] or the Bayes-Dirichlet (BD) score [4  11]. Moreover  in
[7] an approximate MCMC sampler was proposed for full Bayesian structure learning.
In this paper  we propose a well-principled Bayesian approach to SPN learning  by simultaneously
performing inference over both structure and parameters. We ﬁrst decompose the structure learning
problem into two steps  namely i) proposing a computational graph  laying out the arrangement of
sums  products and leaf distributions  and ii) learning the so-called scope-function  which assigns
to each node its scope.1 The ﬁrst step is straightforward  computational graphs have only very few
requirements  while the second step  learning the scope function  is more involved in full generality.
Therefore  we propose a parametrisation of the scope function for a widely used special case of SPNs 
namely so-called tree-shaped region graphs [5  27]. This restriction allows us to encode the scope
function via categorical variables elegantly. Now Bayesian learning becomes conceptually simple:
We equip all latent variables and the leaves with appropriate priors and perform monolithic Bayesian
inference  implemented via Gibbs-updates. Figure 1 illustrates our approach of disentangling structure
learning and performing Bayesian inference on an encoding of the scope function.
In summary  our main contributions in this paper are:

• We propose a novel and well-principled approach to SPN structure learning  by decomposing

the problem into ﬁnding a computational graph and learning a scope-function.

• To learn the scope function  we propose a natural parametrisation for an important sub-type
of SPNs  which allows us to formulate a joint Bayesian framework simultaneously over
structure and parameters.

• Bayesian SPNs are protected against overﬁtting  waiving the necessity of a separate vali-
dation set  which is beneﬁcial for low data regimes. Furthermore  they naturally deal with
missing data and are the ﬁrst – to the best of our knowledge – which consistently and
robustly learn SPN structures under missing data. Bayesian SPNs can easily be extended to
nonparametric formulations  supporting growing data domains.

1The scope of a node is a subset of random variables  the node is responsible for and needs to fulﬁl the

so-called completeness and decomposability conditions  see Section 3.

2

2 Related Work

The majority of structure learning approaches for SPNs  such as LearnSPN [10] and its variants 
e.g. [18  35  42  22] or other approaches  such as [5  23  1  6  13]  heuristically generate a structure
by optimising some local criterion. However  none of these approaches deﬁnes an overall goal of
structure learning  and all of them lack a sound objective to derive SPN structures. Bayesian SPNs 
on the other hand  follow a well-principled approach using posterior inference over structure and
parameters.
The most notable attempts for principled structure learning of SPNs include ABDA [43] and the
existing nonparametric variants of SPNs [17  41]. Even though the Bayesian treatment in ABDA 
i.e. posterior inference over the parameters of the latent variable models located at the leaf nodes  can
be understood as some kind of local Bayesian structure learning  the approach heavily relies on a
heuristically predeﬁned SPN structure. In fact  Bayesian inference in ABDA only allows adaptations
of the parameters and the latent variables at the leaves and does not infer the general structure of the
SPN. Therefore  ABDA can be understood as a particular case of Bayesian SPNs in which the overall
structure is kept ﬁxed  and inference is only performed over the latent variables at the leaves and the
parameters of the SPN.
On the other hand  nonparametric formulations for SPNs  i.e. [17  41]  use Bayesian nonparametric
priors for both structure and parameters. However  the existing approaches do not use an efﬁcient
representation  e.g. [41] uses uniform distributions over all possible partitions of the scope at each
product node  making posterior inference infeasible for real-world applications. Bayesian SPNs
and nonparametric extension of Bayesian SPNs  on the other hand  can be applied to real-world
applications  as shown in Section 6.
Besides structure learning in SPNs  there are various approaches for other tractable probabilistic
models (which also allow exact and efﬁcient inference)  such as probabilistic sentential decision
diagrams (PSDDs) [15] and Cutset networks (CNets) [31]. Most notably  the work by [19] introduces
a greedy approach to optimises a heuristically deﬁned global objective for structure learning in
PSDDs. However  similar to structure learning of selective SPNs [24] (which are a restricted sub-
type of SPNs)  the global objective of the optimisation is not well-principled. Existing approaches
for CNets  on the other hand  mainly use heuristics to deﬁne the structure. In [21] structures are
constructed randomly  while [30] compiles a learned latent variable model  such as an SPNs  into a
CNet. However  all these approaches lack a sound objective which deﬁnes a good structure.

3 Background
Let X = {X1  . . .   XD} be a set of D random variables (RVs)  for which N i.i.d. samples are
available. Let xn d be the nth observation for the dth dimension and xn := (xn 1  . . .   xn D). Our
goal is to estimate the distribution of X using a sum-product network (SPN). In the following we
review SPNs  but use a more general deﬁnition than usual  in order to facilitate our discussion below.
In this paper  we deﬁne an SPN S as a 4-tuple S = (G  ψ  w  θ)  where G is a computational graph 
ψ is a scope-function  w is a set of sum-weights  and θ is a set of leaf parameters. In the following 
we explain these terms in more detail.
Deﬁnition 1 (Computational graph). The computational graph G is a connected directed acyclic
graph  containing three types of nodes: sums (S)  products (P) and leaves (L). A node in G has no
children if and only if it is of type L. When we do not discriminate between node types  we use N for a
generic node. S  P  L  and N denote the collections of all S  all P  all L  and all N in G  respectively.
The set of children of node N is denoted as ch(N). In this paper  we require that G has only a single
root (node without parent).
Deﬁnition 2 (Scope function). The scope function is a function ψ : N (cid:55)→ 2X  assigning each node in
G a sub-set of X (2X denotes the power set of X). It has the following properties:

1. If N is the root node  then ψ(N) = X.

2. If N is a sum or product  then ψ(N) =(cid:83)

N(cid:48)∈ch(N) ψ(N(cid:48)).

3. For each S ∈ S we have ∀N  N(cid:48) ∈ ch(S) : ψ(N) = ψ(N(cid:48)) (completeness).
4. For each P ∈ P we have ∀N  N(cid:48) ∈ ch(P) : ψ(N) ∩ ψ(N(cid:48)) = ∅ (decomposability).

3

Each node N in G represents a distribution over the random variables ψ(N)  as described in the
following. Each leaf L computes a pre-speciﬁed distribution over its scope ψ(L) (for ψ(L) = ∅  we
set L ≡ 1). We assume that L is parametrised by θL  and that θL represents a distribution for any
possible choice of ψ(L). In the most naive setting  we would maintain a separate parameter set for
each of the 2D possible choices for ψ(L)  but this would quickly become intractable. In this paper 
we simply assume that θL contains D parameters θL 1  . . .   θL D over single-dimensional distributions
(e.g. Gaussian  Bernoulli  etc)  and that for a given ψ(L)  the represented distribution factorises:
Xi∈ψ(L) p(Xi | θL i). However  more elaborate schemes are possible. Note that our deﬁnition
of leaves is quite distinct from prior art: previously  leaves were deﬁned to be distributions over a
ﬁxed scope; our leaves deﬁne at all times distributions over all 2D possible scopes. The set θ = {θL}
denotes the collection of parameters for all leaf nodes. A sum node S computes a weighted sum
N∈ch(S) wS N N. Each weight wS N is non-negative  and can w.l.o.g. [26  45] be assumed to
N∈ch(S) wS N = 1. We denote the set of all sum-weights for S as wS
and use w to denote the set of all sum-weights in the SPN. A product node P simply computes

L =(cid:81)
S =(cid:80)
be normalised: wS N ≥ 0 (cid:80)
P =(cid:81)

N∈ch(P) N.

The two conditions we require for ψ – completeness and decomposability – ensure that each node N
is a probability distribution over ψ(N). The distribution represented by S is deﬁned as the distribution
of the root node in G and denoted as S(x). Furthermore  completeness and decomposability are
essential to render many inference scenarios tractable in SPNs. In particular  arbitrary marginalisation
tasks reduce to marginalisation at the leaves  i.e. simplify to several marginalisation tasks over
(small) subsets of X  followed by an evaluation of the internal part (sum and products) in a simple
feed-forward pass [26]. Thus  exact marginalisation can be computed in linear time in size of the
SPN (assuming constant time marginalisation at the leaves). Conditioning can be tackled similarly.
Note that marginalisation and conditioning are key inference routines in probabilistic reasoning so
that SPNs are generally referred to as tractable probabilistic models.

4 Bayesian Sum-Product Networks

All previous work deﬁne the structure of an SPN in an entangled way  i.e. the scope is seen as an
inherent property of the nodes in the graph. In this paper  however  we propose to decouple the
aspects of an SPN structure by searching over G and nested learning of ψ. Note that G has few
structural requirements  and can be validated like a neural network structure. Consequently  we ﬁx G
in the following discussion and cross-validate it in our experiments. Learning ψ is challenging  as ψ
has non-trivial structure due to the completeness and decomposability conditions. In the following 
we develop a parametrisation of ψ and incorporate it into a Bayesian framework. We ﬁrst revisit
Bayesian parameter learning in SPNs using a ﬁxed ψ.

4.1 Learning Parameters w  θ – Fixing Scope Function ψ

The key insight for Bayesian parameter learning [44  33  43] is that sum nodes can be interpreted
as latent variables  clustering data instances [29  45  25]. Formally  consider any sum node S and
assume that it has KS children. For each data instance xn and each S  we introduce a latent variable
ZS n with KS states and categorical distribution given by the weights wS of S. Intuitively  the sum
node S represents a latent clustering of data instances over its children. Let Zn = {ZS n}S∈S be the
collection of all ZS n. To establish the interpretation of sum nodes as latent variables  we introduce
the notion of induced tree [44]. We omit sub-script n when a distinction between data instances is
not necessary.
Deﬁnition 3 (Induced tree [44]). Let an S = (G  ψ  w  θ) be given. Consider a sub-graph T of G
obtained as follows: i) for each sum S ∈ G  delete all but one outgoing edge and ii) delete all nodes
and edges which are now unreachable from the root. Any such T is called an induced tree of G
(sometimes also denoted as induced tree of S). The SPN distribution can always be written as the
mixture

(cid:88)

(cid:89)

S(x) =

T ∼S

(S N)∈T

wS N

L(xL) 

(1)

where the sum runs over all possible induced trees in S  and L(xL) denotes the evaluation of L on the
restriction of x to ψ(L).

(cid:89)

L∈T

4

zS n

α

wS
∀S ∈ S

xn d
n∈ 1 : N

γd

yP d

vP
∀P ∈ P

β

θL d
∀L ∈ L
d∈ 1 : D

Figure 2: Plate notation of our generative model for Bayesian structure and parameter learning.

We deﬁne a function T (z) which assigns to each value z of Z the induced tree determined by z 
i.e. where z indicates the kept sum edges in Deﬁnition 3. Note that the function T (z) is surjective  but
not injective  and thus  T (z) is not invertible. However  it is “partially” invertible  in the following
sense: Note that any T splits the set of all sum nodes S into two sets  namely the set of sum nodes
ST which are contained in T   and the set of sum nodes ¯ST which are not. For any T   we can identify
(invert) the state zS for any S ∈ ST   as it corresponds to the unique child of S in T . On the other
hand  the state of any S ∈ ¯ST is arbitrary. In short  given an induced tree T   we can perfectly retrieve
the states of the (latent variables of) sum nodes in T   while the states of the other latent variables are
arbitrary.

Now  deﬁne the conditional distribution p(x| z) =(cid:81)
(cid:89)

(cid:88)

(cid:89)

(cid:89)

S∈G wS zS 
where wS zS is the sum-weight indicated by zS. When marginalising Z from the joint p(x  z) =
p(x| z) p(z)  we yield

L∈T (z) L(xL) and prior p(z) =(cid:81)

(cid:89)

wS zS

L(xL) =

S∈S

z

L∈T (z)

z∈T −1(T )

S∈S

L∈T (z)

(cid:88)
(cid:88)

T

(cid:88)
(cid:89)

wS zS

(cid:89)

L∈T

L(xL)

(cid:88)
(cid:124)

¯z

(cid:89)
(cid:123)(cid:122)

S∈¯ST

=1

(2)

= S(x) 

(3)


(cid:125)

wS ¯zS

=

wS N

L(xL)

T

(S N)∈T

establishing the SPN distribution (1) as latent variable model  with Z marginalised out. In (2)  we
split the sum over all z into the double sum over all induced trees T   and all z ∈ T −1(T )  where
T −1(T ) is the pre-image of T under T   i.e. the set of all z for which T (z) = T . As discussed above 
the set T −1(T ) is made up by a unique z-assignment for each S ∈ ST   corresponding to the unique
sum-edge (S  N) ∈ T   and all possible assignments for S ∈ ¯ST   leading to (3).
It is now conceptually straightforward to extend the model to a Bayesian setting  by equipping the
sum-weights w and leaf-parameters θ with suitable priors. In this paper  we assume Dirichlet priors
for sum-weights and some parametric form L(·| θL) for each leaf  with conjugate prior over θL 
leading to the following generative model:
wS | α ∼ Dir(wS | α) ∀S  
θL | γ ∼ p(θL | γ) ∀L  

zS n | wS ∼ Cat(zS n | wS) ∀S∀n 
L(xL n | θL) ∀n.

xn | zn  θ ∼ (cid:89)

(4)

L∈T (zn)

We now extend the model to also comprise the SPN’s “effective” structure  the scope function ψ.

Jointly Learning w  θ and ψ

4.2
Given a computational graph G  we wish to learn ψ  additionally to the SPN’s parameters w and θ 
and adopt it in our generative model (4). In general graphs G  representing ψ in an amenable form
is rather involved. Therefore  in this paper  we restrict to the class of SPNs whose computational G
follows a tree-shaped region graph  which leads to a natural encoding of ψ. Region graphs can be
understood as a “vectorised” representation of SPNs  and have been used in several SPN learners
e.g. [5  23  27].

5

either a region with children or a partition  then ψ(Q) =(cid:83)

Deﬁnition 4 (Region graph). Given a set of random variables X  a region graph is a tuple (R  ψ)
where R is a connected directed acyclic graph containing two types of nodes: regions (R) and
partitions (P ). R is bipartite w.r.t. to these two types of nodes  i.e. children of R are only of type
P and vice versa. R has a single root (node with no parents) of type R  and all leaves are also
of type R. Let R be the set of all R and P be the set of all P . The scope function is a function
ψ : R ∪ P (cid:55)→ 2X  with the following properties: 1) If R ∈ R is the root  then ψ(R) = X. 2) If Q is
Q(cid:48)∈ch(Q) ψ(Q(cid:48)). 3) For all P ∈ P we
have ∀R  R(cid:48) ∈ ch(P ) : ψ(R)∩ ψ(R(cid:48)) = ∅. 4) For all R ∈ R we have ∀P ∈ ch(R) : ψ(R) = ψ(P ).
Note that  we generalised previous notions of a region graph [5  23  27]  also decoupling its graphical
structure R and the scope function ψ (we are deliberately overloading symbol ψ here). Given a
region graph (R  ψ)  we can easily construct an SPN structure (G  ψ) as follows. To construct the
SPN graph G  we introduce a single sum node for the root region in R; this sum node will be the
output of the SPN. For each leaf region R  we introduce I SPN leaves. For each other region R 
which is neither root nor leaf  we introduce J sum nodes. Both I and J are hyper-parameters of
the model. For each partition P we introduce all possible cross-products of nodes from P ’s child
regions. More precisely  let ch(P ) = {R1  . . . RK}. Let Nk be the assigned sets of nodes in each
child region Rk. Now  we construct all possible cross-products P = N1 ×···× NK  where Nk ∈ Nk 
for 1 ≤ k ≤ K. Each of these cross-products is connected as children of each sum node in each
parent region of P . We refer to the supplement for a detailed description  including the algorithm to
construct region-graphs used in this paper.
The scope function ψ of the SPN is inherited from the ψ of the region graph: any SPN node introduced
for a region (partition) gets the same scope as the region (partition) itself. It is easy to check that  if
the SPN’s G follows R using above construction  any proper scope function according to Deﬁnition 4
corresponds to a proper scope function according to Deﬁnition 2.
In this paper  we consider SPN structures (G  ψ) following a tree-shaped region graph (R  ψ)  i.e. each
node in R has at most one parent. Note that G is in general not tree-shaped in this case  unless
I = J = 1. Further note  that this sub-class of SPNs is still very expressive  and that many SPN
learners  e.g. [10  27]  also restrict to it.
When the SPN follows a tree-shaped region graph  the scope function can be encoded as follows. Let
P be any partition and R1  . . .   R|ch(P )| be its children. For each data dimension d  we introduce a
discrete latent variable YP d with 1  . . .  |ch(P )| states. Intuitively  the latent variable YP d represents
a decision to assign dimension d to a particular child  given that all partitions “above” have decided to
assign d onto the path leading to P (this path is unique since R is a tree). More formally  we deﬁne:
Deﬁnition 5 (Induced scope function). Let R be a tree-shaped region graph structure  let YP d be
deﬁned as above  let Y = {YP d}P∈R d∈{1...D}  and let y be any assignment for Y. Let Q denote
any node in R  let Π be the unique path from the root to Q (exclusive Q). The scope function induced
by y is deﬁned as:

(cid:40)

(cid:41)
1[RyP d ∈ Π] = 1

(cid:12)(cid:12)(cid:12)(cid:12) (cid:89)

P∈Π

ψy(Q) :=

Xd

 

(5)

i.e. ψy(Q) contains Xd if for each partition in Π also the child indicated by yP d is in Π.
It is easy to check that for any tree-shaped R and any y  the induced scope function ψy is a proper
scope function according to Deﬁnition 4. Conversely  for any proper scope function according to
Deﬁnition 4  there exists a y such that ψy ≡ ψ.2
We can now incorporate Y in our model. Therefore  we assume Dirichlet priors for each YP d and
extend the generative model (4) as follows:
wS | α ∼ Dir(wS | α) ∀S  
vP | β ∼ Dir(vP | β) ∀P  
θL | γ ∼ p(θL | γ) ∀L 

zS n | wS ∼ Cat(zS n | wS) ∀S∀n 
yP d | vP ∼ Cat(vP d | vP ) ∀P ∀d 
L(xy n | θL) ∀n.

xn | zn  y  θ ∼ (cid:89)

(6)

L∈T (zn)

2Note that the relation between y and ψy is similar to the relation between z and T   i.e. each ψy corresponds
in general to many y’s. Also note  the encoding of ψ for general region graphs is more envolved  since the path
to each Q is not unique anymore  requiring consistency among y.

6

Here  the notation xy n denotes the evaluation of L on the scope induced by y. Figure 2 illustrates our
generative model in plate notation  in which directed edges indicate dependencies between variables.
Furthermore  our Bayesian formulation naturally allows for various nonparametric formulations of
SPNs. In particular  one can use the stick-breaking construction [36] of a Dirichlet process mixture
model with SPNs as mixture components. We illustrate this approach in the experiments.3

5 Sampling-based Inference
Let X = {xn}N
n=1 be a training set of N observations xn  we aim to draw posterior samples from
our generative model given X . For this purpose  we perform Gibbs sampling alternating between i)
updating parameters w  θ (ﬁxed y)  and ii) updating y (ﬁxed w  θ).

Updating Parameters w  θ (ﬁxed y) We follow the same procedure as in [43]  i.e. in order to
sample w and θ  we ﬁrst sample assignments zn for all the sum latent variables Zn in the SPN 
and subsequently sample new w and θ. For a given set of parameters (w and θ)  each zn can be
drawn independently and follows standard SPN ancestral sampling. The latent variables not visited
during ancestral sampling  are drawn from the prior. After sampling all zn  the sum-weights are
sampled from the posterior distributions of a Dirichlet  i.e. Dir(α + cS 1  . . .   α + cS KS ) where
1[zS n = k] denotes the number of observations assigned to child k. The parameters at

cS k =(cid:80)N

n=1

leaf nodes can be updated similarly; see [43] for further details.

Updating the Structure y  (ﬁxed w  θ) We use a collapsed Gibbs sampler to sample all yP d
assignments. For this purpose  we marginalise out v (c.f. the dependency structure in Figure 2)
and sample yP d from the respective conditional. Therefore  let yP denote the set of all dimension
assignments at partition P and let yP (cid:54)d denote the exclusion of d from yP . Further  let yP\P d denote
the assignments of dimension d on all partitions except partition P   then the conditional probability
of assigning dimension d to child k under P is:

j=1

β+mP k

β+mP k

  where mP k =(cid:80)

p(yP d = k | yP (cid:54)d  yP\P d X   z  θ  β) = p(yP d = k | yP (cid:54)d  β)p(X | yP d = k  yP\P d  z  θ) .
(7)
Note that the conditional prior in Equation 7 follows standard derivations  i.e. p(yP d = k | yP (cid:54)d  β) =
(cid:80)|ch(P )|
1[yP d = k] are component counts. The second term
in Equation 7 is the product over marginal likelihood terms of each product node in P . Intuitively 
values for yP d are more likely if other dimensions are assigned to the same child (rich-get-richer)
and if the product of marginal likelihoods of child k has low variance in d.
Given a set of T posterior samples  we can compute predictions for an unseen datum x∗ using an
approximation of the posterior predictive distribution  i.e.

d∈ψ(P )\d

T(cid:88)

t=1

p(x∗ |X ) ≈ 1
T

S(x∗ |G  ψy(t)  w(t)  θ(t))  

(8)

where S(x∗ |G  ψy(t)  w(t)  θ(t)) denotes the SPN of the tth posterior sample with t = 1  . . .   T .
Note that we can represent the resulting distribution as a single SPN with T children (sub-SPNs).

6 Experiments

We assessed the performance of our approach on discrete [10] and heterogeneous data [43] as well
as on three datasets with missing values. We constructed G using the algorithm described in the
supplement and used a grid search over the parameters of the graph. Further  we used 5 · 103 burn-in
steps and estimated the predictive distribution using 104 samples from the posterior. Since the
Bayesian framework is protected against overﬁtting  we combined training and validation sets and
followed classical Bayesian model selection [34]  i.e. using the Bayesian model evidence. Note that
within the validation loop  the computational graph remains ﬁxed. We list details on the selected

3See https://github.com/trappmartin/BayesianSumProductNetworks for and implementation of

Bayesian SPNs in form of a Julia package accompanied by codes and datasets used for the experiments.

7

Table 1: Average test log-likelihoods on discrete datasets using SOTA  Bayesian SPNs (ours) and
inﬁnite mixtures of SPNs (ours∞). Signiﬁcant differences are underlined. Overall best result is in
bold. In addition we list the best-to-date (BTD) results obtained using SPNs  PSDDs or CNets.
Dataset
NLTCS
MSNBC
KDD
Plants
Audio
Jester
Netﬂix
Accidents
Retail
Pumsb-star
DNA
Kosarak
MSWeb
Book
EachMovie
WebKB
Reuters-52
20 Newsgrp
BBC
AD

ours∞
BTD
ours
ID-SPN
CCCP
−6.00
−5.97
−6.02
−6.03
−6.02
−6.03
−6.03
−6.06
−6.05
−6.04
−2.12
−2.11
−2.13
−2.13
−2.13
−12.87 −12.54
−11.84
−12.68
−12.94
−39.79 −39.77
−39.39
−40.02
−39.79
−52.86 −52.42
−51.29
−52.86
−52.88
−56.36 −56.31
−55.71
−56.78
−56.80
−27.70 −26.98
−34.10
−26.98
−33.89
−10.83 −10.83
−10.72
−10.92
−10.85
−24.23 −22.41
−22.41
−31.34
−31.96
−84.92 −81.21
−81.07
−92.84
−92.95
−10.88 −10.60
−10.52
−10.77
−10.74
−9.73
−9.62
−9.97
−9.88
−9.89
−34.14 −34.13
−34.14
−35.01
−34.34
−51.66 −50.94
−50.34
−52.56
−51.51
−157.49 −151.84 −156.02 −157.33 −149.20
−84.63 −83.35
−84.31
−84.44
−81.87
−153.21 −151.47 −151.99 −151.95 −151.02
−248.93 −249.70 −254.69 −229.21
−27.20 −19.05
−14.00

−63.80

LearnSPN RAT-SPN
−6.01
−6.04
−2.13
−13.44
−39.96
−52.97
−56.85
−35.49
−10.91
−32.53
−97.23
−10.89
−10.12
−34.68
−53.63
−157.53
−87.37
−152.06
−252.14 −248.60
−48.47

−6.11
−6.11
−2.18
−12.98
−40.50
−53.48
−57.33
−30.04
−11.04
−24.78
−82.52
−10.99
−10.25
−35.89
−52.49
−158.20
−85.07
−155.93
−250.69
−19.73

−63.80

parameters and the runtime for each dataset in the supplement  c.f. Table 3. For posterior inference in
inﬁnite mixtures of SPNs  we used the distributed slice sampler [8].
Table 1 lists the test log-likelihood scores of state-of-the-art (SOTA) structure learners  i.e. LearnSPN
[10]  LearnSPN with parameter optimisation (CCCP) [46] and ID-SPN [35]  random region-graphs
(RAT-SPN) [27] and the results obtained using Bayesian SPNs (ours) and inﬁnite mixtures of Bayesian
SPN (ours∞) on discrete datasets. In addition we list the best-to-date (BTD) results  collected based
on the most recent works on structure learning for SPNs [12]  PSDDs [19] and CNets[21  30]. Note
that the BTD results are often by large ensembles over structures. Signiﬁcant differences to the best
SOTA approach under the Mann-Whitney-U-Test [20] with p < 0.01 are underlined. We refer to the
supplement for an extended results table and further details on the signiﬁcance tests. We see that
Bayesian SPNs and inﬁnite mixtures generally improve over LearnSPN and RAT-SPN. Further  in
many cases  we observe an improvement over LearnSPN with additional parameter learning and often
obtain results comparable to ID-SPN or sometimes outperforms BTD results. Note that ID-SPN uses
a more expressive SPN formulation with Markov networks as leaves and also uses a sophisticated
learning algorithm.
Additionally  we conducted experiments on heterogeneous data  see: [22  43]  and compared against
mixed SPNs (MSPN) [22] and ABDA [43]. We used mixtures over likelihood models as leaves 
similar to Vergari et al. [43]  and performed inference over the structure  parameters and likelihood
models. Further details can be found in the supplement. Table 2 lists the test log-likelihood scores
of all approaches  indicating that our approaches perform comparably to structure learners tailored
to heterogeneous datasets and sometimes outperform SOTA. 4 Interestingly  we obtain  with a large
margin  better test scores for Autism which might indicate that existing approaches overﬁt in this
case while our formulation naturally penalises complex models.
We compared the test log-likelihood of LearnSPN  ID-SPN and Bayesian SPNs against an increasing
number of observations having 50% dimensions missing completely at random [28]. We evaluated
LearnSPN and ID-SPN by i) removing all observations with missing values and ii) using K-nearest
neighbour imputation [2] (denoted with an asterisk). Note that we selected k-NN imputation because
it arguably provides a stronger baseline than simple mean imputation (while being computationally

4Note that we did not apply a signiﬁcance test as implementations of existing approaches are not available.

8

Table 2: Average test log-likelihoods on heterogeneous datasets using SOTA  Bayesian SPN (ours)
and inﬁnite mixtures of SPNs (ours∞). Overall best result is indicated in bold.
ours∞
MSPN
ABDA
ours
Dataset
Abalone
9.73
2.22
3.92
3.99
−5.91 −4.62
−44.07
−4.68
Adult
−36.14 −16.44
−21.51 −21.99
Australian
−27.93 −0.47
−39.20
−1.16
Autism
−25.48 −25.02 −25.76
−28.01
Breast
−12.30 −11.54 −11.76
−13.01
Chess
−36.26 −12.82
−19.38 −19.62
Crx
−24.98 −23.95 −24.33
Dermatology −27.71
−31.22 −17.48
−21.21 −21.06
Diabetes
−26.05 −25.83
−26.76 −26.63
German
−30.18 −28.73
−29.51
−29.9
Student
−0.13
−8.65
−8.62
−10.12
Wine

−52

−54

−56

d
o
o
h
i
l
e
k
i
l
-
g
o
l

t
s
e
t

−155

−160

−165

LearnSPN
ID-SPN
BSPN

LearnSPN(cid:63)
ID-SPN(cid:63)

−260

−280

−300

20

40

60

80

20

40

60

80

20

40

60

80

% obs. with missing values

% obs. with missing values

% obs. with missing values

(a) EachMovie (D: 500  N: 5526)

(b) WebKB (D: 839  N: 3361)

(c) BBC (D: 1058  N: 1895)

Figure 3: Performance under missing values for discrete datasets with increasing dimensionality (D).
Results for LearnSPN are shown in dashed lines  results for ID-SPN in dotted lines and our approach
is indicated using solid lines. Star ((cid:63)) indicates k-NN imputation while (◦) means no imputation.

more demanding). All methods have been trained using the full training set  i.e. training and validation
set combined  and were evaluated using default parameters to ensure a fair comparison across methods
and levels of missing values. See supplement Section A.3 for further details. Figure 3 shows that our
formulation is consistently robust against missing values while SOTA approaches often suffer from
missing values  sometimes even if additional imputation is used.

7 Conclusion

Structure learning is an important topic in SPNs  and many promising directions have been proposed
in recent years. However  most of these approaches are based on intuition and refrain from declaring
an explicit and global principle to structure learning. In this paper  our primary motivation is to
change this practice. To this end  we phrase structure (and joint parameter) learning as Bayesian
inference in a latent variable model. Our experiments show that this principled approach competes
well with prior art and that we gain several beneﬁts  such as automatic protection against overﬁtting 
robustness under missing data and a natural extension to nonparametric formulations.
A critical insight for our approach is to decompose structure learning into two steps  namely con-
structing a computational graph and separately learning the SPN’s scope function – determining the
“effective” structure of the SPN. We believe that this novel approach will be stimulating for future
work. For example  while we used Bayesian inference over the scope function  it could also be
optimised  e.g. with gradient-based techniques. Further  more sophisticated approaches to identify the
computational graph  e.g. using AutoML techniques or neural structural search (NAS) [47]  could be
fruitful directions. The Bayesian framework presented in this paper allows several natural extensions 
such as parameterisations of the scope-function using hierarchical priors or using variational inference
for large-scale approximate Bayesian inference  and relaxing the necessity of a given computational
graph  by incorporating nonparametric priors in all stages of the model formalism.

9

Acknowledgements

This work was partially funded by the Austrian Science Fund (FWF): I2706-N31 and has received
funding from the European Union’s Horizon 2020 research and innovation programme under the
Marie Skłodowska-Curie Grant Agreement No. 797223 — HYBSPN.

References
[1] T. Adel  D. Balduzzi  and A. Ghodsi. Learning the structure of sum-product networks via an SVD-based

algorithm. In Proceedings of UAI  2015.

[2] L. Beretta and A. Santaniello. Nearest neighbor imputation algorithms: a critical evaluation. BMC medical

informatics and decision making  16(3):74  2016.

[3] C. J. Butz  J. S. Oliveira  A. E. dos Santos  and A. L. Teixeira. Deep convolutional sum-product networks.

In Proceedings of AAAI  2019.

[4] G. F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data.

Machine learning  9(4):309–347  1992.

[5] A. W. Dennis and D. Ventura. Learning the architecture of sum-product networks using clustering on

variables. In Proceedings of NeurIPS  pages 2042–2050  2012.

[6] A. W. Dennis and D. Ventura. Greedy structure search for sum-product networks. In Proceedings of IJCAI 

pages 932–938  2015.

[7] N. Friedman and D. Koller. Being Bayesian about network structure. a Bayesian approach to structure

discovery in Bayesian networks. Machine learning  50(1-2):95–125  2003.

[8] H. Ge  Y. Chen  M. Wan  and Z. Ghahramani. Distributed inference for Dirichlet process mixture models.

In Proceedings of ICML  pages 2276–2284  2015.

[9] R. Gens and P. Domingos. Discriminative learning of sum-product networks. In Proceedings of NeurIPS 

pages 3248–3256  2012.

[10] R. Gens and P. Domingos. Learning the structure of sum-product networks. Proceedings of ICML  pages

873–880  2013.

[11] D. Heckerman  D. Geiger  and D. M. Chickering. Learning Bayesian networks: The combination of

knowledge and statistical data. Machine learning  20(3):197–243  1995.

[12] P. Jaini  A. Ghose  and P. Poupart. Prometheus: Directly learning acyclic directed graph structures for

sum-product networks. In Proceedings of PGM  pages 181–192  2018.

[13] A. Kalra  A. Rashwan  W.-S. Hsu  P. Poupart  P. Doshi  and G. Trimponias. Online structure learning for
feed-forward and recurrent sum-product networks. In Proceedings of NeurIPS  pages 6944–6954  2018.

[14] H. Kang  C. D. Yoo  and Y. Na. Maximum margin learning of t-SPNs for cell classiﬁcation with ﬁltered

input. IEEE Journal of Selected Topics in Signal Processing  10(1):130–139  2016.

[15] D. Kisa  G. Van den Broeck  A. Choi  and A. Darwiche. Probabilistic sentential decision diagrams. In

Proceedings of KR  2014.

[16] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press  2009.

[17] S. Lee  C. Watkins  and B. Zhang. Non-parametric bayesian sum-product networks. In Proceedings of

LTPM workshop at ICML  2014.

[18] S.-W. Lee  M.-O. Heo  and B.-T. Zhang. Online incremental structure learning of sum-product networks.

In Proceedings of NeurIPS  pages 220–227  2013.

[19] Y. Liang  J. Bekker  and G. Van den Broeck. Learning the structure of probabilistic sentential decision

diagrams. In Proceedings of UAI  2017.

[20] H. B. Mann and D. R. Whitney. On a test of whether one of two random variables is stochastically larger

than the other. The annals of mathematical statistics  pages 50–60  1947.

[21] N. Di Mauro  A. Vergari  T. M. Altomare Basile  and F. Esposito. Fast and accurate density estimation

with extremely randomized cutset networks. In Proceedings of ECML/PKDD  pages 203–219  2017.

10

[22] A. Molina  A. Vergari  N. Di Mauro  S. Natarajan  F. Esposito  and K. Kersting. Mixed sum-product

networks: A deep architecture for hybrid domains. In Proceedings of AAAI  2018.

[23] R. Peharz  B. C. Geiger  and F. Pernkopf. Greedy part-wise learning of sum-product networks.

Proceedings of ECML/PKDD  pages 612–627  2013.

In

[24] R. Peharz  R. Gens  and P. Domingos. Learning selective sum-product networks. In Proceedings of LTPM

workshop at ICML  2014.

[25] R. Peharz  R. Gens  F. Pernkopf  and P. Domingos. On the latent variable interpretation in sum-product

networks. IEEE Transactions on Pattern Analysis and Machine Intelligence  39(10):2030–2044  2017.

[26] R. Peharz  S. Tschiatschek  F. Pernkopf  and P. Domingos. On theoretical properties of sum-product

networks. In Proceedings of AISTATS  2015.

[27] R. Peharz  A. Vergari  K. Stelzner  A. Molina  X. Shao  M. Trapp  K. Kersting  and Z. Ghahramani. Random
sum-product networks: A simple but effective approach to probabilistic deep learning. In Proceedings of
UAI  2019.

[28] D. F. Polit and C. T. Beck. Nursing research: Generating and assessing evidence for nursing practice.

Lippincott Williams & Wilkins  2008.

[29] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In Proceedings of UAI  pages

337–346  2011.

[30] T. Rahman  S. Jin  and V. Gogate. Look ma  no latent variables: Accurate cutset networks via compilation.

In Proceedings of ICML  pages 5311–5320  2019.

[31] T. Rahman  P. Kothalkar  and V. Gogate. Cutset networks: A simple  tractable  and scalable approach for

improving the accuracy of Chow-Liu trees. In Proceedings of ECML/PKDD  pages 630–645  2014.

[32] A. Rashwan  P. Poupart  and C. Zhitang. Discriminative training of sum-product networks by extended

Baum-Welch. In Proceedings of PGM  pages 356–367  2018.

[33] A. Rashwan  H. Zhao  and P. Poupart. Online and distributed Bayesian moment matching for parameter

learning in sum-product networks. In Proceedings of AISTATS  pages 1469–1477  2016.

[34] C. E. Rasmussen and Z. Ghahramani. Occam’s Razor. In Proceedings of NeurIPS  pages 294–300  2001.

[35] A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect variable interactions.

In Proceedings of ICML  pages 710–718  2014.

[36] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica sinica  pages 639–650  1994.

[37] O. Sharir  R. Tamari  N. Cohen  and A. Shashua. Tractable generative convolutional arithmetic circuits.

arXiv preprint arXiv:1610.04167  2016.

[38] J. Suzuki. A construction of Bayesian networks from databases based on an MDL principle. In Proceedings

of UAI  pages 266–273  1993.

[39] M. Trapp  T. Madl  R. Peharz  F. Pernkopf  and R. Trappl. Safe semi-supervised learning of sum-product

networks. In Proceedings of UAI  2017.

[40] M. Trapp  R. Peharz  and F. Pernkopf. Optimisation of overparametrized sum-product networks. CoRR 

abs/1905.08196  2019.

[41] M. Trapp  R. Peharz  M. Skowron  T. Madl  F. Pernkopf  and R. Trappl. Structure inference in sum-product

networks using inﬁnite sum-product trees. In Proceedings of BNP workshop at NeurIPS  2016.

[42] A. Vergari  N. Di Mauro  and F. Esposito. Simplifying  regularizing and strengthening sum-product

network structure learning. In Proceedings of ECML/PKDD  pages 343–358  2015.

[43] A. Vergari  A. Molina  R. Peharz  Z. Ghahramani  K. Kersting  and I. Valera. Automatic Bayesian density

analysis. In Proceedings of AAAI  2019.

[44] H. Zhao  T. Adel  G. J. Gordon  and B. Amos. Collapsed variational inference for sum-product networks.

In Proceedings of ICML  pages 1310–1318  2016.

[45] H. Zhao  M. Melibari  and P. Poupart. On the relationship between sum-product networks and Bayesian

networks. In Proceedings of ICML  pages 116–124  2015.

11

[46] H. Zhao  P. Poupart  and G. J. Gordon. A uniﬁed approach for learning the parameters of sum-product

networks. In Proceedings of NeurIPS  pages 433–441  2016.

[47] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In Proceedings of ICLR 

2017.

12

,Stanislav Pidhorskyi
Ranya Almohsen
Gianfranco Doretto
Martin Trapp
Robert Peharz
Hong Ge
Franz Pernkopf
Zoubin Ghahramani