2018,Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks,Many neurons in the brain  such as place cells in the rodent hippocampus  have localized receptive fields  i.e.  they respond to a small neighborhood of stimulus space. What is the functional significance of such representations and how can they arise? Here  we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs.  Numerical simulations of such networks on standard datasets yield manifold-tiling localized receptive fields. More generally  we show analytically that  for data lying on symmetric manifolds  optimal solutions of objectives  from which similarity-preserving networks are derived  have localized receptive fields. Therefore  nonnegative similarity-preserving mapping (NSM) implemented by neural networks can model representations of continuous manifolds in the brain.,Manifold-tiling Localized Receptive Fields are

Optimal in Similarity-preserving Neural Networks

†Rutgers University

‡Flatiron Institute

§NYU Langone Medical Center

Anirvan M. Sengupta†‡

Mariano Tepper‡⇤

Cengiz Pehlevan‡⇤

Alexander Genkin§

Dmitri B. Chklovskii‡§

anirvans@physics.rutgers.edu  alexander.genkin@gmail.com
{mtepper cpehlevan dchklovskii}@flatironinstitute.org

Abstract

Many neurons in the brain  such as place cells in the rodent hippocampus  have lo-
calized receptive ﬁelds  i.e.  they respond to a small neighborhood of stimulus space.
What is the functional signiﬁcance of such representations and how can they arise?
Here  we propose that localized receptive ﬁelds emerge in similarity-preserving
networks of rectifying neurons that learn low-dimensional manifolds populated by
sensory inputs. Numerical simulations of such networks on standard datasets yield
manifold-tiling localized receptive ﬁelds. More generally  we show analytically
that  for data lying on symmetric manifolds  optimal solutions of objectives  from
which similarity-preserving networks are derived  have localized receptive ﬁelds.
Therefore  nonnegative similarity-preserving mapping (NSM) implemented by
neural networks can model representations of continuous manifolds in the brain.

1

Introduction

A salient and unexplained feature of many neurons is that their receptive ﬁelds are localized in the
parameter space they represent. For example  a hippocampus place cell is active in a particular spatial
location [1]  the response of a V1 neuron is localized in visual space and orientation [2]  and the
response of an auditory neuron is localized in the sound frequency space [3]. In all these examples 
receptive ﬁelds of neurons from the same brain area tile (with overlap) low-dimensional manifolds.
Localized receptive ﬁelds are shaped by neural activity as evidenced by experimental manipulations
in developing and adult animals [4  5  6  7]. Activity inﬂuences receptive ﬁelds via modiﬁcation  or
learning  of synaptic weights which gate the activity of upstream neurons channeling sensory inputs.
To be biologically plausible  synaptic learning rules must be physically local  i.e.  the weight of a
synapse depends on the activity of only the two neurons it connects  pre- and post-synaptic.
In this paper  we demonstrate that biologically plausible neural networks can learn manifold-tiling
localized receptive ﬁelds from the upstream activity in an unsupervised fashion. Because analyzing
the outcome of learning in arbitrary neural networks is often difﬁcult  we take a normative approach 
Fig. 1. First  we formulate an optimization problem by postulating an objective function and
constraints  Fig. 1. Second  for inputs lying on a manifold  we derive an optimal ofﬂine solution and
demonstrate analytically and numerically that the receptive ﬁelds are localized and tile the manifold 
Fig. 1. Third  from the same objective  we derive an online optimization algorithm which can be
implemented by a biologically plausible neural network  Fig. 1. We expect this network to learn
localized receptive ﬁelds  the conjecture we conﬁrm by simulating the network numerically  Fig. 1.
Optimization functions considered here belong to the family of similarity-preserving objectives which
dictate that similar inputs to the network elicit similar outputs [8  9  10  11  12]. In the absence of sign

⇤M. Tepper and C. Pehlevan contributed equally to this work.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: A schematic illustration of our normative approach.

constraints  such objectives are provably optimized by projecting inputs onto the principal subspace
[13  14  15]  which can be done online by networks of linear neurons [8  9  10]. Constraining the sign
of the output leads to networks of rectifying neurons [11] which have been simulated numerically
in the context of clustering and feature learning [11  12  16  17]  and analyzed in the context of
blind source extraction [18]. In the context of manifold learning  optimal solutions of Nonnegative
Similarity-preserving Mapping objectives have been missing because optimization of existing NSM
objectives is challenging. Our main contributions are:
• Analytical optimization of NSM objectives for input originating from symmetric manifolds.
• Derivation of biologically plausible NSM neural networks.
• Ofﬂine and online algorithms for manifold learning of arbitrary manifolds.
The paper is organized as follows. In Sec. 2  we derive a simpliﬁed version of an NSM objective.
Much of our following analysis can be carried over to other NSM objectives but with additional
technical considerations. In Sec. 3  we derive a necessary condition for the optimal solution. In Sec. 4 
we consider solutions for the case of symmetric manifolds. In Sec. 5  we derive online optimization
algorithm and an NSM neural network. In Sec. 6  we present the results of numerical experiments 
which can be reproduced with the code at https://github.com/flatironinstitute/mantis.

2 A Simpliﬁed Similarity-preserving Objective Function

To introduce similarity-preserving objectives  let us deﬁne our notation. The input to the network is a
set of vectors  xt 2 Rn  t = 1  . . .   T   with components represented by the activity of n upstream
neurons at time  t. In response  the network outputs an activity vector  yt 2 Rm  t = 1  . . .   T   m
being the number of output neurons.
Similarity preservation postulates that similar input pairs  xt and xt0  evoke similar output pairs  yt
and yt0. If similarity of a pair of vectors is quantiﬁed by their scalar product and the distance metric
of similarity is Euclidean  we have

min

8t2{1 ... T}: yt2Rm

1
2

TXt t0=1

(xt · xt0  yt · yt0)2 = min
Y2Rm⇥T

1

2kX>X  Y>Yk2
F  

(1)

2

where we introduced a matrix notation X ⌘ [x1  . . .   xT ] 2 Rn⇥T and Y ⌘ [y1  . . .   yT ] 2 Rm⇥T
and m < n. Such optimization problem is solved ofﬂine by projecting the input data to the principal
subspace [13  14  15]. The same problem can be solved online by a biologically plausible neural
network performing global linear dimensionality reduction [8  10].
We will see below that nonlinear manifolds can be learned by constraining the sign of the output and
introducing a similarity threshold  ↵ (here E is a matrix of all ones):

min
Y0

1

2kX>X  ↵E  Y>Yk2

F = min

8t: yt0

1

2Xt t0

(xt · xt0  ↵  yt · yt0)2 

(2)

In the special case  ↵ = 0  Eq. (2) reduces to the objective in [11  19  18].
Intuitively  Eq. (2) attempts to preserve similarity for similar pairs of input samples but orthogonalizes
the outputs corresponding to dissimilar input pairs. Indeed  if the input similarity of a pair of samples
t  t0 is above a speciﬁed threshold  xt · xt0 >↵   output vectors yt and yt0 would prefer to have
yt · yt0 ⇡ xt · xt0  ↵  i.e.  it would be similar. If  however  xt · xt0 <↵   the lowest value yt · yt0 for
yt  yt0  0 is zero meaning that that they would tend to be orthogonal  yt · yt0 = 0. As yt and yt0 are
nonnegative  to achieve orthogonality  the output activity patterns for dissimilar patterns would have
non-overlapping sets of active neurons. In the context of manifold representation  Eq. (2) strives to
preserve in the y-representation local geometry of the input data cloud in x-space and let the global
geometry emerge out of the nonlinear optimization process.
As the difﬁculty in analyzing Eq. (2) is due to the quartic in Y term  we go on to derive a simpler
quadratic in Y objective function that produces very similar outcomes. To this end  we  ﬁrst  introduce
an additional power constraint: Tr Y>Y  k as in [9  11]. We will call the input-output mapping
obtained by this procedure NSM-0:

argmin

Y0

Tr Y>Yk

1
2kX>X  ↵E  Y>Yk2

F = argmin
Tr Y>Yk

Y0

 Tr((X>X  ↵E)Y>Y) +

1
2kY>Yk2
F  

(NSM-0)

where we expanded the square and kept only the Y-dependent terms.
We can redeﬁne the variables and drop the last term in a certain limit (see the Supplementary Material 
Sec. A.1  for details) leading to the optimization problem we call NSM-1:

min
Y0

diag(Y>Y)I

 Tr((X>X  ↵E)Y>Y) =

min

8t2{1 ... T}:
yt0  kytk2
2

Xt t0

(xt · xt0  ↵)yt · yt0. (NSM-1)

Conceptually  this type of objective has proven successful for manifold learning [20]. Intuitively  just
like Eq. (2)  NSM-1 preserves similarity of nearby input data samples while orthogonalizing output
vectors of dissimilar input pairs. Indeed  a pair of samples t  t0 with xt · xt0 >↵   would tend to have
yt· yt0 as large as possible  albeit with the norm of the vectors controlled by the constraint kytk2  .
Therefore  when the input similarity for the pair is above a speciﬁed threshold  the vectors yt and
yt0 would prefer to be aligned in the same direction. For dissimilar inputs with xt · xt0 <↵   the
corresponding output vectors yt and yt0 would tend to be orthogonal  meaning that responses to these
dissimilar inputs would activate mostly nonoverlapping sets of neurons.

3 A Necessary Optimality Condition for NSM-1

In this section  we derive the necessary optimality condition for Problem (NSM-1). For notational
convenience  we introduce the Gramian D ⌘ X>X and use [z]+  where z 2 RT   for the component-
wise ReLU function  ([z]+)t ⌘ max(zt  0).
Proposition 1. The optimal solution of Problem (NSM-1) satisﬁes

where y(a) designates a column vector which is the transpose of the a-th row of Y and ⇤ =
diag(1  . . .   T ) is a nonnegative diagonal matrix.

[(D  ↵E)y(a)]+ = ⇤y(a) 

(3)

3

The proof of Proposition 1 (Supplementary Material  Sec. A.2) proceeds by introducing Lagrange
multipliers ⇤ = diag(1  . . .   T )  0 for the constraint diag(Y>Y)  I  and writing down the
KKT conditions. Then  by separately considering the cases tyat = 0 and tyat > 0 we get Eq. (3).
To gain insight into the nature of the solutions of (3)  let us assume t > 0 for all t and rewrite it as

yat = 1

tXt0

(Dtt0  ↵)yat0+

.

(4)

Eq. (4) suggests that the sign of the interaction within each pair of yt and yt0 depends on the similarity
of the corresponding inputs. If xt and xt0 are similar  Dtt0 >↵   then yat0 has excitatory inﬂuence on
yat. Otherwise  if xt and xt0 are farther apart  the inﬂuence is inhibitory. Such models often give rise
to localized solutions [21]. Since  in our case  the variable yat gives the activity of the a-th neuron
as the t-th input vector is presented to the network  such a solution would deﬁne a receptive ﬁeld
of neuron  a  localized in the space of inputs. Below  we will derive such localized-receptive ﬁeld
solutions for inputs originating from symmetric manifolds.

4 Solution for Symmetric Manifolds via a Convex Formulation

So far  we set the dimensionality of y  i.e.  the number of output neurons  m  a priori. However  as
this number depends on the dataset  we would like to allow for ﬂexibility of choosing the output
dimensionality adaptively. To this end  we introduce the Gramian  Q ⌘ Y>Y  and do not constrain
its rank. Minimization of our objective functions requires that the output similarity expressed by
Gramian  Q  captures some of the input similarity structure encoded in the input Gramian  D.
Redeﬁning the variables makes the domain of the optimization problem convex. Matrices like D and
Q which could be expressed as Gramians are symmetric and positive semideﬁnite. In addition  any
matrix  Q  such that Q ⌘ Y>Y with Y  0 is called completely positive. The set of completely
positive T ⇥ T matrices is denoted by CP T and forms a closed convex cone [22].
Then  NSM-1  without the rank constraint  can be restated as a convex optimization problem with
respect to Q belonging to the convex cone CP T :

(NSM-1a)

min
Q2CP T
diag(Q)I

 Tr((D  ↵E)Q).

Despite the convexity  for arbitrary datasets  optimization problems in CP T are often intractable for
large T [22]. Yet  for D with a high degree of symmetry  below  we will ﬁnd the optimal Q.
Imagine now that there is a group G ✓ ST   ST being the permutation group of the set {1  2  . . .   T} 
so that Dg(t)g(t0) = Dtt0 for all g 2 G. The matrix with elements Mg(t)g(t0) is denoted as gM 
representing group action on M. We will represent action of g on a vector w 2 RT as gw  with
(gw)t = wg(t).
Theorem 1. If the action of the group G is transitive  that is  for any pair t  t0 2{ 1  2  . . .   T} there
is a g 2 G so that t0 = g(t)  then there is at least one optimal solution of Problem (NSM-1a) with
Q = Y>Y  Y 2 Rm⇥T and Y  0  such that

(i) for each a  the transpose of the a-th row of Y  termed y(a)  satisﬁes
[(D  ↵E)y(a)]+ = y(a)  8a 2{ 1  2  . . .   m} 

(5)

(ii) Let H be the stabilizer subgroup of y(1)  namely  H = Stab y(1) ⌘{ h 2 G|hy(1) = y(1)}.

Then  m = |G/H| and Y can be written as

Y> = 1pm [g1y(1)g2y(1) . . . gmy(1)] 

(6)

where gi are members of the m distinct left cosets in G/H.

In other words  when the symmetry group action is transitive  all the Lagrange multipliers are the
same. Also the different rows of the Y matrix could be generated from a single row by the action
of the group. A sketch of the proof is as follows (see Supplementary Material  Sec. A.3  for further

4

details). For part (i)  we argue that a convex minimization problem with a symmetry always has a
solution which respects the symmetry. Thus our search could be limited to the G-invariant elements
of the convex cone  CP G = {Q 2CP T | Q = gQ 8g 2 G}  which happens to be a convex cone
itself. We then introduce the Lagrange multipliers and deﬁne the Lagrangian for the problem on
the invariant convex cone and show that it is enough to search over ⇤ = I. Part (ii) follows from
optimality of Q = Y>Y implying optimality of ¯Q = 1
Eq. (5) is a non-linear eigenvalue equation that can have many solutions. Yet  if those solutions
are related to each other by symmetry they can be found explicitly  as we show in the following
subsections.

|G|Pg gQ.

4.1 Solution for Inputs on the Ring with Cosine Similarity in the Continuum Limit
In this subsection  we consider the case where inputs  xt  lie on a one-dimensional manifold shaped
as a ring centered on the origin:

T ! ✓ 

(7)

(8)

(9)

= yat  8a 2{ 1  2  . . .   m}.

xt =⇥cos( 2⇡t
⇤  ↵⌘ yat0#+

T

T )⇤>  
⇤ and Eq. (5) becomes
where t 2{ 1  2  . . .   T}. Then  we have Dtt0 = cos⇥ 2⇡(tt0)

T )  sin( 2⇡t

T

In the limit of large T   we can replace a discrete variable  t  by a continuous variable  ✓: 2⇡t

"Xt0 ⇣cos⇥ 2⇡(tt0)
⇤ ! cos(✓  ✓0)  yat ! Cu(✓) and  ! T µ  leading to
0  cos(✓  ✓0)  ↵ u(✓0)d✓0#+
" 1
2⇡Z 2⇡

Dtt0 = cos⇥ 2⇡(tt0)
with C adjusted so thatR u(✓)2dm() = 1 for some measure m in the space of   which is a
angle and the constraint becomesR 2⇡

Eq. (8) has appeared previously in the context of the ring attractor [21]. While our variables have a
completely different neuroscience interpretation  we can still use their solution:

continuous variable labeling the output neurons. We will see that  could naturally be chosen as an

0 u(✓)2d = 1.

= µu(✓) 

T

u(✓) = A⇥ cos(✓  )  cos( )]+

whose support is the interval [     + ].
Eq. (9) gives the receptive ﬁelds of a neuron    in terms of the azimuthal coordinate  ✓  shown
in the bottom left panel of Fig. 1. The dependence of µ and on ↵ is given parametrically (see
Supplementary Material  Sec. A.4). So far  we have only shown that Eq. (9) satisﬁes the necessary
optimality condition in the continuous limit of Eq. (8). In Sec. 6  we conﬁrm numerically that the
optimal solution for a ﬁnite number of neurons approximates Eq. (9)  Fig. 2.
While we do not have a closed-form solution for NSM-0 on a ring  we show that the optimal solution
also has localized receptive ﬁelds (see Supplementary Material  Sec. A.5).

4.2 Solution for Inputs on Higher-dimensional Compact Homogeneous Manifolds
Here  we consider two special cases of higher dimensional manifolds. The ﬁrst example is the
2-sphere  S2 = SO(3)/SO(1). The second example is the rotation group  SO(3)  which is a
three-dimensional manifold. It is possible to generalize this method to other compact homogeneous
spaces for particular kernels.
We can think of a 2-sphere via its 3-dimensional embedding: S2 ⌘{ x 2 R3|kxk = 1}. For two
points ⌦  ⌦0 on the 2-sphere let D(⌦  ⌦0) = x(⌦)· x(⌦0)   where x(⌦)  x(⌦0) are the corresponding
unit vectors in the 3-dimensional embedding.
Remarkably  we can show that solutions satisfying the optimality conditions are of the form

u⌦0(⌦) = A⇥x(⌦0) · x(⌦)  cos ⇤+.

5

(10)

ug0(⌦) = A

This means that the center of a receptive ﬁeld on the sphere is at ⌦0. The neuron is active while the
angle between x(⌦) and x(⌦0) is less than . For the derivation of Eq. (10) and the self-consistency
conditions  determining   µ in terms of ↵  see Supplementary Material  Sec. A.6.
In the case of the rotation group  for g  g0 2 SO(3) we adopt the 3 ⇥ 3 matrix representations
3 TrR(g)R(g0)> to be the similarity kernel. Once more  we index a
R(g)  R(g0) and consider 1
receptive ﬁeld solution by the rotation group element  g0  where the response is maximum:

2⇥ TrR(g0)>R(g)  2 cos  1⇤+

(11)
with   µ being determined by ↵ through self-consistency equations. This solution has support over
g 2 SO(3)  such that the rotation gg1
To summarize this section  we demonstrated  in the continuum limit  that the solutions to NSM
objectives for data on symmetric manifolds possess localized receptive ﬁelds that tile these manifolds.
What is the nature of solutions as the datasets depart from highly symmetric cases? To address this
question  consider data on a smooth compact Riemannian manifold with a smooth metric resulting
in a continuous curvature tensor. Then the curvature tensor sets a local length scale over which the
effect of curvature is felt. If a symmetry group acts transitively on the manifold  this length scale
is constant all over the manifold. Even if such symmetries are absent  on a compact manifold  the
curvature tensor components are bounded and there is a length scale  L  below which the manifold
locally appears as ﬂat space. Suppose the manifold is sampled well enough with many data points
within each geodesic ball of length  L  and the parameters are chosen so that the localized receptive
ﬁelds are narrower than L. Then  we could construct an asymptotic solution satisfying the optimality
condition. Such asymptotic solution in the continuum limit and the effect of uneven sampling along
the manifold will be analyzed elsewhere.

0 has a rotation angle less than .

5 Online Optimization and Neural Networks

Here  we derive a biologically plausible neural network that optimizes NSM-1. To this end  we
transform NSM-1 by  ﬁrst  rewriting it in the Lagrangian form:

min

8t: yt0

max

8t: zt0

1

T Xt t0

(xt · xt0  ↵)yt · yt0 +Xt

zt · zt(yt · yt  ).

(12)

Here  unconventionally  the nonnegative Lagrange multipliers that impose the inequality constraints
are factorized into inner products of two nonnegative vectors (zt · zt). Second  we introduce auxiliary
variables  W  b  Vt [10]:
max
8t: zt0

T Tr(W>W)  T kbk2
2 +

8t: Vt0

8t: yt0

min
W

max

max

min

b

+Xt ⇣2xtW>yt + 2p↵yt · b   kztk2

2 + 2ztVtyt  Tr(V>t Vt)⌘ .

The equivalence of (13) to (12) can be seen by performing the W  b  and Vt optimizations explicitly
and plugging the optimal values back. (13) suggests a two-step online algorithm (see Appendix A.8
for full derivation). For each input xt  in the ﬁrst step  one solves for yt  zt and Vt  by projected
gradient descent-ascent-descent 

(13)

zt

Vt # 24
" yt

yt + yWxt  V>t zt  p↵b

zt + z (zt + Vtyt)

Vt + V zty>t  Vt

35+

 

(14)

where y z V are step sizes. This iteration can be interpreted as the dynamics of a neural circuit
(Fig. 1  Top right panel)  where components of yt are activities of excitatory neurons  b is a bias term 
zt – activities of inhibitory neurons  W is the feedforward connectivity matrix  and Vt is the synaptic
weight matrix from excitatory to inhibitory neurons  which undergoes a fast time-scale anti-Hebbian
plasticity. In the second step  W and b are updated by gradient descent-ascent:

(15)
where W is going through a slow time-scale Hebbian plasticity and b through homeostatic plasticity.
⌘ is a learning rate. Application of this algorithm to symmetric datasets is shown in Fig. 2 and Fig. 3.

W  W + ⌘ytx>t  W  

b  b + ⌘p↵yt  b  

6

n
o
i
t
a
z
i
m

i
t
p
o
e
n
i
ﬂ
f
O

n
o
i
t
a
z
i
m

i
t
p
o
e
n
i
l
n
O

Figure 2: Solution of NSM-1 on a ring in 2D. From left to right  the input dataset X  the output
similarity  Q  the output neural activity matrix Y  a few localized receptive ﬁelds  and the aligned
receptive ﬁelds. The receptive ﬁelds are truncated cosines translated along the ring.

Ofﬂine optimization

Online optimization

Figure 3: Solution of NSM-1 tiles the sphere with overlapping localized receptive ﬁelds (soft-clusters) 
providing an accurate and useful data representation. We show a few receptive ﬁelds in different
colors over three different views of the sphere. An advantage of the online optimization is that it can
handle arbitrarily large number of points.

6 Experimental Results

In this section  we verify our theoretical results by solving both ofﬂine and online optimization
problems numerically. We conﬁrm our theoretical predictions in Sec. 4 for symmetric manifolds
and demonstrate that they hold for deviations from symmetry. Moreover  our algorithms yield
manifold-tiling localized receptive ﬁelds on real-world data.

Synthetic data. Recall that for the input data lying on a ring  optimization without a rank constraint
yields truncated cosine solutions  see Eq. (9). Here  we show numerically that ﬁxed-rank optimization
yields the same solutions  Fig. 2: the computed matrix Y>Y is indeed circulant  all receptive ﬁelds
are equivalent to each other  are well approximated by truncated cosine and tile the manifold with
overlap. Similarly  for the input lying on a 2-sphere  we ﬁnd numerically that localized solutions tile
the manifold  Fig. 3.
For the ofﬂine optimization we used a Burer-Monteiro augmented Lagrangian method [23  24].
Whereas  conventionally  the number of rows m of Y is chosen to be T (observe that diag(Y>Y) 
I implies that Tr(Y>Y)  T   making T an upper bound of the rank)  we use the non-standard
setting m  T   as a small m might create degeneracies (i.e.  hard-clustering solutions).
Also  we empirically demonstrate that the nature of the solutions is robust to deviations from symmetry
in manifold curvature and data point density. See Fig. 4 and its caption for details.

Real-world data. For normalized input data with every diagonal element Dtt = kxtk2
threshold ↵  the term ↵ Tr(EQ) = ↵Ptt0 yt · yt0 in NSM-1 behaves as described in Sec. 2. For
unnormalized inputs  it is preferable to control the sum of each row of Q  i.e.Pt0 yt · yt0  with an

individual ↵t  instead of the total sum.

2 above the

7

InputdatasetY>Y0.000.010.020.030.040.050.060.070.08Y>0.000.010.020.030.040.050.060.070.08ReceptiveﬁeldsReceptiveﬁeldssummaryMean±3STDTruncatedcosineInputdatasetY>Y0.00.20.40.60.81.0Y>0.000.020.040.060.080.10ReceptiveﬁeldsReceptiveﬁeldssummaryMean±3STDTruncatedcosineSmooth curve evolution: from a bunny to a circle

Density change: from quasi-uniformity to clusters

l

y
s
u
o
u
n

i
t

n
o
C

i

g
n
v
o
v
e

l

s
d
o

l

f
i

n
a
m

y
t
i
r
a

l
i

m
s

i

Q
x
i
r
t

a
m

t

t

u
p
u
O

0
0

0
0

2
0

2
0

4
0

4
0

6
0

6
0

8
0

8
0

0
1

0
1

2
1

2
1

4
1

4
1

0
0

0
0

2
0

2
0

4
0

4
0

6
0

6
0

8
0

8
0

0
1

0
1

2
1

2
1

0
0

0
0

2
0

2
0

4
0

4
0

6
0

6
0

8
0

8
0

0
1

0
1

2
1

2
1

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0
0

0
0

2
0

.

.

.

0

0

0

4
0

0
2
0
0
0
0
0
.

6
0

6
0

0
5
0
0

5
4
2
0
0
0
0
.

0

0
1

5
8
7
0
0
0
0
.

0
8
0
0
1
0
0
.

5
0
2
1
1
0
0
.

0

0

0

2
1

2
1

0
5
1
0

5
7
1
0

0

0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

0
0
2
0

.

0
0
0
0
0
0
0
0

5
5
2
2
0
0
0
0

0
0
5
5
0
0
0
0

5
5
7
7
0
0
0
0

0
0
0
0
1
1
0
0

5
5
2
2
1
1
0
0

0
0
5
5
1
1
0
0

5
5
7
7
1
1
0
0

0
0
0
0
2
2
0
0

0
0
0
0
0
0
0
0

5
5
2
2
0
0
0
0

0
0
5
5
0
0
0
0

5
5
7
7
0
0
0
0

0
0
0
0
1
1
0
0

5
5
2
2
1
1
0
0

0
0
5
5
1
1
0
0

5
5
7
7
1
1
0
0

0
0
0
0
2
2
0
0

0
0
0
0
0
0
0
0

5
5
2
2
0
0
0
0

0
0
5
5
0
0
0
0

5
5
7
7
0
0
0
0

0
0
0
0
1
1
0
0

5
5
2
2
1
1
0
0

0
0
5
5
1
1
0
0

5
5
7
7
1
1
0
0

0
0
0
0
2
2
0
0

0
0
0
0
0
0
0
0

5
5
2
2
0
0
0
0

0
0
5
5
0
0
0
0

5
7
0
0

5
7
0
0

0
0
1
0

0
0
1
0

5
2
1
0

5
2
1
0

0
5
1
0

0
5
1
0

5
7
1
0

5
7
1
0

0
0
2
0

0
0
2
0

0
0
0
0
0
0
0
0

5
5
2
2
0
0
0
0

0
0
5
5
0
0
0
0

5
5
7
7
0
0
0
0

0
0
0
0
1
1
0
0

5
5
2
2
1
1
0
0

0
0
5
5
1
1
0
0

5
5
7
7
1
1
0
0

0
0
0
0
2
2
0
0

0
0
0
0

5
2
0
0

0
5
0
0

5
7
0
0

0
0
1
0

5
2
1
0

0
5
1
0

5
7
1
0

0
0
2
0

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0

0

0

0

0

0

0

0

0

0

0

0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

s
d
e
fi

l

e
v
i
t

p
e
c
e
R

Figure 4: Robustness of the manifold-tiling solution to symmetry violations. Left sequence: Despite
non-uniform curvature  the localized manifold-tiling nature of solutions is preserved for the wide
range of datasets around the symmetric manifold. We start from a curve representing a bunny and
evolve it using the classical mean curvature motion. Right sequence: Despite non-uniform point
density  the localized manifold-tiling nature of solutions is preserved in the wide range of datasets
around the symmetric manifold. For high density variation there is a smooth transition to the hard-
clustering solution. The points are sampled from a mixture of von Mises distributions with means
0  ⇡

2 and equal variance decreasing from left to right.

2  ⇡  3⇡

X>X

Y>Y

Y>

Output embedding

0.1

0.5

0.3

0.2

0.0

0.4

0.15

0.10

0.06

0.30

0.25

0.20

0.05

0.08

0.04

0.02

0.00

0.00

0.02
Figure 5: NSM-2 solution learns the manifold from a 100 images obtained by viewing a teapot from
different angles. The obtained 1d manifold uncovers the change in orientation (better seen with zoom)
by tiling it with overlapping localized receptive ﬁelds. The input size n is 23028 (76 ⇥ 101 pixels  3
color channels). We build a 2d linear embedding (PCA) from the solution Y.
Additionally  enforcing Pt kytk2
Pt kytk2
Y0 Tr(X>XY>Y)

which  for some choice of ↵t  is equivalent to (here  1 2 RT is a column vector of ones)

2   but makes the optimization easier. We thus obtain the objective function

2  T is in many cases empirically equivalent to enforcing

s.t. Y>Y1 = 1  Tr(Y>Y)  T 

(xt · xt0  ↵t)yt · yt0 

Xt t0

(NSM-2)

kytk2

8t: yt0 

2T

(16)

min

min

For highly symmetric datasets without constraints on rank  NSM-2 has the same solutions as NSM-1
(see Supplementary Material  Sec. A.7). Relaxations of this optimization problem have been the
subject of extensive research to solve clustering and manifold learning problems [25  26  27  28]. A
biologically plausible neural network solving this problem was proposed in [12]. For the optimization
of NSM-2 we use an augmented Lagrangian method [23  24  28  29].
We have extensively applied NSM-2 to datasets previously analyzed in the context of manifold
learning [28  30] (see Supplementary Material  Sec. B). Here  we include just two representative
examples  ﬁgs. 5 and 6  showing the emergence of localized receptive ﬁelds in a high-dimensional
space. Despite the lack of symmetry and ensuing loss of regularity  we obtain neurons whose receptive

8

ReceptiveﬁeldsFigure 6: NSM-2 solution learns the manifold of MNIST digit 0 images by tiling the dataset with
overlapping localized receptive ﬁelds. Input size is n = 28 ⇥ 28 = 784. Left: Two-dimensional
linear embedding (PCA) of Y. The data gets organized according to different visual characteristics
of the hand-written digit (e.g.  orientation and elongation). Right: A few receptive ﬁelds in different
colors over the low-dimensional embedding.

ﬁelds  taken together  tile the entire data cloud. Such tiling solutions indicate robustness of the method
to imperfections in the dataset and further corroborate the theoretical results derived in this paper.

7 Discussion

In this work  we show that objective functions approximately preserving similarity  along with
nonnegativity constraint on the outputs  learn data manifolds. Neural networks implementing NSM
algorithms use only biologically plausible local (Hebbian or anti-Hebbian) synaptic learning rules.
These results add to the versatility of NSM networks previously shown to cluster data  learn sparse
dictionaries and blindly separate sources [11  18  16]  depending on the nature of input data. This
illustrates how a universal neural circuit in the brain can implement various learning tasks [11].
Our algorithms  starting from a linear kernel  D  generate an output kernel  Q  restricted to the sample
space. Whereas the associations between kernels and neural networks was known [31]  previously
proposed networks used random synaptic weights with no learning. In our algorithms  the weights are
learned from the input data to optimize the objective. Therefore  our algorithms learn data-dependent
kernels adaptively.
In addition to modeling biological neural computation  our algorithms may also serve as general-
purpose mechanisms for generating representations of manifolds adaptively. Unlike most existing
manifold learning algorithms [32  33  34  35  36  37]  ours can operate naturally in the online setting.
Also  unlike most existing algorithms  ours do not output low-dimensional vectors of embedding
variables but rather high-dimensional vectors of assignment indices to centroids tiling the manifold 
similar to radial basis function networks [38]. This tiling approach is also essentially different from
setting up charts [39  40]  which essentially end up modeling local tangent spaces. The advantage
of our high-dimensional representation becomes obvious if the output representation is used not for
visualization but for further computation  e.g.  linear classiﬁcation [41].

Acknowledgments
We are grateful to Yanis Bahroun  Johannes Friedrich  Victor Minden  Eftychios Pnevmatikakis  and
the other members of the Flatiron Neuroscience group for discussion and comments on an earlier
version of this manuscript. We thank Sanjeev Arora  Afonso Bandeira  Moses Charikar  Jeff Cheeger 
Surya Ganguli  Dustin Mixon  Marc’Aurelio Ranzato  and Soledad Villar for helpful discussions.

References
[1] John O’Keefe and Lynn Nadel. The hippocampus as a cognitive map. Oxford: Clarendon Press  1978.

[2] David H Hubel and Torsten N Wiesel. Receptive ﬁelds  binocular interaction and functional architecture in

the cat’s visual cortex. The Journal of Physiology  160(1):106–154  1962.

9

[3] Eric I Knudsen and Masakazu Konishi. Center-surround organization of auditory receptive ﬁelds in the

owl. Science  202(4369):778–780  1978.

[4] Michael P Kilgard and Michael M Merzenich. Cortical map reorganization enabled by nucleus basalis

activity. Science  279(5357):1714–1718  1998.

[5] Daniel E Feldman and Michael Brecht. Map plasticity in somatosensory cortex. Science  310(5749):810–

815  2005.

[6] Takao K Hensch. Critical period plasticity in local cortical circuits. Nature Reviews Neuroscience 

6(11):877  2005.

[7] Valentin Dragoi  Jitendra Sharma  and Mriganka Sur. Adaptation-induced plasticity of orientation tuning

in adult visual cortex. Neuron  28(1):287–298  2000.

[8] Cengiz Pehlevan  Tao Hu  and Dmitri B Chklovskii. A Hebbian/anti-Hebbian neural network for linear
subspace learning: A derivation from multidimensional scaling of streaming data. Neural Computation 
27(7):1461–1495  2015.

[9] Cengiz Pehlevan and Dmitri Chklovskii. A normative theory of adaptive dimensionality reduction in neural

networks. In NIPS  2015.

[10] Cengiz Pehlevan  Anirvan M Sengupta  and Dmitri B Chklovskii. Why do similarity matching objectives

lead to Hebbian/anti-Hebbian networks? Neural Computation  30(1):84–124  2018.

[11] Cengiz Pehlevan and Dmitri B Chklovskii. A Hebbian/anti-Hebbian network derived from online non-

negative matrix factorization can cluster and discover sparse features. In ACSSC  2014.

[12] Cengiz Pehlevan  Alex Genkin  and Dmitri B Chklovskii. A clustering neural network model of insect

olfaction. In ACSSC  2017.

[13] Christopher KI Williams. On a connection between kernel PCA and metric multidimensional scaling. In

NIPS  2001.

[14] Trevor F Cox and Michael AA Cox. Multidimensional scaling. CRC press  2000.

[15] John M Bibby  John T Kent  and Kanti V Mardia. Multivariate analysis  1979.

[16] H Sebastian Seung and Jonathan Zung. A correlation game for unsupervised learning yields computational
interpretations of Hebbian excitation  anti-Hebbian inhibition  and synapse elimination. arXiv preprint
arXiv:1704.00646  2017.

[17] Yanis Bahroun and Andrea Soltoggio. Online representation learning with single and multi-layer Hebbian

networks for image classiﬁcation. In ICANN  2017.

[18] Cengiz Pehlevan  Sreyas Mohan  and Dmitri B Chklovskii. Blind nonnegative source separation using

biological neural networks. Neural Computation  29(11):2925–2954  2017.

[19] Chris Ding  Xiaofeng He  and Horst D Simon. On the equivalence of nonnegative matrix factorization and

spectral clustering. In ICDM  2005.

[20] Raia Hadsell  Sumit Chopra  and Yann LeCun. Dimensionality reduction by learning an invariant mapping.

In CVPR  2006.

[21] Rani Ben-Yishai  Ruth Lev Bar-Or  and Haim Sompolinsky. Theory of orientation tuning in visual cortex.

Proceedings of the National Academy of Sciences  92(9):3844–3848  1995.

[22] Abraham Berman and Naomi Shaked-Monderer. Completely positive matrices. World Scientiﬁc  2003.
[23] Samuel Burer  Kurt M. Anstreicher  and Mirjam Dür. The difference between 5 ⇥ 5 doubly nonnegative

and completely positive matrices. Linear Algebra and its Applications  431(9):1539–1552  2009.

[24] Samuel Burer and Renato D.C. Monteiro. A nonlinear programming algorithm for solving semideﬁnite

programs via low-rank factorization. Mathematical Programming  95(2):329–357  2003.

[25] Arash A Amini and Elizaveta Levina. On semideﬁnite relaxations for the block model. arXiv preprint

arXiv:1406.5647  2014.

[26] Pranjal Awasthi  Afonso S Bandeira  Moses Charikar  Ravishankar Krishnaswamy  Soledad Villar  and

Rachel Ward. Relax  no need to round: Integrality of clustering formulations. In ITCS  2015.

10

[27] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideﬁnite programming. SIAM

Journal on Optimization  18(1):186–205  2007.

[28] Mariano Tepper  Anirvan M Sengupta  and Dmitri Chklovskii. Clustering is semideﬁnitely not that hard:

Nonnegative SDP for manifold disentangling. arXiv preprint arXiv:1706.06028  2017.

[29] Nicolas Boumal  Vlad Voroninski  and Afonso Bandeira. The non-convex Burer-Monteiro approach works

on smooth semideﬁnite programs. In NIPS  2016.

[30] Killan Q. Weinberger and Lawrence K. Saul. An introduction to nonlinear dimensionality reduction by

maximum variance unfolding. AAAI  2006.

[31] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In NIPS  2009.

[32] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding.

Science  290(5500)  2000.

[33] Joshua B Tenenbaum  Vin de Silva  and John C Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(5500)  2000.

[34] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representa-

tion. Neural Computation  15(6):1373–1396  2003.

[35] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning

Research  9(Nov):2579–2605  2008.

[36] Kilian Q Weinberger and Lawrence K Saul. Unsupervised learning of image manifolds by semideﬁnite

programming. International Journal of Computer Vision  70(1):77–90  2006.

[37] David L Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for

high-dimensional data. Proceedings of the National Academy of Sciences  100(10):5591–5596  2003.

[38] David S Broomhead and David Lowe. Radial basis functions  multi-variable functional interpolation and
adaptive networks. Technical report  Royal Signals and Radar Establishment Malvern (United Kingdom) 
1988.

[39] Matthew Brand. Charting a manifold. In NIPS  2003.

[40] Nikolaos Pitelis  Chris Russell  and Lourdes Agapito. Learning a manifold as an atlas. In CVPR  2013.

[41] Sanjeev Arora and Andrej Risteski. Provable beneﬁts of representation learning. arXiv preprint

arXiv:1706.04601  2017.

[42] Christine Bachoc  Dion C Gijswijt  Alexander Schrijver  and Frank Vallentin. Invariant semideﬁnite
programs. In Handbook on semideﬁnite  conic and polynomial optimization  pages 219–269. Springer 
2012.

[43] Nathan Jacobson. Basic algebra I. Courier Corporation  2012.

[44] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive ﬁeld properties by learning a

sparse code for natural images. Nature  381(6583):607  1996.

[45] Sanjeev Arora  Rong Ge  Tengyu Ma  and Ankur Moitra. Simple  efﬁcient  and neural algorithms for

sparse coding. In COLT  2015.

11

,Anirvan Sengupta
Cengiz Pehlevan
Mariano Tepper
Alexander Genkin
Dmitri Chklovskii