2019,UniXGrad: A Universal  Adaptive Algorithm with Optimal Guarantees for Constrained Optimization,We propose a novel adaptive  accelerated algorithm for the stochastic constrained convex optimization setting.Our method  which is inspired by the Mirror-Prox method   \emph{simultaneously} achieves the optimal rates for smooth/non-smooth problems with either deterministic/stochastic first-order oracles. This is done without any prior knowledge of the smoothness nor the noise properties of the problem. To the best of our knowledge  this is the first adaptive  unified algorithm that achieves the optimal rates in the constrained setting. We demonstrate the practical performance of our framework through extensive numerical experiments.,UniXGrad: A Universal  Adaptive Algorithm with
Optimal Guarantees for Constrained Optimization

Ali Kavis∗
EPFL

ali.kavis@epfl.ch

Kﬁr Y. Levy∗
Technion

kfirylevy@technion.ac.il

Francis Bach

INRIA

francis.bach@inria.fr

Volkan Cevher

EPFL

volkan.cevher@epfl.ch

Abstract

We propose a novel adaptive  accelerated algorithm for the stochastic constrained
convex optimization setting. Our method  which is inspired by the Mirror-Prox
method  simultaneously achieves the optimal rates for smooth/non-smooth prob-
lems with either deterministic/stochastic ﬁrst-order oracles. This is done without
any prior knowledge of the smoothness nor the noise properties of the problem.
To the best of our knowledge  this is the ﬁrst adaptive  uniﬁed algorithm that
achieves the optimal rates in the constrained setting. We demonstrate the practical
performance of our framework through extensive numerical experiments.

1

Introduction

√
T ) and O(LD2/T 2 + σD/

Stochastic constrained optimization with ﬁrst-order oracles (SCO) is critical in machine learning.
Indeed  the scalability of classical machine learning tasks  such as support vector machines (SVMs) 
linear/logistic regression and Lasso  rely on efﬁcient stochastic optimization methods. Importantly 
generalization guarantees for such tasks often rely on constraining the set of possible solutions. The
latter induces simple solutions in the form of low norm or low entropy  which in trun enables to
establish generalization guarantees.
√
In the SCO setting  the optimal convergence rates for the cases of non-smooth and smooth objectives
are given by O(GD/
T )  respectively; where T is the total number
of (noisy) gradient queries  L is the smoothness constant of the objective  σ2 is the variance of the
stochastic gradient estimates  D is the effective diameter of the decision set  and G is a bound on the
magnitude of gradient estimates. These rates cannot be improved without additional assumptions.
The optimal rate for the non-smooth case may be obtained by the current state-of-the-art optimization
algorithms  such as Stochastic Gradient Descent (SGD)  AdaGrad [Duchi et al.  2011]  Adam [Kingma
and Ba  2014]  and AmsGrad [Reddi et al.  2018]. However  in order to obtain the optimal rate for
the smooth case  one is required to use more involved accelerated methods such as [Hu et al.  2009 
Lan  2012  Xiao  2010  Diakonikolas and Orecchia  2017  Cohen et al.  2018  Deng et al.  2018].
Unfortunately  all of these accelerated methods require a-priori knowledge of the smoothness parame-
ter L  as well as the variance of the gradients σ2  creating a setup barrier for their use in practice. As
a result  accelerated methods are not very popular in machine learning tasks.
This work develops a new universal method for SCO that obtains the optimal rates in both smooth
and non-smooth cases  without any prior knowledge regarding the smoothness of the problem L  nor

∗Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the noise magnitude σ. Such universal methods that implicitly adapt to the properties of the learning
objective may be very beneﬁcial in practical large-scale problems where these properties are usually
unknown. To our knowledge  this is the ﬁrst work that achieves this desiderata in the constrained
SCO setting.

GD

T

(cid:17)

(cid:17)

(cid:17)

T

(cid:17)

√
1/

T

L log LD2/T + σD

√

√
log T /

T

√

√
log T /

T

and O(cid:16)

and O(cid:16)

√
D2L/T 3/2 + σD/

constrained setting.
Our work completely resolves the open problem in Levy et al. [2018]  Cutkosky [2019]  and
and

√
GD/

Our contributions in the context of related work For the unconstrained setting  Levy et al. [2018]
and Cutkosky [2019] have recently presented a universal scheme that obtains (almost) optimal rates
for both smooth and non-smooth cases.
More speciﬁcally  Levy et al. [2018] designs AcceleGrad—a method that obtains respective rates
. Unfortunately  this result only
holds for the unconstrained setting  and the authors leave the constrained case as an open problem.
An important progress towards this open problem is achieved only recently by Cutkosky [2019]  who
for SCO in the

of O(cid:16)
proves suboptimal respective rates of O(cid:16)
(cid:17)
proposes the ﬁrst universal method that obtains respective optimal rates of O(cid:16)
O(cid:16)

for the constrained setting. When applied to the unconstrained setting  our
analysis tightens the rate characterizations by removing the unnecessary logarithmic factors appearing
in [Levy et al.  2018  Cutkosky  2019].
Our method is inspired by the Mirror-Prox method [Nemirovski  2004  Rakhlin and Sridharan  2013 
Diakonikolas and Orecchia  2017  Bach and Levy  2019]  and builds on top of it using additional
techniques from the online learning literature. Among  is an adaptive learning rate rule [Duchi et al. 
2011  Rakhlin and Sridharan  2013]  as well as recent online-to-batch conversion techniques [Levy 
2017  Cutkosky  2019].
The paper is organized as follows. In the next section  we specify the problem setup  and give the
necessary deﬁnitions and background information. In Section 3  we motivate our framework and
explain the general mechanism. We also introduce the convergence theorems with proof sketches
to highlight the technical novelties. We share numerical results in comparison with other adaptive
methods and baselines for different machine learning tasks in Section 4  followed up with conclusions.

√
D2L/T 2 + σD/

(cid:17)

T

2 Setting and preliminaries
Preliminaries. Let (cid:107) · (cid:107) be a general norm and (cid:107) · (cid:107)∗ be its dual norm. A function f : K (cid:55)→ R is
µ-strongly convex over a convex set K  if for any x ∈ K and any ∇f (x)  a subgradient of f at x 

f (x) − f (y) − (cid:104)∇f (y)  x − y(cid:105) ≥ µ
2

(cid:107)x − y(cid:107)2  

∀x  y ∈ K

(1)

A function f : K (cid:55)→ R is L-smooth over K if it has L-Lipschitz continuous gradient  i.e. 

(cid:107)∇f (x) − ∇f (y)(cid:107)∗ ≤ L(cid:107)x − y(cid:107)  

(2)
Consider a 1-strongly convex differentiable function R : K → R. The Bregman divergence with
respect to a distance-generating function R is deﬁned as follows ∀x  y ∈ K 
DR(x  y) = R(x) − R(y) − (cid:104)∇R(y)  x − y(cid:105) .

∀x  y ∈ K.

(3)
2 (cid:107)x − y(cid:107)2 for all x  y ∈ K  due

An important property of Bregman divergence is that DR(x  y) ≥ 1
to the strong convexity of R.

Setting This paper focuses on (approximately) solving the following constrained problem 

where f : K (cid:55)→ R is a convex function  and K ⊂ Rd is a compact convex set.

min
x∈K f (x)  

(4)

2

We assume the availability of a ﬁrst order oracle for f (·)  and consider two settings: a deterministic
setting where we may access exact gradients  and a stochastic setting where we may only access
unbiased (noisy) gradient estimates. Concretely  we assume that by querying this oracle with a point
x ∈ K  we receive ˜∇f (x) ∈ Rd such 

= ∇f (x) .

(5)

E(cid:104) ˜∇f (x)(cid:12)(cid:12)x

(cid:105)

Throughout this paper we also assume the norm of the (sub)-gradient estimates is bounded by G  i.e 

(cid:107) ˜∇f (x)(cid:107)∗ ≤ G 

∀x ∈ K .

3 The algorithm

In this section  we present and analyze our Universal eXtra Gradient (UniXGrad) method. We ﬁrst
discuss the Mirror-Prox (MP) algorithm of [Nemirovski  2004]  and the related Optimistic Mirror
Descent (OMD) algorithm of [Rakhlin and Sridharan  2013]. Later we present our algorithm which
builds on top of the Optimistic Mirror Descent (OMD) scheme. Then in Sections 3.1 and 3.2  we
present and analyze the guarantees of our method in nonsmooth and smooth settings  respectively.
Our goal is to optimize a convex function f over a compact domain K  and Algorithm 1 offers
a framework for solving this template  which is inspired by the Mirror-Prox (MP) algorithm of
[Nemirovski  2004] and the Optimistic Mirror Descent (OMD) algorithm of [Rakhlin and Sridharan 
2013]. Let us motivate this particular template. Basically  the algorithm takes a step from yt−1 to
xt  using ﬁrst order information based on yt−1. Then  it goes back to yt−1 and takes another step 
but this time  gradient information relies on xt. Each step is a generalized projection with respect to
Bregman divergence DR(· ·).

Algorithm 1 Mirror-Prox Template
Input: Number of iterations T   y0 ∈ K  learning rate {ηt}t∈[T ]
1: for t = 1  ...  T do
2:

DR(x  yt−1)

xt = arg min

x∈K (cid:104)x  Mt(cid:105) + 1
y∈K (cid:104)y  gt(cid:105) + 1

ηt

ηt
DR(y  yt−1)

3:

yt = arg min

4: end for

2  instead of general Bregman divergences.

Now  let us explain the salient differences between UniXGrad and MP as well as OMD using the
particular choices of Mt  gt and the distance-generating function R.
Optimistic Mirror Descent takes gt = ∇f (xt) and computes Mt = ∇f (xt−1)  i.e.  based on gradient
information from previous iterates. This vector is available at the beginning of each iteration and
the “optimism” arises in the case where Mt ≈ gt. When Mt = ∇f (yt−1) and gt = ∇f (xt) 
the template is known as the famous Mirror-Prox algorithm. One special case of Mirror-Prox is
Extra-Gradient scheme [Korpelevich  1976] where the projections are with respect to Euclidean norm 
i.e. R(x) = 1/2(cid:107)x(cid:107)2
MP has been well-studied  especially in the context of variational inequalities and convex-concave
saddle point problems. It achieves fast convergence rate of O(1/T ) for this class of problems 
however  in the context of smooth convex optimization  this is the standard slow rate [Nesterov  2003].
To date  MP is not known to enjoy the accelerated rate of O(1/T 2) for smooth convex minimization.
We propose three modiﬁcations to this template  which are the precise choice of gt and Mt  the
adaptive learning rate and the gradient weighting scheme.
The notion of averaging:
In different interpretations of acceleration [Nesterov  1983  Tseng  2008 
Allen Zhu and Orecchia  2014]  the notion of averaging is always central and we incorporate this
notion via gradients taken at weighted average of iterates. Let us deﬁne the weight αt = t and the
following quantities

αtxt +(cid:80)t−1
(cid:80)t

i=1 αi

i=1 αixi

¯xt =

αtyt−1 +(cid:80)t−1

(cid:80)t

i=1 αi

i=1 αixi

.

(6)

 

˜zt =

3

Then  UniXGrad algorithm takes gt = ∇f (¯xt) and Mt = ∇f (˜zt)  which provides a naive interpre-
tation of averaging. Our choice of gt and Mt coincide with that of the accelerated Extra-Gradient
scheme of Diakonikolas and Orecchia [2017]. While their decision relies on implicit Euler discretiza-
tion of an accelerated dynamics  we arrive at the same conclusion as a direct consequence of our
convergence analysis.
Adaptive learning rate: A key ingredients of our algorithm is the choice of adaptive learning rate
ηt. In light of Rakhlin and Sridharan [2013]  we deﬁne our lag-one-behind learning rate as

(cid:115)

ηt =

t−1(cid:80)

i=1

1 +

2D
i (cid:107)gi − Mi(cid:107)2∗
α2

 

(7)

where D2 = supx y∈K DR(x  y) is the diameter of the compact set K with respect to Bregman
divergences. Algorithm 2 summarizes our framework.
Gradient weighting scheme: We introduce the weights αt in the sequence updates. One can
interpret this as separating step size into learning rate and the scaling factors. It is necessary that
αt = Θ(t) in order to achieve optimal rates  in fact we precisely choose αt = t. Also notice that they
appear in the learning rate  compatible with the update rule.

Algorithm 2 UniXGrad
Input: # of iterations T   y0 ∈ K  diameter D  weight αt = t  learning rate {ηt}t∈[T ]
1: for t = 1  ...  T do
2:

DR(x  yt−1)

xt = arg min

x∈K αt (cid:104)x  Mt(cid:105) + 1
y∈K αt (cid:104)y  gt(cid:105) + 1

ηt

ηt
DR(y  yt−1)

(Mt = ∇f (˜zt))
(gt = ∇f (¯xt))

3:

yt = arg min

4: end for
5: return ¯xT

In the remainder of this section  we will present our convergence theorems and provide proof sketches
to emphasize the fundamental aspects and novelties. With the purpose of simplifying the analysis  we
borrow classical tools in the online learning literature and perform the convergence analysis in the
sense of bounding “weighted regret”. Then  we use a simple yet essential conversion strategy which
enables us to directly translate our weighted regret bounds to convergence rates. Before we proceed 
we will present the conversion scheme from weighted regret to convergence rate  by deferring
the proof to Appendix. In a concurrent work  [Cutkosky  2019] proves a similar online-to-ofﬂine
conversion bound.
t=1 αt (cid:104)xt − x∗  gt(cid:105) denote
the weighted regret after T iterations  αt = t and gt = ∇f (¯xt). Then 

Lemma 1. Consider weighted average ¯xt as in Eq. (6). Let RT (x∗) =(cid:80)T

f (¯xT ) − f (x∗) ≤ 2RT (x∗)

T 2

.

3.1 Non-smooth setting
Deterministic setting: First  we will focus on the convergence analysis in the case of non-smooth
objective functions with deterministic/stochastic ﬁrst-order oracles. We will follow the regret analysis
as in Rakhlin and Sridharan [2013] with essential adjustments that suit our weighted scheme and
particular choice of adaptive learning rate.
Remark 1. It is important to point out that we do not completely exploit the precise deﬁnitions of gt
and Mt in the presence of non-smooth objectives. As far as the regret analysis is concerned  it sufﬁces
that these quantities are functions of ∇f (·) and that  as a corollary  their dual norm is upper bounded.
However  in order to bridge the gap between weighted regret and the objective sub-optimality  i.e.
f (¯xT ) − f (x∗)  we require gt = ∇f (¯xt).
Now  we can exhibit our convergence bounds for the case of deterministic oracles.

4

Theorem 1. Consider the constrained optimization setting in Problem (4)  where f : K → R is a
proper  convex and G-Lipschitz function deﬁned over compact  convex set K. Let x∗ ∈ minx∈K f (x).
Then  Algorithm 2 guarantees

f (¯xT ) − min

x∈K f (x) ≤ 7D

t (cid:107)gt − Mt(cid:107)2∗ − D
t=1 α2
T 2

≤ 6D

T 2 +

14GD√
T

.

(8)

(cid:113)
1 +(cid:80)T

We establish the basis of our analysis through Lemma 1 and Corollary 2 of Rakhlin and Sridharan
[2013]. Then  we build upon this base by exploiting the structure of the adaptive learning rate  the
weights αt and the bound on gradient norms to give adaptive convergence bounds.
Stochastic setting: Now  we further consider the case of stochastic gradients. We assume that the
ﬁrst-order oracles are unbiased (see Eq. (5)). We want to emphasize that our stochastic setting is not
restricted to the notion of additive noise  i.e. gradients corrupted with zero-mean noise. It essentially
includes any estimate that recovers the full gradient in expectation  e.g. estimating gradient using
mini batches. Additionally  we propagate the bounded gradient norm assumption to the stochastic
oracles  such that (cid:107) ˜∇f (x)(cid:107)∗ ≤ G  ∀x ∈ K.
Theorem 2. Consider the optimization setting in Problem (4)  where f is non-smooth  convex and
G-Lipschitz. Let {xt}t=1 .. T be a sequence generated by Algorithm 2 such that gt = ˜∇f (¯xt) and
Mt = ˜∇f (˜zt). With αt = t and learning rate as in Eq. (7)  it holds that

E [f (¯xT )] − min

x∈K f (x) ≤ 6D
T 2 +

14GD√
T

.

The analysis in the stochastic setting is similar to deterministic setting. The difference is up to
replacing gt ↔ ˜gt and Mt ↔ ˜Mt. With the bound on stochastic gradients  the same rate is achieved.

3.2 Smooth setting

Deterministic setting:
In terms of theoretical contributions and novelty  the case of L-smooth
objective is of greater interest. We will ﬁrst start with the deterministic oracle scheme and then
introduce the convergence theorem for the noisy setting.
Theorem 3. Consider the constrained optimization setting in Problem (4)  where f : K → R is a
proper  convex and L-smooth function deﬁned over compact  convex set K. Let x∗ ∈ minx∈K f (x).
Then  Algorithm 2 ensures the following

f (¯xT ) − min

x∈K f (x) ≤ 20

7D2L
T 2

.

(9)

Remark 2. In the non-smooth setting  we assume that gradients have bounded norms. Our algorithm
does not need to know this information  but it is necessary for the analysis in that case. However 
when the function is smooth  neither the algorithm nor the analysis requires bounded gradients.

Proof Sketch (Theorem 3). We follow the proof of Theorem 1 until the point where we obtain

T(cid:88)

t=1

T(cid:88)

ηt+1α2

αt (cid:104)xt − x∗  gt(cid:105) ≤ 1
2

1
η1
By smoothness of the objective function  we have (cid:107)gt − Mt(cid:107)∗ ≤ L(cid:107)¯xt − ˜zt(cid:107)  which implies
− 1

t (cid:107)gt − Mt(cid:107)2∗ − 1
2

(cid:107)xt − yt−1(cid:107)2 + D2

(cid:107)xt − yt−1(cid:107)2 ≤ − α2

(cid:107)gt − Mt(cid:107)2∗. Hence 

ηT +1

ηt+1

t=1

t=1

+

1

t

4L2ηt+1

ηt+1

(cid:18) 3

(cid:19)

.

√

T(cid:88)

(cid:18)

T(cid:88)

t=1

≤ 1
2

(cid:110)

(cid:19)

4L2ηt+1

≤ 7L2(cid:111)

ηt+1 −

1

t (cid:107)gt − Mt(cid:107)2∗ + D2
α2

(cid:18) 3

+

1
η1

ηT +1

(cid:19)

.

Now we will introduce a time variable to characterize the growth of the learning rate. Deﬁne
τ∗ = max

such that ∀t > τ∗  ηt+1 −

t ∈ {1  ...  T} :

≤ − 3

1

4 ηt+1. Then 

1
η2

4L2ηt+1

t+1

5

t=1

τ∗(cid:88)
(cid:113)
1 +(cid:80)t
(cid:118)(cid:117)(cid:117)(cid:116)1 +

T(cid:88)

≤ D

(cid:124)

+

3D
2

(cid:124)

+

(A)

D
2

t (cid:107)gt − Mt(cid:107)2∗
α2
(cid:123)(cid:122)
(cid:125)
i (cid:107)gi − Mi(cid:107)2∗
i=1 α2
t (cid:107)gt − Mt(cid:107)2∗ − T(cid:88)
(cid:123)(cid:122)

α2

t=1

t=τ∗+1

(B)

(cid:113)

1 +(cid:80)t

t (cid:107)gt − Mt(cid:107)2∗
α2
i=1 α2

i (cid:107)gi − Mi(cid:107)2∗


(cid:125)

 

where we wrote ηt+1 in open form and used the deﬁnition of τ∗. To complete the proof  we will need
the following lemma.
Lemma 2. Let {ai}i=1 ... n be a sequence of non negative numbers. Then  it holds that

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

ai ≤ n(cid:88)

i=1

i=1

ai(cid:113)(cid:80)i

j=1 aj

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

≤ 2

ai.

√
Please refer to [McMahan and Streeter  2010  Levy et al.  2018] for the proof. We jointly use Lemma 2
and the bound on ητ∗+1 to upper bound terms (A) and (B) with 4
7D2L  respectively.
Lemma 1 immediately establishes the convergence bound.

√
7D2L and 6

Stochastic setting: Next  we will present our results for the stochastic extension. In addition to
unbiasedness and boundedness  we will introduce another classical assumption: bounded variance 

E[(cid:107)∇f (x) − ˜∇f (x)(cid:107)2∗|x] ≤ σ2 

∀x ∈ K.

(10)

The analysis proceeds along similar lines as its deterministic counterpart. However  we execute the
analysis using auxiliary terms and attain the optimal accelerated rate without the log factors.
Theorem 4. Consider the optimization setting in Problem (4)  where f is L-smooth and convex. Let
{xt}t=1 .. T be a sequence generated by Algorithm 2 such that gt = ˜∇f (¯xt) and Mt = ˜∇f (˜zt).
With αt = t and learning rate as in (7)  it holds that
x∈K f (x) ≤ 224

E [f (¯xT )] − min

√
2σD√
14
T

14D2L
T 2

√

+

.

Proof Sketch (Theorem 4). We start in the same spirit as the stochastic  non-smooth setting 

T(cid:88)
(cid:124)

t=1

(cid:125)

αt (cid:104)xt − x∗  ˜gt(cid:105)

+

αt (cid:104)xt − x∗  gt − ˜gt(cid:105)

.

(cid:123)(cid:122)

(B)

(cid:125)

T(cid:88)

t=1

αt (cid:104)xt − x∗  gt(cid:105) ≤ T(cid:88)
(cid:124)
(cid:118)(cid:117)(cid:117)(cid:116)1 +

t=1

(cid:123)(cid:122)

(A)

T(cid:88)

t=1

T(cid:88)

t=1

Recall that term (B) is zero in expectation given ¯xt. Then  we follow the proof steps of Theorem 1 

αt (cid:104)xt − x∗  gt(cid:105) ≤ 7D
2

t(cid:107)˜gt − ˜Mt(cid:107)2∗ − 1
α2
2

(cid:107)xt − yt−1(cid:107)2 .

1

ηt+1

(11)

T(cid:88)

t=1

We will obtain (cid:107)gt − Mt(cid:107)2∗ from (cid:107)xt − yt−1(cid:107)2 due to smoothness and the challenge is to handle
(cid:107)˜gt − ˜Mt(cid:107)2∗ and (cid:107)gt − Mt(cid:107)2∗ together. So let’s denote  B2
t := min{(cid:107)gt − Mt(cid:107)2∗ (cid:107)˜gt − ˜Mt(cid:107)2∗}. Using
this deﬁnition  we could declare an auxiliary learning rate which we will only use for the analysis 

6

(cid:115)

˜ηt =

2D

t−1(cid:80)

i=1

.

i B2
α2
i

1 +

Clearly  for any t ∈ [T ] we have − 1

ηt+1

(cid:107)gt − Mt(cid:107)2∗ ≤ − 1

˜ηt+1

B2

t . Also  we can write 

(cid:107)˜gt − ˜Mt(cid:107)2∗ ≤ 2(cid:107)gt − Mt(cid:107)2∗ + 2(cid:107)ξt(cid:107)2∗ 

and 

Therefore  we could rewrite Eq. (11) as 

T(cid:88)

t=1

αt (cid:104)xt − x∗  gt(cid:105) ≤ 7
2

(cid:124)

t=1

(cid:107)˜gt − ˜Mt(cid:107)2∗ ≤ 2B2
(cid:18)
T(cid:88)

t + 2(cid:107)ξt(cid:107)2∗.
(cid:19)

˜ηt+1 −

α2
t B2

t +

28L2 ˜ηt+1

1

(cid:123)(cid:122)

(A)

(cid:118)(cid:117)(cid:117)(cid:116) T(cid:88)
(cid:123)(cid:122)

t=1

(B)

t (cid:107)ξt(cid:107)2∗
(cid:125)
α2

+

7D
2

(cid:125)

7D√
2

(cid:124)

(12)

(13)

.

Using Lemma 2 and deﬁning a time variable τ∗ in the sense of Theorem 3 (with correct constants) 
√
14D2L. By taking expectation conditioned on ¯xt and using
term (A) is upper bounded by 112
√
Jensen’s inequality  we could upper bound term (B) as 14σDT 3/2/
2  which leads us to the optimal
rate of 224
2σD/

T through Lemma 1.

14D2L/T 2 + 14

√

√

√

4 Experiments

We compare performance of our algorithm for two different tasks against adaptive methods of various
characteristics  such as AdaGrad  AMSGrad and AcceleGrad  along with a recent non-adaptive
method AXGD. We consider a synthetic setting where we analyze the convergence behavior  as
well as a SVM classiﬁcation task on some LIBSVM dataset. In all the setups  we tuned the hyper-
parameters of each algorithm by grid search. In order to compare the adaptive methods on equal
grounds  AdaGrad is implemented with a scalar step size based on the template given by Levy [2017].
We implement AMSGrad exactly as it is described by Reddi et al. [2018].

4.1 Convergence behavior

We take the least squares problem with L2-norm ball constraint
i.e. 
2  where A ∈ Rn×d  A ∼ N (0  σ2I) and b = Ax(cid:92) +  such that  is a
min(cid:107)x(cid:107)2<r
random vector ∼ N (0  10−3). We pick n = 500 and d = 100. For the rest of this section  we refer
to the solution of constrained problem as x∗.

2n (cid:107)Ax − b(cid:107)2

this setting 

for

1

(a) Average Iterate

(b) Last Iterate

Figure 1: Convergence rates in the deterministic oracle setting when x∗ ∈ Boundary(K)

7

10010110210310410510-1010-810-610-410-210010210410010110210310410510-1510-1010-5100105(a) Average Iterate

(b) Last Iterate

Figure 2: Convergence rates in the stochastic oracle setting when x∗ ∈ Boundary(K)

In Figure 1 and 2  we present the convergence rates under deterministic and stochastic oracles  and we
pick a problem in which the solution is on the boundary of the constraint set  i.e.  x∗ ∈ Boundary(K).
In this setting  our algorithm shows matching performance in comparison with other methods. AXGD
has convergence issues in the stochastic setting  as it only handles additive noise and their step size
routine does not seem to be compatible with stochastic gradients. Another key observation is that
AMSGrad suffers a decrease in its performance when the solution is on the boundary of the set.

4.2 SVM classiﬁcation

In this section  we will tackle SVM classiﬁcation problem on “breast-cancer” data set taken from
LIBSVM. We try to minimize squared Hinge loss with L2 norm regularization. We split the data set
as training and test sets with 80/20 ratio. The models are trained using random mini batches of size
5. Figure 3 demonstrates convergence rates and test accuracies of the methods. They represent the
average performance of 5 runs  with random initializations. For UniXGrad  AcceleGrad and AXGD 
we consider the performance with respect to the average iterate ¯xt as it shows a more stable behavior 
whereas AdaGrad and AMSGrad are evaluated based on their last iterates. AXGD  which has poor
convergence behavior in stochastic setting due to its step size rule  shows the worst performance
both in terms of convergence and generalization. UniXGrad  AcceleGrad  AdaGrad and AMSGrad
achieve comparable generalization performances to each other. AMSGrad achieves a slightly better
performance as it has diagonal preconditioner which translates to per-coordinate learning rate. It
could possibly adapt to the geometry of the optimization landscape better.

(a) Convergence rates with respect to training data

(b) Test Accuracy

Figure 3: Convergence behavior with respect to training data and resulting test accuracies for binary
classiﬁcation task on breast-cancer dataset from LIBSVM Chang and Lin [2011]

5 Discussion and Future Work

In this paper we presented an adaptive and universal framework that achieves the optimal convergence
rates in constrained convex optimization setting. To our knowledge  this is the ﬁrst method that

8

10010110210310410510-310-210-110010110210310010110210310410510-410-310-210-110010110210310010110210310410-410-21001021040500100015002000707580859095100√
D2L/T 2 + σD/

T

(cid:17)

achieves O(cid:16)

√
GD/

(cid:17)

and O(cid:16)

T

rates in the constrained setting  without log
dependencies. Without any prior information  our algorithm adapts to smoothness of the objective
function as well as the variance of the possibly noisy gradients.
One would interpret that our guarantees are extensions of [Levy et al.  2018] to the constrained
setting  through a completely different algorithm and a simpler  classical analysis. Our study of their
algorithm and proof strategies concludes that:

• It does not seem possible to remove log T dependency in non-smooth setting for their
• Extending their algorithm to constrained setting (via projecting y sequence) is not trivial  as

algorithm  due to their Lemma A.3

the analysis requires y sequence to be unbounded (refer to their Appendix A  Eq. (16)).

As a follow up to our work  we would like to investigate three main extensions:

• Proximal version of our algorithm that could handle composite problems with nonsmooth
terms  including indicator functions  in a uniﬁed manner. It seems like a rather simple
extension as the main difference would be replacing optimality condition for constrained
updates with that of proximal operator.
• Extending scalar adaptive learning rate to per-coordinate matrix-like preconditioner. This
direction of research would help us create a robust algorithm that is applicable to non-convex
problems  such as training deep neural networks.
• Adaptation to strong convexity along with smoothness and noise variance  simultaneously. A
ﬁrst step towards tackling this open problem is proving an improved rate of O(1/T 2 + σ/T )
for smooth and strongly convex problems  with stochastic gradients.

Acknowledgment

AK and VC are supported by the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement no 725594 - time-data) and the
Swiss National Science Foundation (SNSF) under grant number 200021_178865 / 1.

References
Z. Allen Zhu and L. Orecchia. A novel  simple interpretation of nesterov’s accelerated method as a

combination of gradient and mirror descent. CoRR  abs/1407.1537  2014.

F. Bach and K. Y. Levy. A universal algorithm for variational inequalities adaptive to smoothness and

noise. arXiv preprint arXiv:1902.01637  2019.

C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on

Intelligent Systems and Technology  2:27:1–27:27  2011.

M. B. Cohen  J. Diakonikolas  and L. Orecchia. On acceleration with noise-corrupted gradients.

arXiv preprint arXiv:1805.12591  2018.

A. Cutkosky. Anytime online-to-batch conversions  optimism  and acceleration. the International

Conference on Machine Learning (ICML)  June 2019.

Q. Deng  Y. Cheng  and G. Lan. Optimal adaptive and accelerated stochastic gradient descent. arXiv

preprint arXiv:1810.00553  2018.

J. Diakonikolas and L. Orecchia. Accelerated extra-gradient descent: A novel accelerated ﬁrst-order

method. arXiv preprint arXiv:1706.04680  2017.

J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of Machine Learning Research  12(Jul):2121–2159  2011.

C. Hu  W. Pan  and J. T. Kwok. Accelerated gradient methods for stochastic optimization and online

learning. In Advances in Neural Information Processing Systems  pages 781–789  2009.

9

D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 

2014.

G. M. Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Matecon 

12:747–756  1976.

G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming  133

(1-2):365–397  2012.

K. Levy. Online to ofﬂine conversions  universality and adaptive minibatch sizes. In Advances in

Neural Information Processing Systems  pages 1612–1621  2017.

K. Y. Levy  A. Yurtsever  and V. Cevher. Online adaptive methods  universality and acceleration. In

Neural and Information Processing Systems (NeurIPS)  December 2018.

H. B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. COLT

2010  page 244  2010.

A. Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal
on Optimization  15(1):229–251  2004.

Y. Nesterov. A method for solving the convex programming problem with convergence rate o(1/k2).

Dokl. Akad. Nauk SSSR  269:543–547  1983.

Y. Nesterov. Introductory lectures on convex optimization. 2004  2003.

S. Rakhlin and K. Sridharan. Optimization  learning  and games with predictable sequences. In

Advances in Neural Information Processing Systems  pages 3066–3074  2013.

S. J. Reddi  S. Kale  and S. Kumar. On the convergence of adam and beyond. In International

Conference on Learning Representations  2018.

P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted to

SIAM Journal on Optimization  01 2008.

L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal

of Machine Learning Research  11(Oct):2543–2596  2010.

10

,Ali Kavis
Kfir Y. Levy
Francis Bach
Volkan Cevher