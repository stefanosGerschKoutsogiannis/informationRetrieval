2013,Solving inverse problem of Markov chain with partial observations,The Markov chain is a convenient tool to represent the dynamics of complex systems such as traffic and social systems  where probabilistic transition takes place between internal states. A Markov chain is characterized by initial-state probabilities and a state-transition probability matrix. In the traditional setting  a major goal is to figure out properties of a Markov chain when those probabilities are known. This paper tackles an inverse version of the problem: we find those probabilities from partial observations at a limited number of states. The observations include the frequency of visiting a state and the rate of reaching a state from another. Practical examples of this task include traffic monitoring systems in cities  where we need to infer the traffic volume on every single link on a road network from a very limited number of observation points. We formulate this task as a regularized optimization problem for probability functions  which is efficiently solved using the notion of natural gradient. Using synthetic and real-world data sets including city traffic monitoring data  we demonstrate the effectiveness of our method.,Solving inverse problem of Markov chain

with partial observations

Tetsuro Morimura
IBM Research - Tokyo

tetsuro@jp.ibm.com

Takayuki Osogami
IBM Research - Tokyo

osogami@jp.ibm.com

Tsuyoshi Id´e

IBM T.J. Watson Research Center

tide@us.ibm.com

Abstract

The Markov chain is a convenient tool to represent the dynamics of complex sys-
tems such as trafﬁc and social systems  where probabilistic transition takes place
between internal states. A Markov chain is characterized by initial-state proba-
bilities and a state-transition probability matrix. In the traditional setting  a major
goal is to study properties of a Markov chain when those probabilities are known.
This paper tackles an inverse version of the problem: we ﬁnd those probabilities
from partial observations at a limited number of states. The observations include
the frequency of visiting a state and the rate of reaching a state from another. Prac-
tical examples of this task include trafﬁc monitoring systems in cities  where we
need to infer the trafﬁc volume on single link on a road network from a limited
number of observation points. We formulate this task as a regularized optimiza-
tion problem  which is efﬁciently solved using the notion of natural gradient. Us-
ing synthetic and real-world data sets including city trafﬁc monitoring data  we
demonstrate the effectiveness of our method.

1 Introduction

The Markov chain is a standard model for analyzing the dynamics of stochastic systems  including
economic systems [29]  trafﬁc systems [11]  social systems [12]  and ecosystems [6]. There is a large
body of the literature on the problem of analyzing the properties a Markov chain given its initial
distribution and a matrix of transition probabilities [21  26]. For example  there exist established
methods for analyzing the stationary distribution and the mixing time of a Markov chain [23  16].
In these traditional settings  the initial distribution and the transition-probability matrix are given a
priori or directly estimated.
Unfortunately  it is often impractical to directly measure or estimate the parameters (i.e.  the initial
distribution and the transition-probability matrix) of the Markov chain that models a particular sys-
tem under consideration. For example  one can analyze a trafﬁc system [27  24]  including how the
vehicles are distributed across a city  by modeling the dynamics of vehicles as a Markov chain [11].
It is  however  difﬁcult to directly measure the fraction of the vehicles that turns right or left at every
intersection.
The inverse problem of a Markov chain that we address in this paper is an inverse version of the
traditional problem of analyzing a Markov chain with given input parameters. Namely  our goal is
to estimate the parameters of a Markov chain from partial observations of the corresponding system.
In the context of the trafﬁc system  for example  we seek to ﬁnd the parameters of a Markov chain 
given the trafﬁc volumes at stationary observation points and/or the rate of vehicles moving between

1

Figure 1: An inverse Markov chain problem. The trafﬁc volume on every road is inferred from
trafﬁc volumes at limited observation points and/or the rates of vehicles transitioning between these
points.

these points. Such statistics can be reliably estimated from observations with web-cameras [27] 
automatic number plate recognition devices [10]  or radio-frequency identiﬁcation (RFID) [25] 
whose availability is however limited to a small number of observation points in general (see Figure
1). By estimating the parameters of a Markov chain and analyzing its stationary probability  one can
infer the trafﬁc volumes at unobserved points.
The primary contribution of this paper is the ﬁrst methodology for solving the inverse problem of
a Markov chain when only the observation at a limited number of stationary observation points are
given. Speciﬁcally  we assume that the frequency of visiting a state and/or the rate of reaching a
state from another are given for a small number of states. We formulate the inverse problem of a
Markov chain as a regularized optimization problem. Then we can efﬁciently ﬁnd a solution to the
inverse problem of a Markov chain based on the notion of natural gradient [3].
The inverse problem of a Markov chain has been addressed in the literature [9  28  31]  but the
existing methods assume that sample paths of the Markov chain are available. Related work of
inverse reinforcement learning [20  1  32] also assumes that sample paths are available.
In the
context of the trafﬁc system  the sample paths corresponds to probe-car data (i.e.  sequence of GPS
points). However  the probe-car data is expensive and rarely available in public. Even when it is
available  it is often limited to vehicles of a particular type such as taxis or in a particular region. On
the other hand  stationary observation data is often less expensive and more obtainable. For instance 
web-camera images are available even in developing countries such as Kenya [2].
The rest of this paper is organized as follows. In Section 2  preliminaries are introduced. In Section
3  we formulate an inverse problem of a Markov chain as a regularized optimization problem. A
method for efﬁciently solving the inverse problem of a Markov chain is proposed in Section 4. An
example of implementation is provided in Section 5. Section 6 evaluates the proposed method with
both artiﬁcial and real-world data sets including the one from trafﬁc monitoring in a city.

2 Preliminaries

A discrete-time Markov chain [26; 21] is a stochastic process  X = (X0; X1; : : : )  where Xt
is a random variable representing the state at time t 2 Z(cid:21)0. A Markov chain is deﬁned by
the triplet fX ; pI; pTg  where X = f1; : : : ;jXjg is a ﬁnite set of states  where jXj (cid:21) 2 is
the number of states. The function  pI : X ! [0; 1]  speciﬁes the initial-state probability  i.e. 
pI(x) ≜ Pr(X0 = x)  and pT : X (cid:2) X ! [0; 1] speciﬁes the state transition probability from x to
′ j Xt = x); 8t 2 Z(cid:21)0. Note the state transition is conditionally
′  i.e.  pT(x
x
independent of the past states given the current state  which is called the Markov property.
Any Markov chain can be converted into another Markov chain  called a Markov chain with restart 
by modifying the transition probability. There  the initial-state probability stays unchanged  but the
state transition probability is modiﬁed into p such that

′ j x) ≜ Pr(Xt+1 = x

′ j x) ≜ (cid:12)pT(x

′ j x) + (1 (cid:0) (cid:12))pI(x
′

p(x

(1)
where (cid:12) 2 [0; 1) is a continuation rate of the Markov chain1. In the limit of (cid:12) ! 1  this Markov
chain with restart is equivalent to the original Markov chain. In the following  we refer to p as the
(total) transition probability  while pT as a partial transition (or p-transition) probability.

);

1The rate (cid:12) can depend on the current state x so that (cid:12) can be replaced with (cid:12)(x) throughout the paper. For

readability  we assume (cid:12) is a constant.

2

(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:1)(cid:10)(cid:8)(cid:5)(cid:1)(cid:10)(cid:13)(cid:9)(cid:14)(cid:14)(cid:3)(cid:15)(cid:1)(cid:4)(cid:16)(cid:17)(cid:18)(cid:19)(cid:5)(cid:1)(cid:20)(cid:21)(cid:16)(cid:6)(cid:1)(cid:5)(cid:9)(cid:15)(cid:8)(cid:1)(cid:18)(cid:6)(cid:16)(cid:22)(cid:12)(cid:5)(cid:13)(cid:4)(cid:5)(cid:23)(cid:1)(cid:17)(cid:3)(cid:6)(cid:24)(cid:25)Our main targeted applications are (massive) multi-agent systems such as trafﬁc systems. So  restart-
ing a chain means that an agent’s origin of a trip is decided by the initial distribution  and the trip
ends at each time-step with probability 1 (cid:0) (cid:12).
We model the initial probability and p-transition probability with parameters (cid:23) 2 Rd1 and ! 2 Rd2 
respectively  where d1 and d2 are the numbers of those parameters. So we will denote those as pI(cid:23)
and pT!  respectively  and the total transition probability as p(cid:18)  where (cid:18) is the total model parameter 
(cid:0)1((cid:12)) with the inverse of sigmoid function
⊤
(cid:18) ≜ [(cid:23)
(cid:0)1. That is  Eq. (1) is rewritten as
&

⊤ 2 Rd where d = d1 +d2 +1 and ~(cid:12) ≜ &

⊤
; !

; ~(cid:12)]

′ j x) ≜ (cid:12)pT!(x

p(cid:18)(x

′ j x) + (1 (cid:0) (cid:12))pI(cid:23)(x
′

):

(2)

The Markov chain with restart can be represented as M((cid:18)) ≜ fX ; pI(cid:23); pT!; (cid:12)g.
Also we make the following assumptions that are standard for the study of Markov chains and their
variants [26  7].
Assumption 1 The Markov chain M((cid:18)) for any (cid:18) 2 Rd is ergodic (irreducible and aperiodic).
Assumption 2 The initial probability pI(cid:23) and p-transition probability pT! are differentiable every-
where with respect to (cid:18) 2 Rd.2
Under Assumption 1  there exists a unique stationary probability  (cid:25)(cid:18)((cid:1))  which satisﬁes the balance
equation:
′
(3)
(cid:25)(cid:18)(x
This stationary probability is equal to the limiting distribution and independent of the initial state:
′ j X0 = x; M((cid:18))); 8x2X . Assumption 2 indicates that the transition
′
(cid:25)(cid:18)(x
) 2 X (cid:2) X with respect to any (cid:18) 2 Rd.
probability p(cid:18) is also differentiable for any state pair (x; x
Finally we deﬁne hitting probabilities for a Markov chain of indeﬁnite-horizon. The Markov chain
is represented as ~M((cid:18)) = fX ; pT!; (cid:12)g  which evolves according to the p-transition probability pT! 
not to p(cid:18)  and terminates with a probability 1 (cid:0) (cid:12) at every step. The hitting probability of a state x
′
given x is deﬁned as

) = limt!1 Pr(Xt = x

′ j x)(cid:25)(cid:18)(x); 8x

′ 2 X ;

x2X p(x

∑

) =

′

′ j x) ≜ Pr(x

′ 2 ~X j X0 = x; ~M((cid:18)));

h(cid:18)(x

(4)

where ~X = ( ~X0; : : : ; ~XT ) is a sample path of ~M((cid:18)) until the stopping time  T .

3 Inverse Markov Chain Problem

Here we formulate an inverse problem of the Markov chain M((cid:18)). In the inverse problem  the model
family M 2 fM((cid:18))j (cid:18) 2 Rdg  which may be subject to a transition structure as in the road network 
is known or given a priori  but the model parameter (cid:18) is unknown. In Section 3.1  we deﬁne inputs
of the problem  which are associated with functions of the Markov chain. Objective functions for
the inverse problem are discussed in Section 3.2.

3.1 Problem setting

The input and output of our inverse problem of the Markov chain is as follows.
(cid:15) Inputs are the values measured at a portion of states x 2 Xo  where Xo (cid:26) X and usually jXoj ≪
jXj. The measured values include the frequency of visiting a state  f (x); x 2 Xo. In addition 
) 2 Xo (cid:2) Xo 
the rate of reaching a state from another  g(x; x
where g(x; x) is equal to 1.
In the context of trafﬁc monitoring  f (x) denotes the number of vehicles that went through an
′ in this
observation point  x; g(x; x
order divided by f (x).
(cid:15) Output is the estimated parameter (cid:18) of the Markov chain M((cid:18))  which speciﬁes the total-

) denotes the number of vehicles that went through x and x

)  might also be given for (x; x

′

′

′

transition probability function p(cid:18) in Eq. (2).

2We assume @
@(cid:18)i

log pI(cid:23) (x) = 0 when pI(cid:23) (x) = 0  and an analogous assumption applies to pT!.

3

The ﬁrst step of our formulation is to relate f and g to the Markov chain. Speciﬁcally  we assume
that the observed f is proportional to the true stationary probability of the Markov chain:

(5)
where c is an unknown constant to satisfy the normalization condition. We further assume that the
observed reaching rate is equal to the true hitting probability of the Markov chain:

(x) = cf (x);

(cid:25)

x 2 Xo;

(cid:3)

(cid:3)
h

′ j x) = g(x; x
(x

′

);

(x; x

′

) 2 Xo (cid:2) Xo:

3.2 Objective function

Our objective is to ﬁnd the parameter (cid:18)
Eqs. (5) and (6). We use the following objective function to be minimized 

(cid:3) such that (cid:25)(cid:18)(cid:3) and h(cid:18)(cid:3) well approximate (cid:25)

L((cid:18)) ≜ (cid:13)Ld((cid:18)) + (1 (cid:0) (cid:13))Lh((cid:18)) + (cid:21)R((cid:18));

(7)
(cid:3) and h
(cid:3) 
where Ld and Lh are cost functions with respect to the quality of the approximation of (cid:25)
respectively. These are speciﬁed in the following subsections. The function R((cid:18)) is the regular-
2 or jj(cid:18)jj1. The parameters (cid:13) 2 [0; 1] and (cid:21) (cid:21) 0 balance these cost
ization term of (cid:18)  such as jj(cid:18)jj2
functions and the regularization term  which will be optimized by cross-validation. Altogether  our
problem is to ﬁnd the parameter  (cid:18)

= arg min(cid:18)2Rd L((cid:18)):

(cid:3)

(6)

(cid:3) and h

(cid:3) in

3.2.1 Cost function for stationary probability function

∑

∑

∑

(cid:3)

x2Xo

Because the constant c in Eq. (5) is unknown  for example  we cannot minimize a squared error
(x) (cid:0) (cid:25)(cid:18)(x))2. Thus  we need to derive an alternative cost function of (cid:25)(cid:18) that is
such as
((cid:25)
independent of c.
For Ld((cid:18))  one natural choice might be a Kullback-Leibler (KL) divergence 

d ((cid:18)) ≜
LKL

(cid:3)

(cid:25)

(x) log

(cid:3)

(x)
(cid:25)
(cid:25)(cid:18)(x)

= (cid:0)c

f (x) log (cid:25)(cid:18)(x) + o;

x2Xo

∑

x2Xo
where o is a term independent of (cid:18). The minimizer of LKL
minimization of LKL
∑
); 8x; x
increasing
that  because of
effect of overvaluing

d will lead to a biased estimate. This is because LKL
(cid:25)(cid:18)(x) when the ratios (cid:25)(cid:18)(x)=(cid:25)(cid:18)(x
x2Xo

x2(XnXo)(cid:25)(cid:18)(x) = 1; minimizing LKL

(cid:25)(cid:18)(x) and undervaluing

d ((cid:18)) is independent of c. However 
d will be decreased by
′ 2 Xo are unchanged. This implies
has an unwanted side-

∑

∑

x2Xo

(cid:25)(cid:18)(x) +
x2Xo

x2(XnXo)(cid:25)(cid:18)(x).

d

′

Here we propose an alternative form of Ld that can avoid this side-effect. It uses a logarithmic ratio
of the stationary probabilities such that

∑
∑

(

∑

)2

∑

∑

(

Ld((cid:18)) ≜ 1
2

log

i2Xo

j2Xo

(cid:3)
(cid:25)
(i)
(cid:25)(cid:3)(j)

(cid:0) log

(cid:25)(cid:18)(i)
(cid:25)(cid:18)(j)

=

1
2

log

i2Xo

j2Xo

(cid:0) log

f (i)
f (j)

(cid:25)(cid:18)(i)
(cid:25)(cid:18)(j)

(8)

(cid:3)

The log-ratio of probabilities represents difference of information contents between these probabil-
ities in the sense of information theory [17]. Thus this function can be regarded as a sum of squared
(x) and (cid:25)(cid:18)(x) over x 2 Xo with respect to relative information contents. In a
error between (cid:25)
different point of view  Eq. (8) follows from maximizing the likelihood of (cid:18) under the assumption
that the observation “log f (i) (cid:0) log f (j)” has a Gaussian white noise N (0; ϵ2). This assumption
p
is satisﬁed when f (i) has a log-normal distribution  LN ((cid:22)i; (ϵ=
2)2)  independently for each i 
where (cid:22)i is the true location parameter  and the median of f (i) is equal to e(cid:22)i.

3.2.2 Cost function for hitting probability function

)2

Unlike Ld((cid:18))  there are several options for Lh((cid:18)). Examples of this cost function include a mean
squared error and mean absolute error. Here we use the following standard squared errors in the log
space  based on Eq. (6) 

log g(i; j) (cid:0) log h(cid:18)(j j i)

:

(9)

∑

∑

(

i2Xo

j2Xo

Lh((cid:18)) ≜ 1
2

)2

Eq. (9) follows from maximizing the likelihood of (cid:18) under the assumption that the observation
log g(i; j) has a Gaussian white noise  as with the case of Ld((cid:18)).

4

4 Gradient-based Approach

(cid:0)1
(cid:18)t

Let us consider (local) minimization of the objective function L((cid:18)) in Eq. (7). We adopt a gradient-
descent approach for the problem  where the parameter (cid:18) is optimized by the following iteration 
with the notation ∇(cid:18)L((cid:18)) ≜ [@L((cid:18))=@(cid:18)1; : : : ; @L((cid:18))=@(cid:18)d]
⊤ 

(cid:18)t+1 = (cid:18)t (cid:0) (cid:17)tG

f(cid:13)∇(cid:18)Ld((cid:18)t) + (1 (cid:0) (cid:13))∇(cid:18)Lh((cid:18)t) + (cid:21)∇(cid:18)R((cid:18)t)g ;

(10)
2 Rd(cid:2)d  called the metric of the parameter (cid:18)  is
where (cid:17)t > 0 is an updating rate. The matrix G(cid:18)t
an arbitrary bounded positive deﬁnite matrix. When G(cid:18)t is set to the identity matrix of size d  Id 
the update formula in Eq. (10) becomes an ordinary gradient descent. However  since the tangent
space at a point of a manifold representing M((cid:18)) is generally different from an orthonormal space
with respect to (cid:18) [4]  one can apply the idea of natural gradient [3] to the metric G(cid:18)  expecting to
make the procedure more efﬁcient. This is described in Section 4.1.
The gradients of Ld and Lh in Eq. (10) are given as

(
(

∑
∑

i2Xo

∑
∑

j2Xo

i2Xo

j2Xo

∇(cid:18)Ld((cid:18)) =
∇(cid:18)Lh((cid:18)) =

log

(cid:0) log

f (i)
f (j)

(cid:25)(cid:18)(i)
(cid:25)(cid:18)(j)

log g(i; j) (cid:0) log h(cid:18)(j j i)

)
∇(cid:18) log (cid:25)(cid:18)(j) (cid:0) ∇(cid:18) log (cid:25)(cid:18)(i)

)(
)∇(cid:18) log h(cid:18)(j j i):

;

In order to implement the update rule of Eq. (10)  we need to compute the gradient of the logarithmic
stationary probability ∇(cid:18) log (cid:25)(cid:18)  the hitting probability h(cid:18)  and its gradient ∇(cid:18) h(cid:18). In Sections 4.2 
we will describe how to compute them  which will turn out to be quite non-trivial.

4.1 Natural gradient
Usually  a parametric family of Markov chains  M(cid:18) ≜ fM((cid:18)) j (cid:18) 2 Rdg  forms a manifold struc-
ture with respect to the parameter (cid:18) under information divergences such as a KL divergence  instead
of the Euclidean structure. Thus the ordinary gradient  Eq. (10) with G(cid:18) = Id  does not properly
reﬂect the differences in the sensitivities and the correlations between the elements of (cid:18). Accord-
ingly  the ordinary gradient is generally different from the steepest direction on the manifold  and
the optimization process with the ordinary gradient often becomes unstable or falls into a learning
plateau [5].
For efﬁcient learning  we consider an appropriate G(cid:18) based on the notion of the natural gradient
(NG) [5]. The NG represents the steepest descent direction of a function b((cid:18)) in a Riemannian
space3 by (cid:0)R
∇(cid:18)b((cid:18)) when the Riemannian space is deﬁned by the metric matrix R(cid:18). An appro-
information matrix (FIM):4∑
priate Riemannian metric on a statistical model  Y   having parameters  (cid:18)  is known to be its Fisher

(cid:0)1
(cid:18)

y Pr(Y = y j (cid:18))∇(cid:18) log Pr(Y = y j (cid:18))∇(cid:18) log Pr(Y = y j (cid:18))
⊤

:

′ 2 X   fully speciﬁes M((cid:18)) at the steady
In our case  the joint probability  p(cid:18)(x
state  due to the Markovian property. Thus we propose to use the following G(cid:18) in the update rule of
Eq. (10) 

′jx)(cid:25)(cid:18)(x) for x; x

∑

G(cid:18) = F(cid:18) + (cid:27)Id;

(
∇(cid:18) log (cid:25)(cid:18)(x)∇(cid:18) log (cid:25)(cid:18)(x)
⊤

′jx)(cid:25)(cid:18)(x) 

∑

x2X

(cid:25)(cid:18)(x)

where F(cid:18) is the FIM of p(cid:18)(x
F(cid:18) ≜
The second term with (cid:27) (cid:21) 0 in Eq. (11) will be needed to make G(cid:18) positive deﬁnite.
3A parameter space is a Riemannian space if the parameter (cid:18) 2 Rd is on a Riemannian manifold deﬁned
by a positive deﬁnite matrix called a Riemannian metric matrix R(cid:18) 2 Rd(cid:2)d. The squared length of a small
incremental vector ∆(cid:18) connecting (cid:18) to (cid:18) + ∆(cid:18) in a Riemannian space is given by ∥∆(cid:18)∥2
R(cid:18)∆(cid:18).
4The FIM is the unique metric matrix of the second-order Taylor expansion of the KL divergence  that is 

′jx)∇(cid:18) log p(cid:18)(x

′jx)∇(cid:18) log p(cid:18)(x

⊤
R(cid:18) = ∆(cid:18)

′jx)
⊤

x′2X

p(cid:18)(x

+

:

∑

y Pr(Y = y j (cid:18)) log

Pr(Y=yj(cid:18))

Pr(Y=yj(cid:18)+∆(cid:18))

≃ 1

2

∥∆(cid:18)∥2
F(cid:18) :

(11)

)

5

4.2 Computing the gradient
To derive an expression for computing ∇(cid:18) log (cid:25)(cid:18)  we use the following notations for a vector and
a matrix: (cid:25)(cid:18) ≜ [(cid:25)(cid:18)(1); : : : ; (cid:25)(cid:18)(jXj)]
′jx). Then the logarithmic stationary
probability gradients with respect to (cid:18)i is given by
log (cid:25)(cid:18) ≜ ∇(cid:18)ilog (cid:25)(cid:18) = Diag((cid:25)(cid:18))

⊤ and (P(cid:18))x;x′ ≜ p(cid:18)(x

(cid:0)1(Id (cid:0) P

⊤
⊤
(cid:18) + (cid:25)(cid:18)1
d )

(cid:0)1(∇(cid:18)i P

⊤
(cid:18) )(cid:25)(cid:18);

(12)

@
@(cid:18)i

∑

(cid:0)1 = limK!1

⊤
(cid:18) )(cid:25)(cid:18) + P
(cid:18) )Diag((cid:25)(cid:18))∇(cid:18)i log (cid:25)(cid:18) = (∇(cid:18)iP
⊤

⊤
(cid:18) (cid:25)(cid:18): Note that (cid:25)(cid:18) is equal to a normalized eigenvector
⊤
(cid:18) whose eigenvalue is 1. By taking a partial differential of Eq. (3) with respect to (cid:18)i 
(cid:18) Diag((cid:25)(cid:18))∇(cid:18)i log (cid:25)(cid:18) is obtained. Though we get the
⊤

where Diag(a) is a diagonal matrix whose diagonal elements consist of a vector a  log a is the
element-wise logarithm of a  and 1d denotes a column-vector of size d  whose elements are all 1.
In the remainder of this section  we prove Eq. (12) by using the following proposition.
Proposition 1 ([7]) If A 2 Rd(cid:2)d satisﬁes limK!1 AK = 0  then the inverse of (I (cid:0) A) exists 
and (I (cid:0) A)
K
k=0 Ak:
Equation (3) is rewritten as (cid:25)(cid:18) = P
of P
Diag((cid:25)(cid:18))∇(cid:18)i log (cid:25)(cid:18) = (∇(cid:18)iP
following linear simultaneous equation of ∇(cid:18)i log (cid:25)(cid:18) 
(Id (cid:0) P
⊤
(13)
(cid:18) )(cid:25)(cid:18);
the inverse of (Id(cid:0)P
(cid:18) )Diag((cid:25)(cid:18)) does not exist. It comes from the fact (Id(cid:0)P
⊤
⊤
(cid:18) )Diag((cid:25)(cid:18))1d = 0.
d Diag((cid:25)(cid:18))∇(cid:18)i log (cid:25)(cid:18) = 1
∇(cid:18)i (cid:25)(cid:18) = ∇(cid:18)i
f1
d (cid:25)(cid:18)g = 0 to Eq. (13) 
⊤
⊤
⊤
So we add a term including 1
such that (Id (cid:0) P
d )Diag((cid:25)(cid:18))∇(cid:18)ilog (cid:25)(cid:18) = (∇(cid:18)iP
(cid:18) )(cid:25)(cid:18): The inverse of (Id (cid:0) P
⊤
⊤
⊤
⊤
⊤
d
(cid:18) + (cid:25)(cid:18)1
(cid:18) + (cid:25)(cid:18)1
d )
(cid:0) (cid:25)(cid:18)1
(cid:0) (cid:25)(cid:18)1
⊤
⊤
⊤
exists  because of Proposition 1 and the fact limk!1(P
d )k = limk!1 P
d = 0:
The inverse of Diag((cid:25)(cid:18)) also exists  because (cid:25)(cid:18)(x) is positive for any x 2 X under Assumption 1.
(cid:18)
Hence we get Eq. (12).
To derive expressions for computing h(cid:18) and ∇(cid:18) log h(cid:18)  we use the following notations: h(cid:18)(x) ≜
′ j x) for
[h(cid:18)(xj 1); : : : ; h(cid:18)(xjjXj)]
p-transition probabilities in Eq. (1). The hitting probabilities and those gradients with respect to (cid:18)i
can be computed as the following closed forms 
nx
T(cid:18) )
∇(cid:18)i log h(cid:18)(x) = (cid:12) Diag(h(cid:18)(x))

⊤ for the hitting probabilities in Eq. (4) and (PT(cid:18))x;x′ ≜ pT!(x

(15)
where exjXj denotes a column-vector of size jXj  where x’th element is 1 and all of the other elements
jXj)PT(cid:18). We will derive Eqs. (14) and (15) as
are zero. The matrix P
follows. The hitting probabilities in Eq. (4) can be represented as the following recursive form 

T(cid:18) is deﬁned as (IjXj (cid:0) exjXjex⊤
nx

(cid:0)1exjXj;
(cid:0)1(IjXj (cid:0) (cid:12)P

h(cid:18)(x) = (IjXj (cid:0) (cid:12)P

nx
(cid:18) )h(cid:18)(x);

(cid:0)1(∇(cid:18)iP

nx
T(cid:18) )

⊤k
(cid:18)

(14)

{

′ j x) =

h(cid:18)(x

∑
y2X pT!(y j x) h(cid:18)(x

1
(cid:12)

′
if x
otherwise:

= x

′ j y)

This equation can be represented with the matrix notation as h(cid:18)(x) = exjXj + (cid:12)P
the inverse of (IjXj (cid:0) (cid:12)P
In a similar way  one can prove Eq. (15).

nx
T(cid:18) ) exists by Proposition 1 and limk!1((cid:12)P

nx
T(cid:18) h(cid:18)(x): Because
nx
T(cid:18) )k = 0  we get Eq. (14).

5 Implementation

For implementing the proposed method  parametric models of the initial probability pI(cid:23) and the p-
transition probability pT! in Eq. (1) need to be speciﬁed. We provide intuitive models based on the
logit function [8].
The initial probability is modeled as

∑

pI(cid:23)(x) ≜

exp(sI(x; (cid:23)))
y2X exp(sI(y; (cid:23)))

;

(16)
⊤ 2 Rd1 consisting of
]

where sI(x; (cid:23)) is a state score function with its parameter (cid:23) ≜ [(cid:23)loc⊤
a local parameter (cid:23)loc 2 RjXj and a global parameter (cid:23)glo 2 Rd1(cid:0)jXj. It is deﬁned as

; (cid:23)glo⊤

sI(x; (cid:23)) ≜ (cid:23)loc

⊤
x + ϕI(x)

(cid:23)glo;

(17)

6

where ϕI(x) 2 Rd1(cid:0)jXj is a feature vector of a state x. In the case of the road network  a state
corresponds to a road segment. Then ϕI(x) may  for example [18]  be deﬁned with the indicators of
whether there are particular types of buildings near the road segment  x. We refer to the ﬁrst term
and the second term of the right-hand side in Eq. (17) as a local preference and a global preference 
respectively. If a simpler model is preferred  either of them would be omitted.
; !glo⊤
Similarly  a p-transition probability model with the parameter ! ≜ [!loc⊤

⊤ is given as
]

{

/∑

; !glo⊤
) 2 X (cid:2) Xx;

2

1
′
if (x; x
otherwise;

(18)

′jx) ≜

pT!(x

′

; !))

exp(sT(x; x
0

y2Xx

exp(sT(x; y; !));

where Xx is a set of states connected from x  and sT(x; x
deﬁned as

′

; !) is a state-to-state score function. It is
′

′

′

′

⊤
)

!glo
2 ;

sT(x; x

; !) ≜ !loc

′
(x;x′) + ϕT(x
(x;x′) is the element of !loc (2 R

⊤
∑
!glo
)
1 +  (x; x
x2X jXxj) corresponding to transition from x to x
′

′  and
where !loc
ϕT(x) and  (x; x
) are feature vectors. For the road network  ϕT(x) may be deﬁned based on
the type of the road segment  x  and  (x; x
) may be deﬁned based on the angle between x and
′. Those linear combinations with the global parameters  !glo
2   can represent drivers’
x
preferences such as how much the drivers prefer major roads or straight routes to others.
′jx) presented in this section can be differentiated analytically.
Note that the pI(cid:23)(x) and pT!(x
Hence  F(cid:18) in Eq. (11)  ∇(cid:18)i log (cid:25)(cid:18) in Eq. (12)  and ∇(cid:18)ih(cid:18) in Eq. (15) can be computed efﬁciently.

) 2 X (cid:2) Xx;

and !glo

(x; x

1

6 Experiments

6.1 Experiment on synthetic data

′

′

) for every x; x

′ 2 Xo from this synthesized Markov chain.

To study the sensitivities of the performance of our algorithm to the ratio of observable states  we
applied it to randomly synthesized inverse problems of 100-state Markov chains with a varying
number of observable states  jXoj 2 f5; 10; 20; 35; 50; 70; 90g. The linkages between states were
randomly generated in the same way as [19]. The values of pI and pT are determined in two stages.
First  the basic initial probabilities  pI(cid:23)  and the basic transition probabilities  pT!  were determined
based on Eqs. (16) and (18)  where every element of (cid:23)  !  ϕI(x)  ϕT(x)  and  T(x; x
) was drawn
independently from the normal distribution N (0; 12). Then we added noises to pI(cid:23) and pT!  which
are ideal for our algorithm  by using the Dirichlet distribution Dir  such that pI = 0:7pI(cid:23) + 0:3(cid:27)
with (cid:27) (cid:24) Dir(0:3(cid:2) 1jXj). Then we sampled the visiting frequencies f (x) and the hitting rates
g(x; x
In
We used Eqs. (16) and (18) for the models and Eq. (7) for the objective of our method.
Eq. (7)  we set (cid:13) = 0:1 and R((cid:18)) = ∥(cid:18)∥2
2  and (cid:21) was determined with a cross-validation. We
evaluated the quality of our solution with the relative mean absolute error (RMAE)  RMAE =
f (x). As
jXnXoj
a baseline method  we use Nadaraya-Watson kernel regression (NWKR) [8] whose kernel is com-
puted based on the number of hops in the minimum path between two states. Note that the NWKR
could not use g(x; x
) as an input  because this is a regression problem of f (x). Hence  for a fair
comparison  we also applied a variant of our method that does not use g(x; x
Figure 2 (A) shows the mean and standard deviation of the RMAEs. The proposed method gives
clearly better performance than the NWKR. This is mainly due to the fact that the NWKR assumes
that all propagations of the observation from a link to another connected link are equally weighted.
In contrast  our method incorporates such weight in the transition probabilities.

maxff (x); 1g   where ^c is a scaling value given by ^c = 1=jXoj∑

jf (x)(cid:0)^c(cid:25)(cid:18)(x)j

x2XnXo

∑

x2Xo

).

′

′

1

6.2 Experiment on real-world trafﬁc data

We tested our method through a city-wide trafﬁc-monitoring task as shown in Fig. 1. The goal is to
estimate the trafﬁc volume along an arbitrary road segment (or link of a network)  given observed
trafﬁc volumes on a limited number of the links  where a link corresponds to the state x of M((cid:18))  and
the trafﬁc volume along x corresponds to f (x) of Eq. (5). The trafﬁc volumes along the observable
links were reliably estimated from real-world web-camera images captured in Nairobi  Kenya [2 

7

(A)

(B)

(C)

Figure 2: (A) Comparison of RMAE for the synthetic task between our methods and the NWKR
(baseline method). (B) Trafﬁc volumes for a city center map in Nairobi  Kenya  I: Web-camera
observations (colored)  II: Estimated trafﬁc volumes by our method. (C) Comparison between the
NWKR and our method for the real trafﬁc-volume prediction problem.

′

) here because of its unavailability. Note that this task
15]  while we did not use the hitting rate g(x; x
is similar to network tomography [27  30] or link-cost prediction [32  14]. However  unlike network
tomography  we need to infer all of the link trafﬁcs instead of source-destination demands. Unlike
link-cost prediction  our inputs are stationary observations instead of trajectories. Again  we use the
NMKR as the baseline method. The road network and the web-camera observations are shown in
Fig. 2 (B)-I. While the total number of links was 1; 497  the number of links with observations was
only 52 (about 3:5%). We used the parametric models in Section 5  where ϕT(x) 2 [(cid:0)1; 1] was set
based on the road category of x such that primary roads have a higher value than secondary roads
′. However  we omitted the
[22]  and  (x; x
terms of ϕI(x) in Eq. (17).
Figure 2 (B)-II shows an example of our results  where the red and yellow roads are most congested
while the trafﬁc on the blue roads is ﬂowing smoothly. The congested roads from our analysis
are consistent with those from a local trafﬁc survey report [13]. Figure 2 (C) shows comparison
between predicted and observed travel volumes. In the ﬁgures  the 45o line corresponds to perfect
agreement between the actual and predicted values. To evaluate accuracy  we employed the leave-
one-out cross-validation. We can see that the proposed method gives a good performance. This is
rather surprising  because the rate of observation links is very limited to only 3:5 percent.

) 2 [(cid:0)1; 1] was the cosine of the angle between x and x

′

7 Conclusion

We have deﬁned a novel inverse problem of a Markov chain  where we infer the probabilities about
the initial states and the transitions  using a limited amount of information that we can obtain by
observing the Markov chain at a small number of states. We have proposed an effective objective
function for this problem as well as an algorithm based on natural gradient.
Using real-world data  we have demonstrated that our approach is useful for a trafﬁc monitoring
system that monitors the trafﬁc volume at limited number of locations. From this observation the
Markov chain model is inferred  which in turn can be used to deduce the trafﬁc volume at any
location. Surprisingly  even when the observations are made at only several percent of the locations 
the proposed method can successfully infer the trafﬁc volume at unobserved locations.
Further analysis of the proposed method is necessary to better understand its property and effec-
tiveness. In particular  our future work includes an analysis of model identiﬁability and empirical
studies with other applications  such as logistics and economic system modeling.

Acknowledgments
The authors thank Dr. R. Morris  Dr. R. Raymond  and Mr. T. Katsuki for fruitful discussion.

8

030609000.511.522.5# of observation statesRMAE  Proposed methodProposed method with no use of gNadaraya−Watson kernel regression36.7936.836.8136.8236.8336.8436.85−1.31−1.305−1.3−1.295−1.29−1.285−1.28−1.275−1.27−1.265−1.26I36.7936.836.8136.8236.8336.8436.85−1.31−1.305−1.3−1.295−1.29−1.285−1.28−1.275−1.27−1.265−1.26II10−110010110−1100101ObservationEstimationNWKR(RMAE: 1.01 ± 0.917)10−110010110−1100101ObservationEstimationProposed method(RMAE: 0.517 ± 0.669)References
[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. of Interna-

tional Conference on Machine learning  2004.

[2] AccessKenya.com. http://traffic.accesskenya.com/.
[3] S. Amari. Natural gradient works efﬁciently in learning. Neural Computation  10(2):251–276  1998.
[4] S. Amari and H. Nagaoka. Method of Information Geometry. Oxford University Press  2000.
[5] S. Amari  H. Park  and K. Fukumizu. Adaptive method of realizing natural gradient learning for multilayer

perceptrons. Neural Computation  12(6):1399–1409  2000.

[6] H. Balzter. Markov chain models for vegetation dynamics. Ecological Modelling  126(2-3):139–154 

2000.

[7] J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence

Research  15:319–350  2001.

[8] C. M. Bishop. Pattern Recognition and Machine Learning. Springer  2006.
[9] H. H. Bui  S. Venkatesh  and G. West. On the recognition of abstract Markov policies. In Proc. of AAAI

Conference on Artiﬁcial Intelligence  pages 524–530  2000.

[10] S. L. Chang  L. S. Chen  Y. C. Chung  and S. W. Chen. Automatic license plate recognition. In Proc. of

IEEE Transactions on Intelligent Transportation Systems  pages 42–53  2004.

[11] E. Crisostomi  S. Kirkland  and R. Shorten. A Google-like model of road network dynamics and its

application to regulation and control. International Journal of Control  84(3):633–651  1995.

[12] M. Gamon and A. C. K¨onig. Navigation patterns from and to social media. In Proc. of AAAI Conference

on Weblogs and Social Media  2009.

[13] J. E. Gonzales  C. C. Chavis  Y. Li  and C. F. Daganzo. Multimodal transport in Nairobi  Kenya: Insights
and recommendations with a macroscopic evidence-based model. In Proc. of Transportation Research
Board 90th Annual Meeting  2011.

[14] T. Id´e and M. Sugiyama. Trajectory regression on road networks.

Artiﬁcial Intelligence  pages 203–208  2011.

In Proc. of AAAI Conference on

[15] T. Katasuki  T. Morimura  and T. Id´e. Bayesian unsupervised vehicle counting. In Technical Report. IBM

Research  RT0951  2013.

[16] D. Levin  Y. Peres  and E. Wilmer. Markov Chains and Mixing Times. American Mathematical Society 

2008.

[17] D. MacKay. Information theory  inference  and learning algorithms. Cambridge University Press  2003.
In Proc. of
[18] T. Morimura and S. Kato. Statistical origin-destination generation with multiple sources.

International Conference on Pattern Recognition  pages 283–290  2012.

[19] T. Morimura  E. Uchibe  J. Yoshimoto  and K. Doya. A generalized natural actor-critic algorithm. In

Proc. of Advances in Neural Information Processing Systems  volume 22  2009.

[20] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proc. of International Con-

ference on Machine Learning  2000.

[21] J. R. Norris. Markov Chains. Cambridge University Press  1998.
[22] OpenStreetMap. http://wiki.openstreetmap.org/.
[23] C. C. Pegels and A. E. Jelmert. An evaluation of blood-inventory policies: A Markov chain application.

Operations Research  18(6):1087–1098  1970.

[24] J.A. Quinn and R. Nakibuule. Trafﬁc ﬂow monitoring in crowded cities. In Proc. of AAAI Spring Sympo-

sium on Artiﬁcial Intelligence for Development  2010.

[25] C. M. Roberts. Radio frequency identiﬁcation (RFID). Computers & Security  25(1):18–26  2006.
[26] S. M. Ross. Stochastic processes. John Wiley & Sons Inc  1996.
[27] S. Santini. Analysis of trafﬁc ﬂow in urban areas using web cameras. In Proc. of IEEE Workshop on

Applications of Computer Vision  pages 140–145  2000.

[28] R. R. Sarukkai. Link prediction and path analysis using Markov chains. Computer Networks  33(1-

6):377–386  2000.

[29] G. Tauchen. Finite state Markov-chain approximations to univariate and vector autoregressions. Eco-

nomics Letters  20(2):177–181  1986.

[30] Y. Zhang  M. Roughan  C. Lund  and D. Donoho. An information-theoretic approach to trafﬁc matrix es-
timation. In Proc. of Conference on Applications  technologies  architectures  and protocols for computer
communications  pages 301–312. ACM  2003.

[31] J. Zhu  J. Hong  and J. G. Hughes. Using Markov chains for link prediction in adaptive Web sites. In
Proc. of Soft-Ware 2002: Computing in an Imperfect World  volume 2311  pages 60–73. Springer  2002.
[32] B. D. Ziebart  A. L. Maas  and A. K. Dey J. A. Bagnell. Maximum entropy inverse reinforcement learning.

In Proc. of AAAI Conference on Artiﬁcial Intelligence  pages 1433–1438  2008.

9

,Tetsuro Morimura
Takayuki Osogami
Tsuyoshi Ide
Harikrishna Narasimhan
Rohit Vaish
Shivani Agarwal