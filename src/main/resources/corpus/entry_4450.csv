2019,Program Synthesis and Semantic Parsing with Learned Code Idioms,Program synthesis of general-purpose source code from natural language specifications is challenging due to the need to reason about high-level patterns in the target program and low-level implementation details at the same time. In this work  we present Patois  a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step. It accomplishes this by automatically mining common code idioms from a given corpus  incorporating them into the underlying language for neural synthesis  and training a tree-based neural synthesizer to use these idioms during code generation. We evaluate Patois on two complex semantic parsing datasets and show that using learned code idioms improves the synthesizer's accuracy.,Program Synthesis and Semantic Parsing

with Learned Code Idioms

Richard Shin∗
UC Berkeley

Miltiadis Allamanis  Marc Brockschmidt & Oleksandr Polozov

Microsoft Research

ricshin@berkeley.edu

{miallama mabrocks polozov}@microsoft.com

Abstract

Program synthesis of general-purpose source code from natural language speciﬁ-
cations is challenging due to the need to reason about high-level patterns in the
target program and low-level implementation details at the same time.
In this
work  we present PATOIS  a system that allows a neural program synthesizer to
explicitly interleave high-level and low-level reasoning at every generation step.
It accomplishes this by automatically mining common code idioms from a given
corpus  incorporating them into the underlying language for neural synthesis  and
training a tree-based neural synthesizer to use these idioms during code genera-
tion. We evaluate PATOIS on two complex semantic parsing datasets and show
that using learned code idioms improves the synthesizer’s accuracy.

1

Introduction

Program synthesis is a task of translating an incomplete speciﬁcation (e.g. natural language  input-
output examples  or a combination of the two) into the most likely program that satisﬁes this speciﬁ-
cation in a given language [15]. In the last decade  it has advanced dramatically thanks to the novel
neural and neuro-symbolic techniques [5  10  19]  ﬁrst mass-market applications [28]  and massive
datasets [9  39  41]. Table 1 shows a few examples of typical tasks of program synthesis from natural
language. Most of the successful applications apply program synthesis to manually crafted domain-
speciﬁc languages (DSLs) such as FlashFill and Karel  or to subsets of general-purpose functional
languages such as SQL and Lisp. However  scaling program synthesis to real-life programs in a
general-purpose language with complex control ﬂow remains an open challenge.

We conjecture that one of the main current challenges of synthesizing a program is insufﬁcient
separation between high-level and low-level reasoning. In a typical program generation process 
be it a neural model or a symbolic search  the program is generated in terms of its syntax tokens 
which represent low-level implementation details of the latent high-level patterns in the program. In
contrast  humans switch between high-level reasoning (“a binary search over an array”) and low-
level implementation (“while l < r: m = (l+r)/2 . . . ”) repeatedly when writing a single function.
Reasoning over multiple abstraction levels at once complicates the generation task for a model.

This conjecture is supported by two key observations. First  recent work [12  25] has achieved great
results by splitting the synthesis process into sketch generation and sketch completion. The ﬁrst stage
generates a high-level sketch of the target program  and the second stage ﬁlls in missing details in
the sketch. Such separation improves the accuracy of synthesis as compared to an equivalent end-
to-end generation. However  it allows only one stage of high-level reasoning at the root level of
the program  whereas (a) real-life programs involve common patterns at all syntactic levels  and (b)
programmers often interleave high-level and low-level reasoning during implementation.

∗Work done partly during an internship at Microsoft Research.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Representative program synthesis tasks from real-world semantic parsing datasets.

Dataset

Natural Language Speciﬁcation

Program

Hearthstone
[24]

Mana Wyrn (1  3  1  Minion  Mage  Common)
Whenever you cast a spell  gain +1 Attack.

# . . .
def create_minion(self  player):

return Minion(1  3  effects=[Effect(

SpellCast()  ActionTag(Give(
ChangeAttack(1))  SelfSelector()))])

Spider
[41]

For each stadium  how many concerts are there?

Schema:
stadium = {stadium_id  name  ...}  ...

SELECT T2.name  COUNT(*)
FROM concert AS T1 JOIN stadium AS T2

ON T1.stadium_id = T2.stadium_id

GROUP BY T1.stadium_id

Second  many successful applications of inductive program synthesis such as FlashFill [14] rely on
a manually designed DSL to make the underlying search process scalable. Such DSLs include high-
level operators that implement common subroutines in a given domain. Thus  they (i) compress
the search space  ensuring that every syntactically valid DSL program expresses some useful task 
and (ii) enable logical reasoning over the domain-speciﬁc operator semantics  making the search
efﬁcient. However  DSL design is laborious and requires domain expertise. Recently  Ellis et al.
[13] showed that such DSLs are learnable in the classic domains of inductive program synthesis; in
this work  we target general-purpose code generation  where DSL design is difﬁcult even for experts.

In this work  we present a system  called PATOIS  that equips a program synthesizer with automat-
ically learned high-level code idioms (i.e. common program fragments) and trains it to use these
idioms in program generation. While syntactic by deﬁnition  code idioms often represent useful
semantic concepts. Moreover  they compress and abstract the programs by explicitly representing
common patterns with unique tokens  thus simplifying generative process for the synthesis model.

PATOIS has three main components  illustrated in Figure 1. First  it employs nonparameteric
Bayesian inference to mine the code idioms that frequently occur in a given corpus. Second  it
marks the occurrences of these idioms in the training dataset as new named operators in an extended
grammar. Finally  it trains a neural generative model to optionally emit these named idioms instead
of the original code fragments  which allows it to learn idiom usage conditioned on a task speciﬁ-
cation. During generation  the model has the ability to emit entire idioms in a single step instead
of multiple steps of program tree nodes comprising the idioms’ deﬁnitions. As a result  PATOIS
interleaves high-level idioms with low-level tokens at all levels of program synthesis  generalizing
beyond ﬁxed top-level sketch generation.

We evaluate PATOIS on two challenging semantic parsing datasets: Hearthstone [24]  a dataset of
small domain-speciﬁc Python programs  and Spider [41]  a large dataset of SQL queries over various
databases. We ﬁnd that equipping the synthesizer with learned idioms improves its accuracy in
generating programs that satisfy the task description.

2 Background

Program Synthesis We consider the following formulation of the program synthesis problem.
Assume an underlying programming language L of programs. Each program P ∈ L can be rep-
resented either as a sequence y1 · · · y|P | of its tokens  or  equivalently  as an abstract syntax tree
(AST) T parsed according to the context-free grammar (CFG) G of the language L. The goal of a
program synthesis model f : φ 7→ P is to generate a program P that maximizes the conditional
probability Pr (P | φ) i.e. the most likely program given the speciﬁcation. We also assume a train-
ing set D = {⟨φj  Pj⟩}|D|
j=1  sampled from an unknown true distribution D  from which we wish to
estimate the conditional probability Pr (P | φ).

In this work  we consider general-purpose programming languages L with a known context-free
grammar G such as Python and SQL. Each speciﬁcation φ is represented as a natural language
task description  i.e. a sequence of words X = x1 · · · x|X| (although the PATOIS synthesizer can be
conditioned on any other type of incomplete spec). In principle  we do not impose any restrictions
on the generative model f apart from it being able to emit syntactically valid programs. However 
as we detail in Section 4  the PATOIS framework is most easily implemented on top of structural
generative models such as sequence-to-tree models [38] and graph neural networks [7  21].

2

t
n
i

:
1
ℓ

t
s
i
l

:
0
ℓ

x
e
d
n

I

t
p
i
r
c
s
b
u
S

e
r
a
p
m
o
C

f
I

2

m ℓ
u
N

q
E

3
ℓ

i

n
g
s
s
A

*
t

m
t
s

3
ℓ

d
d
A

m 1
u
N

p
O
n
B

i

.

.

.

Figure 1: Top: An overview of PATOIS. A miner 1⃝ extracts common idioms from the programs
in a given dataset. All the idiom occurrences in the dataset programs are 2⃝ marked as optional
alternative grammar operators. The dataset with marked occurrences is used to 3⃝ train a neural
generative model. At inference time  the model 4⃝ generates programs with named idioms  which
are inlined before program execution. Note that idioms may have named subexpressions  may repeat 
and may occur at any program level. For clarity  we typeset idioms using function-like syntax
Ij(ℓ1  . . .   ℓk) in this paper  although they are actually represented as AST fragments with no syntax.
Bottom: AST fragment representation of the idiom I2 in Python. Here sans-serif nodes are ﬁxed
non-terminals  monospaced nodes are ﬁxed terminals  and boxed nodes are named arguments.

Code Idioms Following Allamanis and Sutton [2]  we deﬁne code idioms as fragments I of valid
ASTs T in the CFG G  i.e. trees of nonterminals and terminals from G that may occur as subtrees
of valid parse trees from G. The grammar G extended with a set of idiom fragments forms a tree
substitution grammar (TSG). We also associate a non-unique label ℓ with each nonterminal leaf
in every idiom  and require that every instantiation of an idiom I must have its identically-labeled
nonterminals instantiated to identical subtrees. This enables the role of idioms as subroutines  where
labels act as “named arguments” in the “body” of an idiom. See Figure 1 for an example.

3 Mining Code Idioms

The ﬁrst step of PATOIS is obtaining a set of frequent and useful AST fragments as code idioms.
The trade-off between frequency and usefulness is crucial: it is trivial to mine commonly occurring
short patterns  but they are often meaningless [1]. Instead  we employ and extend the methodology
of Allamanis et al. [3] and frame idiom mining as a nonparameteric Bayesian problem.

We represent idiom mining as inference over probabilistic tree substitution grammars (pTSG). A
pTSG is a probabilistic context-free grammar extended with production rules that expand to a whole
AST fragment instead of a single level of symbols [8  29]. The grammar G of our original language L
induces a pTSG G0 with no fragment rules and with choice probabilities estimated from the corpus D.
To construct a pTSG corresponding to the extension of L with common tree fragments representing
idioms  we deﬁne a distribution G over pTSGs as follows.

We ﬁrst choose a Pitman-Yor process [36] as a prior distribution G0 over pTSGs. It is a nonpa-
rameteric process that has proven to be effective for mining code idioms in prior work thanks to its
modeling of production choices as a Zipﬁan distribution (in other words  it implements the desired
“rich get richer” effect  which encourages a smaller number of larger and more common idioms).

3

Formally  it is a “stick-breaking” process [31] that
deﬁnes G0 as a distribution for each set of id-

ioms eIN rooted at a nonterminal symbol N as
Pr(I ∈ eIN ) def=
k−1∏

πk δ (I = Ik)  

def= uk

∞∑

k=0

Ik ∼ G0

πk

(1 − uj)  uk ∼ Beta (1 − d  α + kd)

j=1

where δ(·) is the delta function  and α  d are hyper-
parameters. See Allamanis et al. [3] for details.

Figure 2: MCMC sampling for an AST (ﬁg-
ure from [2]). Dots show the inferred nodes
where the AST is split into fragments.

PATOIS uses G0 to compute a posterior distribu-
tion G1 = Pr (G1 | T1  . . .   TN ) using Bayes’ rule 
where T1  . . .   TN are concrete AST fragments in the training set D. As this calculation is compu-
tationally intractable  we approximate it using type-based MCMC [23]. At each iteration t of the
MCMC process  PATOIS generates a pTSG Gt whose distribution approaches G1 as t → ∞. It
works by sampling splitting points for each AST T in the corpus D  which by construction deﬁne a
set of fragments constituting Gt (see Figure 2). The split probabilities of this Gibbs sampling are set
in a way that incentivizes merging adjacent tree fragments that often cooccur in D. The ﬁnal idioms
are then extracted from the pTSG obtained at the last MCMC iteration.

While the Pitman-Yor process helps avoid overﬁtting the idioms to D  not all sampled idioms are
useful for synthesis. Thus we rank and ﬁlter the idioms before using them in the training. In this
work  we reuse two ranking functions deﬁned by Allamanis et al. [3]:

ScoreCov (I) def= coverage = count(T ∈ D | I ∈ T )

ScoreCXE (I) def= coverage · cross-entropy gain =

count(T ∈ D | I ∈ T )

|D|

·

1
|I|

log

PrG1 (I)
PrG0 (I)

and also ﬁlter out any terminal idioms (i.e. those that do not contain any named arguments ℓ).

We conclude with a brief analysis of computational complexity of idiom mining. Every iteration of
the MCMC sampling traverses the entire dataset D once to sample the random variables that deﬁne
the splitting points in each AST. When run for M iterations  the complexity of idiom mining is

O(M ·∑T ∈D |T |). Idiom ranking adds an additional step with complexity O(|eI| log |eI|) where eI

is the set of idioms obtained at the last iteration. In our experiments (detailed in Section 5) we set
M = 10  and the entire idiom mining takes less than 10 minutes on a dataset of |D| ≈ 10 000 ASTs.

4 Using Idioms in Program Synthesis

Given a set of common idioms eI = {I1  . . .   IN } mined by PATOIS  we now aim to learn a syn-

thesis model f that emits whole idioms Ij as atomic actions instead of individual AST nodes that
comprise Ij . Achieving this involves two key challenges.

First  since idioms are represented as AST fragments without concrete syntax  PATOIS works best
when the synthesis model f is structural  i.e. it generates the program AST instead of its syntax.
Prior work [7  38  40] also showed that tree- and graph-based code generation models outperform
sequence-to-sequence models  and thus we adopt a similar architecture in this work.

Second  exposing the model f to idiom usage patterns is not obvious. One approach could be to
extend the grammar with new named operators opI(ℓ1  . . .   ℓk) for each idiom I  replace every oc-
currence of I with opI in the data  and train the synthesizer on the rewritten dataset. However  this
would not allow f to learn from the idiom deﬁnitions (bodies). In addition  idiom occurrences often
overlap  and any deterministic rewriting strategy would arbitrarily discard some occurrences from
the corpus  thus limiting the model’s exposure to idiom usage. In our experiments  we found that
greedy rewriting discarded as many as 75% potential idiom occurrences from the dataset. There-
fore  a successful training strategy must preserve all occurrences and instead let the model learn a
rewriting strategy that optimizes end-to-end synthesis accuracy.

4

To this end  we present a novel training setup for code generation that encourages the model to
choose the most useful subset of idioms and the best representation of each program in terms of the

idioms. It works by (a) marking occurrences of the idioms eI in the training set D  (b) at training

time  encouraging the model to emit either the whole idiom or its body for every potential idiom oc-
currence in the AST  and (c) at inference time  replacing the model’s state after emitting an idiom I
with the state the model would have if it had emitted I’s body step by step.

4.1 Model Architecture

The synthesis model f of PATOIS combines a spec encoder fenc and an AST decoder fdec  fol-
lowing the formulation of Yin and Neubig [38]. The encoder fenc embeds the NL speciﬁcation
X = x1 · · · xn into word representations ˆX = ˆx1 · · · ˆxn. The decoder fdec uses an LSTM to model
the sequential generation of the AST in the depth-ﬁrst order  wherein each timestep t corresponds to
an action at — either (a) expanding a production from the grammar  (b) expanding an idiom  or (c)
generating a terminal token. Thus  the probability of generating an AST T given ˆX is

Pr(T | ˆX) = ∏t

Pr(at | Tt  ˆX)

(1)

where at is the action taken at timestep t  and Tt is the partial AST generated before t. The proba-
bility Pr(at | Tt  ˆX) is computed from the decoder’s hidden state ht−1 depending on at.

Production Actions For actions at = APPLYRULE[R] corresponding to expanding production
rules R ∈ G from the original CFG G  we compute the probability Pr(at | Tt  ˆX) by encoding the
current partial AST structure similarly to Yin and Neubig [38]. Speciﬁcally  we compute the new
hidden state as ht = fLSTM ([at−1 ∥ ct ∥ hpt ∥ apt ∥ nft ]  ht−1) where at−1 is the embedding
of the previous action  ct is the result of soft attention applied to the spec embeddings ˆX as per
Bahdanau et al. [4]  pt is the timestep corresponding to expanding the parent AST node of the
current node  and nft is the embedding of the current node type. The hidden state ht is then used
to compute probabilities of the syntactically appropriate production rules R ∈ G:

Pr(at = APPLYRULE[R] | Tt  ˆX) = softmaxR (g(ht))

(2)

where g(·) is a 2-layer MLP with a tanh non-linearity.

Terminal Actions For actions at = GETTOKEN[y]  we compute the probability Pr(at | Tt  ˆX) by
combining a small vocabulary V of tokens commonly observed in the training data with a copying
mechanism [24  30] over the input X to handle UNK tokens. Speciﬁcally  we learn two functions
pgen(ht) and pcopy(ht  X) such that pgen produces a score for each vocabulary token y ∈ V and pcopy
computes a score for copying the token y from the input. The scores are then normalized across the
entries corresponding to the same constant  as in [7  38].

4.2 Training to Emit Idioms

As discussed earlier  training the model to emit idioms presents computational and learning chal-
lenges. Ideally  we would like to extend Eq. (1) to maximize

J = ∑

τ ∈T

|τ |∏

i=1

Pr(aτi | Tτi   ˆX)

(3)

where T is a set of different action traces that may produce the output AST T . The traces τ ∈ T dif-
fer only in their possible choices of idiom actions APPLYRULE[opI] that emit some tree fragments
of T in a single step. However  computing Eq. (3) is intractable because idiom occurrences overlap
and cause combinatorial explosion in the number of traces T . Instead  we apply Jensen’s inequality
and maximize a lower bound:

log J = log ∑

τ ∈T

|τ |∏

i=1

Pr(aτi | Tτi   ˆX) ≥ log(|T |) +

5

1

|T | ∑

τ ∈T

|τ |∑

i=1

log Pr(aτi | Tτi   ˆX)

(4)

Let A(Tt) = {a∗
t } ∪ I(Tt) be the set of all valid actions to expand the AST Tt at timestep t. Here
a∗
t is the action from the original action trace that generates T using the original CFG and I(Tt)
is the set of idiom actions APPLYRULE[opI] also applicable at the node to be expanded in Tt. Let
c(T   t) also denote the number of traces τ ∈ T that admit an action choice for the AST Tt from
the original action trace. Since each action a ∈ A(Tt) occurs in the sum in Eq. (4) with probability

c(T   t)/ |A(Tt)|  we can rearrange this sum over traces as a sum over timesteps of the original trace:
|τ |∑
|T | ∑
= ∑
|A(Tt)| ∑
|A(Tt)|[log Pr(a∗
≈ ∑

log Pr(at = APPLYRULE[opI] | Tt  ˆX)]

log Pr(a | Tτi   ˆX) = E
Tt∼T

t | Tt  ˆX) +∑

1

|A(Tt)| ∑

log Pr(aτi | Tτi   ˆX) =

log Pr(a | Tτi   ˆX)

log Pr(a | Tτi   ˆX)

c(T   t)
|A(Tt)|

c(T   t)

|T |

1

|T | ∑

∑

t

a∈A(Tt)

1

τ ∈T

i=1

a∈A(Tt)

a∈A(Tt)

1

1

t

t

I∈M (Tt)

(5)

In the last step of Equation (5)  we approximate
the expectation over ASTs randomly drawn
from all traces T using only the original trace
(containing all possible Tt) as a Monte Carlo
estimate.

Intuitively  at each timestep during training we
encourage the model to emit either the orig-
inal AST action for this timestep or any ap-
plicable idiom that matches the AST at this
step  with no penalty to either choice. However 
to avoid the combinatorial explosion  we only
teacher-force the original generation trace (not
the idiom bodies)  thus optimizing the bound
in Eq. (5). Figure 3 illustrates this optimization
process on an example.

At inference time  whenever the model emits
an APPLYRULE[opI] action  we teacher-force
the body of I by substituting the embedding
of the previous action at−1 with embedding of
the previous action in the idiom deﬁnition  thus
emulating the tree fragment expansion. Out-
side the bounds of I (i.e. within the hole sub-
trees of I) we use the actual at−1 as usual.

5 Evaluation

Figure 3: Decoding the AST sorted(my_list 
reverse=True)  ﬁgure adapted from [38]. Sup-
pose an idiom I = sorted( ℓ   reverse=True) is
mined and added as an operator opI(ℓ) to the
grammar. At training time  PATOIS adjusts the
cross-entropy objective at timestep t2 to addition-
ally allow opI as a valid production  with no
change to further decoding. At inference time  if
decoder emits an action at2 = APPLYRULE[opI] 
PATOIS unrolls I on the ﬂy by teacher-forcing the
shaded portion of the AST generation.

Datasets We evaluate PATOIS on two semantic parsing datasets: Hearthstone [24] and Spider [41].

Hearthstone is a dataset of 665 card descriptions from the trading card game of the same name  along
with the implementations of their effects in Python using the game APIs. The descriptions act as NL
specs X  and are on average 39.1 words long.

Spider is a dataset of 10 181 questions describing 5 693 unique SQL queries over 200 databases
with multiple tables each. Each question pertains to a particular database  whose schema is given to
the synthesizer. Database schemas do not overlap between the train and test splits  thus challenging
the model to generalize across different domains. The questions are on average 13 words long and
databases have on average 27.6 columns and 8.8 foreign keys.

Implementation We mine the idioms using the training split of each dataset. Thus PATOIS cannot
indirectly overﬁt to the test set by learning its idioms  but it also cannot generalize beyond the idioms
that occur in the training set. We run type-based MCMC (Section 3) for 10 iterations with α = 5

6

Table 2: Ablation tests on the Hearthstone dev set.

Table 3: Ablation tests on the Spider dev set.

Model

K Exact
match

Sentence
BLEU

Corpus
BLEU

Baseline decoder — 0.197

PATOIS  ScoreCov

PATOIS  ScoreCXE

10
20
40
80

10
20
40
80

0.151
0.091
0.167
0.197

0.151
0.167
0.182
0.151

0.767

0.781
0.745
0.765
0.780

0.780
0.787
0.773
0.771

0.763

0.785
0.745
0.764
0.774

0.783
0.782
0.770
0.768

Model

K Exact match

Baseline decoder —

PATOIS  ScoreCov

PATOIS  ScoreCXE

10
20
40
80

10
20
40
80

0.395

0.394
0.379
0.395
0.407

0.368
0.382
0.387
0.416

and d = 0.5. After ranking (with either ScoreCOV or ScoreCXE) and ﬁltering  we use K top-ranked
idioms to train the generative model. We ran ablation experiments with K ∈ {10  20  40  80}.

As described in Section 4  for all our experiments we used a tree-based decoder with a pointer mech-
anism as the synthesizer f   which we implemented in PyTorch [27]. For the Hearthstone dataset 
we use a bidirectional LSTM [16] to implement the description encoder ˆX = fenc(X)  similarly to
Yin and Neubig [38]. The word embeddings ˆx and hidden LSTM states h have dimension 256. The
models are trained using the Adadelta optimizer [42] with learning rate 1.0  ρ = 0.95  ε = 10−6 for
up to 2 600 steps with a batch size of 10.

For the Spider dataset  word embeddings ˆx have dimension 300  and hidden LSTM states h have
dimension 256. The models are trained using the Adam optimizer [20] with β1 = 0.9  β2 = 0.999 
ε = 10−9 for up to 40 000 steps with a batch size of 10. The learning rate warms up linearly up to
2.5 × 10−4 during the ﬁrst 2 000 steps  and then decays polynomially by (1 − t/T )−0.5 where T is
the total number of steps. Each model conﬁguration is trained on one NVIDIA GTX 1080 Ti GPU.

The Spider tasks additionally include the database schema as an input in the description. We fol-
low a recent approach of embedding the schema using relation-aware self-attention within the en-
coder [34]. Speciﬁcally  we initialize a representation for each column  table  and word in the
question  and then update these representations using 4 layers of relation-aware self-attention [32]
using a graph that describes the relations between columns and tables in the schema. See Section A
in the appendix for more details about the Spider schema encoder.

5.1 Experimental Results

In each conﬁguration  we compare the performance of equivalent trained models on the same dataset
with and without idiom-based training of PATOIS. For fairness  we show the performance of the same
decoder implementation described in Section 4.1 as a baseline rather than the state-of-the-art results
achieved by different approaches from the literature. Thus  our baseline is the decoder described
in Section 4.1 trained with a regular cross-entropy objective rather than the PATOIS objective in
Equation (5). Following prior work  we evaluate program generation as a semantic parsing task  and
measure (i) exact match accuracy and BLEU scores for Hearthstone and (ii) exact match accuracy
of program sketches for Spider.

respectively.

Tables 2 and 3 show our ablation analysis of different conﬁgurations of PATOIS on the Hearth-
stone and Spider dev sets 
Table 4 shows the test set results of the best
model conﬁguration for Hearthstone (the test instances for the Spider dataset are unreleased).
As the results show  small numbers of idioms do not
signiﬁcantly change the exact match accuracy but
improve BLEU score  and K = 80 gives a signiﬁ-
cant improvement in both the exact match accuracy
and BLEU scores. The improvement is even more
pronounced on the test set with 4.5% improvement
in exact match accuracy and more than 4 BLEU
points  which shows that mined training set idioms

Table 4: Test set results on Hearthstone (us-
ing the best conﬁgurations on the dev set).

Baseline
PATOIS

Sentence
BLEU

Corpus
BLEU

Exact
match

0.152
0.197

0.743
0.780

0.723
0.766

Model

7

def __init__(self):

ℓ0 : id =

super().__init__( ℓ0 : str   ℓ1 : int  

copy.copy( ℓ1 : expr )

SELECT COUNT( ℓ0 : col )  ℓ∗

1 WHERE ℓ∗

2

INTERSECT ℓ?

4 : sql EXCEPT ℓ?

5 : sql

CHARACTER_CLASS. ℓ3 : id  

CARD_RARITY. ℓ4 : id   ℓ?

5 )

class ℓ0 : id ( ℓ1 : id ):
def __init__(self):

WHERE ℓ0 : col = $terminal

Figure 4: Five examples of commonly used idioms from the Hearthstone and Spider datasets.

Figure 5: The distribution of used idioms in the inferred ASTs on the Hearthstone test set. Left: in
the ASTs exactly matched with ground truth; Right: all ASTs.

generalize well to the whole data distribution. As mentioned above  we compare only to the same
baseline architecture for fairness  but PATOIS could also be easily implemented on top of the struc-
tural CNN decoder of Sun et al. [35]  the current state of the art on the Hearthstone dataset.

Figure 4 shows some examples of idioms that were frequently used by the model. On Hearthstone 
the most popular idioms involve common syntactic elements (e.g. class and function deﬁnitions) and
domain-speciﬁc APIs commonly used in card implementations (e.g. CARD_RARITY enumerations or
copy.copy calls). On Spider  they capture the most common combinations of SQL syntax  such
as a SELECT query with a single COUNT column and optional INTERSECT or EXCEPT clauses. Notably 
popular idioms are also often big: for instance  the ﬁrst idiom in Figure 4 expands to a tree fragment
with more than 20 nodes. Emitting it in a single step vastly simpliﬁes the decoding process.

We further conducted qualitative experiments to analyze actual idiom usage by PATOIS on the
Hearthstone test set. Figure 5 shows the distribution of idioms used in the inferred (not ground
truth) ASTs. A typical program involves 7 idioms on average  or 6 for the programs that exactly

match the ground truth. Despite the widespread usage of idioms  not all of the mined idioms eI were

useful: only 51 out of K = 80 idioms appear in the inferred ASTs. This highlights the need for an
end-to-end version of PATOIS where idiom mining would be directly optimized to beneﬁt synthesis.

6 Related Work

Program synthesis & Semantic parsing Program synthesis from natural language and input-
output examples has a long history in Programming Languages (PL) and Machine Learning (ML)
communities (see Gulwani et al. [15] for a survey). When an input speciﬁcation is limited to natu-
ral language  the resulting problem can be considered semantic parsing [22]. There has been a lot
of recent interest in applying recurrent sequence-based and tree-based neural networks to semantic
parsing [11  18  21  38  40]. These approaches commonly use insights from the PL literature  such
as grammar-based constraints to reduce the search space  non-deterministic training oracles to en-
able multiple executable interpretations of intent  and supervision from program execution. They
typically either supervise the training on one or more golden programs  or use reinforcement learn-
ing to supervise the training from a neural program execution result [26]. Our PATOIS approach is

8

applicable to any underlying neural semantic parsing model  as long as it is supervised by a corpus
of golden programs. It is  however  most easily applicable to tree-based and graph-based models 
which directly emit the AST of the target program. In this work we have evaluated PATOIS as applied
on top of the sequence-to-tree decoder of Yin and Neubig [38]  and extended it with a novel training
regime that teaches the decoder to emit idiom operators in place of the idiomatic code fragments.

Sketch generation Two recent works [12  25] learn abstractions of the target program to compress
and abstract the reasoning process of a neural synthesizer. Both of them split the generation process
into sketch generation and sketch completion  wherein the ﬁrst stage emits a partial tree/sequence
(i.e. a sketch of the program) and the second stage ﬁlls in the holes in this sketch. While sketch
generation is typically implemented with a neural model  sketch completion can be either a different
neural model or a combinatorial search. In contrast to PATOIS  both works deﬁne the grammar of
sketches manually by a deterministic program abstraction procedure and only allow a single top-
level sketch for each program. In addition  an earlier work of Bošnjak et al. [6] also formulates
program synthesis as sketch completion  but in their work program sketches are manually provided
rather than learned. In PATOIS  we learn the abstractions (code idioms) automatically from a corpus
and allow them to appear anywhere in the program  as is common in real-life programming.

Learning abstractions Recently  Ellis et al. [13] developed an Explore  Compress & Com-
pile (EC2) framework for automatically learning DSLs for program synthesis from I/O examples
(such as the DSLs used by FlashFill [14] and DeepCoder [5]). The workﬂow of EC2 is similar to
PATOIS  with three stages: (a) learn new DSL subroutines from a corpus of tasks  (b) train a recogni-
tion model that maps a task speciﬁcation to a distribution over DSL operators as in DeepCoder [5] 
and (c) use these operators in a program synthesizer. PATOIS differs from EC2 in three aspects: (i)
we assume a natural language speciﬁcation instead of examples  (ii) to handle NL speciﬁcations 
our synthesizer is a neural semantic parser instead of enumerative search  and (iii) most importantly 
we discover idioms that compress general-purpose languages instead of extending DSLs. Unlike
for inductive synthesis DSLs such as FlashFill  the existence of useful DSL abstractions for general-
purpose languages is not obvious  and our work is the ﬁrst to demonstrate them.

Concurrently with this work  Iyer et al. [17] developed a different approach of learning code idioms
for semantic parsing. They mine the idioms using a variation of byte-pair encoding (BPE) compres-
sion extended to ASTs and greedily rewrite all the dataset ASTs in terms of the found idioms for
training. While the BPE-based idiom mining is more computationally efﬁcient than non-parametric
Bayesian inference of PATOIS  introducing ASTs greedily tends to lose information about overlap-
ping idioms  which we address in PATOIS using our novel training objective described in Section 4.2.

As described previously  our code idiom mining is an extension of the procedure developed by Alla-
manis et al. [2  3]. They are the ﬁrst to use the tree substitution grammar formalism and Bayesian
inference to ﬁnd non-trivial common idioms in a corpus of code. However  their problem formaliza-
tion does not involve any application for the learned idioms beyond their explanatory power.

7 Conclusion

Semantic parsing  or neural program synthesis from natural language  has made tremendous progress
over the past years  but state-of-the-art models still struggle with program generation at multiple lev-
els of abstraction. In this work  we present a framework that allows incorporating learned coding
patterns from a corpus into the vocabulary of a neural synthesizer  thus enabling it to emit high-level
or low-level program constructs interchangeably at each generation step. Our current instantiation 
PATOIS  uses Bayesian inference to mine common code idioms  and employs a novel nondeterminis-
tic training regime to teach a tree-based generative model to optionally emit whole idiom fragments.
Such dataset abstraction using idioms improves the performance of neural program synthesis.

PATOIS is only the ﬁrst step toward learned abstractions in program synthesis. While code idioms
often correlate with latent semantic concepts and our training regime allows the model to learn
which idioms to use and in which context  our current method does not mine them with the intent to
directly optimize their usefulness for generation. In future work  we want to alleviate this by jointly
learning the mining and synthesis models  thus optimizing the idioms’ usefulness for synthesis by
construction. We also want to incorporate program semantics into the idiom deﬁnition  such as data
ﬂow patterns or natural language phrases from task specs.

9

References

[1] C. C. Aggarwal and J. Han. Frequent pattern mining. Springer  2014.

[2] M. Allamanis and C. Sutton. Mining idioms from source code. In Proceedings of the 22nd ACM SIGSOFT

International Symposium on Foundations of Software Engineering (FSE)  pages 472–483. ACM  2014.

[3] M. Allamanis  E. T. Barr  C. Bird  P. Devanbu  M. Marron  and C. Sutton. Mining semantic loop idioms.

IEEE Transactions on Software Engineering  2018.

[4] D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align and translate.

In Proceedings of the 3rd International Conference on Learning Representations (ICLR)  2015.

[5] M. Balog  A. L. Gaunt  M. Brockschmidt  S. Nowozin  and D. Tarlow. DeepCoder: Learning to write
programs. In Proceedings of the 5th International Conference on Learning Representations (ICLR)  2017.

[6] M. Bošnjak  T. Rocktäschel  J. Naradowsky  and S. Riedel. Programming with a differentiable Forth
interpreter. In Proceedings of the 34th International Conference on Machine Learning (ICML)  volume 70 
pages 547–556  2017.

[7] M. Brockschmidt  M. Allamanis  A. L. Gaunt  and O. Polozov. Generative code modeling with graphs.

In Proceedings of the 7th International Conference on Learning Representations (ICLR)  2019.

[8] T. Cohn  P. Blunsom  and S. Goldwater. Inducing tree-substitution grammars. Journal of Machine Learn-

ing Research  11(Nov):3053–3096  2010.

[9] J. Devlin  R. Bunel  R. Singh  M. Hausknecht  and P. Kohli. Neural program meta-induction. In Advances

in Neural Information Processing Systems (NIPS)  pages 2080–2088  2017.

[10] J. Devlin  J. Uesato  S. Bhupatiraju  R. Singh  A.-r. Mohamed  and P. Kohli. RobustFill: Neural program
In Proceedings of the 34th International Conference on Machine Learning

learning under noisy I/O.
(ICML)  2017.

[11] L. Dong and M. Lapata. Language to logical form with neural attention. In Proceedings of the 54th Annual

Meeting of the Association for Computational Linguistics (ACL)  2016.

[12] L. Dong and M. Lapata. Coarse-to-ﬁne decoding for neural semantic parsing. In Proceedings of the 56th

Annual Meeting of the Association for Computational Linguistics (ACL)  2018.

[13] K. Ellis  L. Morales  M. Sablé-Meyer  A. Solar-Lezama  and J. Tenenbaum. Learning libraries of sub-
routines for neurally-guided Bayesian program induction. In Advances in Neural Information Processing
Systems  pages 7816–7826  2018.

[14] S. Gulwani. Automating string processing in spreadsheets using input-output examples. In Proceedings of
the 38th ACM Symposium on Principles of Programming Languages (POPL)  volume 46  pages 317–330 
2011.

[15] S. Gulwani  O. Polozov  and R. Singh. Program synthesis. Foundations and Trends® in Programming

Languages  4(1-2):1–119  2017.

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780  1997.

[17] S. Iyer  A. Cheung  and L. Zettlemoyer. Learning programmatic idioms for scalable semantic parsing. In

EMNLP  2019.

[18] R. Jia and P. Liang. Data recombination for neural semantic parsing. In Proceedings of the 54th Annual

Meeting of the Association for Computational Linguistics (ACL)  volume 1  pages 12–22  2016.

[19] A. Kalyan  A. Mohta  O. Polozov  D. Batra  P. Jain  and S. Gulwani. Neural-guided deductive search
for real-time program synthesis from examples. In Proceedings of the 6th International Conference on
Learning Representations (ICLR)  2018.

[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of 3rd International

Conference on Learning Representations (ICLR)  2015.

[21] Y. Li  D. Tarlow  M. Brockschmidt  and R. Zemel. Gated graph sequence neural networks. In Proceedings

of the 4th International Conference on Learning Representations (ICLR)  2016.

[22] P. Liang. Learning executable semantic parsers for natural language understanding. Communications of

the ACM  59(9):68–76  2016.

10

[23] P. Liang  M. I. Jordan  and D. Klein. Type-based MCMC. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics 
pages 573–581. Association for Computational Linguistics  2010.

[24] W. Ling  P. Blunsom  E. Grefenstette  K. M. Hermann  T. Koˇcisk`y  F. Wang  and A. Senior. Latent
predictor networks for code generation. In ACL  volume 1  pages 599–609  2016. URL https://github.
com/deepmind/card2code.

[25] V. Murali  L. Qi  S. Chaudhuri  and C. Jermaine. Neural sketch learning for conditional program genera-

tion. In Proceedings of the 6th International Conference on Learning Representations (ICLR)  2018.

[26] A. Neelakantan  Q. V. Le  M. Abadi  A. McCallum  and D. Amodei. Learning a natural language interface
with neural programmer. In Proceedings of the 5th International Conference on Learning Representations
(ICLR)  2017.

[27] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison  L. Antiga  and

A. Lerer. Automatic differentiation in PyTorch. 2017.

[28] O. Polozov and S. Gulwani. FlashMeta: A framework for inductive program synthesis.

In Proceed-
ings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming  Systems 
Languages  and Applications (OOPSLA)  pages 107–126  2015.

[29] M. Post and D. Gildea. Bayesian learning of a tree substitution grammar. In Proceedings of the ACL-

IJCNLP 2009 Conference Short Papers  pages 45–48. Association for Computational Linguistics  2009.

[30] A. See  P. J. Liu  and C. D. Manning. Get to the point: Summarization with pointer-generator networks. In
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)  volume 1 
pages 1073–1083  2017.

[31] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica sinica  pages 639–650  1994.

[32] P. Shaw  J. Uszkoreit  and A. Vaswani. Self-attention with relative position representations.

In Pro-
ceedings of the 2018 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies  Volume 2 (Short Papers)  2018.

[33] P. Shaw  J. Uszkoreit  and A. Vaswani. Self-Attention with Relative Position Representations. In Pro-
ceedings of the 2018 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies  Volume 2 (Short Papers)  pages 464–468. Association for
Computational Linguistics  2018. doi: 10.18653/v1/N18-2074.

[34] R. Shin. Encoding database schemas with relation-aware self-attention for text-to-SQL parsers. arXiv

preprint arXiv:1906.11790  2019.

[35] Z. Sun  Q. Zhu  L. Mou  Y. Xiong  G. Li  and L. Zhang. A grammar-based structural CNN decoder for

code generation. In AAAI  2019.

[36] Y. W. Teh and M. I. Jordan. Hierarchical Bayesian nonparametric models with applications. Bayesian

nonparametrics  1:158–207  2010.

[37] A. Vaswani  N. Shazeer  N. Parmar  J. Uszkoreit  L. Jones  A. N. Gomez  Ł. Kaiser  and I. Polosukhin.
In Advances in Neural Information Processing Systems  pages 5998–6008.

Attention is all you need.
Curran Associates  Inc.  2017.

[38] P. Yin and G. Neubig. A syntactic neural model for general-purpose code generation. In ACL  July 2017.

[39] P. Yin  B. Deng  E. Chen  B. Vasilescu  and G. Neubig. Learning to mine aligned code and natural
language pairs from StackOverﬂow. In International Conference on Mining Software Repositories (MSR) 
pages 476–486. ACM  2018.

[40] P. Yin  C. Zhou  J. He  and G. Neubig. StructVAE: Tree-structured latent variable models for semi-
supervised semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computa-
tional Linguistics (ACL)  2018.

[41] T. Yu  R. Zhang  K. Yang  M. Yasunaga  D. Wang  Z. Li  J. Ma  I. Li  Q. Yao  S. Roman  Z. Zhang  and
D. Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing
and text-to-SQL task. In EMNLP  2018. URL https://yale-lily.github.io/spider.

[42] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701  2012.

11

,Eui Chul Shin
Miltiadis Allamanis
Marc Brockschmidt
Alex Polozov