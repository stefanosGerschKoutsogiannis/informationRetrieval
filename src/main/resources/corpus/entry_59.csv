2014,A Differential Equation for Modeling Nesterov’s Accelerated Gradient Method: Theory and Insights,We derive a second-order ordinary differential equation (ODE)  which is the limit of Nesterov’s accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov’s scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov’s scheme. As a byproduct  we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov’s scheme leading to an algorithm  which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.,A Differential Equation for Modeling Nesterov’s

Accelerated Gradient Method: Theory and Insights

Weijie Su1

Stephen Boyd2

Emmanuel J. Cand`es1 3

1Department of Statistics  Stanford University  Stanford  CA 94305

2Department of Electrical Engineering  Stanford University  Stanford  CA 94305

3Department of Mathematics  Stanford University  Stanford  CA 94305

{wjsu  boyd  candes}@stanford.edu

Abstract

We derive a second-order ordinary differential equation (ODE)  which is the limit
of Nesterov’s accelerated gradient method. This ODE exhibits approximate equiv-
alence to Nesterov’s scheme and thus can serve as a tool for analysis. We show that
the continuous time ODE allows for a better understanding of Nesterov’s scheme.
As a byproduct  we obtain a family of schemes with similar convergence rates.
The ODE interpretation also suggests restarting Nesterov’s scheme leading to an
algorithm  which can be rigorously proven to converge at a linear rate whenever
the objective is strongly convex.

1

Introduction

As data sets and problems are ever increasing in size  accelerating ﬁrst-order methods is both of
practical and theoretical interest. Perhaps the earliest ﬁrst-order method for minimizing a convex
function f is the gradient method  which dates back to Euler and Lagrange. Thirty years ago  in a
seminar paper [11] Nesterov proposed an accelerated gradient method  which may take the following
form: starting with x0 and y0 = x0  inductively deﬁne

xk = yk−1 − s∇f (yk−1)
yk = xk +

k − 1
k + 2

(xk − xk−1).

(1.1)

For a ﬁxed step size s = 1/L  where L is the Lipschitz constant of ∇f   this scheme exhibits the
convergence rate

f (xk) − f ⋆ ≤ O(cid:16) Lkx0 − x⋆k2

k2

(cid:17).

Above  x⋆ is any minimizer of f and f ⋆ = f (x⋆). It is well-known that this rate is optimal among
all methods having only information about the gradient of f at consecutive iterates [12]. This is in
contrast to vanilla gradient descent methods  which can only achieve a rate of O(1/k) [17]. This
improvement relies on the introduction of the momentum term xk − xk−1 as well as the particularly
tuned coefﬁcient (k − 1)/(k + 2) ≈ 1 − 3/k. Since the introduction of Nesterov’s scheme  there
has been much work on the development of ﬁrst-order accelerated methods  see [12  13  14  1  2] for
example  and [19] for a uniﬁed analysis of these ideas.

In a different direction  there is a long history relating ordinary differential equations (ODE) to opti-
mization  see [6  4  8  18] for references. The connection between ODEs and numerical optimization
is often established via taking step sizes to be very small so that the trajectory or solution path con-
verges to a curve modeled by an ODE. The conciseness and well-established theory of ODEs provide
deeper insights into optimization  which has led to many interesting ﬁndings [5  7  16].

1

In this work  we derive a second-order ordinary differential equation  which is the exact limit of
Nesterov’s scheme by taking small step sizes in (1.1). This ODE reads

3
t

¨X +

˙X + ∇f (X) = 0
˙X(0) = 0; here  x0 is the starting point in Nesterov’s
˙X denotes the time derivative or velocity dX/dt and similarly ¨X = d2X/dt2 denotes the
acceleration. The time parameter in this ODE is related to the step size in (1.1) via t ≈ k√s. Case

for t > 0  with initial conditions X(0) = x0 
scheme 

studies are provided to demonstrate that the homogeneous and conceptually simpler ODE can serve
as a tool for analyzing and generalizing Nesterov’s scheme. To the best of our knowledge  this work
is the ﬁrst to model Nesterov’s scheme or its variants by ODEs.
We denote by FL the class of convex functions f with L–Lipschitz continuous gradients deﬁned on
Rn  i.e.  f is convex  continuously differentiable  and obeys

(1.2)

k∇f (x) − ∇f (y)k ≤ Lkx − yk

for any x  y ∈ Rn  where k · k is the standard Euclidean norm and L > 0 is the Lipschitz constant
throughout this paper. Next  Sµ denotes the class of µ–strongly convex functions f on Rn with
continuous gradients  i.e.  f is continuously differentiable and f (x) − µkxk2/2 is convex. Last  we
set Sµ L = FL ∩ Sµ.

2 Derivation of the ODE

Assume f ∈ FL for L > 0. Combining the two equations of (1.1) and applying a rescaling give

xk+1 − xk√s

=

k − 1
k + 2

xk − xk−1√s

− √s∇f (yk).

(2.1)

Introduce the ansatz xk ≈ X(k√s) for some smooth curve X(t) deﬁned for t ≥ 0. For ﬁxed t 
as the step size s goes to zero  X(t) ≈ xt/√s = xk and X(t + √s) ≈ x(t+√s)/√s = xk+1 with
k = t/√s. With these approximations  we get Taylor expansions:

(xk+1 − xk)/√s = ˙X(t) +
(xk − xk−1)/√s = ˙X(t) −

1
2
1
2

¨X(t)√s + o(√s)
¨X(t)√s + o(√s)

√s∇f (yk) = √s∇f (X(t)) + o(√s) 

where in the last equality we use yk − X(t) = o(1). Thus (2.1) can be written as

˙X(t) +

1
2

¨X(t)√s + o(√s)
3√s
t (cid:17)(cid:16) ˙X(t) −

= (cid:16)1 −

By comparing the coefﬁcients of √s in (2.2)  we obtain

1
2

¨X(t)√s + o(√s)(cid:17) − √s∇f (X(t)) + o(√s).

(2.2)

¨X +

3
t

˙X + ∇f (X) = 0

for t > 0. The ﬁrst initial condition is X(0) = x0. Taking k = 1 in (2.1) yields (x2 − x1)/√s =
−√s∇f (y1) = o(1). Hence  the second initial condition is simply ˙X(0) = 0 (vanishing initial
velocity). In the formulation of [1] (see also [20])  the momentum coefﬁcient (k − 1)/(k + 2) is
replaced by θk(θ−1

k−1 − 1)  where θk are iteratively deﬁned as
k − θ2

θk+1 = pθ4

k + 4θ2

k

2

starting from θ0 = 1. A bit of analysis reveals that θk(θ−1
O(1/k2)  thus leading to the same ODE as (1.1).

k−1 − 1) asymptotically equals 1 − 3/k +

2

(2.3)

Classical results in ODE theory do not directly imply the existence or uniqueness of the solution to
this ODE because the coefﬁcient 3/t is singular at t = 0. In addition  ∇f is typically not analytic
at x0  which leads to the inapplicability of the power series method for studying singular ODEs.
Nevertheless  the ODE is well posed: the strategy we employ for showing this constructs a series of
ODEs approximating (1.2) and then chooses a convergent subsequence by some compactness argu-
ments such as the Arzel´a-Ascoli theorem. A proof of this theorem can be found in the supplementary
material for this paper.
Theorem 2.1. For any f ∈ F∞
X(0) = x0 

  ∪L>0FL and any x0 ∈ Rn  the ODE (1.2) with initial conditions

˙X(0) = 0 has a unique global solution X ∈ C 2((0 ∞); Rn) ∩ C 1([0 ∞); Rn).

3 Equivalence between the ODE and Nesterov’s scheme

We study the stable step size allowed for numerically solving the ODE in the presence of accumu-
lated errors. The ﬁnite difference approximation of (1.2) by the forward Euler method is

X(t + ∆t) − 2X(t) + X(t − ∆t)

∆t2

+

3
t

X(t) − X(t − ∆t)

∆t

+ ∇f (X(t)) = 0 

(3.1)

which is equivalent to

X(t + ∆t) = (cid:16)2 −

3∆t

t (cid:17)X(t) − ∆t2∇f (X(t)) −(cid:16)1 −

3∆t

t (cid:17)X(t − ∆t).

Assuming that f is sufﬁciently smooth  for small perturbations δx  ∇f (x + δx) ≈ ∇f (x) +
∇2f (x)δx  where ∇2f (x) is the Hessian of f evaluated at x. Identifying k = t/∆t  the char-
acteristic equation of this ﬁnite difference scheme is approximately

det(cid:16)λ2 −(cid:16)2 − ∆t2∇2f −

3∆t

t (cid:17)λ + 1 −

3∆t

t (cid:17) = 0.

(3.2)

The numerical stability of (3.1) with respect to accumulated errors is equivalent to this: all the roots

of (3.2) lie in the unit circle [9]. When ∇2f (cid:22) LIn (i.e.  LIn − ∇2f is positive semideﬁnite)  if
∆t/t small and ∆t < 2/√L  we see that all the roots of (3.2) lie in the unit circle. On the other
hand  if ∆t > 2/√L  (3.2) can possibly have a root λ outside the unit circle  causing numerical
instability. Under our identiﬁcation s = ∆t2  a step size of s = 1/L in Nesterov’s scheme (1.1) is
approximately equivalent to a step size of ∆t = 1/√L in the forward Euler method  which is stable

for numerically integrating (3.1).
As a comparison  note that the corresponding ODE for gradient descent with updates xk+1 = xk −
s∇f (xk)  is
whose ﬁnite difference scheme has the characteristic equation det(λ − (1 − ∆t∇2f )) = 0. Thus 
to guarantee −In (cid:22) 1 − ∆t∇2f (cid:22) In in worst case analysis  one can only choose ∆t ≤ 2/L for a
ﬁxed step size  which is much smaller than the step size 2/√L for (3.1) when ∇f is very variable 
i.e.  L is large.

˙X(t) + ∇f (X(t)) = 0 

Next  we exhibit approximate equivalence between the ODE and Nesterov’s scheme in terms of
convergence rates. We ﬁrst recall the original result from [11].
Theorem 3.1 (Nesterov). For any f ∈ FL  the sequence {xk} in (1.1) with step size s ≤ 1/L obeys

f (xk) − f ⋆ ≤

2kx0 − x⋆k2
s(k + 1)2

.

Our ﬁrst result indicates that the trajectory of ODE (1.2) closely resembles the sequence {xk} in
terms of the convergence rate to a minimizer x⋆.
Theorem 3.2. For any f ∈ F∞  let X(t) be the unique global solution to (1.2) with initial condi-
tions X(0) = x0 

˙X(0) = 0. For any t > 0 

f (X(t)) − f ⋆ ≤

2kx0 − x⋆k2

t2

.

3

Proof of Theorem 3.2. Consider the energy functional deﬁned as

E(t)   t2(f (X(t)) − f ⋆) + 2kX +

t
2

˙X − x⋆k2 

whose time derivative is

t
2

˙X − x⋆ 

˙E = 2t(f (X) − f ⋆) + t2h∇f 

˙Xi + 4hX +
Substituting 3 ˙X/2 + t ¨X/2 with −t∇f (X)/2  (3.3) gives
t
˙E = 2t(f (X) − f ⋆) + 4hX − x⋆ −
2∇f (X)i = 2t(f (X) − f ⋆) − 2thX − x⋆ ∇f (X)i ≤ 0 
where the inequality follows from the convexity of f . Hence by monotonicity of E and non-
negativity of 2kX + t ˙X/2 − x⋆k2  the gap obeys f (X(t)) − f ⋆ ≤ E(t)/t2 ≤ E(0)/t2 =
2kx0 − x⋆k2/t2.

¨Xi.

˙X +

(3.3)

3
2

t
2

4 A family of generalized Nesterov’s schemes

In this section we show how to exploit the power of the ODE for deriving variants of Nesterov’s
scheme. One would be interested in studying the ODE (1.2) with the number 3 appearing in the
coefﬁcient of ˙X/t replaced by a general constant r as in

¨X +

r
t

˙X + ∇f (X) = 0  X(0) = x0 

˙X(0) = 0.

(4.1)

Using arguments similar to those in the proof of Theorem 2.1  this new ODE is guaranteed to assume
a unique global solution for any f ∈ F∞.
4.1 Continuous optimization

To begin with  we consider a modiﬁed energy functional deﬁned as

Since r ˙X + t ¨X = −t∇f (X)  the time derivative ˙E is equal to

4t
r − 1

(f (X) − f ⋆) +

E(t) =

2t2
r − 1

(f (X(t)) − f ⋆) + (r − 1)(cid:13)(cid:13)(cid:13)(cid:13)
r − 1h∇f 

˙Xi + 2hX +

2t2

t

r − 1
4t
r − 1

=

X(t) +

2

.

t

r − 1

˙X(t) − x⋆(cid:13)(cid:13)(cid:13)(cid:13)

˙X − x⋆  r ˙X + t ¨Xi

A consequence of (4.2) is this:
Theorem 4.1. Suppose r > 3 and let X be the unique solution to (4.1) for some f ∈ F∞. Then X
obeys

(f (X) − f ⋆) − 2thX − x⋆ ∇f (X)i.

(4.2)

and

f (X(t)) − f ⋆ ≤

(r − 1)2kx0 − x⋆k2

2t2

Z ∞

0

t(f (X(t)) − f ⋆)dt ≤

(r − 1)2kx0 − x⋆k2

2(r − 3)

.

Proof of Theorem 4.1. By (4.2)  the derivative dE/dt equals
2t(f (X)− f ⋆)− 2thX − x⋆ ∇f (X)i−
(f (X)− f ⋆)  (4.3)
where the inequality follows from the convexity of f . Since f (X) ≥ f ⋆  (4.3) implies that E is

(f (X)− f ⋆) ≤ −

2(r − 3)t
r − 1

2(r − 3)t
r − 1

non-increasing. Hence
2t2
r − 1

(f (X(t)) − f ⋆) ≤ E(t) ≤ E(0) = (r − 1)kx0 − x⋆k2 

4

yielding the ﬁrst inequality of the theorem as desired. To complete the proof  by (4.2) it follows that

Z ∞

0

2(r − 3)t
r − 1

(f (X) − f ⋆)dt ≤ −Z ∞

0

dE
dt

dt = E(0) − E(∞) ≤ (r − 1)kx0 − x⋆k2 

as desired for establishing the second inequality.

We now demonstrate faster convergence rates under the assumption of strong convexity. Given a
strongly convex function f   consider a new energy functional deﬁned as

˜E(t) = t3(f (X(t)) − f ⋆) +

2r − 3
As in Theorem 4.1  a more reﬁned study of the derivative of ˜E(t) gives
Theorem 4.2. For any f ∈ Sµ L(Rn)  the unique solution X to (4.1) with r ≥ 9/2 obeys

8

(2r − 3)2t

X(t) +

2t

2

.

˙X(t) − x⋆(cid:13)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:13)

f (X(t)) − f ⋆ ≤
for any t > 0 and a universal constant C > 1/2.

Cr

5

2kx0 − x⋆k2
t3√µ

The restriction r ≥ 9/2 is an artifact required in the proof. We believe that this theorem should be
valid as long as r ≥ 3. For example  the solution to (4.1) with f (x) = kxk2/2 is

r−1

2

X(t) =

2 Γ((r + 1)/2)J(r−1)/2(t)

r−1

2

t

x0 

(4.4)

where J(r−1)/2(·) is the ﬁrst kind Bessel function of order (r−1)/2. For large t  this Bessel function
obeys J(r−1)/2(t) = p2/(πt)(cos(t − (r − 1)π/4 − π/4) + O(1/t)). Hence 
in which the inequality fails if 1/tr is replaced by any higher order rate. For general strongly convex
functions  such reﬁnement  if possible  might require a construction of a more sophisticated energy
functional and careful analysis. We leave this problem for future research.

f (X(t)) − f ⋆ . kx0 − x⋆k2/tr 

4.2 Composite optimization

Inspired by Theorem 4.2  it is tempting to obtain such analogies for the discrete Nesterov’s scheme
as well. Following the formulation of [1]  we consider the composite minimization:

where g ∈ FL for some L > 0 and h is convex on Rn with possible extended value ∞. Deﬁne the

proximal subgradient

minimize

f (x) = g(x) + h(x) 

x∈Rn

Gs(x)  

x − argminzhkz − (x − s∇g(x))k2/(2s) + h(z)i

s

.

Parametrizing by a constant r  we propose a generalized Nesterov’s scheme 

xk = yk−1 − sGs(yk−1)
yk = xk +

k − 1
k + r − 1

(xk − xk−1) 

(4.5)

starting from y0 = x0. The discrete analog of Theorem 4.1 is below  whose proof is also deferred to
the supplementary materials as well.
Theorem 4.3. The sequence {xk} given by (4.5) with r > 3 and 0 < s ≤ 1/L obeys

and

f (xk) − f ⋆ ≤

(r − 1)2kx0 − x⋆k2

2s(k + r − 2)2

∞

Xk=1

(k + r − 1)(f (xk) − f ⋆) ≤

(r − 1)2kx0 − x⋆k2

2s(r − 3)

.

5

The idea behind the proof is the same as that employed for Theorem 4.1; here  however  the energy
functional is deﬁned as

E(k) = 2s(k + r − 2)2(f (xk) − f ⋆)/(r − 1) + k(k + r − 1)yk − kxk − (r − 1)x⋆k2/(r − 1).
The ﬁrst inequality in Theorem 4.3 suggests that the generalized Nesterov’s scheme still achieves
O(1/k2) convergence rate. However  if the error bound satisﬁes

f (xk′) − f ⋆ ≥

c
k′2

for some c > 0 and a dense subsequence {k′}  i.e.  |{k′} ∩ {1  . . .   m}| ≥ αm for any positive
integer m and some α > 0  then the second inequality of the theorem is violated. Hence  the second
inequality is not trivial because it implies the error bound is in some sense O(1/k2) suboptimal.
In closing  we would like to point out this new scheme is equivalent to setting θk = (r−1)/(k+r−1)
and letting θk(θ−1
k−1 − 1) replace the momentum coefﬁcient (k − 1)/(k + r − 1). Then  the equal
sign “ = ” in (2.3) has to be replaced by “ ≥ ”. In examining the proof of Theorem 1(b) in [20]  we
can get an alternative proof of Theorem 4.3 by allowing (2.3)  which appears in Eq. (36) in [20]  to
be an inequality.

5 Accelerating to linear convergence by restarting

Although an O(1/k3) convergence rate is guaranteed for generalized Nesterov’s schemes (4.5)  the
example (4.4) provides evidence that O(1/poly(k)) is the best rate achievable under strong con-
vexity. In contrast  the vanilla gradient method achieves linear convergence O((1 − µ/L)k) and
[12] proposed a ﬁrst-order method with a convergence rate of O((1 − pµ/L)k)  which  however 
requires knowledge of the condition number µ/L. While it is relatively easy to bound the Lipschitz
constant L by the use of backtracking [3  19]  estimating the strong convexity parameter µ  if not
impossible  is very challenging. Among many approaches to gain acceleration via adaptively esti-
mating µ/L  [15] proposes a restarting procedure for Nesterov’s scheme in which (1.1) is restarted
with x0 = y0 := xk whenever ∇f (yk)T (xk+1 − xk) > 0. In the language of ODEs  this gradi-
˙Xi negative along the trajectory. Although it has been
ent based restarting essentially keeps h∇f 
empirically observed that this method signiﬁcantly boosts convergence  there is no general theory
characterizing the convergence rate.

In this section  we propose a new restarting scheme we call the speed restarting scheme. The un-
derlying motivation is to maintain a relatively high velocity ˙X along the trajectory. Throughout this
section we assume f ∈ Sµ L for some 0 < µ ≤ L.
Deﬁnition 5.1. For ODE (1.2) with X(0) = x0 

˙X(0) = 0  let

T = T (f  x0) = sup{t > 0 : ∀u ∈ (0  t) 

dk ˙X(u)k2

du

> 0}

be the speed restarting time.

In words  T is the ﬁrst time the velocity k ˙Xk decreases. The deﬁnition itself does not imply that
0 < T < ∞  which is proven in the supplementary materials. Indeed  f (X(t)) is a decreasing
function before time T ; for t ≤ T  

df (X(t))

dt

= h∇f (X) 

˙Xi = −

3
t k ˙Xk2 −

1
2

dk ˙Xk2

dt ≤ 0.

The speed restarted ODE is thus

¨X(t) +

3
tsr

˙X(t) + ∇f (X(t)) = 0 

(5.1)

where tsr is set to zero whenever h ˙X  ¨Xi = 0 and between two consecutive restarts  tsr grows just
as t. That is  tsr = t − τ   where τ is the latest restart time. In particular  tsr = 0 at t = 0. The
theorem below guarantees linear convergence of the solution to (5.1). This is a new result in the
literature [15  10].
Theorem 5.2. There exists positive constants c1 and c2  which only depend on the condition number
L/µ  such that for any f ∈ Sµ L  we have

f (X sr(t)) − f (x⋆) ≤

c1Lkx0 − x⋆k2

e−c2t√L.

2

6

5.1 Numerical examples

Below we present a discrete analog to the restarted scheme. There  kmin is introduced to avoid
having consecutive restarts that are too close. To compare the performance of the restarted scheme
with the original (1.1)  we conduct four simulation studies  including both smooth and non-smooth
objective functions. Note that the computational costs of the restarted and non-restarted schemes are
the same.

Algorithm 1 Speed Restarting Nesterov’s Scheme

input: x0 ∈ Rn  y0 = x0  x−1 = x0  0 < s ≤ 1/L  kmax ∈ N+ and kmin ∈ N+
j ← 1
for k = 1 to kmax do
xk ← argminx( 1
yk ← xk + j−1
if kxk − xk−1k < kxk−1 − xk−2k and j ≥ kmin then
else

2skx − yk−1 + s∇g(yk−1)k2 + h(x))

j+2 (xk − xk−1)

j ← 1
j ← j + 1

end if
end for

Quadratic. f (x) = 1
2 xT Ax + bT x is a strongly convex function  in which A is a 500× 500 random
positive deﬁnite matrix and b a random vector. The eigenvalues of A are between 0.001 and 1. The
vector b is generated as i. i. d. Gaussian random variables with mean 0 and variance 25.

Log-sum-exp.

f (x) = ρ logh

m

Xi=1

exp((aT

i x − bi)/ρ)i 

2kXobs − Mobsk2

where n = 50  m = 200  ρ = 20. The matrix A = {aij} is a random matrix with i. i. d. standard
Gaussian entries  and b = {bi} has i. i. d. Gaussian entries with mean 0 and variance 2. This function
is not strongly convex.
Matrix completion. f (X) = 1
F + λkXk∗  in which the ground truth M is a
rank-5 random matrix of size 300 × 300. The regularization parameter is set to λ = 0.05. The 5
singular values of M are 1  . . .   5. The observed set is independently sampled among the 300 × 300
entries so that 10% of the entries are actually observed.
Lasso in ℓ1–constrained form with large sparse design. f = 1
s.t. kxk1 ≤ δ  where
A is a 5000 × 50000 random sparse matrix with nonzero probability 0.5% for each entry and b is
generated as b = Ax0 + z. The nonzero entries of A independently follow the Gaussian distribution
with mean 0 and variance 1/25. The signal x0 is a vector with 250 nonzeros and z is i. i. d. standard
Gaussian noise. The parameter δ is set to kx0k1.
In these examples  kmin is set to be 10 and the step sizes are ﬁxed to be 1/L. If the objective is in
composite form  the Lipschitz bound applies to the smooth part. Figures 1(a)  1(b)  1(c) and 1(d)
present the performance of the speed restarting scheme  the gradient restarting scheme proposed in
[15]  the original Nesterov’s scheme and the proximal gradient method. The objective functions
include strongly convex  non-strongly convex and non-smooth functions  violating the assumptions
in Theorem 5.2. Among all the examples  it is interesting to note that both speed restarting scheme
empirically exhibit linear convergence by signiﬁcantly reducing bumps in the objective values. This
leaves us an open problem of whether there exists provable linear convergence rate for the gradient
restarting scheme as in Theorem 5.2. It is also worth pointing that compared with gradient restarting 
the speed restarting scheme empirically exhibits more stable linear convergence rate.

2kAx − bk2

6 Discussion

This paper introduces a second-order ODE and accompanying tools for characterizing Nesterov’s
accelerated gradient method. This ODE is applied to study variants of Nesterov’s scheme. Our

7

108

106

104

102

*

f
 

−

 
f

100

10−2

10−4

10−6

 
0

102

100

10−2

10−4

*

f
 

−

 
f

10−6

10−8

10−10

10−12

 
0

 

srN
grN
oN
PG

200

400

600

800

1000

1200

1400

iterations

(a) min 1

2 xT Ax + bx

*

f
 

−

 
f

102

100

10−2

10−4

10−6

10−8

10−10

10−12

 
0

 

srN
grN
oN
PG

500

1000

1500

iterations

(b) min ρ log(Pm

i=1 e(aT

i x−bi)/ρ)

 

105

srN
grN
oN
PG

 

srN
grN
oN
PG

*

f
 

−

 
f

100

10−5

10−10

 
0

20

40

60

80

100

iterations

120

140

160

180

200

(c) min 1

2 kXobs − Mobsk2

F + λkXk∗

200

400

600

800

1000

1200

1400

iterations

(d) min 1

2 kAx − bk2

s.t. kxk1 ≤ C

Figure 1: Numerical performance of speed restarting (srN)  gradient restarting (grN) proposed in
[15]  the original Nesterov’s scheme (oN) and the proximal gradient (PG)

approach suggests (1) a large family of generalized Nesterov’s schemes that are all guaranteed to
converge at the rate 1/k2  and (2) a restarted scheme provably achieving a linear convergence rate
whenever f is strongly convex.

In this paper  we often utilize ideas from continuous-time ODEs  and then apply these ideas to
discrete schemes. The translation  however  involves parameter tuning and tedious calculations.
This is the reason why a general theory mapping properties of ODEs into corresponding properties
for discrete updates would be a welcome advance. Indeed  this would allow researchers to only
study the simpler and more user-friendly ODEs.

7 Acknowledgements

We would like to thank Carlos Sing-Long and Zhou Fan for helpful discussions about parts of this
paper  and anonymous reviewers for their insightful comments and suggestions.

8

References

[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Imaging Sci.  2(1):183–202  2009.

[2] S. Becker  J. Bobin  and E. J. Cand`es. NESTA: a fast and accurate ﬁrst-order method for sparse

recovery. SIAM Journal on Imaging Sciences  4(1):1–39  2011.

[3] S. Becker  E. J. Cand`es  and M. Grant. Templates for convex cone problems with applications

to sparse signal recovery. Mathematical Programming Computation  3(3):165–218  2011.

[4] A. Bloch (Editor). Hamiltonian and gradient ﬂows  algorithms  and control  volume 3. Amer-

ican Mathematical Soc.  1994.

[5] F. H. Branin. Widely convergent method for ﬁnding multiple solutions of simultaneous non-

linear equations. IBM Journal of Research and Development  16(5):504–522  1972.

[6] A. A. Brown and M. C. Bartholomew-Biggs. Some effective methods for unconstrained op-
timization based on the solution of systems of ordinary differential equations. Journal of
Optimization Theory and Applications  62(2):211–224  1989.

[7] R. Hauser and J. Nedic. The continuous Newton–Raphson method can look ahead. SIAM

Journal on Optimization  15(3):915–925  2005.

[8] U. Helmke and J. Moore. Optimization and dynamical systems. Proceedings of the IEEE 

84(6):907  1996.

[9] J. J. Leader. Numerical Analysis and Scientiﬁc Computation. Pearson Addison Wesley  2004.

[10] R. Monteiro  C. Ortiz  and B. Svaiter. An adaptive accelerated ﬁrst-order method for convex

optimization  2012.

[11] Y. Nesterov. A method of solving a convex programming problem with convergence rate

O(1/k2). In Soviet Mathematics Doklady  volume 27  pages 372–376  1983.

[12] Y. Nesterov.

Introductory lectures on convex optimization: A basic course  volume 87 of

Applied Optimization. Kluwer Academic Publishers  Boston  MA  2004.

[13] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming 

103(1):127–152  2005.

[14] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE Discus-

sion Papers  2007.

[15] B. O’Donoghue and E. J. Cand`es. Adaptive restart for accelerated gradient schemes. Found.

Comput. Math.  2013.

[16] Y.-G. Ou. A nonmonotone ODE-based method for unconstrained optimization. International

Journal of Computer Mathematics  (ahead-of-print):1–21  2014.

[17] R. T. Rockafellar. Convex analysis. Princeton Landmarks in Mathematics. Princeton University

Press  Princeton  NJ  1997. Reprint of the 1970 original  Princeton Paperbacks.

[18] J. Schropp and I. Singer. A dynamical systems approach to constrained minimization. Numer-

ical functional analysis and optimization  21(3-4):537–551  2000.

[19] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. submit-

ted to SIAM J. 2008.

[20] P. Tseng. Approximation accuracy  gradient methods  and error bound for structured convex

optimization. Mathematical Programming  125(2):263–295  2010.

9

,Weijie Su
Stephen Boyd
Emmanuel Candes